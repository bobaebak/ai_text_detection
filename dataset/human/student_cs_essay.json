[
    {
        "input": "Computer Usage Evolution Through Years Essay\n\nIn the history of mankind, the computer has become one of the most important inventions. Hardly anyone can imagine a modern home or office without computers. While some people argue that the computer has provided us with additional conveniences, according to others, the computer has made our life more complex and stressful. I believe that computers, in the first place, have enriched our society with several advantages due to faster communication and easier access to information.\n\nThe computer provides us with a fast way to communicate. In our time, when modern society is becoming more global, computers play an essential role in communication systems. Previously, communication was limited to means such as mail (Grudin, 2017). If a businessman traveled abroad, the sending of reports and reports, as a rule, was accompanied by significant delays. Since the competitiveness of a business depends on time, this situation was completely unacceptable. The Internet has allowed businesses to send messages instantly. Staff can send emails or instant messages to a manager anywhere in the world (Grudin, 2017). This has added efficiency to businesses and expanded business opportunities.\n\nFor me, a computer is a technological necessity that makes my life more enjoyable and interesting. First of all, a computer is an excellent leisure tool. Watching movies and various sci-fi programs through my electronic companion is very convenient and comfortable. Also, my daily work, which brings me the lion\u2019s share of my cash income, is not in the least tied to the computer, so it is not just a means of interactivity and entertainment but also a tool for making money.\n\nPhysicians and other healthcare workers use computers for many essential uses. Computer hardware is widely used in making a diagnosis, conducting examinations and preventive examinations are examples of computer devices and methods of treatment and diagnosis (Farinella & Marco, 2018). Computed tomography and nuclear medical diagnostics provide accurate layered images of internal organ structures (Farinella & Marco, 2018). Ultrasound diagnostics and probing, using the effects of the interaction of incident and reflected ultrasonic waves, opens countless possibilities for obtaining images of internal organs and studying their condition. The diagnostics and treatment methods will be much easier with the help of computer intervention.\n\nReferences\n\nFarinella, G. M., & Marco, L. (2018). Computer Vision for Assistive Healthcare . Academic Press.\n\nGrudin, J. (2017). From tool to partner: The evolution of human-computer interaction. Synthesis Lectures on Human-Centered Informatics , 10 (1), i-183.\n",
        "label": "human"
    },
    {
        "input": "How to Change a Computer Hard Drive Disk Essay (Critical Writing)\n\nIntroduction\n\nHard disk failure is common among computers and leads to severe damage. The failure results from various causes, including human error, computer hardware failure, malware corruption, media damage, and exposure to heat or water. Changing the HDD before it wears out completely is prudent to protect other hardware and software components (Fern\u00e1ndez-Fuentes et al., 2022). While tech-savvies easily replace the HDD, persons with little knowledge of computer hardware find it difficult to replace it. The persons with little knowledge include first-time computer users and those who rarely interact with computer hardware. Novice computer users can follow this instruction guide to effectively change HDD without causing any damage to the computer hardware and software components.\n\nThese instructions will allow the readers to change the HDD from a faulty computer step by step and switch on the computer to test the new HDD. While changing the HDD seems easy following this instruction guide, the computer user should have basic computer knowledge. The assistance should be sought if the users interact with computer hardware for the first time. Appropriate assistants include computer experts and authorized computer hardware dealers at the place of HDD purchase.\n\nOverall Steps\n\nHDD can be upgraded in a fully functioning computer or a faulty laptop. To get started, the readers must ensure that their critical data is backed up in a secondary device such as flash disks or cloud-based storage like Google Drive. The computer must be switched off by following the appropriate booting process. After that, the power supply should be disconnected to avoid any instant of electric shock. Proper instructions, as indicated on the HDD guide book should be followed. The users should avoid placing the new HDD near heat or magnetic fields that interfere with the storage mechanism. Before inserting the new HDD remove the existing HDD carefully. The computer should be allowed to boot up upon replacing the HDD. The process takes about fifteen to twenty minutes and should be done in a well-ventilated room at average room temperatures.\n\nList of Materials and Tools Needed\n\nBefore starting the process of changing the HDD, several tools and materials are required. The tools will help avoid static electricity, dust, electric shock, and other hazardous conditions to the computer and the user. Therefore, the user must acquire a list of protective tools before starting the process. The following list of tools is needed when changing the HDD in a computer.\n\n  * Anti-static mat to shield the users and computer equipment from static electricity discharge\n  * Anti-static vacuum cleaner to cause suction to remove dirt from the computer equipment\n  * Anti-static wristband to be worn on the writs to prevent static electricity build-up.\n  * \n  * Philips-head screwdriver to tighten and drive out screws\n\nThe readers should also ensure that appropriate materials are available before the process. The following list of materials is needed to get the process started.\n\n  * A can of compressed air\n  * \n  * A can of refrigerant-based propellant cleaner\n  * A new hard drive disk\n\nList of Steps\n\nThe following steps should be followed when changing HDD, and the readers should remember to disconnect the power supply and carefully read the HDD\u2019s manufacturers\u2019 instructions.\n\nBack up data on the existing HDD\n\nBacking up of data should be done if the existing HDD still works. You can use an external USB or online using OneDrive or Google Drive.\n\nObtain a full, bootable Operating system copy\n\nYou should do this step if you replace a primary HDD and are not using a cloning tool. The software can be bought on a DVD or downloaded on a flash drive by creating recovery media.\n\nYou should decide between an HDD or an SDD (Solid State Drive)\n\nAlthough SDDs are faster and more durable than HDD, they are more expensive. However, the two operate similarly, and you can choose either depending on your budget.\n\nChoose the right driver for your computer\n\nDifferent types of computers require different types of HDDs with varying sizes. If you are replacing a laptop\u2019s HDD, you need a 2.5-inch HDD, and if you are replacing a desktop\u2019s HDD, you need a 3.5-inch HDD.\n\nShut down your computer by following the right procedure\n\nFollowing the appropriate computer booting process help protect the data and prevent damage to the existing hardware. Unplug the computer from the power supply upon shutting it down.\n\nGround yourself before opening your computer\u2019s system unit\n\nUse the anti-static wrist band while standing on the anti-static mat to avoid improper handling of the electronic components. The tools help avoid damaging the computer by electrostatic discharge.\n\nOpen the system unit case\n\nThis step varies depending on the computer being worked on: desktop or laptop. If you are working on a desktop, you need to twist off the screws using the Philips-head screwdriver. Meanwhile, laptops either have special cases with special doors for easy insertion and removal of HDD or need unscrewing of various components and battery removal.\n\nLocate HDD in the system unit\n\nHDDs are often screwed within a cage in the computer\u2019s system unit. Upon locating the HDD, identify power and data connectors and disconnect them from the HDD.\n\nMake sure you have removed all the screws and disconnected the HDD\n\nUse the Philips-head screwdriver to remove all the screws attaching the HDD to the system unit. If the computer case does not support the drive, you can use your hand to support the HDD. After that, carefully slide the HDD out of the cage and system unit.\n\nSet jumpers on an IDE drive if not using a SATA drive\n\nHDDs have diagrams illustrating the jumpers\u2019 location and can set the jumpers. Jumper setting sets the drive as Master, Slave, or Select depending on your choice and the original drive settings.\n\nInsert the new HDD into position\n\nThe insertion should match the original positioning of the old HDD. After that, carefully screw the HDD in, reconnect the power and data cables, and close the system unit.\n\nBoot the computer up with the recovery media inserted\n\nThis step can be skipped if you use cloning software, but the DVD tray should be ejected first if the recovery media is on a DVD. In case of a USB, insert the USB before turning on the computer. The computer automatically boots up into the Operating System (OS) installer to boot up from USB or DVD drives.\n\nFollow on-screen instructions to reinstall the OS\n\nUpon reinstalling the OS, the computer automatically re-registers the OS. With a fully operational computer, you can restore the backed-up data. The computer is ready to use, and can be run through diagnostic test to identify any error. Remember to keep the old HDD somewhere safe in case you need it for future use.\n\nReferences\n\nAmazon. (n.d.). Phillips-Head Screwdriver . Web.\n\nESUS-IT. (n.d.). Hard Disc Drive dedicated for HP server 2.5 . Web.\n\nFern\u00e1ndez-Fuentes, X., Pena, T. F., & Cabaleiro, J. C. (2022). Digital forensic analysis methodology for private browsing: Firefox and Chrome on Linux as a case study. Computers & Security .\n\nIBM. (n.d.). Hard Disk Drive Jumper Settings . Web.\n\nTrustcomputer. (n.d.). Windows 10 Pro 64 Bit System Disc. Web.\n",
        "label": "human"
    },
    {
        "input": "Researching of Computer-Aided Design: Theory Essay (Article)\n\nTechnical Drawing\n\nTechnical drawings can be described as plans that show how a human-made object is constructed. A technical drawing is usually a diagram that illustrates an object from different angles (projections) to explain its structure (fig. 1). These diagrams are used in engineering, construction, and other industries involved in creating, constructing, and repairing ( Technical drawing , 2022). They also act as a tool for communicating ideas between creators and makers. Thus, it is vital for a technical drawing to follow its own universal language that can be recognized by all professionals ( Design Handbook , 2022). For example, a mechanical engineering drawing shows a component or product\u2019s requirements (Singh and Singh, 2021). It can also reveal weaknesses in the design and guide producers on how to manufacture it perfectly.\n\nFigure 1: A technical drawing\n\nAnother field where technical drawings are essential is architecture. Sketches and diagrams are also necessary for healthcare and software engineering (Vergara et al., 2021). Historically, technical drawings were drafted by hand, using tools (Maguire, Phelps and Simmons, 2020). These included a drafting table, a straightedge, T-square, compass, and more. The manual method can still be used today, although many of all drafting processes have been automated.\n\nComputer-Aided Design\n\nComputer-aided design (CAD) is software used to create digital technical drawings. There are two main subcategories of CAD products \u2013 programs working with 2D and 3D graphics (fig. 2). CAD offers many benefits to creators and producers of products. First, the programs were developed on the basis of the universal language to which technical drawings must adhere (Fuller, Ramirez and Smith, 2021). Therefore, an engineer using such software can ensure that all markings and lines correspond to those understood by other professionals.\n\nFigure 2: 3D CAD software\n\nMoreover, the use of CAD leads to quicker project completion and higher productivity rates ( Inc. , 2020). A user can quickly change a drawing, zoom in or out, remove and add elements, and observe a drafted object from different perspectives (Llach, 2021; Sarkar, 2018). Therefore, CAD software is vital in manufacturing industries where the quality of one detail can impact the whole product. However, each CAD program has unique traits, meaning that an engineer must know the specifics of using various software. One of the most significant advantages of advanced CAD software is the ability to simulate an object\u2019s performance in motion. Thus, the use of CAD simplifies the initial trial step in any design process.\n\nTheory of Projections\n\nOrthographic\n\nTo depict a 3D object in a 2D form, one has to represent it from one or several angles. Orthographic projection is used to represent a 3D object by using 2D drawings of its different viewpoints. The main views are called plan (top), front, and side, and a technical drawing is separated into four quadrants where these views are placed (Khumaedi et al., 2021). In some cases, an object\u2019s diagram does not need all three views; in others, additional projections are required. There are two types of orthographic projection \u2013 first and third angle \u2013 they differ only in the way of putting the views onto the drawing (fig. 3).\n\nFigure 3: An orthographic projection\n\nFirst Angle\n\nThe first type of orthographic projection is the first angle projection. This approach to drawing is primarily used in European and Asian countries (Rahbarianyazd and Nia, 2019). In the first angle projection, the first quadrant contains the front view of an object, while the second and third quadrants depict the side and plan views (Rahbarianyazd and Nia, 2019). To draw a first-angle projection, one can imagine that the object is placed between the person drawing and the projection (fig. 4). Therefore, the product is placed above the horizontal plane and in front of the vertical plane, and the projection plays the role of paper (Rahbarianyazd and Nia, 2019). As a result, the observer sees through the object. To distinguish the first angle projection, technical drawings are marked with a specific symbol (fig. 4).\n\nFigure 4: First-degree projection\n\nThird Angle\n\nThe third angle projection imagines the object and the diagram differently. This projection theory is used mainly in the United States and has a specific marking (fig. 5). One can describe the drawing as if the object were put in a clear box or behind a transparent wall. This is the projection \u2013 it is placed between the observer and the detail being drawn (fig. 5). On the technical drawing, the object is in the third quadrant, and the other views are placed in the first and second ones (Rahbarianyazd and Nia, 2019). It is imagined that the object is situated behind the vertical and below the horizontal plane. As a result, the engineer sees the side and top of the object the same way they are depicted on the drawing.\n\nFigure 5: Third-degree projection\n\nAxonometric (Pictorial)\n\nWhile objects\u2019 views are separated into sides in orthographic projections, axonometric projections display the plan, front, and side views in one drawing. This provides more visualization but a less clear representation of dimensions than an orthographic projection. As all three views are put into one illustration, the axes of the objects\u2019 bodies have to be changed (foreshortened) according to the perspective that the observer assumes (Bi and Wang, 2020). For example, when one looks at a detail from one of its axes at an angle, some sides appear shorter than they are in real life (fig. 6). There are three categories of axonometric projection: dimetric, isometric, and trimetric, based on the three axes of projection.\n\nFigure 6: An axonometric projection\n\nDimetric\n\nThe differences between the projections are based on the ways in which axes are changed in relation to each other. In a dimetric projection, two of the three axes are foreshortened equally, and the third axis\u2019 foreshortening is scaled separately (fig. 7). Therefore, one side is more distorted than the two others, which leads to the drawing being more approximate than precise (Tornincasa, 2020). A dimetric projection does not have a strict angle at which it is used \u2013 the observer determines it. However, as two axes are scaled in the same way, some knowledge of measurements is necessary to represent the object. Dimetric projection is not the most commonly utilized method as it requires complex angle calculations and approximation of the third axis\u2019 scaling.\n\nFigure 7: Dimetric projections\n\nIsometric\n\nIsometric projections are more common in technical drawings than in other axonometric types. In this projection, all three axes are foreshortened the same amount (fig. 8). Therefore, there is a standard angle of 120\u00b0 between all axes \u2013 this angle separates the whole projection into three equal parts (Tornincasa, 2020). This approach is also the easiest to work with, as one can use simple tools to construct a technical drawing. In isometric projections, all axes change equally, which means that no approximations have to be made. These characteristics of isometric drawings make them the most reliable and precise out of all axonometric projections. They are also helpful for the design element of engineering, depicting a 3D object as it could be seen in real life.\n\nFigure 8: An isometric projection\n\nTypes of Lines and Their Use\n\nLines in a technical drawing can be continuous, dashed, or chain; the thickness of lines is important as well as their form (fig. 9). There are ten mainline types that can be used in a technical drawing. The first type is a continuous thick line that represents the visual outlines of an object (Saif, 2021). A thin continuous line can note an imaginary, dimension, or projection line (Saif, 2021). For example, if a drawn detail is a part of a larger product, thin lines can help place it in the structure.\n\nFigure 9: A technical drawing with different lines\n\nZig-zag and wavy (freehand) lines denote the object\u2019s boundaries, limits, and breaks (Saif, 2021). Dashed lines usually depict outlines and edges that appear hidden in the chosen view, as if the detail were transparent (Saif, 2021). The last category of mainline types is chain lines, where short and long dashes are combined. For example, a thin chain line represents the lines of symmetry in an object as well as the center and trajectory of the drawing (Saif, 2021). A thick chain line can denote the area or specific line that requires a different treatment.\n\nScaling\n\nIt is clear that not all details can be drawn to their full size. For this reason, all technical drawings are drawn to a specific scale for producers to build the object according to its true dimensions (Tornincasa, 2020). For example, a full-scale drawing means that the diagram and the detail have the same measurements (such as length, width, and other parameters) (fig. 10). Smaller objects can be scaled up \u2013 all numbers being multiplied by the same number to draw the detail larger than it is in reality. In the same way, large products are scaled down in a technical drawing.\n\nFigure 10: An example of scaling\n\nMany types of scale exist, according to the drawing\u2019s purpose and the measurement system used in the project. For instance, large objects (such as aircraft) have to be significantly scaled, where 1 inch can represent up to 100 feet (Tornincasa, 2020). There is also a metric-based scaling system, where one millimeter is used as the base measurement ( Design Handbook , 2022). Here, a small detail can be drawn on a 2:1 scale (2 millimeters of a drawing for 1 millimeter in real life), while a 1:50 scale can help illustrate large objects.\n\nDimensioning\n\nWhile the visual representation of detail is important, it cannot be used in production without knowing its actual measurements. Thus, another critical part of the drawing process is dimensioning \u2013 the addition of size information ( Design Handbook , 2022). A well-dimensioned and described part can be easily rebuilt as the drawing contains all the needed data ( Design Handbook , 2022). However, it is vital to dimension the object properly so that all numbers are easily interpreted by manufacturers (fig. 11).\n\nFigure 11: An example of dimensioning\n\nFor that reason, dimensioning also follow the universal rules of technical drawing. The numerical value (dimension) is placed near the dimension lines that show the extent of the size (Maguire, Phelps and Simmons, 2020). An arrow is placed at the ends of these lines to show where the dimensioned line ends (Maguire, Phelps and Simmons, 2020). Together, these elements create a clear representation of each part of the object. Furthermore, a drawing can also include tolerance \u2013 a number to which the dimension can vary ( Design Handbook , 2022). Also, some drawings can feature limits of the size that represent the maximum and minimum measurements for the specific detail (Maguire, Phelps and Simmons, 2020). The addition of these elements depends on each particular drawing.\n\nAuxiliary Views, Sections, and Sectional Views\n\nAn auxiliary view is an additional view that is necessary when an object\u2019s plan, front, and side perspectives are not enough. For example, if a detail has an inclined surface, it may not fully appear on a simple orthographic projection (Singh and Singh, 2021). For this reason, an auxiliary view of this particular surface and its connection to the figure is added to the drawing (Singh and Singh, 2021). It should be noted that the auxiliary view is represented on the drawing following its perspective \u2013 it is placed perpendicular to the angle where the unseen surface is shown from the side (fig. 12).\n\nFigure 12: An auxiliary view\n\nSome details cannot be reproduced without knowing how they are structured on the inside. Here, one may use a section or a sectional view of the object (fig. 13). It is a look inside the detail as if it were cut and opened up (Singh and Singh, 2021). A sectional view usually shows an object cut in half, and special lines show the symmetry and center of the object to signify the cut\u2019s location (Singh and Singh, 2021). In most cases, the material is also indicated with shading to show the hollow parts of the detail.\n\nFigure 13: A sectional view\n\nReference List\n\nAssembly (2020) \u2018Updated 3D CAD program,\u2019 Web.\n\nBasic Civil Engineering (2015) \u2018Different between 1st angle projection and 3rd angle projection in drawing\u2019, Web.\n\nBi, Z. and Wang, X. (2020) Computer-aided design and manufacturing . New York: John Wiley & Sons.\n\nCamba, J. D. (2014) \u201816 different angles for dimetric projections\u2019, ResearchGate . Web.\n\nCostin, R. (2022) Basic blueprint reading . Portland, OR: Open Oregon Educational Resources.\n\nDesign handbook: engineering drawing and sketching (2022) Web.\n\nFuller, A., Ramirez, A., and Smith, D. (2021) Technical drawing 101 with AutoCAD 2022: a multidisciplinary guide to drafting theory and practice with video instruction. New York: SDC Publications.\n\nInc. (2020) \u2018Computer-Aided Design (CAD) and Computer-Aided Manufacturing (CAM)\u2019, Web.\n\nKhumaedi, M., Widjanarko, D., Setiadi, R. and Setiyawan, A. (2021) Evaluating the impact of audio-visual media on learning outcomes of drawing orthographic projections. International Journal of Education and Practice , 9(3), pp. 613-624.\n\nLlach, D. C. (2021) Between form and information: early philosophies of computer-aided design. Nexus Network Journal , 23(4), pp. 933-943.\n\nMaguire, D., Phelps, N. and Simmons, C. (2020) Manual of engineering drawing: British and international standards . London: Elsevier Science.\n\nRahbarianyazd, R. and Nia, H. A. (2019) Introduction to architectural and technical drawing: a practical handbook . Fatih: Cinius Yay\u0131nlar\u0131.\n\nSaif, M. (2021) \u201810 different types of lines used in engineering drawing\u2019, The Engineers Post , Web.\n\nSarkar, C. K. (ed.) (2018) Technology computer-aided design . Boca Raton, FL: CRC Press.\n\nScale drawings (2022) Web.\n\nSingh, L. P. and Singh, H. (2021) Engineering drawing: principles and applications . Cambridge: Cambridge University Press.\n\nTechnical drawing (2022) Web.\n\nTornincasa, S. (2020) Technical drawing for product design . Cham: Springer.\n\nVergara, D., Rubio, M.P., Extremera, J. and Lorenzo, M. (2021) Interdisciplinary learning methodology for supporting the teaching of industrial radiology through technical drawing. Applied Sciences , 11(12), p. 5634.\n",
        "label": "human"
    },
    {
        "input": "Computerized Animation as a Useful Study Tool Proposal\n\nTable of Contents\n 1. Introduction\n 2. The Boundary of the Research\n 3. Key Concepts in The Study\n 4. Project Purpose\n 5. Review of Existing Literature\n 6. Plan of Work\n 7. Conclusion\n 8. Works Cited\n\nIntroduction\n\nThe evolvement of technological innovations and varying perspectives of film, combined with the growing prominence of visual effects, has made describing animations in words a difficult endeavor. Animation is a flexible and ageless notion that can be traced back to toys from the 1800s, scribblings on the margins of papers, and some of the most renowned films ever made (Ekinci 4). To construct profitable, impactful, and motivating pieces, an artist should strike a balance between the following three variables: researching, discussing, and experiencing, as they are all important parts of the animation triangle. To examine the evolution of computerized animation as a useful study tool, the research will be answering the following questions:\n\n 1. What is the role of writing in the process of animation?\n 2. How could the ideology of \u2018computerized animation as a useful study tool\u2019 transform animation?\n 3. What novel approaches could artists be able to utilize in animations as a result of this ideology?\n 4. What will be the next stages in this research?\n\nIn addition, the researcher will be analyzing the evolution of animation from steam-boat-based animation to computerized animation while highlighting the key themes of change. Conversely, the study will outline previous research on the topic and propose new recommendations that could be useful in advancing the field of animation.\n\nThe Boundary of the Research\n\nThe proposed research will focus on the evolution of computerized animation as a useful study tool while exploring the transitions witnessed in the animating style of The Enchanted Drawing (Blackton) and Make love, not warcraft (Parker and Matt). It will highlight the key themes of change while relating animation with other study tools. In the time boundary, the study will utilize scholarly materials dated between 2016 and 2021.\n\nKey Concepts in The Study\n\nAn artistic existence can be defined as a condition of being that alternates amongst the states of thinking, creating, and performing. The research will demonstrate how a self-reflective blueprint of generating animations may be essential to other areas like the academic field. It will describe animation as a tool for creating, exploring, and connecting ideas (Crawford 481). There is a plethora of expertise in the creation of animated films that cannot be shared. For this reason, there is a need to illuminate the comprehension of the animation field by integrating the creating and writing operations entrenched into the creative process.\n\nProject Purpose\n\nThe term \u201canimate\u201d refers to the process of bringing something to life. The strength of animation lies in its ability to envision the unseen and recreate that which exists only in the artist\u2019s mind (Giensen and Anna n.p.). With abstraction or simplicity, an animation may convey emotions, sentiments, and experiences (Winokur 54). While a film camera records what is currently happening in the world, animations may go much further by establishing a new existence. The main aim of the research project will be to contribute to the sounding literature of animation by explaining the transitions witnessed from the 1800s to 2021 in the field of animation.\n\nIn the crystalline multi-dimensional framework that is the human perception of the world, animation introduces a unique prism to one\u2019s perspective of realism (Ehrlich 2). This makes animation an important aspect of human life that needs to be studied in depth.\n\nThe goal of the research is to give a review of the work of other animators and designers who use animation, as well as study it. The purpose is to consider the value of animation as a kind of creative expression. This research is an analytical examination of an artistic practice, informed by education, skills, and research from other scholarly works. The study also proposes animation as a technique of research in the anticipation that it will highlight the need for animation to be recognized as scholarly work and motivate extra study in this area. The research on \u2018Computerized Animation as a Research Tool\u2019 is presented from the perspective that different ways of reasoning and exploration will provide new and creative insight.\n\nReview of Existing Literature\n\nA literature review is an important aspect of research as it helps in identifying information gaps and further contributing ideas to a topic of study while integrating what is already known about the subject. This means that the role played by works of other artists in the animation field cannot be underestimated as they can assist in gathering motivation for one\u2019s creations or discovering missing elements.\n\nThis study will review related literature that defines animation, studies the history of animation from the 1800s to date, and also studies that highlight the role of animation in other fields of study. Schallert (504) states that if one fails to ponder their artwork, they cannot be able to truly comprehend it. Chen et al. (260) believe that animation can be a useful tool in simplifying complex academic concepts and improving learning. Computer-produced animation is a pretty modern but sophisticated art genre, which is naturally constrained by the medium\u2019s technical restrictions and thus the need to have an in-depth look at computerized animations. It is important to note however, that very little literature has been produced concerning the evolution of animations and thus the need to conduct this research.\n\nPlan of Work\n\nThe research study will utilize the core aspects of qualitative research in defining animation and describing computerized animation as a useful research tool. After a definition of the basics of animation, the section that follows will evaluate the importance of identifying animation as a mode of investigation. It is also the possible contribution it might make to a variety of fields of study. Afterward, the study will question the idea that writing is just a means of comprehending and reflecting artistic work (Ginting et al. 233) and instead propose a symbiotic link between writing and creation modes of communication.\n\nAdditionally, the study will address the artwork in animations and digital entertainment. It will summarize the works of other academic scholars in the creative area, and delve into what computerized animation as a useful tool of study can entail. Finally, the study will focus on the future, establishing on what this implies for computerized animation, as well as making recommendations for future research.\n\nConclusion\n\nAnimation can be traced back to 1800s\u2019 toys and scribblings on the margins of papers and tracked to modern-day computerized animations that have brought varying perspectives of film, and prominent visual effects. Of the notable concepts of animation, researching, discussing, and experiencing are the most important. The research study will employ qualitative research methodology in defining animation and describing computerized animation as a useful research tool. It will also review previously done works of literature and purpose to fill the information gaps created. This is especially because there is a limited number of studies produced concerning the evolution of animations. Finally, the research will build into the future of animation studies by recommending further areas of interest.\n\nWorks Cited\n\nBaglama, Basak, Yucehan Yucesoy, and Ahmet Yikmis. \u201cUsing Animation as a Means of Enhancing the Learning of Individuals With Special Needs.\u201d TEM Journal, vol. 7, no. 3, 2018, pp 670. Web.\n\nBlackton, James Stuart. The Enchanted Drawing . The Library of Congress. Web.\n\nChen, Chien-Hsu, Chun-Yen Huang, and Yin-Yu Chou. \u201cEffects of Augmented Reality-Based Multidimensional Concept Maps on Students\u2019 Learning Achievement, Motivation, and Acceptance.\u201d Universal Access in the Information Society, vol. 18, no. 2, 2019 pp. 257-268. Web.\n\nCrawford, Pat. \u201cDigital Animation as a Participatory Tool for Exploring Community Visions.\u201d Environment and Planning B: Planning and Design, vol. 33, no. 4, 2006, pp. 481-484. Web.\n\nEkinci, Bar\u0131\u015f Tolga. \u201cA Hybrid Documentary Genre: Animated Documentary and The Analysis of Waltz With Bashir (2008) Movie.\u201d CINEJ Cinema Journal, vol. 6, no.1, 2017 pp. 4-24. Web.\n\nEhrlich, Nea. Animating Truth: Documentary and Visual Culture in the 21st Century . 2021.\n\nGiesen, Rolf, and Anna Khan. Acting and Character Animation: The Art of Animated Films, Acting, and Visualizing . CRC Press, 2017.\n\nGinting, Kristiani Lisma Vera Br, et al. \u201cImproving Students\u2019 Skill in Writing Narrative Text Through Animation Movie.\u201d Linguistic, English Education and Art (LEEA) Journal, vol. 3, no.1, 2019, pp. 230-237. Web.\n\nMorton, Drew. \u201cSketching Under the Influence? Winsor McCay and the Question of Aesthetic Convergence Between Comic Strips and Film.\u201d Animation , vol. 5, no.3, 2010, pp. 295-312.\n\nParker, Trey, and Matt Stone. \u201cMake love, not warcraft.\u201d South Park (2006).\n\nSchallert, Diane Lemonnier. \u201cThe Role of Illustrations in Reading Comprehension.\u201d Theoretical Issues in Reading Comprehension . Routledge, 2017, pp. 503-524.\n\nWinokur, Mark. \u201c2. Creole Cartoons.\u201d The Politics of Humour . University of Toronto Press, 2017, pp. 2-81.\n",
        "label": "human"
    },
    {
        "input": "Computerized Physician Order Entry in Clinical Practice Research Paper\n\nIntroduction\n\nComputerized physician order entry or CPOE systems are an increasingly popular alternative to the manual entry for clinician prescribing. Currently, about 15% of US hospitals have already adopted this practice, although reviews of the physicians remain controversial (Charles et al. 2018). Healthcare providers likely need additional clarification or training on the use of CPOE. It is equally important to consult scientists and developers to create the most effective and safe interface for the convenience of clinicians. Therefore, there is a need for a literature review, where the listed issues would be discussed in more detail, with the provision of actual research results, opinions of clinicians, and proposals for the implementation and optimization of CPOE. This paper presents the literature review as an integral part of the report regarding the benefits, errors, and implementation potential of the CPOE.\n\nLiterature Review\n\nMajor Themes Found in the Articles\n\nCPOE Concept\n\nThe CPOE concept is defined by most researchers in the same way, as it is a technical term. Specifically, Connelly and Korvek (2017) define a concept as a system that \u201cimproves clinician-patient care by reducing the number of medication errors in hospitalized patients\u201d (p. 1). CPOE is also defined as \u201ca solution to reduce medical errors by implementing a computerized order entry system for healthcare providers\u201d (Amiri et al., 2018, p. 1). It is also noted that CPOE as a digital medicine tool is used everywhere today, and this transformation took place very rapidly, since 10 years ago clinicians wrote out most of the orders by hand.\n\nIn particular, the introduction of the CPOE was stimulated by the 2009 federal law HITECH, and the purpose of using the CPOE was to improve the safety of drug orders. Later, the system was improved with the ability to \u201corder analyzes, procedures and consultations in electronic form\u201d (\u201cComputerized provider order entry,\u201d 2019, p. 3). Noteworthy, there are several stages of prescribing and administering drugs: ordering, transcribing, dispensing, and administration (\u201cComputerized provider order entry,\u201d 2019). At each stage, CPOE helps to make your workflow safer and more efficient.\n\nCPOE in Research\n\nReckmann et al. (2009) looked at the evidence from 12 studies to determine how CPOE systems affect the reduction of medical errors. Although the studies characterize the processes that take place even before the legal obligation to implement systems, they provide some important insights into the use of systems. The use of CPOE has been found to introduce new types of errors, indicating the need for special monitoring of systems in hospitals. In general, the scientists concluded that the systems help to formulate orders for medicines more clearly and fully. At the same time, security remains in question, and there is a need to adapt systems to avoid errors. Scientists also recommend more standardized studies on prescribing errors, including comparisons between different CPOE systems.\n\nCPOE Errors\n\nKhanna and Yenn (2014) note that the most common mistakes associated with CPOE systems are incorrect dosing and duplication. Many errors are also \u201cdue to a hybrid workflow with paper and CPOE and due to poor design decisions\u201d (Khanna & Yenn, 2014, p. 30). It is noteworthy that scientists analyze the medical experience of 2011 when the system was just being mastered by doctors at the first stages. As a rule, health workers complained that the system slows down the workflow, although some processes, on the contrary, accelerated. The reduction in cognitive load after initial learning and adaptation to the system allowed physicians to eventually appreciate its benefits.\n\nCPOE Benefits\n\nThe scientists paid special attention to the advantages of the system, in particular the trend towards increased patient safety. In particular, a significant reduction in medical errors when ordering drugs was positively associated with the introduction of the CPOE system (Charles et al. 2018). The system also increased speed \u201cin sending orders for drugs, laboratory and radiology examinations to the appropriate departments or institutions\u201d (Charles et al. 2018, p. 3). According to scientists, the introduction of additional functions of the Support System into the CPOE programs is the most significant advantage. A key element of the Support System is the function of coercive precautions due to compatibility or side effects of drugs, including the side effects of various comorbid conditions. Scientists believe that introducing clues about such problems will significantly improve patient safety.\n\nAmiri et al. (2018) conducted a study by interviewing healthcare practitioners about the comparative advantages of CPOE. The majority of those surveyed identified the following benefits: better decision making, correction of prescription errors, support in clinical decision making, improved data reliability, improved communication between healthcare professionals, and assistance in documenting treatment processes (Amiri et al. 2018). Other benefits include reduced psychological impact due to lack of information in patient records, correct dosage according to patient data, improved coordination of pharmacies and clinical departments, savings in staff time, and reduced risk of error. Doctors also noted increased patient satisfaction, improved hospital profitability, improved readability, improved patient safety, and reduced repetitive actions.\n\nCPOE Implementation Potential\n\nInterestingly, one of the studies seems to be especially important as it discusses the nuances of CPOE implementation and optimization, including improving the user interface. For instance, scientists propose the introduction of a radio button that allows you to select only one item from the list in the case when the simultaneous administration of certain drugs can cause side effects (Connelly and Korvek, 2017). Also, as part of the workflow, the user can be warned of potential dangers from drug interactions. Standardization of choices and default drug choices are other possible additional options. Default selection and standardization are convenient when prescribing medicines for the most common illnesses, such as the common cold, and can be a significant time-saver.\n\nMoreover, the CPOE can offer support in decision making and dosage determination, for example, in identifying allergies, or drug interactions with food, other drugs, and comorbid conditions. The CPOE may also include information material in the form of monograph links, policy and protocol links, and toxicological information (Connelly and Korvek, 2017). The advice can also be provided to guide clinicians to follow protocols and prescribe any necessary tests for related conditions, such as advice on treating pneumonia or chest pain. At the same time, the advice may be specific to the treatment of the same disease in different settings, such as on the hospital floor or in the intensive care unit.\n\nThe studied articles used various methods to obtain information and form conclusions on the results related to the application of CPOE. Amiri et al. (2018) conducted a cross-sectional study in March-June 2017 using a survey of doctors from hospitals at Urmia University of Health Sciences. 200 doctors were interviewed, selected at random. For the survey, we used the technique of spreading innovations by E.M. Rogers, and the data were analyzed through SPSS 16.0. Reckmann et al. (2009) used a literature search to identify studies evaluating the association between CPOE and prescribing errors. The search strategy focused on investigating errors in prescribing and taking medications. The journal database included Ovid MEDLINE (1950-2007), CINAHL (1982-2007), EMBASE (1974-2007), and other journals. Scientists analyzed 954 articles, and also took additional links from reference books and review articles.\n\nDifferences and Similarities between Papers\n\nThe articles reviewed present many similar approaches to analyzing the disadvantages and advantages of CPOE. Most scientists agree that CPOE systems have good potential for implementation and that clinicians are easily adaptable to work with them. There is also widespread recognition of the potential for reducing medical errors associated with prescribing and dosing drugs. Equally important, improved communication between healthcare professionals and increased efficiency in hospital operations is recognized. At the same time, scientists associate possible errors with imperfections in the user interface and recommend optimizing the systems by introducing prompts of the Support System. Therefore, it is obvious that the widespread implementation of systems with training and taking into account the comments of scientists regarding the optimization of systems will be of great benefit to both patients and doctors of medical institutions of all types.\n\nConclusion\n\nThus, the literature review of the articles analyzing the implementation potential of CPOE was presented. The articles discuss the CPOE advantages and disadvantages, the perceptions of the system by the clinicians, and possible ways of its optimization for implementation in the hospitals. The mentioned topics present the scope of the scholarly discourse regarding the CPOE systems, and the articles as of 2009 and 2014 give an additional depth and perspective on introducing these systems into the medical practice. Further research concerning the issue is recommended to provide a more functional overview of the CPOE system use.\n\nReferences\n\nAmiri, P., Rahimi, B., & Khalkhali, H. R. (2018). Determinant of successful implementation of Computerized Provider Order Entry (CPOE) system from physicians\u2019 perspective: feasibility study prior to implementation. Electronic Physician , 10 (1), 6201.\n\nCharles, M., DelVecchio, A., & Eastwood, B. (2018). Computerized physician order entry (CPOE). TechTarget. Web.\n\nConnelly, T. P., & Korvek, S. J. (2017). Computer provider order entry . StatPearls. Web.\n\nComputerized provider order entry . (2019). PS Net. Web.\n\nKhanna, R., & Yen, T. (2014). Computerized physician order entry: promise, perils, and experience. The Neurohospitalist , 4 (1), 26-33.\n\nReckmann, M. H., Westbrook, J. I., Koh, Y., Lo, C., & Day, R. O. (2009). Does computerized provider order entry reduce prescribing errors for hospital inpatients? A systematic review. Journal of the American Medical Informatics Association , 16 (5), 613-623.\n",
        "label": "human"
    },
    {
        "input": "Computerized Management Systems and Quality of Care of Patients Research Paper\n\nHealthcare system is implementing more computerized management systems, providing high quality care for patients. During my practicum experience I have been working with various tools that helped me to track the patients and collect information about them. Yet, I had difficulties in learning software applications of those computerized management systems. Ang (2019) argues that after collecting electronic health records, nurses should further organize and process the information, placing them in a position of developers of computerized systems. As such, nurses should have good knowledge of computer and data science. For me and for my peers, applying technology to the patients\u2019 care was difficult due to lack of experience.\n\nThe issue of new nurses\u2019 education about different computer applications is not new. There are some tools that help nurses to learn computerized management systems. Barra et al. (2016) examine the Computerized Nursing Process based on the International Classification for Nursing Practice and conclude that such tool allows nurses to structure their knowledge for future education of new nurses. The tool provides grouping of clinical situations to reduce the amount of information that nurses will evaluate, thus making nursing practice less complex. Moreover, there is training applications that instruct nurses before allowing them to work with computerized management systems as each application requires special knowledge and skills. As such, some scholars examine the effectiveness of such training systems.\n\nFor example, Roos et al. (2020) analyze the readiness of nurses to use the Integrated Management of Childhood Illness Computerised Adaptation and Training Tool. The scholars point put that aside from knowing the application, independent learning ability and creativity of nurses are also critical factors contributing to their education of computerized management systems.\n\nReferences\n\nAng, R. J. (2019). Use of content management systems to address nursing workflow. International Journal of Nursing Sciences , 6 (4), 454-459.\n\nBarra, D. C. C., Almeida, S. R. W. D., Sasso, G. T. M. D., Paese, F., & Rios, G. C. (2016). Method for the modeling and structuring of computerized nursing in intensive care. Texto & Contexto-Enfermagem , 25.\n\nRoos, J., Naidoo, U., & Sandy, P. (2020). Determinants of Nursing Campuses\u2019 Readiness to Use a Computerised Training Tool. Africa Journal of Nursing and Midwifery , 22 (2), 18.\n",
        "label": "human"
    },
    {
        "input": "Researching of Computer Simulation Theory Essay\n\nIn the modern days, human creatures still cannot explain many things that happen due to the lack of knowledge or study in many areas. Therefore, various theories are permissible, including the one that a person exists in a computer simulation, which he does not know about or only suspects. The theory is developed and analyzed in the article Are We Living in a Computer Simulation? by Clara Moskowitz. I take the position of an agnostic regarding this statement as I think that at this stage of human development, it is impossible to unequivocally assert the correctness or incorrectness of this theory.\n\nBoth statements and cases from practice suggest the possibility of the existence of people in a computer simulation. First of all, due to the fact that people do not have an exact definition of their origin, although there are various assumptions, some of which are positioned as \u201cproven\u201d but contradict each other, alternative versions of the origin of the human race are acceptable. Perhaps we and people around us were created in a computer simulation by people in the future or by a much more developed intellect, representing another form of life (Moskowitz, 2016). For people of the future, it might be interesting how people live on planet Earth at the level of development we are at. Given the technical capabilities, it would be realistic to implement such a computer simulation.\n\nAt the same time, there is not a single proof for the idea that there is some kind of simulation in which we live every day. It is conceivable that humans are too self-centered and therefore assume that another race would want to simulate them, while in reality, they are not of real interest to higher levels of intelligence (Moskowitz, 2016). The presence of such concepts as luck can be explained only by a coincidence of circumstances, and therefore there is nothing that would contradict the logical development of events in the life of every person. If simulation exists, it most likely does not program people to live a particular scenario of life.\n\nThere are other thoughts and assumptions about the existence of a computer simulation. Some suggest that if there was a simulation of us as people from the past, we would be something similar to artificial intelligence (Moskowitz, 2016). At the same time, if the program itself was composed on the basis of artificial intelligence, which simulates human activity at our level of development, would such intelligence come to the very idea of the possibility of existence in a computer simulation? Most likely, the functionality of artificial intelligence would be limited to the necessary functions, while with the ability to learn, but without the emergence of ideas that are not relevant to the topic.\n\nSumming up, I take the position that while there is no evidence, it cannot be argued about the existence or absence of such a simulation. Perhaps in the future, scientists will be able to come to more advanced technologies that will test this hypothesis and find confirmation or refutation of it, which will be objective. Technology and science should develop in the future, leading the human race to new conclusions. Until then, people can only continue to study and try to come to unambiguous arguments regarding the possibility of human life in a computer simulation.\n\nReference\n\nMoskowitz, C. (2016). Are we living in a computer simulation? Scientific American , 7 . Web.\n",
        "label": "human"
    },
    {
        "input": "Systems Development Life Cycle and Implementation of Computer Assisted Coding Essay\n\nAdvancements in technology continue to revolutionize all aspects of healthcare, including healthcare records management. Among the many technologies that a high-quality technology-conscious healthcare facility should implement is Computer-Assisted Coding software. This software enables quick and efficient management of healthcare documentation, integrating patient data with financial information. As with other initiatives companies implement in the healthcare industry, Computer-Assisted Coding must overcome potential inherent risks and threats. While eliminating all risks and threats is a near impossibility, following the steps outlined in the systems development life cycle (SDLC) could mitigate severe dangers and threats to Computer-Assisted Coding software implementation.\n\nThe first phase of the system development lifecycle is needs identification. During this phase, identifying a company\u2019s security needs commences. When identified, the security needs to help draft the system architecture with the impact and mitigation efforts taken into account (Maria & Costas, 2021). This phase is essential in implementing the Computer-Assisted Coding software because it initializes project risk management, enabling managers to prioritize the more severe software implementation risks. Specific tasks include imagining different positive and negative scenarios, identifying critical risks and their origins, and classifying them in their order of severity.\n\nThe second phase entails specifying requirements. At this stage, the Computer Assisted Coding software implementation team has chosen the best scenario that optimizes company requirements while keeping risks and threats to the minimum. The potential risks the software must deal with are identified at this phase in addition to other system and hardware specifications. A vivid concept of the complete software together with in-built safeguards against identified potential risks and threats and other systems is conceived in the specifications of the requirement phase. After the success of the first two stages, the team responsible for implementing or acquisition of Computer-Assisted Coding software can proceed to purchase. The CAC software configuration, functional unit creation, testing, and verification are undertaken during this phase (Maria & Costas, 2021). The implementers must then physically realize the imagination into functioning software from a concept imagined in phase two. In actuality, these two stages involve combining the software specifications and security requirements to realize a relatively secure software for the organization. Where the capability to build CAC software is not available, acquisition from a pre-qualified vendor must occur.\n\nThe fifth stage involves the maintenance of the CAC software. For the CAC software to function correctly and at the rate envisioned by the implementers, support software such as the natural language processing (NLP) engine must be provided (Maria & Costas, 2021). Personnel with the requisite qualifications for reviewing the codes generated must also be provided. Given the rapid advancements in technology, the company must ensure the latest version of the software is available promptly. Updating CAC offers several benefits, including patching potential vulnerabilities that could be exploited to steal or manipulate client information. Maintenance also includes having backup systems to augment security measures in place. With backup systems, an organization can function without shutting down if the main CAC software is compromised.\n\nFinally, the sixth stage involves monitoring the CAC software (Maria & Costas, 2021). Monitoring consists of measuring the performance of the CAC against set targets and taking corrective actions to ensure performance criteria are met. Where vulnerabilities of security issues are identified due to the monitoring exercise, the optimal alternative is chosen to patch them. Monitoring is not a one-off event, especially where sensitive software such as CAC is involved. The management and officers with particular CAC software knowledge, including external and internal auditors, must monitor its performance to ensure the best possible results and protection.\n\nReferences\n\nMaria, M., & Costas, L. (2021). Software development lifecycle for survivable mobile telecommunication systems . Advances in Science, Technology and Engineering Systems Journal, 6(4), 259\u2013277. Web.\n",
        "label": "human"
    },
    {
        "input": "Why Is Speed More Important in Computer Storage Systems? Essay\n\nThere is a common misconception that the larger amount of storage in a computer system allows for better and faster performance. However, the storage issue is that a slow processor, while requiring significantly more time to retrieve and operate the data to access certain files, is more likely to fill the storage with redundant information. In some cases, when using slow processors to access webpages or files that require high speed and performance, the system has to back up the data for it not to crash.\n\nThus, it uses a considerably larger amount of random access memory (RAM). A high-speed computer system secures quality data processing, storage, and immediate retrieval of files (Donatus et al., 2017). Hence, it may be concluded that speed is more important in a computer storage system because, without high processing speed, any amount of storage may be filled by duplicating and saving some irrelevant information.\n\nWhile there are indications of how speed may be more significant than storage in the context of a computer system, both storage and speed are important to efficiency. Thus, while speed stands for the processor\u2019s ability to quickly and efficiently access local or web-based files, the notion of storage stands for the amount of data that could be simultaneously stored on the computer\u2019s hard drive. If the storage size is limited, the computer is more likely to be overloaded with information and slow down the processor\u2019s performance.\n\nOn the other hand, a slow processor may catalyze faster storage fill. Hence, while speed and storage are responsible for different functions in a computer system, their proper cooperation is vital for optimal performance. A primary distinction between speed and storage is the fact that speed predominantly deals with short-term system memory, whereas storage is a computer tool responsible for storing data on a long-term basis.\n\nReference\n\nDonatus, N. O., Agbaeze, E., Ikenna, N. C., Kizito, U. K., & Andrew, M. K. (2017). Positioning performance improvement of a servomechanism of hard disk drive in a computer. International Journal of Engineering Sciences & Research Technology, 6 (11), 102-107. Web.\n",
        "label": "human"
    },
    {
        "input": "Choosing a Computer for a Home Recording Studio Essay\n\nTable of Contents\n 1. Introduction\n 2. Computer Type\n 3. Parts Identification\n 4. Configuration Detail\n 5. References\n\nIntroduction\n\nWhen choosing a computer for a home recording studio, it is worth spending most of the time on the processor, RAM, and hard drives. It is these components that are most actively used by programs for working with sound. Another essential component is a sound card, but its choice depends on the task at hand: recording vocals, acoustic or electronic instruments, mixing, or live performances. The most important properties of this computer are the absence of noise that can damage sound recordings and reliability since the computer should not fail at a crucial moment.\n\nComputer Type\n\nIt is imperative to create an understanding of the aims of the computer. Sound recording and working with music have many different functions: from recording podcasts and broadcasts, developing soundtracks for films to a full recording of acoustic instruments or an orchestra. In any case, when the required options go beyond simple single-track recording, the computer must quickly handle the Digital Audio Workstation (DAW), a highly resource-intensive program (Miyara, 2017). In addition to the software itself, the processor is also forced to handle plugins, virtual synthesizers of sounds, various samplers, and many mastering effects applied to each track: compressor, gate, limiter, equalizer, and many others.\n\nVirtual instruments place a heavy load on RAM. The motherboard is responsible for the speed and stability of the system and should also have a large number of ports in case of many purposes of the computer in the studio. It is possible to save money from the entire computer configuration only on a video card since the graphic load in the case of mixing music tracks is minimal. Only in the case of an integrated graphics core, the processor will need to replace it with a full-fledged video card since the graphics core will take over part of the RAM (Liang et al., 2021). On the other hand, an external video card creates unnecessary noise but opens up the possibility of connecting multiple monitors, which is a highly recommended option in a large studio (Kalliris et al., 2019). The computer case is also essential, as it can neutralize all unnecessary noise, provide the necessary cooling, which will preserve both the performance of the parts and their durability. In durable memory, SSDs look preferable due to the lack of noise and higher speed and reliability, and it should also be paid attention to the power supply and cooler, which are responsible for the safety of the entire PC configuration.\n\nParts Identification\n\nI chose the Intel Core i7-1195G7 processor (12MB cache, up to 5.00 GHz) for its high clock speed to handle most recording tasks. I chose the GIGABYTE Z390 UD motherboard for many RAM slots and good compatibility with the above processor. The GTX 1060 6 GB graphics card was chosen for its low cost and characteristics, which are pretty enough for music editing. The Fractal Design Meshify case was selected for its superior sound insulation performance. The EVGA 750BQ is a suitable 750 Watt PSU with good energy efficiency. RAM for most tasks of this kind requires at least 16 GB; therefore, the most profitable option would be Crucial 16 GB DDR4, a proven and promising brand on the market. As a ROM, it is needed to choose an SSD with a volume of at least 200 GB, which is the minimum for any DAW, a large number of plugins, and the system itself. Ideally, it is necessary to take 512 GB as music libraries tend to take up much space. In this case, the best choice would be XPG Gammix 512 GB. The main characteristics when choosing a monitor are the presence of an IPS matrix for better response and high resolution. It is often required to have several monitors for musical editing, but in this case, the focus will be on one, to which in the future it can be added either the same or a smaller one. LG 24MK430H 23.8 is suitable for this assembly since it has all the above parameters and a low price. A sound card, in this case, is one of the essential elements of a computer, and it is needed to choose external ones. A professional but still relatively inexpensive option is the Focusrite Scarlett Solo, which offers phantom power, minimal latency, and reasonable sample rates. The mouse and keyboard functionality is not critical, but the presence of additional buttons on the mouse allows working more quickly and intuitively; in this regard, the Redragon S101 Wired Keyboard and Mouse Combo set is suitable. Finally, the necessary wiring includes an HDMI cable for the monitor and three USB cables for a mouse, keyboard, and sound card. The rest of the equipment, such as headphones and a microphone, is already specific to each specific purpose; therefore, it is described in the particular configuration section. Table 1 shows the prices for the specified components.\n\nTable 1. Computer Parts List\n\nComputer Parts List      \nPart                       Description                                   Price   Link      \nCPU                        Intel Core i7-1195G7                          $400    Click Here\nIntel Core i7                                                                              \nMotherboard                GIGABYTE Z390 UD                              $110    Click Here\nGigabyte                                                                                   \nCase                       Fractal Design Meshify                        $90     Click Here\nFractal                                                                                    \nVideo Card                 GTX 1060 6 GB                                 $320    Click Here\nNvidia                                                                                     \nROM                        XPG Gammix 512 GB                             $60     Click Here\nXPG                                                                                        \nRAM                        Crucial 16 GB DDR4                            $122    Click Here\nCrucial                                                                                    \nKeyboard & Mouse Redragon  Redragon S101 Wired Keyboard and Mouse Combo  $30     Click Here\nSound Card                 Focusrite Scarlett Solo                       $120    Click Here\nFocusrite                                                                                  \nPower Unit                 EVGA 750BQ                                    $70     Click Here\nEVGA                                                                                       \nMonitor                    LG 24MK430H 23.8                              $184    Click Here\nLG                                                                                         \nTotal                      Grand Total                                   $1,506            \n\n\nConfiguration Detail\n\nSince this computer assembly assumes work with sound, it is worth clarifying the recommended models of headphones, microphone, and speakers, as the most common and mandatory additional components. The Shure SM-58 is a classic and ideal low-cost microphone solution that can record vocals, podcasts, and more. The JBL 305P MKII active studio monitors in the amount of two, together with the equal frequency distribution in the Sennheiser HD 200 Pro headphones, is a professional solution for sound extraction and work with it.\n\nThe configuration of a computer for professional work with sound is a relatively budget solution compared to gaming models and computers sharpened for working with graphics or mining. One of the most expensive components is a video card, which is not critical in the described configuration. Another thing is that an impressive part of the budget is spent on additional musical equipment, which has a wide range of prices depending on the goals and quality. Nevertheless, this paper describes an assembly that allows solving most problems at a professional level, second only to ultra-fast, expensive studio equipment.\n\nReferences\n\nKalliris, G., Dimoulas, C. A., & Matsiola, M. (2019). Media management, sound editing and mixing. In Foundations in Sound Design for Linear Media (pp. 82-112). London: Routledge.\n\nLiang, C. C., Huang, C. C., & Liou, C. F. (2021). The impact of hardware buffer size settings on digital audio production: The model example of the Avid pro tools digital audio workstation. In Smart Design, Science & Technology: Proceedings of the IEEE 6th International Conference on Applied System Innovation (ICASI 2020), 2020, Taitung, Taiwan (p. 40). Boca Raton, FL: CRC Press.\n\nMiyara, F. (2017). Digital Audio Editing. In Software-Based Acoustical Measurements (pp. 167-186). New York, NY: Springer, Cham.\n",
        "label": "human"
    },
    {
        "input": "Computer Programming and Code Coursework\n\nBefore doing this assignment, I knew several basic concepts about computer programming and code. For example, I was aware that there are many programming languages, like JavaScript or Python, that are used for various purposes and different targeted platforms. Then, I also knew that for every task, there is a particular algorithm \u2013 a set of actions needed to achieve the desired programming output. Finally, I heard that, after designing a program, it is essential for the developer to test it out and check whether there are any problems so that users do not face them later (Google Developers, 2017).\n\nCertainly, the easiest game for me was the first one \u2013 Puzzle. I managed to finish it in a few seconds and naively assumed that all other tasks would be just as easy. The Maze game was the one I probably enjoyed the most since it was both engaging and not challenging, and I quickly understood what I needed to do. However, all last levels took me several attempts to finally see the solution, but every failure to get the person to the destination made me even more interested. I also enjoyed making music \u2013 it was satisfying and relaxing. Unfortunately, all the games with degree orientations made me doubt my skills and abilities, and I gave up quickly and decided to return to them later. Overall, I enjoyed this experience more than I thought I would, and I am excited that we have it as a part of our computer programming course.\n\nAfter getting acquainted with Blockly games, I managed to learn several new things about coding. First, some multi-leveled loops and conditionals may be used and connected in numerous ways. Second, I did not think that mathematical equations and math in general play such a significant role in computer programming. Finally, now I realize that apart from having talent, experience, and programming skills, it is also crucial to develop certain personal characteristics like patience, determination, and creativity.\n\nReference\n\nGoogle Developers. (2017). Blockly: Using block based coding in your app [Video]. YouTube. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer-Aided-Design, Virtual and Augmented Realities in Business Research Paper\n\nKnowledge work systems have revolutionized the capabilities of an organization in concern to data management and utilization. Computer-aided-design (CAD), or virtual, augmented (VR, AR) realities, and blockchain helped businesses to reach new heights. The usual applications of these technologies are in the field of data management, product visualization, and training; however, there is infinite potential in their development and integration with one another \u2013 and this is why they are important today.\n\nGenerally, CAD systems are used in visualization, prototyping, and understanding functions of a concept \u2013 they in great demand in architecture, product design, and engineering. According to Gao et al., their team used a CAD system to help to design a museum exhibition more efficiently (157). Since this process implies knowledge application from a great number of fields, such as \u201clandscape architecture, audiovisual engineering, and digital media,\u201d mistakes are likely to happen. With CAD, which allows testing the project, these mistakes are easier to spot and get rid of at an early stage. In doing so, VR and AR are extremely helpful and go hand in hand with CAD systems.\n\nVR and AR technologies are usually used as a tool for employee training and education \u2013 while also being efficient for 3D visualization purposes. This constitutes the main value of these technologies for a business. As Thies et al. write, \u201cvirtual training is one of the main VR applications\u201d (181). However, the usage of CAD with its integration with VR is still \u201cat an early stage\u201d in regards to the true collaborative system, such as cloud computing (Lemes & Lemes 25). In this case, blockchain can be extremely helpful with its capabilities to provide security of information. Since its primary characteristic is that information, once published, cannot be altered, every member can verify any action (Lemes & Lemes 25). Thus, in order for CAD systems to move forward, the technology of blockchain could be implemented in a variety of ways.\n\nIt is important to note that these technologies are of major importance because, at their core, they make use of innovative ideas that are simple but are in contradiction to the \u201cnorm\u201d in the industry \u2013 like decentralized systems and blockchain. I have only experienced the benefits of CAD as a user, seeing how much they actually simplify the process. Although my personal experience with them remains scarce, having studied their applications, I believe that they may benefit future society immensely.\n\nWorks Cited\n\nGao, X., Wang, X., Yang, B., & Liu, Y. \u201cDesign of a Computer-Aided-Design System for Museum Exhibition Based on Virtual Reality\u201d. Advances in Image and Graphics Technologies , 2017, pp. 157\u2013167.\n\nLemes, Samir and Lemes, Lamija. \u201cBlockchain in Distributed CAD Environments\u201d. Springer (76), 2020, pp. 25-32.\n\nThies, L et al. \u201cCompiling VR/AR Trainings from Business Process Models\u201d. International Symposium on Mixed and Augmented Reality Adjunct, 2019, pp. 181-186.\n",
        "label": "human"
    },
    {
        "input": "Computerized Ordering System Dissertation\n\nIntroduction\n\nThe word system can be traced to a Greek word that suggests combination of various features or properties (Dixit & Kumar 2007). Based on this a system is thought of as a combination between related functioning components or units with the purpose of achieving a single goal (Shelly & Rosenblatt 2011). A component of a system is single irreducible part that allows for changes to be made in a modular fashion (Dixit & Kumar 2007). Due to this nature of systems changes can be made to the various components with little or no effect on the entire system. In this report, the discussion will involve the considerations that prevail in the design and development of a computerized ordering system.\n\nAgile Systems and Software Development Cycle\n\nAs it has been mentioned in the previous section a system is composed of components. The main objective of systems analysis and design is to gain an understanding of how the system works and take steps to improve this system. The process of system analysis and design typically involves six distinct stages namely; preliminary investigation, requirements analysis, design, coding and development, implementation and maintenance (Dixit & Kumar 2007).\n\nFor the production of a suitable system these steps can be followed to completion using a variety of methodologies. The traditional approaches to this have been noted to be cumbersome and often hamper the speedy completion of a software project (Dixit & Kumar 2007). For this and other similar reasons this project was undertaken using an alternative approach to software development. This approach which is categorized under Agile methodologies was chosen due to the ability to counter problems inherent in traditional approaches to software design and development (Caine 2011).\n\nThe there are three key principles that are used in Agile methodologies that were found suitable for this project. First is the fact that Agile methodologies focus on adaptive rather than predictive procedures (Dixit and Kumar 2007). Traditional methods are especially poor at this due to their sequential nature thus making it difficult to adapt to changes in the requirements. In addition to that Agile methodologies also focus on people rather than roles. This allows for the creation of the most suitable solution for the users. Lastly Agile methodologies focus on self adaptive processes (Dixit & Kumar 2007). Based on these specific traits it was observed that Agile methods were more likely to be useful in achieving the desired objectives.\n\nSoftware Selection\n\nThe decision to utilize an Agile approach to the development of this software was also based on several considerations that were made after the design stage. It should be noted that during the design stage the development team makes decisions on how the new system will operate (Denis, Wixom & Roth 2009). The design stage comes after analysis which involves identification of business needs the software to accomplish. Through the use of various tools during the design stage the development team will create a blue print of the desired system (Denis, Wixom & Roth 2009). Upon completion of the blue print the team then makes a decision on what software to use in the development of the software. (See Appendix A)\n\nOne approach that can be adopted to acquiring the relevant software is the purchase of already completed products available in the market. This approach is suitable where there already exists a large market for similar needs. Based on the large ready market it is common to find that there are several available commercial packages that can be purchased to solve the problem. In such cases it would be unwise to spend additional funds in creation of custom software unless the commercial packages available do not cater for a major specific need of the business (Denis, Wixom & Roth 2009). An example of such an instance occurs if the company was in search of payroll software. In addition to saving time wasted in creation of a custom package it should be noted that the company would benefit from expertise already invested in creation of the available packages.\n\nThe main problem with this option arises from the fact that the company has to accept the functionality provided by the system. This is sometimes unsuitable as it may not necessarily provide a perfect fit based on business needs (Denis, Wixom & Roth 2009). Though most packaged applications allow for some degree of customization to change system parameters, these must allow for a perfect fit based on the requirements identified. In cases where such a fit is not possible this choice would be considered unwise based on the fact that some of the requirements identified may not be met suggesting an unsatisfactory solution to the problem.\n\nAs an alternative to the above problems another solution that may be utilized following completion of design is outsourcing. This involves enlisting the services of an external vendor to create or supply the system based on requirements (Denis, Wixom & Roth 2009). This approach involves giving the task of construction and maintenance of a software system to an external vendor. Though this approach comes with the advantage of getting the perfect fit it also bears inherent disadvantages. Among the disadvantages related to this approach include the issues that arise in formulating suitable agreement in relation to the software and rights.\n\nBased on the vendor in question the agreement for the software may vary considerably. In some cases the vendor may offer a single price for the product. In other cases the vendor may suggest a monthly payment for use of the product. In other cases vendors may sell a product but insist on maintaining ownership rights related to the product (Denis, Wixom & Roth 2009). However, careful consideration and planning are useful in guiding through such negotiations. Given the specific nature of this software project it has been determined that outsourcing is the most suitable option to pursue. This is due to the fact that the requirements identified are very specific to the organization and are not suitable for the available packaged software solutions.\n\nSelection of Software Tools\n\nBased on the strong need for reliable data in relation to this system it has been established that there is a serious need to store relevant data in a database. This arises due to the fact that databases allow for improved management of information (Coronel, Morris & Rob 2009). The improved management of information stored in a database can be attributed to the improved information sharing that is possible with modern database management systems (Coronel, Morris & Rob 2009). In addition to that it has been noted that the use of database management systems leads to improved information security. This is especially crucial since the proposed system may need to keep sensitive information such as credit card information.\n\nIn addition to improvements related to security these systems also provide advantages by improving the process of integration (Coronel, Morris & Rob 2009). This is especially useful given that the information stored is likely to be used in various parts of the system and thus may need to be available to these various components. It becomes easier to trace how activities in one sector affect other sectors of business (Coronel, Morris & Rob 2009). In addition to that it has been noted that database management systems are useful in minimizing data inconsistency (Coronel, Morris & Rob 2009). This is very important in that it ensures changes made in various sections within the system can be made without affecting the consistency of data stored on the system.\n\nFor the above reasons this project was developed making use of a popular database management system known as MySQL. This software provides most of the features available in commercial high end database servers (Williams & Lane 2004). The main reasons behind the selection of this software can be traced to the fact that it is available free of charge thus reducing the costs associated with the development of the project. In addition to that this particular software is very well suited for web based applications. For this reason it will allow easier transition to a fully web based version that will run across several branches.\n\nIn relation to the design of the user interface and the application for the ordering system it was proposed that Microsoft Visual Studio is utilized. This comes in light of the fact that Microsoft has made adjustments to the Visual Studio to support concepts used in Agile system development (Strober & Hansmann 2010). These features have been designed to allow for continuous integration that is primary characteristic of Agile systems. An example of these changes can be seen in the addition of tools such as Simple Design , which is used in creation of technical design documentation (Strober & Hansmann 2010). This provides a big advantage to the development process owing to the fact that the Microsoft tool family is already a consistent part of the Visual Studio system (Strober & Hansmann 2010). In addition to this the choice was influenced by the vast experience in the use of ASP provided in the Visual Studio package.\n\nBased on considerations such as those made in this report it can be seen that the proposed system is likely to be successfully implemented and easily altered in the future. These considerations are thus considered given the fact that they cater both for functionality of the completed system, integration and future needs.\n\nBenefits of Proposed System\n\nThe proposed system is intended to solve the problems identified during requirements gathering and it is expected it should improve the following procedures in the restaurant.\n\n 1. Improve management of orders\n 2. Improve management of Inventory\n 3. Improve bookings, cancelations and allow remote bookings\n 4. Improve the provision of services\n 5. Reduce time spent in processing orders\n\nReferences\n\nCoronel, C, Morris, S & Rob, P 2009, Database Systems: Design, Implementation, and Management , 8 th edn. Cengage Learning, Boston.\n\nDenis, A, Wixom, BH & Roth, RM 2009, System Analysis and Design , 4 th edn. John Wiley & Sons Inc, Hoboken.\n\nDixit, JB & Kumar R 2007, Structured System Analysis and Design , Laxmi Publications (P) Ltd., New Delhi.\n\nCaine, M & Associates 2011, DSDM Atern Enables More Than Just Agility. DSDM Consortium, Web.\n\nShelly, GB & Rosenblatt, HJ 2011, Systems Analysis and Design , 9 th edn. Cengage Course Technology, Boston.\n\nStrober, T & Hansmann, U 2010, Agile Software Development: Best Practices for Large Software Development Projects , Springer-verlag, Heidelberg.\n\nWilliams, HJ & Lane, D 2004, Web database applications with PHP and MySQL , 2 nd edn. O\u2019Reilly Media Inc., Sebastopol.\n\nAppendix\n\nAppendix A: ERD Diagram\n\nAppendix B: Use Case Diagrams (Internal View)\n",
        "label": "human"
    },
    {
        "input": "Anticipated Growth in Computer and Monitor Equipment Sales Presentation\n\nIntroduction\n\nEMI (Essential Monitors Inc) has suffered slow growth in its sale of computer LCD monitors. This presentations explores the computer equipment market to identify opportunities and device ways of using the opportunities to the advantage of EMI.\n\nOpportunities available for growth\n\nThe following are the reason why the computer equipment industry is perfectly suited for growth.\n\n  * Increased use of technology.\n  * The dynamic nature of technology.\n  * A variety of equipment to suit user preferences.\n  * High quality innovations giving users unlimited choices.\n\nThese reasons have worked together to ensure continuous entry of new computer equipment users and necessitate continuous change of existing equipment by users and thus they have contributed to market expansion.\n\nThe opportunities available for growth in computer monitor sales are as follows:\n\nFirstly, there is the increased use of technological equipment in the contemporary society. This means that more people and organizations need technology equipment in order to survive in the society today. This gives this industry a ready market for its products. The second reason is that the technology industry is very dynamic and thus new and better equipment are continually replacing outmoded technologies. Consider for example the introduction of LCD monitors in the industry, these monitors have replaced the CRT monitors and thus brought about revolution in the computer equipment industry. The computer equipment industry is also characterized with continuous introduction of a variety of equipments to suit user preferences.\n\nFor instance, introduction of bigger monitors than the existent monitors has helped in implementing several computer projects with commendable efficacy. The innovations evident in the computer monitor industry are also a contributor to the growth opportunities in the computer equipment industry. The quest for high-definition display has seen innovations leading to introduction of plasma and electroluminescent displays. These give a perfect opportunity for computer monitor companies to beef up their sales (Baker 2006, p. 1).\n\nPossible growth alternatives\n\nEMI should take advantage of the available opportunities for growth to increase its computer monitor sales.\n\nThe following strategies can be used to improve the sales of the company.\n\n  * Determining the size of their market and coming up with market expansion plans like the contracting of resellers in unexploited markets.\n  * Collecting data related to the sales volumes that are realized with the current high prices, predicting sales upon reduction of selling prices of equipment and comparing the prices with their respective profit margins to see whether a reduction in prices of equipment will yield good results.\n  * Analyzing different markets to determine their contribution to the overall sales of the company and implementation of product selection to suit different markets.\n  * The company should also analyze its network of resellers and determine the output of each reseller and the reasons for the success of the successful resellers.\n  * The company should also analyze its competitive advantages and study the reasons for the relative success of the most successful competitors.\n\nThe appointment of resellers in unexploited markets will definitely lead to increased sales but comparisons should be made to ensure that the best market is chosen. EMI\u2019s sales are low because of the high prices of LCD screens. This means that if the profit margin per product is reduced and large volumes of sales are realized, the company will make more profits (Law 2003, p. 1).\n\nAn analysis of the contribution of different markets to the overall sales will make the company to know where to concentrate its efforts on. If the leading markets have several other characteristics that are similar to other markets, a research can be conducted to deduce the reason for the relative low sales of the latter. This way, problems can be solved to increase sales. Product selection will be very crucial in the efficiency of sales. This is because different markets are characterized with different user preferences and abilities. This way, the company will know where to supply CRTs, LCDs, plasma displays etc. and at what price to sell them. The reseller analysis will enable the company to distribute resellers better and get new resellers with guaranteed performance. Finally, competitor analysis will help EMI develop policies that will enable it to beat its competitors and thus realize larger sales volumes (Baker 2006, p. 1).\n\nThe best strategy\n\nThe best among the stated strategies is an integration of the price reduction strategy with the strategy involving addition and placement of resellers. This can be substantiated by the fact that lower prices will attract more customers and reseller addition and placement will provide new markets and thus increase the market size. This will ensure an exponential increase in sales that will increase the profit margin.\n\nPossible difficulties\n\nIn implementing the above stated strategy, the company may encounter a number of difficulties.\n\n  * Firstly, respondents may provide wrong information during research leading to incorrect information about the market. This will in turn lead sales that do not agree with projections.\n  * A reduction in monitor prices may not attract the sales volumes required to exceed existent profit margins. This could lead to poorer performance.\n\nControls to avoid these problems\n\nAppropriate controls should be put in place to ensure that these problems are not encountered. These controls include:\n\n  * Proper projections for future sales after market expansion by conducting a thorough research on demographics of markets and technology use in the markets.\n  * Ensuring that a large number of respondents are contacted during data collection to ensure that accurate information is obtained.\n\nConclusion\n\nWith the current extensive use of technology and continuous introduction of new, sophisticated and desirable technology products, companies in the technology sector should beat all odds to realize growth. EMI should, therefore, follow the stated guidelines for growth to ensure that it improves its sales and profit.\n\nReference List\n\nBaker, J. B. (2006). \u201cMarket definition: an analytical overview\u201d. Web.\n\nLaw, G. (2003) \u201cMonitor sales go flat.\u201d. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer-Mediated Communication Competence in Learning Essay\n\nTheories of computer-mediated communications (CMC) focus on how people learn to use them as active agents. This approach is different from the debates in terms of positive and negative aspects of the computer medium, which puts a human into a passive role. Research by Sherblom et al. examined the cognitive influences of knowledge, motivation, apprehension, skill, and medium on student participation in computer-supported collaborative learning (CSCL) environments (34). In particular, the researchers attempted to clarify which influence has the most power over the students\u2019 willingness to participate in CSCL discussions.\n\nThe researchers combined two main approaches to CMC competence analysis to achieve those goals. The first approach identified participants\u2019 knowledge and motivation as the primary influences, while the second focused on affective predispositions such as apprehension and reticence. CMC apprehension determines an overall negative perception towards text-based communication, and reticence describes anxiety emerging during the use of CMC to express emotions and personal meanings (Sherblom et al., 33). After defining these factors of influence, the researchers conducted a questionnaire-based qualitative analysis of the control group.\n\nDuring the first stage, 91 participants completed an anonymous online survey that queried their self-perception of the following factors: CMC knowledge, motivation, skill, apprehension, medium, and degree of participation in CSCL discussions. After completion of the data collection stage, the items loading of six factors were entered into a linear regression analysis that produced a regression model. That model provided a clear representation of influence, contributed to participation in CSCL discussions by each factor.\n\nThe study showed that knowledge of the CMC medium was the strongest influence on participation with a \u03b2=.41. Perceived skill occupied second place in terms of importance with a \u03b2=.23. Apprehension has also contributed to negative participation with a \u03b2= -.17 score. Motivation \u2014 an overall reliance and trust in CMC, took fourth place with a \u03b2=.11. Finally, the influence of the CMC medium showed little to no contribution with a \u03b2=.01 score.\n\nIn regard to limitations, one should realize that the study included quite a small sample of participants. Due to that fact, it would be wrong to claim that researchers confirmed a universal rule. Despite this limitation, the study contains practical value since it provides an insight that can be examined in other classrooms or bigger groups of participants. In addition to that, teachers can use the results of this study to improve students\u2019 experience with CSCL.\n\nIn a broader sense, the study\u2019s findings can improve the quality of learning in an online format. For example, the revealed importance of knowledge shows that instructors need to pay attention to students\u2019 training before engaging them in online discussions. The relatively small scope of research created certain limitations; however, the study still highlighted weak spots that hinder the use of technology for educational purposes.\n\nIn general, the study did a decent job at revealing problems that emerge during online communication in an educational setting. Society still faces troubles created by seemingly endless waves of the COVID-19 pandemic. In that regard, it is impossible to predict whether education will return to the usual format, or teachers and students will have to master online communication because it will stay relevant for a long time. If the latter becomes the case, the awareness about CMC and CSCL will grow in importance, changing the face of education in the long perspective.\n\nFinally, I would like to commend the paper for clarity in research methods and result presentation. This study is not difficult to comprehend not only for education system professionals but also for the students. I especially approve that researchers provided practical suggestions on solving problems related to such issues like the lack of CMC knowledge or CMC apprehension. I find this information valuable for organizing the educational process in the online format, both for the students and their teachers.\n\nWork Cited\n\nSherblom, John C., et al. \u201cThe Influence of Computer-Mediated Communication (CMC) Competence on Computer-Supported Collaborative Learning (CSCL) in Online Classroom Discussions.\u201d Human Communication, vol. 16, no. 1, 2013, pp. 31-39.\n",
        "label": "human"
    },
    {
        "input": "Getting to Know Laptop Computers Report\n\nA laptop computer is a crucial component of modern life that allows instant access to education, entertainment, and communication almost anywhere. It is light and portable, allowing users to take the device with them during long and short trips. A laptop differs from a desktop computer by its size and other features (GCFLearnFree.org, 2012). Still, it has almost the same functionality as its bigger analog. This report aims to discuss the composition of a laptop computer and the purpose of each element.\n\nEach part of a laptop is necessary for its proper operation. Like a desktop computer, it contains a monitor and keyboard (GCFLearnFree.org, 2012). However, the distinction is in the size of these two parts, which are smaller in a notebook computer. Furthermore, instead of a regular mouse, a laptop has a touchpad to control the pointer on the screen (GCFLearnFree.org, 2012). One of the essential advantages of this computer is a built-in rechargeable battery that allows relative independence from the power source and incredible mobility. The laptop\u2019s other beneficial feature is the possibility to connect a regular monitor, keyboard, and mouse through the unique ports, transforming it into a stationary computer (GCFLearnFree.org, 2012). Overall, laptops have some important peculiarities that distinguish them from stationary devices, increasing their usability and portability.\n\nTo summarize, a laptop possesses the same functions as a desktop computer but is smaller in size. Installed battery, keyboard, and touchpad make this device convenient and easily transportable. Although it may seem that a compact monitor and the lack of a regular mouse can complicate its utilization, specific ports allow connecting these components to a laptop and turn it into a desktop computer. Finally, a rechargeable battery allows to continue working on a computer even in the absence of electricity.\n\nReference\n\nGCFLearnFree.org. (2012). Computer basics: Getting to know laptop computers [Video]. YouTube.\n",
        "label": "human"
    },
    {
        "input": "The Evolution of Computers and Digitalization Essay\n\nThe evolution of computers was a long process, which started with the emergence of calculating machines. This event marked the beginning of the attempts to create devices intended for performing thinking work (Code.org, 2020). Consequently, the first inventions became more complex, and they were developed by numerous mathematicians, including Blaise Pascal, Gottfried Leibniz, Charles Babbage, and many others (Riggs, 2021).\n\nThese instruments performed arithmetic operations, and they were further developed and turned into automatic devices. The repeated operations, which were the basis of their functioning, determined further progress. Their use was facilitated by the creation of universal punch cards, binary language, and other technological solutions (Riggs, 2021). Meanwhile, the ultimate digitalization happened after the introduction of an integrated circuit, which connected numerous transistors. This occasion led to a fast evolution of hardware and software, reduced power consumption, and made the computers smaller.\n\nReferences\n\nCode.org. (2020). What is a computer? [Video]. YouTube. Web.\n\nRiggs, N. (2021). The history of computing [Video]. YouTube. Web.\n",
        "label": "human"
    },
    {
        "input": "Current Trends and Projects in Computer Networks and Security Research Paper\n\nSoftware-Defined Networking\n\nSoftware-defined networking (SDN) is a cloud-based architecture intended to enhance a network to be more flexible and efficient to handle. When using SDN, there is centralization of management by conceptualizing the regulator plane from the data advancing function in the separate networking (Abbas et al., 2020). SDN offers a centralized and programmable network that consists of various elements. First, there is a control element responsible for the centralization and control management, automating and enforcing other task commands.\n\nThe second element is the southbound Application Programming Interface (APIs) used to relay the data and information between the independent network gadgets and controller planes. Such devices that information is relayed include switches, routers, firewalls, and access points (Pradhan & Mathew, 2020). The last element used in the centralization of the programs is northbound APIs that convey data between the applications and policy regulators, whereby an SDN appears like an independent function.\n\nAccording to the Global Networking Trends 2020 report, SDN has been widely adopted across the data quarters. That means a range not less than 64% for the data centers, Wide Area Networks by a range of not less than 58%, and 40% in access networks (Abbas et al., 2020). The main purpose of SDNs is to simplify operations by decoupling the control planes, achieving a faster time to promote through the deployment of applications and services by use of APIs (Pradhan & Mathew, 2020). Therefore, there are four areas that an organization that can rely on SDN technology to make a competitive advantage. First, network programming, logical centralization and control, network abstraction, and interoperability openness.\n\nSDN controllers are in the market because the developed companies have utilized their capability to sell the components. Examples of SDN networking components include Cisco Open SDN, Brocade SDN, and Jupiter Contrail SDN (Rosa, 2019). Additionally, there are open sources of SDN components such as Beacon, Opendaylight, and Ryu. The open-source networks are linear with the SDN designs; therefore, they provide a better interface useful to the companies.\n\nIntent-Based Networking (IBN)\n\nIntent-based networking (IBN) is a developing technology idea that targets to apply an intensive level of artificial intelligence and networking insight. To elaborate on the insights, they replace the monolithic processes of configuring networks and respond to network issues (Yichiet et al., 2021). That means the management of a given organization can send a request to communicate to the network the intended outcome instead of coding and executing the single tasks manually.\n\nThe benefits of using IBN are reduction in manual tasks, fast resolution through effective troubleshooting, reduced risks associated with compliance of networks, improvement of security, and optimization of analysis (Rosa, 2019). Examples of IBN networking include Cisco, Apstra, closed-loop, and Silicon Valley. For example, the given company from the analysis text has various tasks that it wants to perform to improve operations (Benzekki et al., 2016). Assuming that the company has IT experts who have specialized in microservices architecture, they can undertake various tasks to achieve that.\n\nFor example, the Caduceus has 2,000 Microsoft Windows computers in the workstation. It intends to utilize the Dell OptiPlex series, whereby several changes will be required. The IT team can access the cost by use of IBN. First, the desired intention would be \u2018running of Intel Core i5 processor, install windows 10, contain 8 Giga Bites (GB) and configure 256 GB hard disk drive\u2019. Each action will be put against the cost, and the automation would be validating the intent and translating it into action (Abbas et al., 2020). Through the quality control monitor department, the company will check the feasibility and control the actions in axes ensuring that the intent works.\n\nHow Virtualizing the Desktop and Now Back-End Infrastructure Are Complementary and Related\n\nThe notable difference between SDN and monolithic programming is that SDN is software-based while traditional networking is hardware-based. Therefore, SDN is more flexible and allows users to control and manage resources in a virtual way throughout the control command (Yichiet et al., 2021). The SDN controllers have a northbound feature that utilizes APIs to develop a direct communication using the required protocols that would oppose traditional design that uses all the available elements for the user to manually remove. In the Caduceus example, the IT personnel can direct network paths and have a proactive arrangement of network services that can produce linear communication with the financial controller (Benzekki et al., 2016). The fact that the company\u2019s resources can be accessed remotely means that there can be the generation of abstract copies of the physical networks from a central firm\u2019s locations.\n\nFig. 1. A comparison between traditional network and SDN\n\nThe architecture between the traditional and IBN is based on the network complexity due to automation. The two networks are built in a different era. The traditional network\u2019s focus is for the need to revolutionize the networks and process in-line with overall business aims. The pros of IBN are the increased agility by reducing the network infrastructure for the organization. IBN works across all handsets from different levels. Unlike traditional networking, which has delays in operations, IBN improves the control and performance due to the software capability to implement tasks across all networks. The major benefit of IBN is that it can allow IT personnel to alter any constraint realized through the graphical interface instead of the device by a device that traditional ways relied on.\n\nThe cons of SDN and IBN are the controller\u2019s vulnerability because the person needs to manage the network instead of routers and other switches. Therefore, the controller\u2019s security must be enhanced by checking who has access to the controller for security purposes. Additionally, IBNs and SDNs have distributed denial-of-service threats due to the large numbers of undeclared intent into the network line. Since there is the elimination of physical routers and switches, there may be a missing firewall to prevent the network\u2019s vulnerability (Benzekki et al., 2016). Although the traditional networks may not have a full centralization system when being configured, to some extent, they are safer as compared to SDNs and IBNs.\n\nRelationship Between SDN and IBN\n\nIBN can be framed as a modern successor to SDN since it applies most principles and architectures. That means elements such as a division between the applications and the network is used by the two networking methodologies. SDN uses a software configuration, while IBN utilizes the same concept with the physician workforce to execute various components. The two networking architectures can transform the way networks and implemented and managed (Benzekki et al., 2016). For organizations to have an advanced program, they need to combine both SDN and IBN.\n\nIBN takes SDN to a further step by allowing network administrators to have a network configuration to address and support businesses than grow the workforce\u2019s capability. If the organization\u2019s personnel want to accomplish a certain intent, they can adhere to the previous SDN policies for the success of the matter (Karakus & Durresi, 2018). For example, the company wants to set Virtual Private Network (VPN)software to achieve easy tracking. Therefore, the IBN-based command will have to perform self-checks to allow the operator to run the SDN to the approved interface for tracking the data and information. Even though the cost would be approximately $2000000, the returns will be a strong cyber-secured operation that can guarantee streamlined workflow that can bring more income.\n\nConclusion\n\nOrganizations need to adopt the new cloud-based services in operations that require technology. From the example, Caduceus has highlighted moving from monolithic applications by adopting microservices architecture that works more independently than the traditional models. IBN is an advanced program based on SDN principles that seeks to have more functional resources but under strict supervision and monitoring. The two networks are interrelated because they apply the same deployment capability when designing an intent.\n\nThe two networking components have enhanced modern computing because they have centralized data and information points compared to the old configuration forms. One of the key factors that the paper has shown is the ability to have access to a given network without distorting the end-to-end functions within the same organization. From the analysis, it is easy to predict that in the future, IBN and SDN will have wider coverage in cloud computing, where organizations will have improved service implementation and other programs. Any modern organization should learn the benefits of SDN and IBN networks and utilize them to enhance its portfolio.\n\nReferences\n\nAbbas, K., Afaq, M., Ahmed Khan, T., Rafiq, A., & Song, W. (2020). Slicing the core network and radio access network domains through intent-based networking for 5G networks. Electronics , 9 (10), 17-20. Web.\n\nBenzekki, K., El Fergougui, A., & Elbelrhiti Elalaoui, A. (2016). Software-defined networking (SDN): A survey. Security and Communication Networks , 9 (18), 58-78. Web.\n\nKarakus, M., & Durresi, A. (2018). The economic viability of software-defined networking (SDN). Computer Networks , 135 (3), 81-95. Web.\n\nPradhan, A., & Mathew, R. (2020). Solutions to vulnerabilities and threats in software-defined networking. Procedia Computer Science , 171 (2), 81-89. Web.\n\nRosa, M. (2019). Improving security through software-defined networking (SDN): An SDN-based model. International Journal of Recent Technology and Engineering , 8 (4), 295-300. Web.\n\nYichiet, A., Min, J., Lee, G., & Sheng, L. (2021). Intent-based network policy to solution architecting recommendations. International Journal of Business Data Communications and Networking , 17 (1), 55-74. Web.\n",
        "label": "human"
    },
    {
        "input": "History of Computers: From Abacus to Modern PC Essay\n\nDiscussion\n\nIn history computers were only used as machines that performed calculations. These changed over time with more supplicated machines which were being developed to perform more general tasks (Null L.& Lobur J., P. 34) Modern computer is a result of advances in technologies and the need to quantify, record numbers and language. Papyrus was used to make records and write numbers.\n\nAmong the first machines used was the Abacus, which helped the early man to count (History of Computers, Para. 1). It was people who were regarded as the first computer. This is because computers were developed to perform the functions that were assigned to people. The name computer was a job title for people who were used to doing calculations (An Illustrated History of Computers, part. 1). The word \u201ccomputer\u201d is said to have originated from the Latin to refer to a person who computes (Rojas & Hashagen, P. 1). According to Webster\u2019s dictionary, a computer is an electronic that can be programmed to store, retrieve and process data (Zeruzzi, p. 351).\n\nThe Picture Shows Operations Back When People Were Regarded As Computers.\n\nClassification and Development of Computers\n\nComputers can be classified by their technology, their use, how they operated and the era in which they were in use (Rojas, P.1). According to Rojas (P. 3), we can classify computers into two classes that are the electronic programmed computer and others which were developed after the electronic stored programmed concept (P.3). Calculators were among the early machines and an example of this is the Harvard Mark 1 (Zeruzzi, p. 351)\n\nEarly man was in need of a way to count and do calculations. Between 1000 BC and 500 BD, He used the Abacus which had movable beads for calculations (The History of The Computer, Para. 2). A mathematician by the name Charles Babbage proposed that they construct a machine and name it Babbage Difference Engine which could calculate and print mathematical tables (The History of The Computer, Para. 3). In 1979 the United States Department of Defense had to honor Ada Byron Lovelace by naming a computer language she had written. She came up with the first computer program to improve Babbage\u2019s ideas to make them a reality. Her ideas saw the machine\u2019s capability to produce music and graphs (The History of The Computer, Para. 4).\n\nThe Old Abacus\n\nGeorge Boole was a professor of mathematics who wrote an investigation of the laws of thought; he was then recognized to be the founder of computer science (A Brief History of Computers & Networks, Part. 1). A punch card was developed by Herman Hollerith of MIT; it was a machine that used electric power. In 1982, William Burroughs introduced a calculator which could print only that it was a manual machine. He later improved it so as it could use electricity.\n\nA differential analyzer was built by Vannevar Bush of MIT in 1925. It could handle simple calculus only that it was not accurate. The machine was made of gears and shafts. Konrad Zuse was a German engineer who built a calculator to do calculations he handled dairy. Thereafter a programmable calculator was made in 1938 by Zuse. In 1936 at Iowa State campus, John Vincent Atanasoff started developing a digital computer and came up with ABC as the way of solving linear equations (History of Computers, Para. 11).\n\nThe Enigma was another machine that the Germans used in computing algorithms in 1937; it was a complex mechanical encoder. In the same year George Steblitz came up with a model that could solve more complex calculations. The enigma code was broken by the British who built a colossus mark 1 (History of Computers, Para.13).\n\nIn 1943 at Penn state, the development of an electronic numerical integrator and computer began by Mauchly and Presper Eckert of the Moore School. In 1944 Harvard Mark 1 was introduced and then used by the U.S navy. The Harvard 1 used a paper tape as its information storage. IBM came up with a 701 computer which became the first commercially successful computer. They developed languages like FORTRAN, LISP and COBOL that were used with the computer. In 1958 a transistor powered computer was introduced by a team headed by Seymour Cray. This was the year the integrated circuits were as well developed by Kilby and Noyce. At this time computers used integrated circuits instead of the transistors (A Brief History of Computers & Networks, Part 2).\n\nThe IBM 701 computer.\n\nThese first computers were room sized and were considered to be quite powerful. Once again IBM introduced system 360 which was designed for business purposes. The system was then used to demonstrate the very first wide area network TSS (Time Share System). The first microcomputer was used to manage telephone lines. This major development was a joint MIT and Bell design of greatly defined networks featuring shared resources. Through this development, Bell was not happy with MIT and parted ways and thereafter he came up with UNIX an operating system. After UNIX, there came APANet and consequently Alan Keys designed Apple operating system. This is the era in which he proposed the design of personal computers.\n\nA group of technicians seemed not to be happy with all these developments; they planned to form a company and named it Intel in 1969 (A Brief History of Computers & Networks, Part. 2). A pocket calculator was introduced by Texas instruments. Xerox introduced the mouse and proposals then was brought forward to develop the local area network.\n\nThe first personal computer was marketed in kit form with 256 bytes of memory. The machine used BASIC compiler that was developed by Bill Gates and other technicians. Apple followed the trend and went on to advertise also in personal computers in the same kit form. The computers comprised of a monitor, and keyboard. A few years passed and the personal computer took its center stage to the American scene where many computer companies were formed.\n\nMany of these companies did not survive for a long time, they vanished. By 1977 stores were selling personal computers and some of them exist today. Companies are now reducing the size of the personal computer while the performance of the machines is being improved. There is also an effort to reduce their prices to make them affordable and maximize their sales. After a failed attempt, IBM once again introduced a personal computer in 1981 which was successful. (A Brief History of Computers & Networks, Part 2).\n\nIBM Personal Computer.\n\nConclusion\n\nThe reducing size and price of computers have made it a universal component which has made big changes in human lives than any other development. Although it is difficult to tell which computer was the first to be developed, this paper tries to shows their history. Thus it is very important to know who, why and where these developments started (Rojas & Hashagen, P13).\n\nWorks cited\n\nA Brief History of Computers and Networks. 2010. Web.\n\nAn Illustrated History of Computers Part 1 . 2010. Web.\n\nCeruzzi E. A History of Modern Computing 2 nd edition. MIT Press, Cambridge MA. 2003.\n\nHistory of computers. 2010. Web.\n\nIBM 701 , Layout for a 701 Installation. 2010. Web.\n\nIBM Personal Computer. 2010. Web.\n\nNull, Lobur. The essentials of Computer Organization & Architecture. Jones & Bartlet, Sudburn MA. 2006.\n\nRojas, Hashagen. The First Computers: History and Architectures. MIT Press, Cambridge. 2002.\n\nThe History of The Computer. 2010. Web.\n",
        "label": "human"
    },
    {
        "input": "Super Micro Computer Inc.\u2019s Improper Accounting Research Paper\n\nAs a company producing computer servers, Super Micro was found guilty of improper financial accounting by the securities and the exchange commission. In the FY 2015 through to 2017, the company engaged in fraudulent deals breaching a number of sections of securities act of 1933. The company prematurely recognized revenue and deliberately misstated financial statements amounting to grievous violation of the Securities Exchange Act of 1934.\n\nPoor internal control was widely witnessed as employees were pushed to maximize revenue and minimize expenses. Goods were delivered to the customers before due dates as stipulated in the terms and in the process undermining generally accepted accounting principles (GAAP) (Larkin and DiTommaso 391). Through its employees, the company illegally changed shipment terms, delivered incomplete goods, and held the bill of lading abusing the rights of its carriers. Due to these gross violations, the security and exchange commission delivered cease-and-desist injunction barring the company from conducting some of its activities. Although the super micro has been fined $ 19.5 million, a detailed audit should be carried out to ascertain the extent of fraud.\n\nBefore commencing the audit process, auditor should first understand the client, the kind of business being conducted, and the accounting process. An audit company needs to draw a comprehensive strategy audit plan that would be used in the whole process. The accuracy of accounting system including internal control structure being used by the company in question should be verified. The nature of audit to be employed must be decided on earlier enough.\n\nGenerally, auditor is simply an expert of financial matters. However, when faced with complex scenarios like in the case of Super Micro which deals with computer technologies, the auditor may require services of a specialist. According to auditing standards (AS 1210) of the PCAOB, a specialist is an individual with special skills and knowledge in a given field (Krishnan et al. 153). To obtain correct evidential matter from the substantive tests while evaluating the books and technological property of Super Micro, the help of a specialist would be of great significance.\n\nIn the case of Super Micro, it would not be appropriate to design tests on the financial statements assertions provided by the company. The firm has inadequate internal control system and fundamental financial reporting is not done. This can be supported by the fact that it presented wrong financial statements on net sales to the securities and the exchange commission, and was forced to alter them. The company does not keep proper financial records to avoid backlash from its employees who have in some occasions tried to expose some of its dealings. The risks of material mistreatments are higher since a number of its employees engage in fraudulent acts by threatening customers and carriers through personal emails to accomplish company demands. Therefore, the auditing team should carry out its own substantive audit tests to help in exposing the fraud being practiced in the firm.\n\nThe internal control tests that should be adopted include inspection of all the documents such as payrolls, deliveries, and invoices. Approvals in terms of signatures, stamps, and check marks should be inspected to assess any illegal dealings. Observations of internal controls that are used by the firm and their effectiveness must be made. According to PCAOB, transactions should be traced from the origin to their inclusion in the financial statements (DeFond et al. 593). The control element on the business should be put into perspective. This is due to the fact that in some cases recognition of revenue is done upon shipment of goods without customer authorization, prior to delivery, before obtaining customer acceptance, and sometimes with incomplete or damaged shipments.\n\nCommunication records are some of the pieces of evidence that can be used to detect any illegal practices in an organization. To evaluate such, direct communications between third party and Super Micro should be assessed. This can be done through designing confirmation requests and sending them to the other stakeholders involved in the business. The main aim would be to obtain information about particular items affecting financial assertions. According to AICPA auditing standards, upon establishment of confirmations, they can be sent to the third parties and feedbacks evaluated (Swieringa 128). If evidence obtained is inadequate, more confirmations can be sent.\n\nIn the case of Super Micro, validations can be sent to the carriers and customers whose rights have been infringed by the firm through illegal financial practices. Shipment processes between the company and some of its customers were marred with irregularities as they could in some occasions be threatened by employees through emails to accept goods when they do not need them. A number of goods could be hoarded in carriers\u2019 stores against their will. Such information can be obtained through sending confirmations.\n\nComparison of account estimates would prove crucial to ascertain all evidential matters are material to financial statements developed by the firm. It would also ensure approximations are reasonable and records of the company\u2019s business operations are true. It would also help to establish if all estimations conform to the GAAP and Securities Exchange principles. Changes of principles in the industry are always disregarded by the organizations in the business.\n\nAs such, their methods of operation should be evaluated to ensure they conform to the new accounting pronouncements. According to AS 2110, an accounting estimate should be conducted to ensure new entries in the industry adhere to the operating strategies (Acito et al. 15). The comparison of industry\u2019s estimates and super micro\u2019s would be significant to ensure new entries which are conducting business with the firm are following the rules. Pursuant to AS 2505, comparisons should be done to check any form of litigation claims. Such information would enable the auditing team to assess damages or infringements by the firm.\n\nLastly, to ensure all the Super Micro\u2019s financial statements and supporting documents are valid, complete, accurate, and without errors, some substantive tastings must be conducted. Confirmations should be sent to the banks requesting accounting records of end cash balances. Accounts receivable balances for the costumers must be verified to ensure they are correct. Super Micro in most cases failed to update its inventory records; therefore, it would be suitable to evaluate the period-end physical inventory count. A specialist should verify if the values of all the assets are reasonable and conform to the market prices. The records of fixed assets should be substantiated to check if they match physical possessions. Loan balances should be ascertained by the lenders as well as approved dividends.\n\nIn conclusion, the fraudulent acts committed by super micro are serious and need detailed auditing to establish their full extent. A specialist familiar with technological assets should be engaged to help the auditing team in the investigations. Confirmations should be sent to the third parties conducting business with the firm to obtain more information. The firm has no adequate internal control systems as records of financial statements are not sufficient. On one occasion, it gave wrong net sales records to the securities and the exchange commission. Therefore, auditing team should conduct their own substantive testing.\n\nWorks Cited\n\nAcito, Andrew A., Chris E. Hogan, and Richard D. Mergenthaler. \u201cThe Effects of PCAOB Inspections on Auditor-Client Relationships.\u201d The Accounting Review, vol. 93, no.2, 2018, pp. 1-35. Web.\n\nDeFond, Mark L., and Clive S. Lennox. \u201cDo PCAOB Inspections Improve the Quality of Internal Control Audits?\u201d Journal of Accounting Research , vol. 55, no.3, 2017, pp. 591-627. Web.\n\nKrishnan, Jagan, Jayanthi Krishnan, and Hakjoon Song. \u201cPCAOB International Inspections and Audit Quality.\u201d The Accounting Review , vol. 92, no. 5, 2017, pp. 143-166.\n\nLarkin, Richard F., and Marie DiTommaso. Wiley Not-for-profit GAAP 2018: Interpretation and Application of Generally Accepted Accounting Principles . John Wiley & Sons, 2018, pp. 382-413.\n\nSwieringa, Robert J. \u201cThe Early Years of the Financial Accounting Foundation and the Financial Accounting Standards Board, 1972 to 1980: The \u201cSpecial Relationship\u201d with the AICPA.\u201d Journal of Financial Reporting , vol. 3, no.1, 2018, pp. 127-130. Web.\n",
        "label": "human"
    },
    {
        "input": "Acme Corp.: Designing a Better Computer Mouse Case Study\n\nThe current case is taking place at Acme Corp., a modern technological company. At the moment, Acme Corp. is challenged with a task that it needs to complete within six months. Namely, the company seeks to design a new mouse to go with a personal computer, which in the case of success, would guarantee a competitive edge over contenders. The approach that the company is taking toward the early stages of the development process is to only include design engineers and brainstorm ideas. This essay analyzes the case and provides recommendations for improving the robustness of the process.\n\nThe Chief Design Engineer, Michael Caroll plays the role of the facilitator of the meeting: he controls its flow and steers the meeting in the right direction (Straus, 2017). Caroll keeps all the attendants focused on the task and prevents the conversation from digressing. Apart from that, the facilitator does not allow a single person or a group of people to talk over others. Everyone is encouraged to speak out as long as they stay on topic and make valuable contributions.\n\nA skilled facilitator uses a whole range of techniques to keep meetings productive, with some of them being observed in the current case. It is important to review desired outcomes and deliverables. In this case, all the meetings attendants will be sure of what is expected of them (Straus, 2017). One of the techniques used by Caroll is synthesizing the main themes: he accomplished it by categorizing the ideas collected during brainstorming. He assigned roles when he formed three groups for further discussion of the key ideas. A technique that was not present is balancing the meeting. Some members are typically more vocal than others, but those who are often dominated in meetings might still have good ideas (Watanabe et al., 2017). Thus, they should be given a voice and amplified by the facilitator.\n\nThe current meeting only consists of design engineers, which is less than ideal. On the one hand, it is intuitive to only include people whose responsibilities are directly related to the subject matter. Indeed, it is design engineers who will be primarily involved in the process of developing a new product. However, a meeting like the one that is described in the case ignores diverse perspectives on how to make a better mouse. Potentially, a product designer, a computer engineer, or someone from the sales team could chime in and provide their point of view. However, the biggest downside is the exclusion of future users: they are the people who will ultimately decide on the quality of the new product.\n\nThe relevant goal for subsequent meetings should be analyzing and understanding customer needs. Sure, a glove-like mouse or a chip could be genuinely innovative, but it is readily imaginable how an ordinary customer could be cautious of a new, unknown product. There may be two approaches to surveying customers: quantitative and qualitative (Camilleri, 2018). On the one hand, it is important to gather statistics on the market niche, previous sales, and their dynamics. The data do not have to be strictly numerical: it may as well be unstructured data such as text reviews. Their sentiment and topics could tell a lot about customers\u2019 preferences. On the other hand, only in-depth interviews can provide the team with insights into what customers want from a computer mouse.\n\nReferences\n\nCamilleri, M. A. (2018). Understanding customer needs and wants. In Travel marketing, tourism economics and the airline product (pp. 29-50). Springer, Cham.\n\nStraus, D. A. (2017). Managing meetings to build consensus. In Multi-Party Dispute Resolution, Democracy and Decision-Making (pp. 389-425). Routledge.\n\nWatanabe, E., Ozeki, T., & Kohama, T. (2017). Analysis of behaviors of participants in meetings. In International Conference on Interactive Collaborative Learning (pp. 427-438). Springer, Cham.\n",
        "label": "human"
    },
    {
        "input": "Apple Inc.\u2019s Competitive Advantages in Computer Industry Essay\n\nIntroduction\n\n  * Competitive advantage is significant in any company\n  * A prerequisite of success\n  * It enhances sustainable profit growth\n  * It shows the company\u2019s strengths\n  * Apple Inc. explores its core competencies to achieve it\n\nCase Summary\n\n  * Apple Inc. is led by Tim Cook\n  * The company was the first to hit the one trillion market cap\n  * The firm has introduced new products\n  * iPhone sales has grown five times under Tim Cook\n  * Hardware business has declined\n 1. Competitive Advantages of Apple Inc.\n\nUnique Differentiation\n\n  * iOS is different and unique\n  * Highly satisfied customers\n  * Image of Innovation\n      + Positive image (Liu, 2021)\n      + First mover in tech industry\n  * Premium Pricing Strategy\n      + Superiority (Yoffie & Fisher, 2020)\n      + Relatively high price\n  * Innovative Products\n      + Long product life\n      + Unique operating system\n      + High-quality (Mulumba, 2020)\n      + High-perceived value\n  * Consumer Trust\n      + High satisfaction\n      + Unique and sole producer\n  * Brand Value\n      + Most valued brand of all time\n      + Distinctively advantageous (Yoffie & Fisher, 2020)\n  * Advantage of Device Based Monetization\n      + Privacy stance\n      + Keeps user information private\n\nWhy did Apple struggle historically in PCs?\n\n 1. The first to create personal computers\n 2. Did not embrace personal computer mainstream\n 3. Three vendors have complete market domination\n 4. Major players cover different territories (Yoffie & Fisher, 2020)\n 5. Difficulty of new market entry\n 6. High price point of computers\n 7. High specificity of materials related to the devices (Yoffie & Fisher, 2020)\n 8. High cost to the consumer compared to competitors\n 9. Low sales volume (Mulumba, 2020)\n10. Limitations of operating system and software\n\nWhich New Areas Would You Invest Heavily to Grow Business?\n\n  * Software development\n  * Hardware development\n  * Service innovation (Mulumba, 2020)\n  * Affordable raw materials and technology\n\nConclusion\n\n  * To rise, Apple Inc. should\n  * Overcome technological disruptions\n  * Embrace technological change (Macado & Davim, 2018)\n  * Evaluate products before market launch\n  * Rely on technical expertise\n  * Commitment to best potential products\n\nReferences\n\nLiu, Z. (2021). Analysis of Apple Inc.\u2019s innovation dilemma from the perspective of leadership. Proceedings of the 6th Annual International Conference on Social Science and Contemporary Humanity Development (SSCHD 2020) . Web.\n\nMachado, C., & Davim, J. (2018). Enhancing competitive advantage with dynamic management and engineering . IGI Global.\n\nMulumba, B. K. (2020). Knowledge management: A crucial asset toward a sustainable competitive advantage: Apple Inc. and Samsung Ltd. Teaching examples. The International Journal of Business & Management , 8 (8), 21- 34. Web.\n\nYoffie, D. B., & Fisher, D. (2020). Apple Inc. in 2020 . Harvard Business School.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensic Incident Essay\n\nTable of Contents\n 1. Legal Statutes and Considerations\n 2. Steps and Procedures\n 3. Crime Scene Analysis\n 4. Sources\n\nLegal Statutes and Considerations\n\nFirst of all, it is worth noting that the legal right for self-defense does not cover cyberspace. This means that a victim of a hacker attack is not allowed to \u201chack back,\u201d in other words, attack the perpetrator (1). This determines the need for special legal statutes provide governmental regulation in the given sphere. Currently, all 50 states have the laws of such a kind that \u201cencompass a variety of actions that destroy or interfere with normal operation of a computer system\u201d (2). As much as anything else, cybercrime-related laws regularize evidence collection, which should involve the participation of electronic experts and not allow for any privacy right violation (3). Therefore, approaching the crime scene has to be based upon the existing legal considerations.\n\nSteps and Procedures\n\nWhen collecting digital evidence, it is critical to maintain data originality. This means that a forensic analysis should be a so-called clone, which stands for a copy of the original data collected from the device under analysis (3). After the submission of the evidence to the court, it has to be possible to explain any change from its initial state; ideally, there should not be any. Considering the latter fact, neither transfer process nor the analysis itself presupposes changing the data. Another essential point is the supervision of experts, under which only both evidence collection and investigation can happen. To avoid bias, not only the plaintiff should appoint experts, but other parties as well (3). Finally, in case any electronic data have been destroyed or deleted, it is essential to restore them accurately and without delay.\n\nCrime Scene Analysis\n\nThe given diagram represents the workplace of the one who is suspected in hacking. It bears several electronic devices that are to be seized for evidence, notably, a PC, a smartphone, an external media drive, a router, and a wireless access point. All of those may bear traces of the cybercrimes that have been committed by their owner from this location, which has been confirmed by the IP address.\n\nThe gadgets to use for accessing the Internet, in particular, the router, as well as the wireless access point, are important for checking whether the detail of the crime involves their MAC-addresses. It is necessary as well to review both the PC and the smartphone for malicious software that enables an access to logins and passwords of users, hence the data protected by them. Finally, data themselves are doubtlessly incontrovertible evidence, which determines the need for examining all present memory devices. In the given case, those are the PC, the smartphone, and the external media drive. In addition, it is essential to check whether the suspect is using cloud storages, which determines the need for examining all devices that allow for surfing the Internet, such as the PC and the smartphone.\n\nTo summarize, all 50 states today have legal statutes to enable governmental regulation in the sphere of cyber safety. Investigation on cybercrimes involves collecting electronic evidence, that is, data stored on or accessible from the devices that belong to a suspect along with software installed on them. All evidence should be collected in the presence of experts in order to avoid losing data as well as violating privacy rights.\n\nSources\n\n 1. N. Winstead. 2020. Hack-Back: Toward a Legal Framework for Cyber Self-Defense . Web.\n 2. National Conference of State Legislators (NCSL). 2020. Computer Crime Statutes . Web.\n 3. Ya. Wu et al. 2019. Research on Investigation and Evidence Collection of Cybercrime Cases. Journal of Physics: Conference Series, vol. 1176, no. 4. Web.",
        "label": "human"
    },
    {
        "input": "Computer Technology: Evolution and Developments Essay\n\nSummary\n\nThe development of computer technology is characterized by the change in the technology used in building the devices. The evolution of computer technology is divided into several generations, from mechanical devices, followed by analog devices, to the recent digital computers that now dominate the world. This paper examines the evolution of computers and their technology, their use in the early and modern periods, their merits and demerits, and future developments.\n\nEvolution of Computers and their Technology\n\nMechanical Age (1800s -1920s)\n\nThe development of the computer characterized this period to facilitate mathematical calculations that could not be done manually by individuals. The first notable computing device was the \u201canalytical engine\u201d designed by Charles Babbage in 1834, which used electromechanical relays to function (Zakari 1). The mechanical era saw improvements made to the first design by Babbage until the first generation era.\n\nFirst Generation (the 1930s-1950s)\n\nThe first generation era is characterized by the development of three electronic computers that used vacuum tubes, unlike the previous devices that used electromechanical relays to perform their tasks (Enzo 4). In this period, the machines were capable of storing data in the form of instructions written manually by the programmers and installed into the device (Zakari 1). The devices developed in this period were primarily used in applied science and engineering to facilitate solving evaluations.\n\nSecond Generation (Mid-1950s-Early 1960s)\n\nThe second-generation period saw the development of many design areas; there was development in the technology used and the programming language used to write the commands. Unlike in the previous generations, the operations in this era were performed in the hardware (McAfee 141). The period saw the development of the index registers used for numerous operations.\n\nThird Generation (the Early 1960s \u2013 Early1970s)\n\nThe era saw improvement in the technology used in designing the devices; integrated circuits in computer devices were introduced. The period saw the introduction of the microprogramming technique and the development of the operation system (Zakari, 1). The speed of functioning of the devices designed in this period was faster than in the previous eras, and the computers could perform more functions.\n\nFourth Generation (The early 1970s \u2013 Mid 1980s)\n\nThis Generation saw the development in the use of large-scale integration in the computers developed. The size of the microchips was the information for the computers was stored was reduced to allow for data to be stored in the same microchip (Zakari 1). The devices were installed with semiconductors memories to replace the core memories of the previous era. The processors were designed with high speed to allow faster processing speed of operations in the devices (McAfee 141).\n\nFifth Generation (the Mid 1980s- Early 1990s)\n\nThe machines/ devices designed had many processors that worked simultaneously on a single program (Zakari1). The semiconductors in the computers were improved to increase the scale of operation with the development of chips (Enzo 2). In this period, the computer devices developed were capable of performing parallel processing of commands. Which improved their functionality?\n\nSixth Generation (1990 to Date)\n\nThe era is characterized by improvements in all the areas of designing computers. There is a reduction in the size of the devices developed with increased portability of the machines. The era has seen the development of computers to interact more with people and facilitate human functions in society, with an increase in connection due to improved network development linking computers (Zachari 1).\n\nUses of Computers\n\nThe early computers were mainly used to accomplish mathematical functions in applied science and engineering. These machines were primarily used to solve mathematical calculation problems (Zakari 1). The second-generation devices improved on their functionality and were capable of processing information stored in them by the programmer (Zakari 1). Today, individuals use computers to perform various functions, including facilitating communication, storing data, and processing information for individuals. The use of computer technology is now in every section of the world; people in different areas are using computers to perform numerous functions (McAfee 141). The technology is directly applied in agriculture, health and medicine, education and transport, communication, and other regions.\n\nAdvantages of Computers and their Technology\n\nComputer technology has enabled the development of devices like mobile phones that are easy to use and effective, allowing individuals to keep in contact with one another even when at different locations (Golosova and Romanovs 3). Computer technology has improved manufacturing; producing goods is now better and more efficient due to the development of technology that enhances individuals\u2019 performance. Computer technology enhances the development of better healthcare operations by facilitating functions in health. Computer technology also enhances learning as individuals can get the required learning material (Golosova and Romanovs 6). Computers and computer technology improve teacher-student interaction during education by providing a medium that can facilitate lessons.\n\nDisadvantages of Computers and Computer Technology\n\nComputers are hazardous to human health; when used excessively, individuals suffer from health issues like eye problems resulting from extreme exposure to the screen light. Also, sitting for an extended period affects an individual\u2019s health (Golosova and Romanovs 14). Computers and computer technology are artificial, making them susceptible to human manipulation; humans are exposed to risks from those that can harm them by manipulating information (Suma 133). Computers also impact the environment negatively due to the carbon footprint left in the environment when they become obsolete because people can no longer use them.\n\nTrends in Computer Technology\n\nThere is an expected increase in the use of artificial intelligence among people with increased developments in computers and their technology (McAfee 141). Computer technology is expected to increase the automation of processes and functci0ons previously done by humans in society. Computer technology is expected to increase the virtual reality and augmented reality among individuals in society to improve the human experience.\n\nWorks Cited\n\nEnzo, Albert, Charles O. Connors, and Walter Curtis. \u201cThe Evolution of Computer Science.\u201d Computer Science, Murdoch University, Australia.\n\nMcAfee, Andrew. \u201cMastering the Three Worlds of Information Technology.\u201d Harvard Business Review. vol. 84, no. 11, 2006, p. 141. Web.\n\nSuma. V. \u201cComputer Vision for Humans-machines Interaction-review.\u201d Journal of Trends in Computer Science and Smart Technology ( TCSST ), vol. 1, no. 2, 2019, pp. 131-139. doi: 10.36548/jtcsst.2019.2.006\n\nGolosova, Julija, and Andrejs Romanovs. \u201cThe Advantages and Disadvantages of the Blockchain Technology.\u201d 2018 IEEE 6th Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE) . IEEE, 2018.\n\nZakari, Ishaq \u201cHistory of Computers and its Generations.\u201d Umaru Musa Yar\u2019adua University, Katsina State (2019).\n",
        "label": "human"
    },
    {
        "input": "Computer Science Courses Project Management Essay\n\nProposal Selection Process\n\nThe grant provided by the National Science Foundation titled Computer science for all aims at helping students in PreK-12 levels to participate in computer science (NSF, 2021). NSF\u2019s review process consists of four steps described in its official policy. First, Program Officers conduct a preliminary review to ensure completeness and conformance with NSF requirements (NSF, n.d.).\n\nThe officers then select peer reviewers for further analysis. Second, the selected independent reviewers analyze the proposal according to the set criteria and submit the information to the NSF (NSF, n.d.). Third, Program Officers assess the feedback from peer reviewers to check for accuracy (NSF, n.d.). Finally, the division board issues the decision about the proposal. At all stages, the proposal is evaluated in terms of intellectual merit and broader impact (NSF, n.d.). This implies that all projects should be of the highest quality, based on appropriate metrics, and achieve broad social goals (NSF, n.d.). The proposals should also be creative, advance knowledge, benefit society, have a clear implementation plan, and be financially feasible (NSF, n.d.).\n\nThe process is associated with low bias, as it promotes the review of different stakeholders, including outside peer reviewers. Additionally, the process ensures that the proposal aligns with NSF\u2019s goals. However, the process is very long, which may have a negative impact on social change. The information provided by NSF (n.d.) helped me to focus my report on four things: promotion of social change and knowledge, creativity, financial feasibility, and careful planning, as these four things are crucial for the proposal to succeed.\n\nMethodology and Evaluation Plans\n\nProgram Description\n\nThe project proposes to create computer science courses for PreK-4 students in an elementary school. First, the project includes creating relevant course materials for the youngest students through collaboration with the most talented and experienced teachers and policymakers in the school country. Second, the project requires creating a special classroom with the latest equipment, including top-quality personal computers, video and audio recording equipment, network essentials, virtual reality, and presentation equipment. Third, the project supposes that essential training will be provided to teachers in the school to help students learn computer science.\n\nThe project is crucial for the school and the community, as students currently do not have the opportunity to learn computer science appropriately. Even though there are computer classes for elementary school students, the teachers are undertrained, and there are not enough computers to provide all the students with an adequate level of education. Moreover, the computers in the school are outdated, which makes them difficult to use. Implementation of the proposed project can help to improve the prestige of the school country and help the students receive an appropriate education.\n\nThe project was driven by the idea that everyone should have the ability to learn computer science starting from pre-school. The purpose of the project is to increase the level of computer literacy in elementary schools. The initiative is a powerful addition to the current way of improving computer literacy in the community, as it provides the opportunity for children to enroll in a computer science club, which can improve their level of satisfaction with the process of learning and improve the outcomes.\n\nTimeline\n\nThe project is expected to start in July 2021 from the preparation phase. During this phase, the personnel will be trained and all the equipment prepared for the program start. The Computer Science club will be running for an entire academic year; thus, the second stage will start in August 2021 and continue until June 2022. The second stage will be divided into three substages, as a preliminary evaluation of results will be conducted in January 2022. The third stage will start in June 2022, which will be associated with the evaluation of the results and working on possible improvements. The timeline is visualized in Figure 1 below.\n\nFigure 1. Project timeline\n\nRequired Resources\n\nThe project does not require large amounts of resources, as the club will be based in an elementary school, which has all the required resources. The only non-human resources required are new computers and appropriate software for the computer science course. The human resources that will be needed are the program coordinator, who will manage the project, schedule the classes, and evaluate the outcomes, and two teachers. The coordinator will also play a role of a teacher when needed. All the staff members will require training. It is expected that an outside training program will be utilized to raise the level of staff\u2019s competence as a Computer Science teacher for K-4 students.\n\nManagement Plan\n\nThe project will be managed by the project coordinator, who will report to the school\u2019s principal. The coordinator will create schedules for the club at the beginning of every week and control that the teachers provide proper education during the classes. The classes will be provided on a daily basis. The overall number of classes depends upon the number of groups interested in Computer science that will be created. It is expected that there will be between 10 and 15 classes a week provided. The coordinator will run all the communications within the project, make written reports about the progress of the project once a month, and conduct evaluations twice during the project: in January and in June.\n\nEvaluation Research Design, Sample, Reliability, and Validity\n\nOne of the most frequently used research designs in evaluation programs is the pre-experimental design (Arora et al., 2019). This approach implies gathering pretest and post-test data utilizing the statistical analysis to understand if the intervention had a statistically significant effect on the dependent variable. The purpose of the project is to increase the level of interest and knowledge about Computer Science among Granby Elementary students. Thus, the levels of knowledge and interest of 100 students will be measured before and after the implementation of the program. Statistical analysis will be performed to understand if the program had a significant impact on the dependent variables. A statistically significant improvement with a large enough effect size will demonstrate the success of the program.\n\nThe proposed research design is associated with significant benefits, such as simplicity and low cost, which can be crucial for researchers with a small budget and limited experience. However, the research design is associated with significant drawbacks. For instance, the design does not control for many extraneous factors, which are associated with significant threats to validity, as it is difficult to dismiss a rival hypothesis (Evaluation Toolkit, n.d.). As for the reliability of the method, pre-experimental design is not associated with special threats.\n\nData and Procedures of Evaluation\n\nThe evaluations will require the collection of data concerning two variables, including the level of computer science knowledge and interest in computer science. The data will be collected from 100 students using in-class surveys developed to measure the level of computer science knowledge and interest in computer science. The surveys will be conducted three times, at the beginning of the program (August 2021), at the midpoint evaluation (January 2022), and during the final evaluation (June 2022). The instruments for evaluation will be developed specifically for every age group, as they need to consider the reading and writing abilities of elementary school children.\n\nBudget Template\n\nTable 1. Budget\n\nName            Title        Hours a week  Salary   Fringe benefits  Grand total\nName #1         Coordinator  10            $10,000  $1,665           $11,665    \nName #2         Teacher      10            $8,000   $1,332           $9,332     \nName #3         Teacher      10            $8,000   $1,332           $9,332     \nSubtotal:       $30,329    \nStaff Training  $6,000     \nComputers       $8,000     \nSoftware        $4,800     \nGrand Total:    $49,129    \n\n\nBudget Narrative\n\nSalaries\n\n(Total: $26,000)\n\nThe coordinator will spend 100% of his time controlling the quality of the provided services and providing classes to students 10 hours a week (2 hours per day after finishing the classes). The total requested salary is $10,000 per annum.\n\nThe teachers will spend 100% of their time giving classes to students 10 hours a week (2 hours per day after finishing the classes). The total requested salary is $8,000 per annum.\n\nFringes\n\n(Total: $4,329)\n\nFICA will be paid for all salaries: $26,000 x.0765 = $1,989.\n\nUnemployment cost is $26,000 x.03 = $780.\n\nRetirement for full-time employees: $26,000 x.06 =$1,560.\n\nStaff Training\n\n(Total: $6,000)\n\nTraining will be provided for the teachers and the coordinator. The estimated cost of training is $2,000 per person.\n\nComputers and software\n\n(Total: $12,800)\n\nThe estimated cost of every computer is $1,000, with $600 additional spent on software. The program is expected to need eight computers in total.\n\nContingency Plan\n\nWhile there are few risks associated with the program, there are certain costs that should be considered for contingency purposes. The primary risk that may be associated with the additional cost is increased demand for classes from students and parents. As a result, it may be needed to recruit another teacher, which may be associated with another $9,332. Moreover, additional supplies may be needed in case the school will not be able to provide paper, pens, pencils, crayons, and printer ink. Thus, an additional $600 should be prepared in case of supply shortages. The two risks mentioned above are the only parts of the contingency plan for the present project.\n\nReferences\n\nArora, S., Deosthali, P. B., & Rege, S. (2019). Effectiveness of a counseling intervention implemented in antenatal setting for pregnant women facing domestic violence: a pre\u2010experimental study. BJOG: An International Journal of Obstetrics & Gynaecology , 126 , 50-57.\n\nEvaluation Toolkit. (n.d.). Choose an evaluation design. Web.\n\nNational Science Foundation. (2021). Computer science for all. Grants.gov. Web.\n\nNational Science Foundation. (n.d.). Phase II: Proposal review and processing. Web.\n",
        "label": "human"
    },
    {
        "input": "How Computer Works? Presentation\n\nWhat is a Computer?\n\nThese are programmable electronic devices or machines that are designed to accept and perform logical or mathematical operations at a high speed. In other words, computers are multipurpose machines used in the processing of data according to a set of given instructions.\n\nWhat does a Computer do?\n\nEven though computers are used to perform a variety of tasks that are of concern to the modern technological era, they are specifically used in the analysis and processing of data (Igbaria, Iivari & Maragahh, 1995).\n\nInput-processing-Output\n\nThis refers to the basis for computer operations. In other words, input process output, well referred to as IPO is a brief model used to describe or explain the process of computer operation. Computers are programmable and cannot think or operate by themselves without the manipulation of humans.\n\nIn order for a computer to function, stuff such as data or programs have to be put through the necessary hardware, where they would be processed to produce the required output. This output could be in various forms and shapes.\n\nIn this regard, input simply refers to the process of entering information into a computer and sending it into the central processing unit for final operation. Output, on the other hand, refers to the process of acquiring the already processed data from the computer CPU.\n\nThe following links gives further insight on the input process output model of a computer:\n\n  * What Is CPU Usage on a Computer?\n  * Input Process Output. What is this? Can you eat it?\n\nComputer Hardware\n\nComputer hardware are the physical components or elements that comprise computers, especially those parts that can be seen and touched. Even though there may be different hardware for various computer models, all computers will probably have the same components.\n\nSome of the common computer hardware would include things such as hard drives and disks, Random Access Memory (RAM), removable drives, the mouse, and the keyboard. Other important hardware include monitor and the system unit. With the help of computer software, each of these components plays a key role in the operation of computers (Englander & Englander, 2003).\n\nThe following images illustrates both internal and external computer hardware.\n\nComputer Software\n\nComputer software refers to a set of instructions or programs that instruct computers what to do. Unlike the hardware which comprises of physical components, computer software are not tangible. Software are crucial components to computers for they instruct various hardware elements on what to do and how to coordinate with other hardware units. Another importance of the software is that it enables the hardware to be accepted by the computer, thus making them compatible to computers. Common computer software include things such as the operating system (OS), applications, and word processing programs, among others.\n\nThe Keyboard\n\nThis is an input device which acts exactly like a typewriter, and is used to enter data into a computer by typing letters, numbers and symbols on the pad.\n\nHowever, unlike the typewriter, computer keyboards are made into special keys such as Control (Ctrl), Alternate (ALT), and Escape (Esc). In addition, there is also a numeric keypad which serves as an alternate to the numbers provided across the top of the pad.\n\nThese special features makes the keyboard a very useful input component to computers.\n\nFollowing is an image of a modern day computer keyboard.\n\nThe Mouse\n\nAs the name suggests, this is a mouse-like device which is normally operated by hand in controlling the movement of a cursor on the computer screen. This electronic device is used to execute commands to computer programs, and can be used to all types of computers.\n\nMonitors\n\nMonitors or screens are the computer display devices that electronically display images, text and other forms of communication.\n\nMost computer actions are displayed through this device as they are processed by the machine.\n\nPrinters\n\nThese are peripheral devices or machines that can be attached to computers to produce physical media that represents electronic media such as text documents within the computer.See the following link for more about printers .\n\nThe Internet\n\n  * Internet refers to the international computer network that links millions of computers across the world.\n  * As it would be observed, internet is arguably the most useful interactive development that has ever happened to the computer technology (Lee, 2009).\n  * The dawn of the internet back in the 1960s has improved the use of computers, thus making life easier and more enjoyable for humans.\n  * To a further extent, the use of internet on computers has effectively encouraged and facilitated various forms of human interaction, thus making computers more useful in the current world than ever.\n\nSearching Skill\n\nComputers are known for their high memorising ability, among other important aspects that make them useful to the modern society. The high searching skills of the machines, makes them even more useful when it comes to data analysis and processing.\n\nWhen connected to the internet, computers can perform extraordinary tasks by enabling people to search, access and share useful information.\n\nReferences\n\nEnglander, I., & Englander, A. (2003). The architecture of computer hardware and systems software: An information technology approach . New York: Wiley Publishers.\n\nIgbaria, M., Iivari, J., & Maragahh, H. (1995). Why do individuals use computer technology? A Finnish case study. Information & Management, 29 (5), 227-238.\n\nLee, S. (2009). Online Communication and Adolescent Social Ties: Who benefits more from Internet use? Journal of Computer-Mediated Communication, 14 (3), 509-531.\n",
        "label": "human"
    },
    {
        "input": "The Computer Science Club Project\u2019s Budget Planning Essay\n\nBudget DQ\n\nThe project aims at organizing a small-scale computer science club in Granby Elementary school. The school\u2019s computers are rather old, and there is no computer science program running. The students have only computer literacy classes using outdated software and hardware. The program includes developing a special computer science course for anyone who is willing to join and purchasing new computers and software for the computers. The project also requires a coordinator and two teachers who will help to run the program. The budget for the program is provided in Table 1 below.\n\nTable 1. Budget\n\nName            Title        Hours a week  Salary   Fringe benefits  Grand total\nName #1         Coordinator  10            $10,000  $1,665           $11,665    \nName #2         Teacher      10            $8,000   $1,332           $9,332     \nName #3         Teacher      10            $8,000   $1,332           $9,332     \nSubtotal:       $30,329    \nStaff Training  $6,000     \nComputers       $8,000     \nSoftware        $4,800     \nGrand Total:    $49,129    \n\n\nThe narrative for the budget is provided below:\n\nSalaries\n\n(Total: $26,000)\n\nThe coordinator will spend 100% of his time controlling the quality of the provided services and provided classes to students 10 hours a week (2 hours per day after finishing the classes). The total requested salary is $10,000 per annum.\n\nThe teachers will spend 100% of their time giving classes to students 10 hours a week (2 hours per day after finishing the classes). The total requested salary is $8,000 per annum.\n\nFringes\n\n(Total: $4,329)\n\nFICA will be paid for all salaries: $26,000 x.0765 = $1,989.\n\nUnemployment cost is $26,000 x.03 = $780.\n\nRetirement for full-time employees: $26,000 x.06 =$1,560.\n\nStaff Training\n\n(Total: $6,000)\n\nTraining will be provided for the teachers and the coordinator. The estimated cost of training is $2,000 per person.\n\nComputers and software\n\n(Total: 12,800)\n\nThe estimated cost of every computer is $1,000, with $600 additional spent on software. The program is expected to need eight computers in total.\n\nNeeds Statement\n\nGranby Elementary school needs to establish a computer science program for PreK-4 students to increase the level of involvement in computer science and improve computer literacy. Granby Elementary School, located in Norfolk, VA, was the educational site for this study. The mission of Granby Elementary School is to \u201censure that all families and students are engaged in purposeful learning, students are using metacognition and comprehension strategies across all curriculum areas to ensure life-long learning\u201d (Granby Elementary School, n.d., para. 1). The school currently has 581 students, among which 59.2% are African American, 22.7% are white, and 8.1% are Hispanic (Virginia Department of Education [VDoE], 2020). Almost 97% of the students qualify for free or discounted lunch, which demonstrates that the majority of students are from financially disadvantaged families (VDoE, 2020). The students in the school do not have the opportunity to learn computer science at an appropriate level since many families do not have enough money to afford a computer. Moreover, the computers at school are only 12 in the count, and all of them were purchased more than ten years ago.\n\nThe importance of computer science in elementary schools is difficult to overstate. Prottsman (2014) claims that teachers see increased benefits in introducing computer science classes in elementary schools. Computers can be used for a wide variety of activities, including self-education (Prottsman, 2014). Thus, it is crucial for students to become familiar with computer science as early as possible. After conducting longitudinal research, Weintrop et al. (2018) concluded that elementary school students could benefit from learning basic programming skills. Currently, Granby Elementary does not have enough modern computers to provide computer science classes. Moreover, teachers do not have the required knowledge and skills to provide the much-needed computer science education.\n\nThe analysis of needs demonstrated that Granby Elementary needs to establish a comprehensive computer science program for its students. The name of the program was chosen to be \u201cComputer Science for All.\u201d The program should include after-school classes for small groups of children interested in computer science. This can be achieved by organizing a small computer science club open to any student of Granby Elementary. The project requires funds for two major aspects, which are reflected in the goals and objectives provided below. In particular, a comprehensive course for all knowledge levels needs to be developed for the club, teachers need to be trained, and new computers need to be purchased. In particular, three teachers need to be trained, and at least six computers need to be purchased.\n\nIt was decided to apply for the grant of the National Science Foundation (2021), as the aims of the organization are similar to those of the project. The program conducted by National Science Foundation (2021) aims to provide all U.S. students with the opportunity to participate in computer science and computational thinking education in their schools at the PreK-12 levels. The goals and objectives of the program in Granby elementary listed below demonstrate how they align with the aims of the National Science Foundation.\n\nGoals and Objectives\n\nGoal #1: Improve the level of computer science competence among teachers in Granby Elementary through training for the proposed Computer Science for All project.\n\n  * Objective 1A: By August 2021, develop a training program for teachers that can help to improve the current level of knowledge about computer literacy by 10% to 20% among involved students.\n  * Objective 1B: By September 2021, have three teachers complete the developed training course.\n  * Objective 1C: By September 10 th , 2021, assess the skill level of all the teachers involved in the program.\n  * Measure 1A: The program is developed and assessed by a board of experts in Norfolk County, VA. The approval rate of at least 90% of the board of experts will be considered a success.\n  * Measure 1B: Three teachers from Granby Elementary receive a certificate of completion of the course.\n  * Measure 1C: The teachers show a level of knowledge of the material of at least 85% demonstrated during the formal assessment procedure.\n\nGoal #2: Increase the number of modern computers in Granby Elementary for the proposed Computer Science for All project.\n\n  * Objective 2A: By July 2021, select all the needed computers and software needed to run the program.\n  * Objective 2B: By August 2021, acquire sufficient funding to buy six to eight new computers and software needed at Granby Elementary.\n  * Objective 2C: By September 2021, buy and set up six to eight new computers and the needed software for Granby Elementary to start the program.\n  * Measure 2A: A list of computer specifications and needed software is developed with a rationale.\n  * Measure 2B: The required fund are transferred to the school\u2019s account.\n  * Measure 2C: The computers are purchased and set up in the school with all the required software installed and ready for utilization.\n\nReferences\n\nGranby Elementary School. (n.d.). School history. Web.\n\nNational Science Foundation. (2021). Computer science for all. Web.\n\nProttsman, K. (2014). Computer science for the elementary classroom. ACM Inroads , 5 (4), 60-63.\n\nVirginia Department of Education. (2020). Granby Elementary. Web.\n\nWeintrop, D., Hansen, A., Harlow, D., & Franklin, D. (2018). Bringing computer science into elementary school classrooms. American Educational Research Association . Web.\n",
        "label": "human"
    },
    {
        "input": "Using Computerized Provider Order Entry and Clinical Decision Support System Essay\n\nComputerized provider order entry (CPOE) is an integral part of healthcare, greatly improving care provision. It is implemented to reduce prescription errors, which is a prevalent issue, and make health records more accessible, as patients may complain about undecipherable handwriting (V\u00e9lez-D\u00edaz-Pallar\u00e9s et al., 2017). CPOE is usually integrated with a clinical decision support system (CDSS), which offers the necessary supporting information (V\u00e9lez-D\u00edaz-Pallar\u00e9s et al., 2017). They contribute to care safety and cost reduction, although humans are still responsible for supplying accurate and relevant information on conditions, such as stroke, and their treatment (V\u00e9lez-D\u00edaz-Pallar\u00e9s et al., 2017). A sound design is essential for proper implementation and error prevention, as medicals ones are replaced by technical glitches.\n\nCPOE Used to Design CDSS\n\nA typical CPOE system will be employed for CDSS integration. Its interface will include a patient\u2019s chart, an order list, which allows one to plan tasks, follow their progress, and see the results, and a trend tab showing one\u2019s clinical data and important developments. The system will enable a healthcare employee to place orders, connecting to pharmacies. Direct input will be supported, meaning that it is possible to correct the existing data or add new entries without a third party. Lastly, a mobile version will be available for emergency cases and those preferring to work there.\n\nProposed CDSS\n\nThe CDSS will provide clinical information, patient data, and other relevant knowledge for a particular case. It will be connected to a database, matching an initial profile with the general hub, and offer suggestions, assessments, and interventions, including medicine (Bezemer et al., 2019). Based on the provided information, a clinician will be able to make balanced decisions (Zikos & DeLellis, 2018). Such convenience is achieved by combining and reusing data and imputing new feedback through the CPOE system, which is relevant for follow-up visits with additional tests (Zikos & DeLellis, 2018). Still, one is expected to apply critical thinking to carefully analyze the information and select the right approach (Zikos & DeLellis, 2018). The system will implement essential guidelines and offer various forms, including reminders (Zikos & DeLellis, 2018). They can be used to suggest cheaper drug alternatives and notify employees of important clinical events for a patient, prompting the staff to contact them in emergencies (Zikos & DeLellis, 2018). Overall, the CDSS will address multiple goals, one of which is connected with e-prescription.\n\nProposed CDSS Links for a Specific Medication\n\nNon\u2013vitamin K antagonist oral anticoagulants are gaining traction in stroke treatment due to having fewer side effects. However, the CDSS may not suggest them due to the database being flawed. Thus, a clinician should manually add rivaroxaban through the CPOE system and place an order on it. The process is especially important if the side effects of more traditional medications, for example, warfarin, put a patient at an increased risk (Yao et al., 2016). Consequently, a care receiver will be able to access the best available treatment due to the system removing most of the barriers to it.\n\nDetails of Clinical Issue\n\nThe clinical issue at hand is stroke, which is responsible for many deaths and disability cases. The focus will be on the ischemic one, which is associated with infarction of the brain caused by arterial occlusion (Campbell et al., 2019). It is rather prevalent and dependent on such factors as sex, age, and certain diseases, with hypertension considerably increasing its risk (Campbell et al., 2019). Stroke has many potential causes, which determine the treatment; for instance, atrial fibrillation would require the use of anticoagulants, slowing blood clot formation (Campbell et al., 2019). Warfarin is usually suggested for such a purpose, although patients may struggle with adherence (Yao et al., 2016). Newer anticoagulants, including rivaroxaban, address the problem and the cause, especially atrial fibrillation (Yao et al., 2016). Overall, despite the issue being prevalent, more advanced types of treatment exist to prevent it and serve as a post-event alleviation measure.\n\nRationale\n\nThe integration of CPOE and the CDSS allows a clinician to resolve several tasks simultaneously. They include diagnosis prediction, concrete recommendations, physiological measurements observation, and medication suggestion and procurement (Zikos & DeLellis, 2018). The systems improve patient outcomes due to reducing medication errors by implementing dosage and other safeguards, setting reminders for both parties, optimizing clinical management, and saving costs (Zikos & DeLellis, 2018). For example, the CDSS will be able to predict the type of stroke from which an individual suffered, suggest a potential cause, and demonstrate the history of blood pressure measurements and other relevant values. The analysis will be followed by the most appropriate and cheapest medicine, allowing a clinician to place an order via the CPOE (V\u00e9lez-D\u00edaz-Pallar\u00e9s et al., 2017). The process will be less time-consuming and overcome the barriers of unavailability, medical errors, and the need to have additional visits, although they might be necessary. Altogether, the integrated CDSS benefits both the patient and the healthcare employee and helps combat such major diseases as stroke.\n\nImplementation of CDSS\n\nThe implementation of the system may take a considerable amount of time considering its scope. It will require a collaboration of the staff, IT specialists, and pharmacists (Bezemer et al., 2019). Those responsible for the technical side will develop a database, link it to electronic health records, and install the necessary software on personal computers, which will be integrated with the other two. Then, the system will have to be adjusted to connect to pharmacies that should have similar programs to receive orders (Bezemer et al., 2019). Cybersecurity specialists will monitor the CDSS to ensure that all information is encrypted and only reaches the involved personnel. Lastly, the medical personnel will be responsible for the database\u2019s accuracy, relevance, and scope, all of which directly impact patient outcomes. Generally, the process is challenging but fruitful in the long run.\n\nMeasured Outcomes\n\nSeveral values will be relevant for determining whether the system is successful and addresses the persisting issues. One of them is patient outcomes, which should improve after the implementation; here, the target will be stroke cases. The medication error statistics are also likely to decrease, although the system cannot eliminate all potential causes. Medical costs deserve to be monitored, both the budget expenses and patient bills, which might see a reduction. Thus, positive clinical outcomes should increase, while errors and charges will ideally follow the opposite direction.\n\nChallenges/Solutions\n\nA potential challenge is the rise of technical errors, replacing medical ones. However, they are no less dangerous, impacting the overall network and affecting the ordering process (V\u00e9lez-D\u00edaz-Pallar\u00e9s et al., 2017). Moreover, wrong suggestions might appear, although an experienced specialist should be able to recognize them (V\u00e9lez-D\u00edaz-Pallar\u00e9s et al., 2017). Besides collaborating with technical support to fix the errors, one has more traditional ways of communication and a patient in front of them to be inspected manually.\n\nConclusion\n\nA CDSS, in integration with the CPOE system, is a viable tool for providing care and address a patient\u2019s needs. Their proper implementation eliminates numerous issues plaguing healthcare, making treatment more accessible and effective. Such diseases as stroke are no longer a significant threat, as one use of the CDSS can reveal all the relevant aspects and suggest the best therapy. Still, a clinician should not be too reliant on the system and remember more traditional methods.\n\nReferences\n\nBezemer, T., De Groot, M. C., Blasse, E., Ten Berg, M. J., Kappen, T. H., Bredenoord, A. L., van Solinge W. W., Hoefer, I. E., & Haitjema, S. (2019). A human (e) factor in clinical decision support systems. Journal of Medical Internet Research, 21 (3), e11732. Web.\n\nCampbell, B. C. V., De Silva, D. A., Macleod, M. R., Coutts, S. B., Schwamm, L. H., Davis, S. M., & Donnan, G. A. (2019). Ischaemic stroke. Nature Reviews Disease Primers, 5 (1), 70. Web.\n\nV\u00e9lez-D\u00edaz-Pallar\u00e9s, M., \u00c1lvarez D\u00edaz, A. M., Gramage Caro, T., Vicente Oliveros, N., Delgado-Silveira, E., Mu\u00f1oz Garc\u00eda, M., Cruz-Jentoft, A. J., & Bermejo-Vicedo, T. (2017). Technology-induced errors associated with computerized provider order entry software for older patients. International Journal of Clinical Pharmacy, 39 (4), 729\u2013742. Web.\n\nYao, X., Abraham, N. S., Alexander, G. C., Crown, W., Montori, V. M., Sangaralingham, L. R., Gersh, B. J., Shah, N. D., & Noseworthy, P. A. (2016). Effect of adherence to oral anticoagulants on risk of stroke and major bleeding among patients with atrial fibrillation. Journal of the American Heart Association, 5 (2), e003074. Web.\n\nZikos, D., & DeLellis, N. (2018). CDSS-RM: A clinical decision support system reference model. BMC Medical Research Methodology, 18 (1), 137. Web.\n",
        "label": "human"
    },
    {
        "input": "Human-Computer Interface in Nursing Practice Essay\n\nThe means through which individuals interact with computational technologies define human-computer interface (HCI) (Bologva et al., 2016). Different types of software and hardware play an instrumental role in facilitating and supporting the interaction. Examples of HCI include Electronic Health Records (EHR), drug administration system, electronic thermometers, and defibrillator. HCI in the healthcare impacts the quality of the care and patients\u2019 safety since it influences communication among care providers and between the latter and their clients.\n\nHCI improves accessibility for patients\u2019 health information and delivery of care services. For instance, nurses use EHR to faster get such information as lab data, care providers, progress notes, problems, and medications related to their clients. EHR enhances the care given by healthcare professionals since it facilitates recording of accurate and clear medical information (Schenk et al., 2018). Equally, telehealth allows nurses to frequently and effectively communicate with patients when at home to monitor their progress and vital signs or supervise wound care. Indeed, telehealth and EHR reduce the pressure experienced by nurses when more patients visit a hospital and manual searching of health records.\n\nHCI minimizes the chances of making errors when providing care to the patients. Nurses who work in understaffed healthcare facilities or for long hours have greater risks of making mistakes. However, new technologies installed in hospitals simplify routine procedures, decreasing possibility of human inaccuracies. For example, drug administration systems can automatically measure patient\u2019s medication dosage. Also, EHR lessens possible faults at the bedside since patient\u2019s information is readily available (Schenk et al., 2018). Nevertheless, HCI can negatively impact nursing practice, risking patients\u2019 safety. For instance, the suggestive feature in the EHR systems can lead to wrong entry of patient\u2019s information or prescription of wrong medication and dosage.\n\nReferences\n\nBologva, E., Prokusheva, D., Krikunov, A., Zvartau, N., & Kovalchuk, S. (2016). Human-computer interaction in electronic medical records: from the perspectives of physicians and data scientists. Procedia Computer Science , 100 , 915-920.\n\nSchenk, E., Schleyer, R., Jones, C., Fincham, S., Daratha, K., & Monsen, K. (2018). Impact of adoption of a comprehensive electronic health record on nursing work and caring efficacy . CIN: Computers, Informatics, Nursing , 36 (7), 331-339.\n",
        "label": "human"
    },
    {
        "input": "How Computer Based Training Can Help Teachers Learn New Teaching and Training Methods Presentation\n\nInformation Communication Technology\n\nICT:\n\n  * Administration;\n  * Government;\n  * Business;\n  * Education.\n\nThe introduction of technology in various aspects of the day to day activities in administration, government, business, and education, among other fields has brought about both new opportunities and challenges in the twenty first century.\n\nUse of information and communication technologies in educational practices\n\nBenefits\n\n  * Flexible learning processes;\n  * Interactive bi-directional communication systems.\n\nChallenges\n\n  * Acquiring the necessary skills in the implementation of IT products;\n  * Finding Appropriate tools to convey ICT to learners.\n\nAccording to Mikre (2011), the new teaching modalities require trainers to change their attitudes towards the teaching paradigms in order to take up the new educational concepts that incorporate flexible learning processes, and interactive bi-directional communication systems. One of the key benefits of using ICT in education is the ability to interact and collaborate with other participants on a global platform in both real and asynchronous time.\n\nAccording to Sansanwal (2009), the challenges faced by teachers in transforming from the traditional teaching and learning paradigm are the acquisition of necessary skills in the implementation of IT (information technology) products, and the appropriate tools and channels to convey IT knowledge to learners.\n\nTeacher training in ICT\n\nReasons for training\n\nTo increase capacity of teachers to:\n\n  * educate the learners through the use of training programs in order to allow the teacher to;\n  * handle intricate scenarios where students have diverse needs and new communication patterns;\n  * understand the modifications to content being taught and instruments being used to deliver;\n  * cope with the requirement for lifelong learning and regular updating of knowledge.\n\nThe current technological advances provide trainers with a variety of educational contents that allow them to offer both autonomous and diversified learning opportunities. Training in ICT requires teachers to abandon the conventional techniques and adapt to the new guidelines. It is important to train teachers to allow them to: handle intricate scenarios where students have diverse needs and new communication patterns; understand the modifications to content being taught and instruments being used to deliver; and cope with the requirement for lifelong learning and regular updating of knowledge (Gamage, Adams, & McCormack, 2009).\n\nComponents of training\n\n  * Cultural competency: knowledge of the areas that the teacher will teach.\n  * Pedagogic competency: research techniques, didactic abilities, treatment of diversity, and group dynamics.\n  * Personal traits: maturity, self-esteem, empathy, self-confidence, emotional intelligence components.\n  * Instrumental capabilities: awareness of new languages in ICT and audio-visual language.\n\nTeacher training involves two components namely \u201ctraining for the media\u201d, which entails the trainers familiarizing themselves with the use of the means, and \u201ctraining with the media\u201d, which involves a devoted means of training in order to build on cognitive abilities and enhance the comprehension of information, as well as, the development of differentiated environments for learning (Gaible & Burns, (2005). Hence, the various requirements that should be identified before training commences include:\n\n  * Cultural competency: knowledge of the areas that the teacher will teach.\n  * Pedagogic competency: research techniques, didactic abilities, social and psychological knowledge that enables them to comprehend. conflict resolution, treatment of diversity, and group dynamics.\n  * Instrumental capabilities and awareness of new languages in ICT and audio-visual language.\n\nPersonal traits such as maturity, self-esteem, empathy, self-confidence, emotional intelligence components.\n\nContent for training teachers in ICT\n\nOn ICT \u2013 for professional, didactic, personal use\n\n  * Knowledge of hardware and basic maintenance of the equipment;\n  * Essential functionality of the operating system: copying, saving, recording;\n  * Audio-visual language, hyper textual structuring of the information;\n  * Texts: word processor, dictionaries, OCR, creation of simple web pages;\n  * Image and sound: graphic editor, scanner, sound editor, video, photography;\n  * Presentations: applications.\n\nThematic \u2013 related directly to the subject matter of the teaching\n\n  * Sources of information and ICT resources: location, access;\n  * Use of programs specific to the fields of knowledge of the subject.\n\nPsycopedagogic \u2013 involving deep attitudinal components\n\n  * Integration of ICT into the curricular design;\n  * Objective assessment of ICT resources;\n  * New teaching and learning strategies using ICT, cooperative work, self-instruction;\n  * Selection of ICT and design of formative interventions set in a context;\n  * Using ICT to analyze students in the educational action.\n\nOn ICT \u2013 for professional, didactic, personal use\n\n  * Integration of ICT into the curricular design;\n  * Objective assessment of ICT resources;\n  * New teaching and learning strategies using ICT, cooperative work, self-instruction;\n  * Selection of ICT and design of formative interventions set in a context;\n  * Using ICT to analyze students in the educational action;\n  * Using ICT to guide and identify the main traits of the students, reports, follow-up;\n  * Using ICT for the management of the educational centers;\n  * Management of the ICT resources in the classroom centers of resources, coordination;\n  * Elaboration of simple didactic materials: open programs, authoring tools.\n\nDelivery methods\n\nWeb-based tools\n\n  * World Wide Web (WWW) through encyclopedias, search engines,etc;\n  * Internet resources e.g. labs, education portals, help centers.\n\nIn-service teaching materials\n\n  * Institutional educational portal that will provide resources, reference material, projects, etc.\n\nThe channels used to provide training to the teachers will be in the form of web-based tools and in-service teaching materials.\n\nOne of the in-service training techniques involves the establishment of an educational portal for the institution. The portal provides teachers with necessary resources on all subject areas, reference materials and manifold activities that can be conducted online. The website will also provide teachers with access to guidelines and support, a catalogue of the available resources, and innovative school projects regarding the integration of ICT tools in the curricula.\n\nThe World Wide Web (WWW) will be useful for teachers as a source of referencing through providing access to various resources including dictionaries, encyclopedias, news articles and search engines. Internet resources include educational portals, virtual resource centers, on-line writing labs, cyber listening labs, web-based English courses, virtual libraries, creative teaching websites, teacher\u2019s websites, student websites, on-line English grammars, and help centers on English related questions (Leung, Watters, & Ginns, 2005).\n\nResource selection Criteria\n\nSource evaluation\n\n  * Trustworthy source: information obtained from author\u2019s credentials, e-mail, organizational support, rating, grammar, meta data.\n\nContents\n\n  * Provides accurate information that is relevant: up-to-date, comprehensive, objective, reasonable, consistent.\n\nAccess\n\n  * Standard multimedia formats in that it does not require extra plugins or applications to view it, it is free, easy to access, downloadable.\n\nDesign\n\n  * Well structured, easy to navigate, interactive, no distracting visual elements, functional design, no broken links.\n\nTeachers will be given the option of choosing the web-based tools that they will be most comfortable using. This is important in order to prevent the learner from getting lost in all the information that is available online. The SCAD checklist (Source evaluation, Contents, Access, and Design) is an easy-to-use set of criteria that serves as proper guidance for students (teachers) in identifying the most appropriate resources (Leung, Watters, & Ginns, 2005). It involves:\n\n  * Source evaluation: It can be helpful to look for the author\u2019s credentials in order for the learner to find out if the author is knowledgeable and reliable. E.g. biographical information, contact information, reviews, comments, ratings and recommendations.\n  * Contents: It is necessary to find out if the information is correct, in depth, truthful, precise or complete. A publication should be well balanced, moderate and not emotional. It should not comprise wild and irrational claims or arguments.\n  * Access: Students easily get bored if they have to wait too long for a page to download to their computer. So it is good to check if a site is not cluttered with unnecessary graphics that help to slow down download time.\n  * Design: A web page should be well structured, and the individual pages should be concise and short enough to avoid having to scroll. In addition, a website should be easy to navigate and allow the teacher to use it spontaneously.\n\nProgram assessment\n\nIssue questionnaires to determine:\n\n  * right ICT training;\n  * Appropriate ICT-based tools and resources;\n  * Elements that enhance the efficacy of the integration of ICT in teaching learning processes;\n  * ICT competencies to focus on.\n\nThe content will be piloted in one of the high schools, in order to use the teachers as trainers for a reaching more schools with the same methodology. Assessment will be conducted through the issue of questionnaires to monitor the relevance of the content. This will enable the teachers\u2019 trainers to determine:\n\n  * Whether the teachers are receiving the right training regarding information and communication technologies;\n  * Which ICT competencies the different teachers should emphasize in order to account for the successful integration of ICT in teaching and learning processes;\n  * Which ICT-based tools and resources account for the successful integration of ICT in the various classrooms;\n  * The elements that enhance the efficacy of the integration of ICT in teaching learning processes.\n\nMonitoring and Keeping up with Technology\n\nIn order to optimize the benefits and reduce the disadvantage of integrating ICT, various initiatives will be introduced including:\n\n  * The development of workshops to enhance the exchange of experiences among colleagues\n  * Teacher training initiatives to provide a principled, meaningful approach to the creation and harnessing of new literacy such as dedicated websites.\n  * Creation of platforms that provide links to providers and users of various online classes and materials\n  * Creation of new training plans whose contents can be updated and revised periodically.\n\nReferences\n\nGaible, E., & Burns, M. (2005). Using Technology to Train Teachers: Appropriate Uses of ICT for Teacher Professional Development in Developing Countries. ICT and education series , 3(1), 2-17.\n\nGamage, D., Adams, D., & McCormack, A. (2009). How Does a School Leader\u2019s Role Influence Student Achievements? A Review of Research Findings and Best Practices. International Journal of Educational Leadership Preparation , 4(1), 23-43.\n\nLeung, K. P., Watters, J. J., & Ginns, I. S. (2005). Enhancing Teachers\u2019 Incorporation of ICT in Classroom Teaching. Educational Leadership , 40(1), 4-10.\n\nMikre, F. (2011). The Roles of Information Communication Technologies in Education Review Article with Emphasis to the Computer and Internet. The Role of Information communication , 12(1), 1-36.\n\nSansanwal, D. N. (2009). Use of ICT in Teaching \u2013 Learning & Evaluation. Educational Technology Lecture Series , 14(1), 21-29.\n",
        "label": "human"
    },
    {
        "input": "Approaches in Computer-Aided Design Process Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Method/approach/Architecture/Framework\n 3. Results and Discussion\n 4. References\n\nIntroduction\n\nThe field of this research is the computer-aided design. It is a non-linear decision-making process, the principle of which is a specific selection of configurations with optimized parameters.\n\nChallenges:\n\n  * The intricacy of the structure that resulted in the need to understand this process was the reason for this study.\n  * It is also worth noting the desire and opportunity to improve the computer-aided design, which is also the motives for this work.\n\nGoals/Contributions: This work\u2019s findings may become a theoretical basis that will contribute to future improvements in the computer-aided design and the development of new approaches to interaction with the structure. The authors also believe that this work will close the skill gap between students and specialists.\n\nMethod/approach/Architecture/Framework\n\nThe chosen methodology was the semi-structured interview. This approach was chosen because, in order to understand the improvement of the computer-aided design process, it is essential to identify its configuration and parametric problems. The semi-structured interview allows the researchers to assess and define the behaviors and thought patterns of designers effectively. The volunteer group that participated in the semi-structured interview consisted of nine graduate students, five industrial CAD designers, and three Purdue University educators. It is essential to mention that all contributors have at least more than three years of experience in CAD design. Each interview\u2019s duration was about thirty-five minutes in which the interviewer asked the participants the same questions regarding demographic data, work experience, design process, approaches, and software. Then, using a qualitative approach, the collected answers were transformed into specific topics. The reasonability of applying the qualitative approach can be explained by the non-quantitative variables and the aspiration to highlight essential nuances. The most prevalent and important issues were identified through the bottom-up grounded theory and theme analysis. It is important to mention that several irrelevant topics were removed in the process of data analysis. It was found that the selective codes are the most important ones affecting the overall user experiences.\n\nResults and Discussion\n\nThe findings were grouped into three conceptual categories. They are the general engineering design process, detail design process, and common design problems and challenges. It was found that both students and specialists adhere to a general iterative design process. Its main steps are identifying the problem, creating a product concept, and choosing a final product concept. The most common procedural nuances are design problems, customer communication, and data interpretation. Kroll, Condoor, and Jansson identify these procedural nuances as conceptual design [1]. It is also essential to note that CAD students are more likely to work with new concepts, while CAD specialists use already established models and concepts. The most frequent and critical problems of both CAD graduates and CAD experts are multivariate data, pricing policy balancing, company\u2019s conceptual standards, preventing the design mistakes, and parametric modeling. Poli offers another framework for categorizing these types of design problems, namely, conceptual, parametric, and configurational [2]. It is also important to mention that Myung and Han identify multivariate data and parametric modeling issues as parametric design issues [3]. The study may benefit the community as a theoretical basis. This paper can only be considered as initial support for further research.\n\nReferences\n\nKroll, E., Condoor, S.S., Jansson, D.G.: Innovative Conceptual Design: Theory and Application of Parameter Analysis. Cambridge University Press, Cambridge (2001).\n\nPoli, C.: Design for Manufacturing: A Structured Approach. Butterworth-Heinemann, Oxford (2001).\n\nMyung, S., Han, S.: Knowledge-based parametric design of mechanical products based on configuration design method. Expert Syst. Appl. 21, 99\u2013107 (2001).\n",
        "label": "human"
    },
    {
        "input": "Acquiring Knowledge About Computers Coursework\n\nIntroduce the course and the objectives of the course. The course is about computers. Thus, there is a need to discuss the importance of acquiring knowledge about computers and how this knowledge relates to the student\u2019s future career or chosen field of expertise. It is also imperative to focus on the cutting-edge technology available to students studying computers in the 21 st century. In the past, computers were simply viewed as machines. However, in the present time there is the added element of mobility and interconnectivity.\n\nOne of the key features of A.I.U.\u2019s learning platform is the use of the Gradebook. This tool does not only inform the students about his or her grades, but this platform also helps the same student to track his or her progress throughout the semester.\n\nIt is important to note that this platform is more than a report card. In the old way of doing things, the students had no idea how they earned a particular grade. However, with the Gradebook platform, the student can track his or her progress throughout the course. Use this feature after completing an activity or test. The Gradebook provides a clear tracking tool, it is like having a personal score card. In the past, students did not have a realistic assessment of the knowledge acquisition process. However, with the use of the Gradebook, students are empowered to have the capability to figure out areas that require improvement.\n\nThe Announcements are one of the features of the online learning platform that serves as a one-stop-area for students that need to get informed regarding key activities, requirements, and events. In the past, students complained about the lack of access to critical information related to the course. It was a common problem to have instructors or professors thinking about doing an activity or cancelling an event and the absence of the appropriate tool. Broadcasting the said information caused a great deal of confusion.\n\nThe Announcements enable the student to have a tool that is similar to a mobile message board. The student is not bound by the physical limitation of a typical message board. Use this feature once a day. In the absence of a similar tool, students were compelled to walk or commute to a particular area in order to read the messages. However, the presence of this feature within the online learning framework of AIU saves times and effort.\n\nInstant Messaging allows students to communicate with each other or with their instructors. The best feature of the instant messaging tool is the fact that it is easy to install with no additional cost to the student. There are occasions during the course of the day when students require access to certain information. There are also plenty of opportunities for collaboration. Instant Messaging provides a platform that guarantees the ease in the communication process. In the absence of the said tool, students are forced to purchase equipment or subscribe to a telecom network in order to satisfy the need for interconnection and collaboration. There is no need to elaborate the cost of acquiring a subscription from a telecom provider. As a result, students are compelled to spend more if they do not have access to this type of communication platform.\n\nInstant messaging provides students with the speed and convenience of instant communication. However, if there is a need to track and document important communication regarding a project or topic, it is better to use email. Instant Messaging is a powerful tool, however there are limitations. In the course of the semester, students are expected to work within teams and with other members of the class. They are supposed to work on projects or class presentations. Due to the hectic schedule and other pressing matters of different types of academic requirements it is easy to lose track of the developments of a specific project. However, the use of email allows the students to have the capability of storing important information. For example, in the process of dealing with a project the articles that were downloaded or the ideas that were shared during a recent brainstorming session are stored within the servers that are part of the said email system. If there is a need to review a certain key idea, the student simply has to scroll up and down the list of emails received. It is also important to point out that the email system is more than an inbox that receives messages from other users. This feature also enables the student to track the messages that were sent from his or her account.\n\nLive Chat provides the basic experience of online learning because knowledge acquisition is made possible beyond the four walls of the classroom. Live Chat used in conjunction with the Announcements feature as well as the email system of the online learning environment creates the capability to go through the learning process without the need to commute or walk to a specific classroom. Live Chat removes the limitations created by personal schedules and other constraints. The value of the Live Chat platform goes beyond the power of transcending space and time limitations. Live Chat also satisfies the need of certain students for greater levels of interaction with instructors and fellow students. Live Chat is a different approach to interactive learning. Aside from using different tools that allow the learner to access information via the World-Wide-Web or video sharing platforms, Live Chat provides the ability to contact another person. As a result, there is a way to tailor-fit the knowledge acquisition process. For example, in a video that is uploaded to a video-sharing platform, the student watching the video is simply the audience. He or she simply accepts or receives the video presentation. Although this is a good way of acquiring important information regarding a particular subject matter, there is little interaction that occurs in that medium. However, in Live Chat the student can fire back with a question. The great thing about Live Chat is that the person on the other end also has the capacity to interact with the students using the said tool.\n\nLive Chat creates the feeling of limitless learning because students and teachers are no longer constrained by the limitations created by time and space. Use this feature when there is a scheduled activity to use Live Chat with students or instructors.\n\nThe Intellipath is like an efficient tracking system that allows the student to monitor his or her progress. However, there are more features aside from tracking the learning process. One can even say that the Intellipath is a tool that provides interactive learning for the student.\n\nThe Intellipath does not only offer guidance so that a student always stays focused on the materials that require immediate attention. In addition, this system helps the student to figure out weaknesses and plan a way to improve on those weakness. Use this feature everyday to track changes and weak points. With just one glance the student using the said tool receives feedback regarding the learning activities that he or she had completed in the past. There is an automatic validation of a good work that has been logged in and deemed completed. However, if there are problems with completion, then the Intellipath system also provides a gentle reminder on the unfinished tasks that require greater attention. Aside from the tracking process, the great thing about the Intellipath system is the interactive nature of the system. One of the impressive features is the ability of the system to provide a certain measurement value regarding the student\u2019s mastery of a certain area of learning. The system has the capability to provide the forecast of the skill and knowledge acquisition ability of the student. As a result, the student has an overview of the whole learning process and he or she has the opportunity to focus resources or time on certain areas that require greater urgency.\n\nThe Learning Materials feature of the online learning system provides an easy-access location for all the pertinent information required to master a particular course. It also helps students to plan their pace because the materials are made available all at once. In the absence of the Learning Materials feature, students are at the mercy of the instructors. If there are instructors that are not prudent enough to provide advanced notification regarding the progress of the class learning process, the students are limited by that trickling flow of information. However, with the introduction of the Learning Materials feature students are able to develop their own pace with regards to how they are going to spend time and resources in studying the said materials.\n\nUsing the Learning Materials feature is like having an interactive map that provides directions which way to go and what subject areas require immediate attention. Use this feature when studying specific topics.\n\nThe MUSE is an acronym and it stands for My Unique Student Experience. This means that the university created a platform that understands and appreciates the uniqueness of each enrollee. Thus, the learning environment was designed to consider different types of learners. There is no other term to describe the MUSE platform other than the term personalization. This system is a testament to how the educational system has acknowledged the unique capabilities of each learner. It is not prudent to force students to simply learn by reading and going over a text-based knowledge acquisition strategy. There are students that are going to respond better if they are able to have access to videos or exercises that allow them to manipulate objects. This reminds the proponent of this presentation about the ancient way of learning that was espoused by the ancient Greek philosophers. It is already a tremendous revelation to find out that teachers like Socrates, Plato, and Aristotle did not find it wise to simply limit the learner\u2019s experience within the four walls of a classroom. These Greek philosophers encouraged movement of the students as they walked together or went to certain locations in order to ponder some of the most difficult questions about life. In the present time, the use of the MUSE acknowledges the need to go beyond the confines of conventional learning environment.\n\nThe creators of the MUSE platform developed an interactive learning environment. At the same time, students are able to access different tools and materials that suit their learning styles and learning requirements. It is a good thing to know that the university invested in a proprietary system that enables students to see a particular subject matter from different perspectives. There is a need to look into this system and how the students are able to capitalize on this tool. Use this feature on a daily basis.\n\nProvide an overview of the course elements. Provide specific details about the learning outcomes. It is important to look into the details of the learning outcomes in order to figure out if the students were able to comply with the said requirements. However, an overview of the course elements also provide students with insights on how to develop specific skills. They are not only expected to learn more about computers, they are also expected to learn how to communicate what they know. This is a good insight because it also encourages the growth of the leadership capabilities of students as they learn to interact and communicate with others. It is a good skill to polish even as they mature through their stay within the online learning system of the said university. Though the MUSE should be utilized correctly in order to harness its benefits and true potential.\n",
        "label": "human"
    },
    {
        "input": "Computer Network: Data Flow and Protocol Layering Essay\n\nA computer network refers to the connection of two or more computers with the ability to share information. Electronic devices in computers communicate by means of data flow. Data flow enables information to transit from one device to another when they are interconnected. Networks are specifically designed to enable the flow of data between two or more interconnected devices (Winkelman 1).\n\nDefine the three types of data flow\n\nThe modes of data transmission describe the way through which data is transferred from one device to another. There exist three types of data flow namely simple, half-duplex and full-duplex modes (Kuphaldt 1).\n\nSimplex\n\nSimplex mode is one in which the flow of data is unidirectional; that is, information flow in one direction only. In this regard, data flows from a transmitter to a receiver. In this case, no information goes back to the transmitter (Kuphaldt 1). Simplex mode is only applicable in situations where the receiver is not expected to transmit information. An example of this kind of communication is the BogusBus. Other examples include radios, televisions and point-of-sale devices. The diagram below shows a simplex communication mode\n\nHalf-duplex\n\nHalf-duplex mode is one in which the flow of data is multidirectional; that is, information flow in both directions. However, it should be noted that data flow is only allowed in any given direction at a time. In essence, data is received at a time or sent at a time. When data is sent, data cannot be received and vice versa. An example of a half-duplex mode of data flow is the communication between a server and a personal computer where data transmission takes turns (Kuphaldt 1). The diagram below shows a half-duplex communication mode\n\nFull-duplex\n\nFull-duplex mode is one in which the flow of data is in both directions simultaneously. It differs from half-duplex in that data flows simultaneously in a full-duplex while data flows in turns in half-duplex. An example of half-duplex mode of data flow is the communication between two people utilizing telephones (Kuphaldt 1). The diagram below shows a full-duplex communication mode\n\nWhat are the two principles of protocol layering?\n\nThe first principle of protocol layering states that the two layers should have two opposite tasks. Essentially, the first principle infers that each layer should have the capability of performing two opposite tasks (Kim 6). The second principle states that the two layers should be alike. The second principle supposes that objects under each layer should be identical (Kasera 25).\n\nA sine wave is offset \u00bc cycle with respect to time 0. What is its phase in degrees and radians?\n\nIt is worth noting that 1 complete cycle is equivalent to 360 degrees. In this regard, phase value of the sine wave would be as follows (Keiser 126).\n\n1/4 x 360\u00b0 = 90 \u00b0\n\nThe phase value would be 90 , however, to compute in radians, it would be as follows\n\nList three properties of Infrared waves\n\nInfrared waves are electromagnetic waves, which extend from 0.75 micrometers to 1000 micrometers (1millimeter). Infrared waves also have a frequency range of between 300 Gigahertz and 400 Terahertz. Infrared waves have another property in that they can be absorbed or emitted by some atmospheric molecules that change rotational movements. These molecules include carbon dioxide, ozone molecules, water vapor and methane. Additionally, infrared waves also have thermal properties. Warm objects or hot objects emit infrared radiation (Kurtus 1).\n\nWorks Cited\n\nKasera, Sumit. Communication Networks: Principles and Practice , New York City, New York: Mc Graw-Hill education, 2006. Print.\n\nKeiser, Gerd. Local Area Networks . 2 nd ed. 2002. New York City, New York: Mc Graw-Hill education. Print.\n\nKim, Yeongkwun. Chapter 2: Network Models . 2013.\n\nKuphaldt, Tony. Data Flow: Chapter 14-Digital Communication . 2016.\n\nKurtus, Ron. Infrared Radiation . 2010.\n\nWinkelman, Roy. Chapter 1: What is a Network? 2013.\n",
        "label": "human"
    },
    {
        "input": "Future of Forensic Accounting With Regards to Computer Use and CFRA Essay\n\nExecutive summary\n\nForensic accounting is an accounting that offers high levels of assurance and suitability for legal review. It further describes engagements from litigations, anticipated disputes and actual disputes. Forensic accounting is believed to be originated from ancient Egypt while others argue that it was officially coined in Canada. There are several historical cases associated with forensic accounting and the first is believed to be conviction of Al Cappone for evading taxes. Though not popular with most people, forensic accounting is mostly used by banks, insurance companies, and governmental entities among others. There are also six simple basic steps that assist in basic investigation of financials, they include, stating potential causes of problems and establishment of data among others. Finally, considering evolution of forensic accounting and rising concern to control fraud, forensic accountants are expected to be highly demanded. Basically, this paper highlights the importance of choosing a good accounting practice and ensuring quality control in the organization. Accounting control must be effective for a company to realize profits and enhance operations because funds management is the most important aspect in accounting or financial accounting management. This is because finances are the most important resource that can enable an organization achieves their organizational goals and objectives.\n\nIntroduction\n\nManagement is the most important aspect in operation of businesses irrespective of their sizes or nature of businesses they engage in. There are several functions of management. Though all companies and organizations consider management as the most important aspect of operations, companies opt to adopt strategic management systems. This may enhance operations of businesses and increase their profit levels in the long run. Though all functions of management are important, their importance in operations varies. However, they must be effectively coordinated towards achievements of goals of the respective organization or company. Companies should develop both short term and long term strategies to ensure that they achieve both long term and short term goals.\n\nThere are several important aspects of business. They include, marketing, management, accounting among others. These aspects have to be effectively coordinated to enhance performance of the respective organization, for instance, accounting involves financial accounts keeping. This process assists in systematic storage of financial quantitative information. Furthermore, accounting ensures effective appropriation and use of organizational funds. There are different types of accounting; they include management accounting, product control, social accounting, non assurance services, resource consumption accounting, governmental accounting, project accounting, triple accounting, fund accounting and forensic accounting among others. However, the main focus will be on forensic accounting due to rising awareness propelled by World Com and Enron scandals (Ferguson 2010, p.32).\n\nAccounting practice is the most essential and vital practice in an organization. Though management is the core function of business operations, accounting control and management is important in ensuring that organizations maintain operation. Organizations can only achieve their goals and objectives of ensuring that they increase their profit levels and gain competitive advantage in the respective industry. There are different factors that must be considered in accounting control, for instance, forensic accounting. Forensic accounting is considered among the most important basic accounting aspect that organizations must consider enhancing tracking of funds release in the organization. Furthermore, forensic accounting is the most important aspect of accounting control that can ensure that an organization achieve their desired goal and objective of increase profit levels and market share respectively in the respective industry or market where they operate (Ferguson 2010, p.32).\n\nWhat is forensic accounting?\n\nBasically, forensic refers to suitability for court of law use. Furthermore, forensic accounting may be viewed as an area of accounting specialization that describes actual disputes, litigation or anticipated dispute engagement results. This is an accounting that offers high levels of assurance and is suitable for legal review (Singleton, and Singleton 87). Its findings are based on scientific interpretation and detection of phenomena introduced into accounting system records and books. Forensic accounting enhances reporting of economic transactions especially in legal framework and the official accounting system in the economy hence proving accountability or valuation of the respective figures. Forensic accounting integrates auditing, investigative and accounting skills. It therefore, provides an analysis suitable for court debate, discussion and dispute resolution (Ferguson 2010, p.32).\n\nWho uses forensic accountants?\n\nAccounting is concerned with management of funds within an organization. There are several accounting practices that are conducted by organizations. However, forensic accounting is the most important and most recent accounting practice adopted under the efficiency accounting theory. Forensic accounting aims at enhancing accountability and efficiency in operation within and among organizations. Forensic accounting is basically offered by the forensic accountants. Though it might be argued that not all organizations require forensic accountants services, these services are important to all organizations (Silverstone, and Sheetz 2011, p. 547).However, forensic accountants are mostly consulted by large scale companies and entities. This is because such companies engages in large amount transactions on a daily basis and tracking the funds within the organization by the personnel might not be easy considering other accounting personnel have other duties to undertake at the respective organization (Silverstone, and Sheetz 2011, p. 547).\n\nBasically, forensic accountants work with financial data so as to convey complicated issues to others for better understanding. There are several people or organizations that seek services of forensic accountants. However, they seek their assistance for different reasons, they include; insurance companies, governmental entities such as Internal Revenue Service (IRS), police departments among others and banks (Silverstone, and Sheetz 2011, p. 547). Forensic accountant may also be used by victims to determine amount of loss. He or she might also be used as an expert witness in court. They are also used by accused to write summaries for court use and prepare visual aids. Additionally, an attorney may hire a forensic accountant to investigate criminal suspects\u2019 financial trial. Furthermore, a bankruptcy court might sort services of a forensic accountant (Silverstone, and Sheetz 2011, p. 547).\n\nSix simple steps to follow to perform a basic investigation into the financials\n\nThere are several steps that can be used to investigate finances of an organization. However, there are six steps that can be used for basic financial investigations. They include stating how potential cause could lead to the underlying problem (Hopwood, Young, and Leiner 2011, p. 231).Secondly, establishment of data that can easily disapprove or prove potential cause. This assists in development of study plan and identifies action plans actions. Third, preparation of required materials for the study, but training may also be required. Fourth, collection of the required data, and fifth, data analysis through simple statistical tools with emphasis on data graphical illustrations. Lastly, concluding, that is determining whether the data established may be the potential cause of the problem (Hopwood, Young, and Leiner 2011, p. 231).\n\nStating how potential cause could lead to underlying problem\n\nOrganizations must state cause for an underlying accounting problem or setback. This is an opportunistic accounting approach. In case an organization can establish the cause of an underlying problem then it can decide on the most appropriate solution to the accounting problem. Forensic accounting aims at determination of a root cause of an accounting problem. This may enable an organization to take corrective measures based on the findings of the organization on the cause of the accounting error (Hopwood, Young, and Leiner 2011, p. 231). Furthermore, this might assist an organization in avoiding such problems or causes in future hence enhance their accounting practice. Additionally, it also enables an organization to determine how an accounting cause could lead to accounting errors or problems and nature of intensity of the respective errors or problems. This could lead an organization to establish corrective measures and strategies to counter such causes and avoid the problems in future operations (Hopwood, Young, and Leiner 2011, p. 231).\n\nEstablishment of data that could lead to disapproving or approving a potential cause\n\nThere are different causes that can lead to accounting error or affect accounting practices of an organization. However, an organization must establish data that could assist the management approve or disapprove potential causes. An organization is better placed if the management can determine potential causes in advance. This can assist an organization establish strategies that can enable them effectively and adequately respond to problems and counter potential causes. Furthermore, this could enable an organization to review its accounting strategies and practices and hence adjust loop holes and avoid effects of the potential causes (Silverstone, and Sheetz 2011, p. 547).\n\nPreparation of required materials for the study\n\nFinancial investigation is a process and not a function. It is performed under several processes and principles. These processes must be carefully reviewed and taken into consideration in case the process is to succeed and a lasting solution or recommendation established by an organization. After determination of potential causes and approval or disapproval of the respective causes, an organization or company must prepare required materials for the study of the respective finances (Hopwood, Young, and Leiner 2011, p. 231). Materials used for financial studies are determined by the potential cause\u2019s determination and approval or disapproval of the respective potential causes. These materials must be prepared in advance to avoid instances of delayed study. It should also enable an organization to effectively plan in advance and avoid any future inconveniences on financial accounting. In this stage training may be required. This is to ensure that relevant personnel are well equipped with necessary information on the respective accounting potential cause (Hopwood, Young, and Leiner 2011, p. 231).\n\nData collection\n\nStudy materials preparation enables an organization to conduct the financial research and collect the relevant data. Data collection relevance mainly depends with preparation of the research materials and determination of potential causes. This will assist in determination of the research method that opts to be adopted to ensure that relevant and required data are collected. The collected data will assist the respective organization or company determines whether to adopt an opportunistic or qualitative accounting approach (Hopwood, Young, and Leiner 2011, p. 231).\n\nData analysis\n\nThe collected data must be analyzed to ensure that relevant and most appropriate accounting solution is adopted or enforced. Implementation of the accounting policy will also rely on the analysis of the collected data. Ineffective and inadequate analysis of the collected data may not enable an organization to prepare or enhance accounting practice. Therefore, the collected data must be effectively analyzed. This can only be possible in case the organization or company adopts an effective data analysis technique. There is several data analysis technique and choice of the analysis technique to use or apply depends with the nature and type of data collected (Hopwood, Young, and Leiner 2011, p. 231).\n\nConcluding\n\nThe data collected and analyzed must be discussed by the relevant personnel in the organization and decision made. After careful review of the collected and analyzed data, it can be determined whether it represents a potential cause or not. Decision made is influenced by the data collected and analysis made (Hopwood, Young, and Leiner 2011, p. 231). Therefore, data collection method should be effective and analysis realistic to ensure that the decision made positively impacts the organization\u2019s or company\u2019s operation. This is because in case the materials chosen for study were not effective and inadequate training offered then the organization might not be able to ascertain on the potential cause considering ineffective process in determination of the causes ((Hopwood, Young, and Leiner 2011, p. 231).\n\nOrigins of Forensic Accounting\n\nForensic accounting is believed to be among the oldest professions on earth, for instance, in Egypt forensic accountants were famously referred to as \u201cears and eyes\u201d of king. However, according to history of the profession, forensic accounting was first used to convict Al Cappone for evading taxes. It is believed that forensic accounting originated from ancient Egyptian scribes accounting for pharaoh\u2019s assets. However, the first forensic case in court was reported in 1817 when an accountant was expected to testify on bankruptcy hearing. It was later adopted by a Scottish accountant in 1824, but the forensic accounting had not yet been coined (National association of Forensic accountants 2011, p. 1).\n\nForensic accounting is among the oldest accounting practices to be coined and applied in operations of organizations ad companies. Basically, forensic accounting is concerned with tracking of financial expenditure and use of an organization. It also ensures or enhances accountability among accounting personnel. In the last centuries, there has been rising concern on accountability of accounting personnel and management of an organization. Several companies and organizations have gone bankrupt and ceased operation due to poor accounting or financial management. Furthermore, most organizations operated on opportunistic strategies, and ignored the qualitative strategy of accounting (National association of Forensic accountants 2011, p. 1).\n\nForensic accounting is based on efficiency and accountability. Therefore, due to the rising cases of losses and closure of operation, there has been increased concern on importance of forensic accounting. Forensic accounting was initially used in Egypt by the workers of the king to enhance efficiency among palace workers. It aimed at ensuring that employees at the palace efficiently operated and guarded against any practices that could lead to mismanagement of finances at the palace and assets owned by the king (National association of Forensic accountants 2011, p. 1).\n\nHistorical cases of forensic accounting\n\nForensic accounting has been practiced for several years by organizations and companies. However, there are several cases they enhanced awareness for forensic accounting. These cases are basically referred to as the historical forensic accounting cases. This is because decisions on the cases were influenced by conclusion made by forensic accountants during those times. There are several historical cases of forensic accounting, the first and most famous being conviction of Al Cappone. There are several cases that shocked many globally and hence have been referred to as famous and historical forensic accounting, for instance, in 1910 and Dr Hawley Harvey Crippen from America was hanged for found guilty of hanging his wife Cora. Crippen poisoned his wife and then buried her remains personally in their home at London. Secondly, death of Marilyn Sheppard in 1954 and arrest, trial and re-trial of her husband has remained to be one of the unsolved murders in the United States. There are other historical forensic cases such Napoleon Bonaparte\u2019s death, Harry Houdini\u2019s death among others (All-about-forensic-science.com 2011, p. 2). Though forensic accounting was not basically coined for use in accounting practices in organizations, its efficiency and classification under efficiency accounting theories has increased its use in organizations. The historical cases were also complicated and collection of evidence difficult. However, due to forensic accounting, reliable evidence was collected and judgment made based on the evidence collected (All-about-forensic-science.com 2011, p. 2).\n\nFuture of forensic accounting\n\nExtensive use of computers has changed several aspects globally. This is because computer use is widely adopted due to technological advancement and increased internet subscription (Brown 2008, p. 3). In the recent past, legal investigation has extended to use of electronic detection hence proved reliability in searching, locating and securing obtained evidence documents. Furthermore, most documents are created with aid of electronic Medias and computer applications. Digital data nature is also suitable for investigation. This has led to the legal community taking advantage of discoveries by electronics (Brown 2008, p. 3).\n\nComputerized devices have greatly increased efficiency in collecting of evidence since it can be stored for future use. Adoption of computer use will therefore, lead to effective analysis, classification, review, and presentation of obtained data. This is likely to improve quality of data collected by forensic accountants in future (Manning 2010, p. 54). Furthermore, this might lead to presentation of organized and summarized data in future. This might also ease communication and easily persuade or convince. Data stored through computerized devices may not be easily destroyed and this might also improve quality and reliability of evidence in future (Manning 2010, p. 54).\n\nForensic accounting may also improve in future due to development of Center for Financial Research and Analysis (CFRA 2011, p. 1). This is because CFRA is an independent body and reputable for quality financial solutions and research. This body is recognized for leading research in forensic accounting and development of exception-based research products. Due to the quality of services offered by this organization. It has greatly increased confidence of organizations in its services. This might increase awareness and rise in demand in forensic accounting considering rising cases of fraud among and between companies, organizations and businesses (CFRA 2011, p. 1).\n\nForensic accounting was started in the early years, however, it was not well known by many until recently. The discipline was developed some time back, but has been responding positively to changes in the government and business environments. This is a clear indication that it will improve practitioners\u2019 standards in future. Much might not be predicted, but present events have shown quick changes in the globe. Considering the rising cases of fraud, forensic accountants are expected to be highly demanded in future (Poinier 2011, p. 1).\n\nFactors that might complement forensic accounting to ensure improved accounting practices\n\nMarketing changes\n\nA company has to possess a good and experienced marketing team hence respond to the marketing changes in each environment the company opt to operate. This enables the company to avoid unnecessary losses. It is through marketing that a company reaches the target groups hence need to be effective. Marketing is a process and needs to be consistently and frequently reviewed. The company should establish and effective marketing process, this will enable the business to effectively and adequately address dynamic business factors and respond to accounting practice changes in the market. It will also enable the company to establish flexible accounting strategies hence increase their profit level and market share in the respective industry (Silverstone, and Sheetz 2011, p. 547).\n\nChoosing of location\n\nBefore a company sets up a business it has to carry out market research hence determine if the location favors the nature of the business. It should be located where the operation costs are reasonable. Operation requires finances to be successful. However, organizations or companies need to operate under limited budget to ensure it obtains profit. Strategic location of a business entity assist an organization reduces costs it incurs in support services and general cost of operation. In return, this increases profit level for an organization or company (Manning 2010, p. 54). Therefore, an organization opts to locate its premises in strategic places where costs of operations are low to increase profit levels. It will also enhance effective accounting process by the organization (Silverstone, and Sheetz 2011, p. 547).\n\nRisk management\n\nWhen a business is incorporated, it is bound to loss or profit. Therefore, a company should effectively have an effective risk management plan. This will allow the business to have a competitive advantage over other companies and hence increase their level of income. Companies opt to establish an effective risk management plan to ensure that it effectively and efficiently respond to uncertainties in the business environment. This will also boost accounting practice by the company and avoid unnecessary losses by establishing emergency funds to respond to any uncertainties in the business environment (Silverstone, and Sheetz 2011, p. 547).\n\nKey issues and factors and important observations to be made to ensure effective adoption of accounting theory or strategy\n\nCompany strategies suppleness\n\nA company can adequately handle and endure challenges and pressures in the industry due to effective response to the changes in the environment. A company might emerge among the best and the largest in the industry if it adopts an effective accounting practices or theory (Manning 2010, p. 54). Though accounting theory may be opportunistic or efficiency, the most recommendable and commonly used theory is the efficiency accounting theory. Efficiency accounting theory adoption may enable an organization or company to implement its strategies alongside the theory and easily coordinate functions and achieve their goals and objectives (Silverstone, and Sheetz 2011, p. 547).\n\nSeveral companies emerge as global fitness businesses by developing and enforcing their spiritual, physical and mental strengths. This has enabled the firms to effectively operate, gain and sustain competitive advantage in different industries where they operate. In addition, a company may also be able to forecast on the expected outcomes in the business world. Consumers, shareholders, and suppliers\u2019 expectations are vivid to the management of the company (Manning 2010, p. 54). For instance the suppliers\u2019 expectations, the company has an outlined code of conduct for the suppliers of both products and services. This is possible if a company adopts an efficiency accounting theory in operation in the respective industry. This is because opportunistic theory may not be effective in operation of several organizations considering opportunities are limited (Silverstone, and Sheetz 2011, p. 547).\n\nStrategic situation analysis\n\nCareful analysis of the situation gives a company a chance to work out a new strategic plan or alter already existing strategies that have not been recently efficient. The company also needs to define its market so as to enable for competition and consumer analysis. The company should focus and effectively analyze future product-market changes (Manning 2010, p. 54). Analysis of strategies aims at identifying and describing potential buyers, estimating the market size and its expected growth rate, understanding customers\u2019 expectations and identifying close competitors in the insurance industry. This may also enable a company to decide on the accounting theory to adopt and use in the respective industry or market and enhance their financial practices (Silverstone, and Sheetz 2011, p. 547).\n\nDesigning of the market driven strategy\n\nDesigning of the market driven strategies involves several activities that aim at improving operations in the organization, for instance, defining market segments, assessment of strengths and weaknesses, identification of available opportunities in the market and evaluation of the prevailing level of competition in the industry. This research is important for the development of an organization as it enables introduction and development of new products by firms, determining the target market among others (Silverstone, and Sheetz 2011, p. 547).\n\nStrategic positioning and market targeting, there are several situational factors that influence gaining of marketing advantage by a firm. Markets are targeted with an aim of identifying potential customers that have an interest in the products of the company. It aims at matching the company\u2019s capabilities and segment requirements in the respective product markets (Manning 2010, p. 54). On the other hand, positioning strategy incorporates or takes into consideration value chain strategies, price, promotion and products of the respective company, which they use in the respective target market to effectively, compete against other related firms in the market and meet the requirements and expectations of the market (Silverstone, and Sheetz 2011, p. 547).\n\nMarketing relationship, there are several partners of marketing relationship. They may contain the following: \u201cinternal teams, suppliers, members of the marketing channel, competitor alliances, and the end user consumers\u201d. The company should be very careful and value the customers that contribute to the successful productions of the company day after day (Manning 2010, p. 54). This will enhance the provision of superior value to the customers in the long run. The strategic partnering may lead to a firm gaining a competitive advantage over other firms in the industry. Therefore, companies should consider partnering strategically to gain competitive advantage (Silverstone, and Sheetz 2011, p. 547).\n\nPlanning of a new product: the company may decide to introduce new products in the market due to the inferiority of its existing products in the market leading to decreased sales. A Company may produce a high-quality product at a competitive price if it closely monitors and coordinates the planning of new products. Before introducing a new product in the market, a Company has to identify gaps in customer satisfaction hence introduce an improved product (Silverstone, and Sheetz 2011, p. 547).\n\nDeveloping of market-driven program\n\nThe location of the market and its targeting strategies for the already existing products and the ones that are yet to be produced play a very important role when it is necessary to choose and to work out the vital components of the strategic program. Positioning strategy is a combination of promotion, product, price and distribution strategies (Silverstone, and Sheetz 2011, p. 547).\n\nThe positioning strategy is carried into effect by the marketing mix. Its objective is to gain favorable positioning efficiently and effectively as possible. Strategic management of brands, the turning point of positioning strategy is the product. It is worth noting that the strategic plan of the market consists of \u201cdecision on problem products, development of new product plans, and managing successful products\u201d. Strategic management of brands consists of a company\u2019s portfolio management and brand value building for the purpose of the overall company performance (Silverstone, and Sheetz 2011, p. 547).\n\nEffective management largely depends on the foundation of the management principle as well as the process of management being used. On the other hand, the concept of employee engagement is a management approach that has been employed to boost individual as well as organizational activities. Moreover, scholars have shown that there exists a relationship between employee\u2019s activities and organizational dedication, employment fulfillment as well as employee engagement. Research has shown that there exists a direct relationship between employee engagement and managers\u2019 effectiveness (Manning 2010, p. 54). In general, it is important to note that employee engagement incorporated with managers self-efficacy is imperative ingredients of a company\u2019s success that are likely to improve managers\u2019 effectiveness. Consequently, effective management is vital in order to achieve considerable development in any organization (Silverstone, and Sheetz 2011, p. 547).\n\nConclusion\n\nThere are different types of organizations in the global economy. These companies or organizations have different accounting policies based on the accounting strategies and general business policies of the respective organization or company. Generally, accounting policy adopted and established by an organization can either be based on efficiency or opportunity in the respective organization. Therefore, a company or organization may adopt an opportunistic accounting policy or efficiency accounting policy.\n\nConsidering evolution of forensic accounting, it has changed from theft prevention measure to court testifying and lately, creation of investigative accountant. Furthermore, there is increase in fraud cases and need to respond to them adequately is also increasing. Therefore, considering advancements in technology such as adoption of computer technology in gathering and storing evidence and other reputable forensic accounting organizations such as CFRA, forensic accounting is bound to increase in popularity. Finally, forensic accountants are forecasted to experience increase in demand for their services due to rising concern over fraud.\n\nList of References\n\nAll-about-forensic-science.com, 2011, famous forensic cases. Famous forensics. Web.\n\nBrown, M. 2008, Forensic accounting-past, present, & future. Web.\n\nCFRA, 2011, About CFRA. Web.\n\nFerguson, D. 2010, Careers in focus: Forensics . Infobase Publishing, New York.\n\nHopwood, W, Young, G and Leiner, J. 2011, Forensic accounting and fraud Examination. McGraw-Hill Companies, London.\n\nManning, G. 2010, Financial investigation and forensic accounting. CRC Press, Chicago.\n\nNational association of Forensic accountants, 2011, The history of forensic accounting . Web.\n\nPoinier, J. 2011, Forensic accountants use their number-crunching skills to ferret out fraud And provide litigation support. In today\u2019s scandal-prone business climate, it\u2019s the fastest growing job in the accounting field. Web.\n\nSilverstone, H and Sheetz, M. 2011, Forensic accounting and fraud investigation for Non-Experts. John Wiley and Sons, New York.\n\nSingleton, Tommie, and Singleton, A. 2010, Fraud auditing and forensic accounting. John Wiley and Sons, New York.\n",
        "label": "human"
    },
    {
        "input": "An Evaluation of an Investment Decision: Computer Incorporation Report (Assessment)\n\nTable of Contents\n 1. Introduction\n 2. Payback period of the investment\n 3. The rationale for the above computations\n 4. Recommendation\n\nIntroduction\n\nThis assessment seeks to relay to you an evaluation of an investment decision to be made by Computer incorporation. The company has planned to purchase a new machine with the following details. Amount to be invested = $ 100,000. The machine will be assigned to CCA class 8 and will be depreciated on a declining balance basis at a rate of 20% per year. After seven years, the machine has no salvage value. The machine is estimated to provide incremental after-tax cash inflows of $ 40,000 per year for seven years. The cost of capital is estimated at 14% and additional working capital is required. The corporate tax rate is at 40%. The company seeks an expert judgment on whether to purchase the machine or not. Below are two approaches to help the company make the investment decision. The first approach is the use of Net present value and the second approach is the use of the payback period. Below are the evaluations to help make the decision.\n\nPayback period of the investment\n\n(120,000/40,000) = 3\n\nThe payback period is three years. This means that it would take the company three years to fully recover the initial investment (initial cash outflow).\n\nThe rationale for the above computations\n\nThe above initial cash outflow is arrived at by adding the price of the new machine and the additional working capital requirement. The rationale for that is because the working is required purposely to facilitate the operations of the new machine therefore, it forms part of the initial cash outflow. Since the annual inflows as measured by after-tax inflows are constant, the present value annuity factor is suitable for computing the present values of the stated cash inflows as done above. The net present value method is that which takes into account discounting factor or cost of capital and the respective cash flows. Both cash inflows and outflows are discounted to reflect their period of earnings. The initial investment is therefore subtracted from the sum of the discounted inflows. On the other hand, when calculating the payback period, the main idea is to find how long it will take the company to recover the whole initial investment. If the payback period is within the acceptable duration, an investment is accepted.\n\nRecommendation\n\nWhen using the NPV rule to make the decision, Computer incorporation should purchase the new machine. The NPV rule states that if an investment venture has a positive NPV, that is, NPV >_0, then it should be accepted. The above-calculated NPV is a positive figure (51,532.2) and that means that the purchase of the new machine by Computer incorporation is a good venture. The company should purchase the new machine because the move will boost its cash inflows. A company derives its wealth from positive cash inflows. According to the above payback period, it would take the Company three years to recover the total amount invested. The period is less than the useful life of the machine and therefore, there is a high possibility of making profits out of the investment.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Search Strategy Using Evidence-Based Research Methodology Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Research Steps\n 3. Conclusion\n 4. References\n\nIntroduction\n\nIn any given research, it is important to have a well-planned search strategy to aid in collecting all the required information on the topic being researched. In addition to a well-planned search strategy, it is vital that the researcher fully understands the topic of research. In this case, the research concerns formulation of a computer-based search strategy to collect information using evidence-based research methodology only.\n\nResearch Steps\n\nThe first step in this search will be to formulate a question that will guide the research. In evidence-based methodology, for the question to be most comprehensive, it needs to have the PICO aspects (Huang, 2003). The PICO aspects cover the population, the intervention, comparison and the outcome. In this case, the question guiding my research is \u201cCan additional choices of food and places to eat improve appetite and maintain weight in residents with dementia?\u201d The population in this context will be the dementia patients while the intervention will be the provision of additional choices of foods and places to eat. This is in comparison with the current choices that the patients have. The outcome will be the effect that these changes will have on their appetite and consequently their weight.\n\nThe next step will be to conduct a search on the internet to get all the written materials related to this topic. There are various materials written concerning the feeding habits of dementia patients including the types of food recommended for them to eat (Fisher, 2011). In addition, there are researches done on the response of these patients to certain types of food (HelpGuide, 2011). Moreover, there are articles written on the best food for dementia patients and this will help in guiding this research (LiveStrong, 2011). All these collected material will be read through to see all that has been written about the topic which will assist in giving the background information and setting a foundation for this research.\n\nThe other step will be to use a computer-based search strategy to collect data on this research topic. This will involve searching for all the homes that care for patients with dementia around my area. A thorough and careful search will be conducted and all the details concerning these homes will be recorded. These details will include the location of the home, the number of patients within each identified home and the types of diets given to these residents in each and every home. In addition, the details of the caregivers in each home will be recorded so that in case there is a need to contact them to clarify any information, then that will be easily done. Also, weight records of these residents will be retrieved and if this is not possible due to the policy of confidentiality, the researcher will make a personal request to the homes to get all the information that will assist in the conduction of this research.\n\nWhen all the details concerning these homes have been collected, a comparison of the types of foods being given and the residents\u2019 records of weight and eating habits will be made. All the foods which are most liked by the residents will be identified and recorded. These records will be used as evidence of the effect of additional food choices on the residents\u2019 eating habits. Therefore, for the research to be truly evidence-based, the conclusion must be drawn from the best evidence available (DynaMed, 2010).\n\nIn addition to the computer research mentioned above, the following evidence-based research methodology which is experimental and scientific will be used. It will involve use of control and experimental groups (Brown, 2007). Residents from a number of randomly selected care centers or homes will be involved. The researcher will suggest the addition of a number of foods. The residents\u2019 responses will be studied and recorded. In addition, during the research, the residents will get chances to choose where they want to have their meals and the researcher will collect the data on how they respond.\n\nAll the results collected from this research will be recorded in the computer and a comparison of the results from various residents in various centers will be done to see how each resident responds to the provision of additional food variety and the choice of the area to have their meals from. Any weight improvement or deterioration will be recorded. Any significant similarities in response to certain foods and areas will be noted so as to be included in the findings and recommendations where they are needed.\n\nDuring this research, the caregivers will be involved to assist in monitoring and providing information to the researcher concerning the observations made. This will be necessary since the researcher cannot collect sufficient data from all these centers all alone.\n\nTo communicate with all the caretakers involved and to collect sufficient data within the set period of time, communication via telephone and emails will be involved so as to liaise with all the caregivers and collect a lot of information while saving time (Brown, 2007). The caregivers will be sent for tables with information on various identified food types and they will be required to write their observations against each food type and feeding area and include a record of changes in the residents\u2019 weight.\n\nConclusion\n\nWhen all the data has been collected, an analysis will be done to see how the residents respond to the changes of adding food and offering them a chance to choose where they want to eat from (Fischer, 2009). In addition, each resident\u2019s favorite food and the types of food generally most liked by the residents will be identified. All the information collected during the evidence-based research study is used to help in making vital medical decisions (The American Dental Hygienists\u2019 Association, 2010). Therefore, after this analysis, a report will be prepared to record the findings of this research and make the necessary recommendation where needed.\n\nReferences\n\nBrown N., Fitzallen, N. (2007). Evidence-based Research in Practice. Web.\n\nDynaMed, (2010) 7-Step Evidence-based Methodology. Web.\n\nFischer W., Etchegaray J. (2009). Understanding Evidence-Based Research Methods: Descriptive Statistics, The Health Environments Research and Design Journal. Web.\n\nFisher M. (2011) Reducing calorie and carbohydrate intake may affect Alzheimer\u2019s disease (search) risk.\n\nHelpGuide (2011) Alzheimer\u2019s Behavior Management: Managing Common Symptoms and Problems. Web.\n\nHuang, W. (2003) Formulating Clinical Questions During Community Preceptorships: A First Step in Utilizing Evidence-based Medicine, Baylor College of Medicine, Houston, Texas.\n\nLiveStrong (2011) The best Food For Dementia Patients To Eat .\n\nThe American Dental Hygienists\u2019 Association (2010) Evidence-based Methodology. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Museum: Personal Experience Personal Essay\n\nThe world of computers has continued to baffle many people across the world. Ever since the invention of the first computer, there have been many developments done on later models and this trend has continued well into the twenty-first century. By looking at the history of computing, one is left with no doubt that computers will have a great impact on our current society and more so in the future. Every year, computing artifacts keep on evolving thus presenting the computer industry as one that will influence our world in the coming years. (Brian)\n\nRecently, I visited the Computer History Museum situated in Mountain View, California to get a first-hand experience of the changing nature of computers. Right from the moment I entered through the doors of the museum, I stood awed at the numerous computing gadgets on display. While growing up, I took a keen interest in computers and therefore I have basic knowledge of most computer models. However, the moment I entered the museum was a turning point for me since I realized that there are so many things that I do not know about computers. On display was a range of the oldest to the latest computer hardware, software, computer files, ephemera, still photographs, and all sorts of moving images. (Computer History Museum)\n\nWhat baffled me most about the museum was the dynamic website maintained by the museum and the dynamic online exhibition. Although I have spent nearly all my life in front of the internet, nothing prepared me for the extensive online resources displayed at the museum. Another thing that fascinated me was the robot theater that had all sorts of robots. These robots all of which are controlled by computers proved to me that the world was indeed evolving and if a man is to survive, he needs to evolve with it. (Brian)\n\nIn the Computer Museum, the history of computing is presented in a multimedia exhibition dubbed Revolution: The First 2000 Years of Computing . In this exhibition, I was able to see a certain trend in the evolution of computers. From the abacus to the smartphone, it is obvious that computing gadgets are becoming smaller with each passing day. While in the Revolution , I got a chance to see a working replica of the Atanasoff-Berry Computer, which was the first real model of a working computer. By comparing this model with the latest computer models, I was able to realize that computers are evolving from mere storage devices to gadgets that get the work done for us. This is evident with the invention of a robot that can do basic chores for man. Even more amazing to me was the realization that there is a computer that can take on the best human chess players. While at the museum, I had a chance to watch a previous match between Deep Blue and Garry Kasparov who was once the World Chess Champion. Just by watching this dramatic footage, it became apparent to me that we are moving into an era where computers will even be able to read the human mind. I know that this might sound a little bit farfetched but considering that in chess you have to guess what the other person is \u201cthinking\u201d then the idea of computers becoming mind readers is not farfetched, after all. (Computer History Museum)\n\nAs I took my tour around the computer museum, it became obvious to me that a few years from now the computer will greatly affect how companies function. Although computers present numerous challenges for companies, it also brings with them a host of new opportunities. One opportunity that computers will present for companies in the coming days is making work easier. While at the computer museum, I could not help but think about how robots can help manufacturers to increase output. To begin with, robots will in the future be used to perform the chores that workers consider impractical or unhealthy. This will enable the workers to concentrate on their areas of specialization something that will definitely increase production for the companies. (Brian)\n\nThe other opportunity that I noticed computers could bring for companies in the future is in marketing. Given that everyone is predicted to be an owning a computing gadget in the future, it is possible that companies will change the way they conduct advertisements. Currently, companies spent huge sums of money to place adverts on television and print media but this is bound to change in the future. By looking at the smartphone, there is no doubt that this phone is designed to play the role of a computer more than that of a phone. As the world moves into the future, advertisements are bound to move from television and print media to the internet. In any case, it is only the older generation that spends time watching television and when they are gone, companies will be forced to change their mode of advertisements. These will in turn save companies the high cost of advertisement something that will increase their profits. (Brian)\n\nApart from the opportunities for the company, I see numerous opportunities for myself in the near future. Considering the numerous gadgets that I saw at the computer museum, it is likely that there will be an invention that will enable people to carry out their duties from wherever they are. Since I intend to start my own company in the future, it means that I will not have to go to the office but I will instead be able to give orders from the comfort of my living room. While someone might consider this to be a means of fostering laziness, I instead see it as a perfect opportunity for advancement since it will even be possible to work with individuals even from outside the country. This means that it will be possible to get employees from developing countries where there is an availability of cheap labor. (Technology Awards)\n\nAdditionally, I see a situation where I will be able to market my products without necessarily looking for office space. By setting up a website, I will be able to reach billions of people without necessarily having to rent premises. This will not only help me to save money but it will be a good opportunity for me to spend with my family. In matters of doing research, the internet will come in handy hence saving me the time and money needed to conduct virtual research. By using the latest computing gadgets, I will not only be able to stay connected to my friends but I will also be able to know first hand what is happening around the world. (Technology Awards)\n\nConclusion\n\nThere is no doubt that computers are changing the way people do things in society. It is also apparent that computers are evolving from the gigantic models of the past to minute models that can conveniently fit in one\u2019s pocket. In the coming days, computers are bound to alter the way companies and individuals conduct their day-to-day lives. In the future, there is no doubt that there will be computers that can literally decipher what is in an individual\u2019s mind. This is true if the numerous devices displayed at the computer museum are anything to go by.\n\nWorks Cited\n\nBrian, Huse. How Robots will Affect Future Generations , 2008.\n\nComputer History Museum . At the Museum, 2011.\n\nTechnology Awards. How will Technology Affect the Future of our Society?, Web.\n",
        "label": "human"
    },
    {
        "input": "Recovering from Computer System Crashes Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Computer Crashes\n 3. Hard Drive Crash\n 4. Operating System Crashes\n 5. Crash Recovery\n 6. Backing Up\n 7. Identifying Crash Cause\n 8. Conclusion\n 9. References\n\nIntroduction\n\nArguably one of the most epic accomplishments of the 21st century was the invention of the computer and the subsequent creation of computer networks. These two entities have virtually transformed the world as far as information processing and communication is concerned. Although computers are hardly a century old, they have revolutionalized the way in which we carry out our day-to-day activities and hardly any arena in our lives has escaped the influence of these systems. As such, a breakdown of the computer systems would be catastrophic to an individual or an organization. Despite this, Goldsborough (2004) notes that a constant reality when working with computers is that the data stored on the PC can disappear in an instant. For this reason, it is of uttermost importance to ensure that contingencies are taken in the event that a computer system should fail. Crash recovery measures provide the best way to salvage a failed system. This paper will engage in a detailed analysis of how to carry out a crash recovery.\n\nComputer Crashes\n\nAll computing systems are vulnerable to a wide variety of potential threats including; viruses, hacking, bugs, and physical damage to name but a few. All these threats may result in a system crash with varying consequences to the users. Bobrowski (2006) defines a crash as \u201cthe unanticipated failure of the system in question\u201d. Crashes result in the computer system being inoperable until a solution to the crash is arrived at. There exist two major forms of crashes; hard disk crashes and OS crashes, and the recovery process will be dependent on the unique nature of the crash.\n\nHard Drive Crash\n\nA hard disk crash is brought about from a hardware failure on the disk. A hard disk crash implies that all or some of the hard disk sectors are damaged and therefore unreadable. Hard disk failure may be caused by contaminants on the disk which may cause a head crash that damages some of the data on the disk. Head crashes may also arise from the hard disk being jarred while it is in use. Some of the signs of a hard disk failure are clicking or whirring sounds from the hard disk when the computer is turned on. To recover from a hard disk crash, one will be forced to invest in a new hard drive. Even so, one may wish to salvage the data that exists in the damaged drive.\n\nOperating System Crashes\n\nOS crashes are logical failures that make either part of or the entire OS unusable. In the event of a logical failure, the hard drive is still fully functional and the error is only on the OS. An improper shutdown of the computer due to power failure or poorly written software may damage critical system files resulting in an OS crash (Gookin, 2009). OS crashes may also occur as a result of memory overflow where data from one area of memory goes to memory allocated for another program. This may result in data corruption and the eventual crash of the OS. OS crashes can also be caused by viruses, which are malicious scripts written to interfere with the normal operation of the computer.\n\nCrash Recovery\n\nIn the event of a crash, the first step is to identify the type of crash and then determine the best way to recover from the crash. For an OS crash, the primary goal of recovery is to get the OS up and running. This can be done by reinstalling the OS from the original disk. Most OS providers provide users with recovery disks which are very useful in the recovery process since they enable one to return the computer to its factory settings (Gookin, 2009). For hard disk crashes, the aim of recovery is to retrieve data from the damaged drive. One can use data recovery software to try and recover some of the data from the damaged drive. Recovery software is designed to access sectors of the hard drive that are damaged or retrieve information that may have been deleted in the event of the crash. While there is no guarantee that all the data will be recovered from the damaged hard disk, data recovery software present the best means to retrieve at least some of the data that would otherwise be irretrievable from the faulty hard disk.\n\nBacking Up\n\nArguably the most important safeguard against computer crashes is backing up of data. Failure to back up important data may prove to be catastrophic in the instances where vital data is lost following a crash. Backing up is based on the premise that preparations must be made beforehand for the physical loss of an important OS file due to operation error, file corruption, or a disk failure. In this kind of disaster, it is impossible to recover the data from the system by the use of recovery programs. Backup copies of the lost data files are the only means through which the recovery process can be done. Backup software can be used to restore data from backups to a computer that has crashed. This is because backup software has modules for restoring files. However, this is only possible if one has already installed the backup software on the computer. In case the hard drive crashes, it is necessary to reinstall the backup software before commencing the recovery process. When performing backups, it is important to ensure that the integrity of the files is not compromised. Data integrity can be compromised by viruses which can damage files to the point that a computer cannot access the data contained therein. It is therefore important to ensure that files that are backed up are virus-free. This can be done by performing an up-to-date virus check as the first step in every backup routine (Parons & Oja, 2010).\n\nCrash recovery can be assisted by the use of extra disks on the computer system. This can be implemented through the use of Redundant Arrays of Independent Disks (RAID) technology which is hailed as the fastest means to recover from a system crash (Goldsborough, 2004). RAID can enable one to replace their failed hard drive almost seamlessly. This is made possible since RAID mirrors all the data that one store on their main hard drive onto a secondary hard drive. Despite the huge reliability that RAID technology offers through redundancy, it is not commonplace among individual PC owners owing to the additional costs that RAID technology demands.\n\nIdentifying Crash Cause\n\nAn important step in crash recovery is to try and identify the cause of the crash. This is very important since it is desirable to avoid future crashes. There are commercially available software tools that can be used to diagnose the system for physical or logical errors (Miller, 2007). Having identified this, appropriate measures can be taken to ensure that future crashes are avoided. Scan disk utilities can help identify faulty hard drives by providing information on bad physical sectors. One can therefore take up appropriate action and avoid future crashes.\n\nConclusion\n\nAs our society becomes increasingly dependent on information technology for a myriad of operations, the responsibility to maintain and protect computing systems increases proportionately (Kilbridge, 2003). This is because, with the increased use of computers, the cost of system failure becomes significantly higher. There are instances where the cost of a system crash may be too high and in such a scenario, avoiding the crash completely is desirable. Running software tools that warn the user of impending problems is an effective way of subverting a disaster that may arise from disk failure. Third-party software utilities such as Symantec\u2019s Systemworks have tools for monitoring the internal diagnostic capabilities of new hard drives to give an alarm in case of looming problems (Goldsborough, 2004).\n\nThis paper set out to discuss crash recovery measures that can be used to salvage a failed system. This paper began by noting that all computer systems are susceptible to crashing which may result in dire consequences for the person(s) who depend on the computer\u2019s data. It has also been noted that crash recovery procedures will vary depending on whether the crash is an OS crash or a hard disk crash. In either case, the best way to recover from a crash is to take up proactive measures such as backing up all the data on a secondary storage device. This paper has also noted that the use of the redundancy capabilities provided by RAID technology presents the best way to recover from hard disk failure. Successful crash recovery will restore the system to its pre-crash status hence enabling the individual(s) to continue reaping the advantages of his computer system.\n\nReferences\n\nBobrowski, S. (2006). Hands-on Oracle Database 10g Express Edition for Windows. McGraw-Hill Professional.\n\nGoldsborough, R. (2004). Signs of an impending hard disk crash. Teacher Librarian , 14811782, Vol. 31, Issue 3.\n\nGookin, D. (2009). Troubleshooting and Maintaining Your PC All-in-One Desk Reference for Dummies . NY: For Dummies.\n\nKilbridge, P. (2003). Computer Crash: Lessons from a System Failure. New England Journal of Medicine , 00284793, 2003, Vol. 348, Issue 10.\n\nMiller, M. (2007). Beginner\u2019s Guide to Computer Basics . Que Publishing.\n\nParsons, J.J. & OJa, D. (2010). New Perspectives on Computer Concepts 2011 . Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Effective Way to Handle a Computer Seizure Report\n\nIntroduction\n\nWhen conducting a search or a seizure it is necessary to preserve the data or information during the process of investigation. Historically, the consequences of computer crimes have involved a limited number of victims and investigations. However, this trend is changing and the impacts of the digital evidence within the conventional crime investigations have become widespread. The investigations within the private and public contexts are likely to incorporate seizure, as well as, preservation and analysis of digital information. Thus, it is necessary to have an integral mechanism to form part of the investigation process (Casey, 2011).\n\nThe data that can be obtained is of immense significance in court cases since they contain data that can be relayed electronically (Norton, 2011). These are latent evidence obtained when retrieved from the physical objects that contain them. A computer device such as software contains very delicate information that can be disrupted. Failure to deal with this information or data appropriately may lead to unusable or inaccurate decision on a criminal case. Thus, it is important to device a method of investigation that may enhance the preservation and maintenance of the integrity of the evidence (Casey, 2011).\n\nIn a crime scene, a suspect may invoke a program that can interfere with system or data files. Moreover, a single action on the targeted system could most likely affect the system files. Therefore, it is crucial for investigators experienced in the seizure of a computer evidence to preserve and seize the system. When there is nobody around, the monitor should be inspected. When there is an indication of formatting, the power plug should be pulled off very quickly. As a result, shutting down a computer requires making decision that usually depends on several parameters such as experience of the investigator as well as the type of the computer (Wilkinson, 2011).\n\nProblem statement\n\nDue to technological advancements, computers can be used to commission crime, act as evidence of crime, as well as, targets of crime. Thus, it is important to understand the nature of evidence that can be retrieved from the storage devices and how to process the crime scene. Turning off a computer have merits and demerits\u2019, hence it is important to decide if the attempt may protect the data contained in the RAM that forms the fragile memory part of a computer by considering the risks that may be involved in accidentally and unintentionally destroying the files or activating a hotkey (Wilkinson, 2011). When power is plugged off from the computer, all the data in the Random Access Memory gets lost and cannot be retrieved easily.\n\nMethodology\n\nThis study used both primary and secondary sources. For instance, information obtained from public records, opinions from expert investigators, journal articles, case studies from previous experiences, as well as, the experience from the orange county computer forensic unit. The experiences of personnel attending crime scenes and have made an initial contact with a criminal during securing or seizing an equipment from crime scenes with an aim of recovering a computer related evidence and identifying data necessary to investigate a computer related crime. The staff responsible for recovering evidence came from the Oregon County. The study also involved external witnesses whose services are very significant in the recovery, identification, as well as, interpretation of any computer based electronic evidence.\n\nFindings\n\nThe study indicates that computer seizure enables valuable crime evidence to be obtained and should therefore, be treated the same as traditional forensic evidence. The method of retrieving computer data while focusing on the continuity of the evidence as well as the integrity of investigation is very complex and costly. The experiences studied demonstrate that when computer based investigation is done correctly it provides a compelling and cost effective evidence. However, the evidences are very delicate in nature since they destroyed or changed. Therefore, it is very important to be very cautious when documenting, collecting, preserving, and examining this type of evidence\n\nAnalysis\n\nConsidering the basic elements of a computer, it is not advisable to pull the plug of networked computers without the presence of a computer specialist since failure to shut down the computer network correctly may tamper with the system and destroy all the data and other important business records. Thus, it is very important to seek assistance from a computer specialist. On the other hand, other portable devices may lose vital data when power is plugged off. Thus, when power is plugged off from a running system, any vital evidence that is stored in the encrypted volumes will be never be recovered unless the relevant key is found. It is also important to note that any potential important data may be destroyed that may damage the claims, such as the corporate data. In addition, for the retrieval of evidence, the study revealed that the main unit, the monitor, keyboard and the mouse, the leads, power supply units, hard disks, modems, flash disks, dongles and modems, the floppy disks, cameras, routers, backup tapes, CD\u2019s, cartridges, the memory sticks and the memory cards should be seized (Department of Justice, 2002).\n\nConclusion and recommendations\n\nBased on the study, it is evident that computers have been used in committing crimes and can store vital information when used by a crime victim or a subject. The importance of computers should not be taken for granted at a crime scene. Caution should be observed when collecting computer related evidence, which is very crucial because they are very delicate. When collecting evidence necessary to ascertain that the system is switched off. All the materials attached to the system should be seized, as well as, any, material that may have a password that may provide evidence. When collecting evidence all power cable should have a mark and should be disconnect from ports. It is necessary to make hand sketches to assist in setting the systems later. Moreover, it is useful to take pictures that might be helpful in documenting the entire scene in general. The seized materials should be properly marked and if possible, the personnel involved should be noted. Consequently, if a forensic examination is required, a request should be made immediately since it will assist in the determination of the criminal evidence and specific issues that are relevant to the overall investigation. As such, several factors are essential in achieving these goals. These include special materials as well as external help (Department of Justice, 2002).\n\nReferences\n\nCasey, E. (2011). Digital Evidence and Computer Crime. Forensic Science, Computers, and the Internet. New York: Academic Press.\n\nDepartment of Justice (2002). Electronic Evidence and Search & Seizure Legal Resources. Computer Crime & Intellectual Property Section : United States Department of Justice . Web.\n\nNorton, Q. (2011). Byte Rights: A New McCarthyism . Maximum PC. Web.\n\nWilkinson, S. (2011). Good Practice Guide for Computer-Based Electronic Evidence. Metropolitan Police Service, E-Crime Working GROUP . Web.\n\nApppendix\n\nAppendix 1: A graphic of a seized site by Homeland Security due to copyright\n",
        "label": "human"
    },
    {
        "input": "Impact of Computers on Business Essay\n\nThe advent of the 21st century saw mass adoption of commercial electronics in various industries which led to the emergence of the digital age. While operational business computers were introduced as early as the 1970s, the devices everyone is familiar with today and can easily use for daily functions came in the late 1990s. With decades, computers have grown to be more complex but powerful, usable for a variety of purposes, and adaptable. As business massively adopted computers the very core nature of doing business and managing operations changed for all stakeholders (Petersen, 2019). This paper seeks to explore the impact of the computer and technology on the business world.\n\nData Storage and Manipulation\n\nPrior to computers, all information was stored on paper. Information had to be written or typed and organized in complex bookkeeping in each business. This was ineffective, time-consuming, and took-up significant resources. Paper files could be easily misplaced, human error was prevalent, and it was costly. Computers allowed for digital file storage, which completely revamped how businesses collected, stored, and used data and information, both about their own business and outside contexts. Once the Internet arrived, data could be stored even more safely, in virtual clouds, while information was accessible at a moment\u2019s notice (IBM, n.d.). Digital file storage presented many benefits, including organizing and storing data with greater ease, reducing human error, creating back-up copies, and allowing for search of that data to find the necessary information at a moment\u2019s notice. Any given file can be retrieved, shared, and manipulated in any means needed for the business in a matter of seconds.\n\nAs it is well-known, the very concept of digitally collected data has fundamentally changed how businesses operate. While business decisions were reliant on empirical data in the past, it was rudimentary statistics based off data points collected and reported on paper. Digitization of data has now meant that data points from both historic and present operations could be used. First, the shift in various processes as discussed later had allowed for much more sophisticated data point gathering in both internal functions and commercial endeavors. At last, computers provided opportunities to perform highly complex manipulations, statistics, and calculations based on a range of algorithms (Leonard, 2018). Depending on the business, this could provide highly accurate and predicting data strategies in their enterprise decision-making and commercial success.\n\nJob Functions and Processes\n\nAs mentioned, the digital transformation brought companies into a completely new space. Multiple business functions critically changed, redundant ones were removed, and new ones added. The core function of most businesses is now to operate within the digital realm whether through marketing, building logistics chains, e-commerce, and others. Work could be done much faster on computers and new programs were developed for various functions ranging from finance to HR to team management. Many corporations had to transition, with the majority of the global workforce now working from behind a computer. The computer age was one of the catalysts to bring about the white collar \u2018revolution,\u2019 essentially shifting many jobs from blue-collar industrial jobs or low-skilled labor into the office (Browne, 2017). This was especially relevant for highly developed countries, which then outsourced much of the manual labor to the developing world. Therefore, the computer not only brought changes to the companies and their processes but has fundamentally shifted the economy and ultimate geographical distribution of labor, essentially contributing to the globalization of business processes.\n\nCommunications\n\nThe foundation of all businesses from pre-digital times to modernity has been communication. Both internal communication among staff to external communication with stakeholders, clients, and consumers. The computer and digital tools have revolutionized the art of communication in business. Originally, when computers emerged, e-mail became the popular tool of communication, and still is highly used and effective. It allowed to quickly transmit written messages in a matter of seconds, distributing information to whomever and to as many people as needed (LaMarco, 2019). As technological solutions developed, various other tools emerged, such as corporate instant messaging services and the latest of which (under remote working), video conferencing to replace face-to-face meetings but being able to still discuss information in greater detail and human interaction than would be possible via written messages (Harris-Briggs, 2018). These digitally driven communication methods have impacted business structures and operations towards more effective information sharing and reduced inefficient face-to-face contact and meetings (Trint, 2021). Furthermore, the more effective and affordable communication is, which digital solutions are typically free or highly cost-efficient, the more businesses can save.\n\nMobility and Flexibility\n\nComputers brought a certain level of freedom to many businesses. As discussed, the ability of one device to perform virtually all backend tasks to business operations has provided much greater flexibility. As computer technologies developed with powerful laptops, tablet PCs, and smartphones, it has given rise to what is known as IT mobility. An individual does not have to be physically present at the workplace to do their job or even monitor key aspects of manufacturing or other physical operations. This has given much social flexibility, as was demonstrated by the COVID-19 pandemic when people were forced to work from home (ColoradoSupport, n.d.). It also allowed for international business to develop as an employee could be halfway across the world fulfilling their duties.\n\nModern developments such as cloud computing allows to store information and even generate the computing power on remote servers, essentially streaming the data to any eligible device. This means that business employees can engage in virtually any task with a good internet connection, regardless the computing power of their respective device, because the computing power is done on the servers. In contemporary contexts, this IT mobility offers significant benefits to businesses in terms of flexibility and fulfilling their needs as any employee they need can be remotely found to fulfill tasks as needed.\n\nThe Internet of Things\n\nThe internet of things (IoT) is a web of physical electronic devices ranging from computers to sensors to other technologies which are interconnected within the same network, allowing to exchange data with other devices and systems over the Internet and communication networks. It is estimated that by 2025, 55.7 billion connected devices will be in the marketplace, with 75% being part of an IoT platform (Consolidated Technologies Inc., 2021). IoT is a critical technological development in businesses across various industries. This can range from smart light bulbs to networks of industrial machines with oversee the manufacturing process and report data back. Using the series of devices and sensors, IoT allows to collect and then analyze massive amounts of data on behaviors, processes, environments, and other key points. The machines can then enact various forms of change, either under human direction or via artificial intelligence to self-correct and make processes more efficient (Zhang & Wen, 2016). This takes the components of data storage and analysis discussed earlier and takes to a new level.\n\nIoT impacts businesses primary by offering accessibility to tremendous amounts of data in regard to their processes. As mentioned, it can track both physical performance of manufacturing, as well as digital characteristics such as customer behaviors and product sales. IoT allows for continuous optimization of business processes and can serve as a tool for strategic decision-making, improving key performance indicators, and even benefit in human resource management such as team engagement. The more complex and comprehensive the technology becomes, the more data points it can collect and greater range of uses that it can be applied to across industries. This represents computers taken to a new generation of computing devices.\n\nE-Commerce\n\nE-commerce is the latest digital development of businesses, which has grown to a multi-billion-dollar market. E-commerce refers to the buying and selling goods over an electronic network, particularly the Internet. As e-commerce grew in popularity, driven in part by massive online retailers including Amazon and global events such as the COVID-19 pandemic, businesses began to shift their sales online virtually in any way possible. A business typically conducted three major elements online that were payment processing, website development, and advertising. E-commerce added on it, also simplifying the sale process significantly for both, the seller and the consumer. In both cases, business-to-consumer and business-to-business interactions, the buyers could list catalogs of available goods either on the seller\u2019s website or through third party retailers. Upon selecting necessary goods, they can place the order with all necessary details, pay online which would immediately go through, and all would be left is for the seller to ship the goods or offer the service, with logistics costs often built-in to the price (Chai, n.d.).\n\nE-commerce in its rudimentary forms was around as early as the 1990s, but it has had tremendous impact on businesses. In many cases, doing business online has shifted entire business models or led to the creation of new ones. Some examples are Netflix who transitioned from a physical media business into fully online streaming, as well as Paypal, a payment processing company which saw a gap in the market of digital payment systems which at the time was necessary since not every retailer could easily set up credit card processing. In 2022, e-commerce is one of the primary forces of the current market economy and the majority of business is done online via computers and smartphones. It has also shifted the way companies do business, in terms of advertising, ease of access to purchase products, and other means to make the experience for consumers smooth and flawless.\n\nConclusion\n\nComputers are one of humanity\u2019s most innovative inventions, significantly revolutionizing the world. The majority of systems, platforms, and processes that everyone is familiar with today came as a result of the computer and the subsequent digital and technological revolution. Businesses, which are commonly complex organizations, rely heavily on consistency, functionality, communications, and sales of goods and services. The computer was able to effectively combine, simplify, and multiply efficiency of all the core business processes. In turn, this has allowed for the development of ultra large corporations with global logistics and networks that can function to provide goods and services around the world.\n\nReferences\n\nBrowne, C. (2017). How have computers changed the workplace?\n\nChai, W. (n.d.). E-commerce .\n\nColoradoSupport. (n.d.). The importance of IT mobility for your small business .\n\nConsolidated Technologies Inc. (2021). Internet of things in business .\n\nHarris-Briggs, N. (2018). 7 advantages of technology in business communication .\n\nIBM. (n.d.). What is data storage?\n\nLaMarco, N. (2019). Uses for computers in business. Chron .\n\nPetersen, L. (2019). Importance of computers in business . Chron .\n\nLeonard, K. (2018). The role of data in business . Chron .\n\nTrint. (2021). How digital communication has changed the world of business forever .\n\nZhang, Y., & Wen, J. (2016). The IoT electric business model: Using blockchain technology for the internet of things . Peer-To-Peer Networking and Applications, 10 (4), 983\u2013994.\n",
        "label": "human"
    },
    {
        "input": "The Attitudes of Acceptance and Resistance Toward Computerization in Hospital Term Paper\n\nIntroduction\n\nAs computer generation quickly grows, many institutions should comply with it for easy running. The main component attracting attention for these institutions has been the use of Information Communication and Technology (ICT) sometimes referred to as information technology (IT).\n\nHospitals handle extremely sensitive issues concerning people\u2019s health. One way to fulfill has been health record keeping of their patients (Van der Meijden 238). However, most of the hospitals have been using the paper-based health records that are not fully secure in some instances. As a result, there has been a suggestion of computer-based heath recording to replace the previous system that is tedious also physical in handling many records. Hospitals across the United States have decided to computerize their health recording to alleviate these problems. Uses of ICT and Electronic Health Records (EHR) have been on the rise given the benefits accrued to utilization of these technologies (Badger et al. 1).\n\nHowever, adoption of these technologies has not been easy in these hospitals as various stakeholders in hospitals expressed different opinions on computerization of some functions in the hospital (William 1). This study will address how these stakeholders differently perceive computerization in hospitals.\n\nBackground Information and Research Problem Definition\n\nThe United Stated has been using the paper-based health recording for many years until when the Katrina Hurricanes strike the United States gulf coast in 2005. Paper medical records faced massive destruction, which made tracking of health records hard. More so, this disaster devastated many health care providers. Activities in most of these health facilities almost came to a standstill but the U. S. Secretary of the Department of Health and Human Services had to acts quickly to resolve the situation. As a result, the department had to develop better methods to secure health records in the future. The major way to this utilization, of Electronic Health Records to facilitate quick retrieval of patient\u2019s information under any circumstance.\n\nNo sooner had the department aired their plans than various health care providers raised concern on the implementation of these EHR. This has led to a difference in attitudes among them hence; there have been acceptance and resistance to this move by these health care providers. Therefore, this paper investigated the factors that might have contributed to either acceptance or resistance toward computerization of hospitals.\n\nResearch Questions\n\nFrom the introduction and background information, the following were questions that this research sought to answer:\n\n 1. What are the advantages and disadvantages of computerization in hospitals?\n 2. What drives resistance among the concerned stakeholders in the hospitals toward computerization?\n 3. What are the possible computer applications used in a successful computerization and their area of interest in hospitals?\n 4. Could there be any correlation between the advantages of computerization and acceptance of the technology?\n 5. What is the responsibility of SAS data analysis in bioinformatics, in a hospital setup?\n\nResearch Scope\n\nUse of computerized functions has been on strong demand in most institutions given the benefits. However, not all the stakeholders have warmly welcomed such paradigm shift because of reasons either known to them or revealed by research. Therefore, this research study concentrated on the hospital setup where peoples\u2019 health is paramount, and everything always ensured working all the time to alleviate any disruption in handling patients. Computerization of some functions has been the primary method to facilitate this situation, but there are issues surrounding this health care in adopting this initiative, especially the EHR. The research study narrowed down to examine the reasons health care providers have either accepted or rejected computerization. In addition, factors fueling either of these opinions helped in analyzing their validity and contributions.\n\nResearch Limitations and Ethics Consideration\n\nLimitations in many researches are unavoidable because they occur without even the researcher\u2019s knowledge or the prevailing conditions. However, the researcher should be aware of these limitations as well as applying the research ethics to handle some of these limitations. Therefore, these were significant limitations faced during this research.\n\n 1. There was insufficient time and resources to facilitate the whole research period. Time allocated to carry out the investigation was inadequate to investigate all the issues contributing to variation attitudes toward computerization. In addition, there was little financial support to collect primary data, which is especially valuable.\n 2. Not all issues affecting the difference in attitudes could be present in the context by the end of the research because of inadequate information in some questions.\n 3. Lack of basic information led to use of secondary data only. Most of these secondary sources were outdated thus, affecting their validity and reliability.\n\nThe ethics considered during the research process were:\n\n 1. All the quoted or paraphrased data had a reference to give credit to researchers in other previous studies on the same topic.\n 2. Data analysis ensured no modification to maintain its validity and reliability. Any alteration could have led to biasing of the outcome to fit an opinion, which could have undermined the importance of the research.\n 3. Time allocated for research was used properly to avoid lateness and avoidance of some areas required for learning. The results acquisition was in time and the results presented in this paper are data got from the literature reviewed.\n\nResearch Methodology\n\nData collection and analysis from secondary sources utilized both inductive and deductive research approaches because of their complement in data acquisition. Data collected were mainly through a qualitative research method. Unlike the quantitative research method, data collected through qualitative approach are immeasurable using numerical standards. Instead, the figures reflected the perceptions, opinions, emotional, and obscure information expressed in percentages or probabilities.\n\nThe research had to get information to address difference opinions expressed by various health care providers. There were no defined answers to questions asked to them hence; measurement of percentage could have been difficult. Therefore, the research used a qualitative approach.\n\nData required for this research could have been either from secondary or primary sources. Primary data results from primary sources using different methods such as interviews, surveys, case study, focus groups, among others. Data from these sources forms the basic research because the data offer firsthand information from the population studied. However, collection of primary data is expensive because these methods consume many expenses in planning and the activity.\n\nOn the other hand, secondary data were acquired from secondary sources such as journals, online sources, publications, newspapers, among others. The advantage over the primary data is the inexpensiveness in acquiring them. Acquiring these data is less expensive because they are readily available from these sources. Therefore, this study used the secondary data only to meet the research question.\n\nThis research was possible through a conducting an in-depth literature review on the topic from previous work published by other researchers. The literature review addressed various aspects of the research topic in which answering of various research questions was possible. The researcher formulated some recommendations as well as making conclusions on the topic after answering these questions.\n\nLiterature Review\n\nComputerization in hospitals has contributed too many benefits in the United States and the rest of the world. However, agreements among different health providers have ranged from positive to negative attitudes. These differences in attitudes triggered research into this issue to know what the contributing factors to these divisions in perceptions are. As a result, various researchers, academicians, health providers, and scholars have showed interest in this issue. Many researchers have been done on the question by individual researchers who came up with different perspectives to this topic. This continually has expanded the knowledge base of computerization in hospitals and differences in opinions.\n\nThis have led to development of information on how computerization in hospitals has received acceptance or if the level of rejection is declining or increasing. This research conducted an in-depth literature review of the previous work to create a decision on the research topic.\n\nAccording to William (1), knowledge in ICT has been of enormous importance to nurses in their line of duty. ICT has enabled nurses to improve health care service delivery to their patients. It is advisable for nurses to accept ICT progress in their jobs because patients care is essential. ICT will be of much help to nurses in making wise decisions when taking charge and managing their patients.\n\nAttitudes toward ICT phenomenon have been a determinant in adoption of computerization. This situation has slowed efforts by some hospitals toward paperless systems.\n\nFactors Influencing Attitudes toward Computerization\n\nBefore introducing computerized services, addressing the attitudes exhibited by the health workers is important. The attitudes by the workers initially determined computerization. Computerization is achievable if only the nurse managers have a positive attitude. Difference in computer skills levels among the health workers in hospitals might be one major determinant to acceptance or rejection of computerization in hospitals. Health workers with excellent skills in computers easily accepted computerization because it could reduce their work. However, lack of computer skills among the workers can be a hindrance to computerization because the health workers are not used to computer operations.\n\nEfforts to establish computerized functions can be discouraging to these workers because it can be a way of eliminating them from the system. In case the low skilled workers accept computerization, it takes a long time to learn all the requirements, which may affect their work output and effectiveness.\n\nNgin and Simms (55) revealed that variation in skills level could help in dealing with them based on their capability to operate at different levels. Appreciation of workers both skilled and non-skilled in computers can create a basis of motivating the low skilled workers. As a result, they can take computerization after getting little duration training in computers use. However, sidelining the low skilled workers could lead to rejection of computerization in hospitals.\n\nA positive attitude among health workers toward the use of computers is significant because computer use in both academic and practical setup (Mcbride and Nagle 164).\n\nKivuti and Chepchirchir (1) cited various factors that have influenced agreement or rejection of computerization as age, experience in computer use and educational preparations. However, there might be other factors influencing attitudes toward computerization hence; more research is helpful in to coming up with more factors. Various researchers have investigated age factor toward opinion attached to computerization (Krampf and Robinson 32).\n\nPrevious studies revealed that young health workers were more positive to computerization than elders were. However, recent studies have disqualified the age factor as a determinant as the acceptance of computerization has increased in all ages categories (Parker and Abbot 1015). This means that age factors currently is invalid in determining the attitude toward computerization.\n\nThe present generation has been exposed to computer use, unlike in the past, when remarkably few people had experience in using computers. Studies have revealed a continued acceptance of computerization in hospital among the workers. The positive attitude has been on an increase, but a struggle to computerization has a negative attitude toward computerization is still evident in hospitals.\n\nSimpson and Martins (39) have attributed the opposition to lack of knowledge in operating computers to the nature of work and applicability of computers currently and in future (McBride and Nagle 170).\n\nNurses with high education preparedness to computerization had a positive attitude. According to Henderson et al. (30), the relationship between working experience and consent of computerization are being inconsistent. However, Henderson, and others were in the opinion that work experience was a determinant, but Ash, and others denied such claims.\n\nA short-term training session in computer use among health workers was a determinant toward adherence to computerization. Trained workers were more favorable to computer use. According to Braude (150), attitudes toward computerization depended on the area of work. For instance, nurses in pediatrics can take computer use more easily compared to nurses in medical surgical. More so, workers in health recording should be the most suited for computerized functions with the use of electronic health recording.\n\nInteraction with computers in the hospital determined the attitude toward computerization. Workers regularly handling computers in the service area were more likely to accept computerization than those who rarely sit next to a computer (Chan 59).\n\nUse of computers in hospitals has been a significant challenge to physicians and health workers (Mandell 316). This has separated the experienced and non-skilled in computer usage. However, technology has been in a progressive development enormous even to the skilled personnel. Evolution in technology has also separated between the proficient and tertiary health workers in the technological field. Some hospitals have advanced in new technology integration into all the institutions. Nowadays, there is software loaded into a computer for data computation, data review among other significant functions. This software has been on demand from the hospitals to manage their patients in a better manner.\n\nAs noted earlier, bioinformatics remain one major constituent in hospitals, especially in health recordings. Some software such as SAS has been essential in statistical data analysis. SAS among other software have eased duties allocated to health workers. Some software is capable of analyzing data after computation through data entry. The results from this analysis have been of much significance in making a decision on patients.\n\nPerceptions by Non-Health Workers to Computerization\n\nMost of The researches have ignored the attitudes of the patients and their affiliated families and friends toward computerization. Computerization had a tremendous support from people outside the hospital setup such as patients, other workers, and the civilians. Computerization has eased patient-hospital relationship as patients have expressed their satisfaction for services currently delivered by different hospitals. Patients spend less time in making enquiries in hospital as observed before. Use of software to monitor the heath status closely for patients involved in emergency cases saved many lives (Braude 150).\n\nComputerization has helped the patients in knowing their health standards through data access to perform a review of health tips, recommendations, and so on. For example, information on body weight and height submitted to a computer to verify health fitness of a person. The results may recommend physical exercise, and change of diet.\n\nAdvantages of Computerization Ion Hospitals\n\nRegardless of differences the attitudes among different stakeholders, computerization has several advantages.\n\nMaking the work easier\n\nUse of the computer has saved the workers much time that might have been wasted in physical activities. Workers should submit information to a computer, after which the computers have software to perform analysis. This has eased the previously work done by former workers.\n\nImprovement of accuracy in health records\n\nUse of the computer has led to accurateness of patient\u2019s health information. As long as you enter the right information about the patient, the data store in the computer remains constant unless outside forces vary. Alteration may be through hacking processes done by malicious people. However, hospitals can avoid such occurrences through powerful data encrypting methods as well as securing their website by firewalls (Lorenzi and Robert 117).\n\nQuick retrieval of patients records\n\nUnlike the paper-base health recording, computerization has enabled quick access of patients\u2019 health information on a request. Only few clicks done to gain access to the information in retrieving information. Previously, some minutes or even hours became wastage in retrieving a patient\u2019s heath record.\n\nDisadvantages of Computerization in Hospitals\n\nComputerization has its better and the worse part. Therefore, the following are the obstacle associated with computerization in hospitals.\n\nHigh costs of establishing a computer system and purchases of software\n\nIt is expensive to buy computers, especially if the hospital targets to receive fast and efficient computers. The cost of buying original software is even higher unless the hospital settles on locally developed software.\n\nLack of computer experts in hospitals\n\nComputers like any other machine require regular maintenance to maintain their functionality. Software loading into computers makes them more susceptible to virus infection. This may hinder functioning of these computers to avoid further destruction of data stored in as a computer\n\nLaying off some hospital workers\n\nComputerization leads to ease of service in hospital because one computer can supply different departments simultaneously. Therefore, adoption of computerization in hospital might lead to laying of some workers because computers replace them.\n\nConclusions and Recommendations\n\nFrom the above in-depth literature review, various issues were open to the research conducted. The division in attitude toward hospitalization has continually taken different dimensions as time goes on. One factor might have led to either acceptance or rejection of computerization at one time, but with time, it became less determinant, for example, age.\n\nThe investigation revealed age, level of experience, preparedness to work, among other factors as a determinant to either positive or negative attitude toward hospitalization. However, there were deficits in researching for more factors as revealed by many researchers.\n\nThere was a correlation between the advantages of computerization and the amount of compliance in the hospitals. Most of the involved stakeholders accepted computerization based on the benefits it had to patients. On the other hand, rejection of computerization had little to do with disadvantages of computerization. Instead, those rejecting computerization had a personal problem to it or the conditions they worked suppressed efforts to incorporate computerization.\n\nEvolution in technology has improved service delivery in hospitals as new software are aiding in the proper handling of patients. However, problem in latest and improved software has proven hard for many people (Lorenzi 204). As computerization in hospitals become a reality, there are opinion divide among the associated stakeholders such as doctors, clinicians, nurses, registrars, among others. The move to switch from manual activities to electronic and digitalized functions has not been easy. Some of the directly affected stakeholders warmly have accepted this initiative, but there is yet another group still criticizing such action. Therefore, more researchers have shown interest on this issue to determine the potential drivers to either acceptance or hostility toward computerization in hospitals. An exciting showdown may emerge as the computer-based health records replace the paper-based health records.\n\nThere were challenges faced in the completion of this research study. For instance, there was no in-depth data analysis in most of the literature review. Therefore, this research came up with different recommendations.\n\n 1. There is a need to do more research studies on this topic because the already documented data have not been consistent. There is unclear trend in the factors influencing the attitudes toward computerization.\n 2. Hospitals in the United States should conduct a research before computerizing the functions in their institutions to avoid rejection by their workers. This may suspend the functions of the hospitals, something that avoidable by consulting the workers on the suitability of such proposal (Ash et al. 234).\n 3. Patients should request for more computerization ensure fit bodies. Leaving decisions to computerization to health workers might not produce any fruits as the exhibit different attitudes.\n 4. A formal strategic plan execution should promote a more welcoming response to computerization. Rejection should distance from personal interest. Instead, addressing the factors leading to dismissal could enable a smooth transition from paper-base systems to electronic-based systems.\n\nWorks Cited\n\nAsh, Joan, Zoe Stavri, and Gilad Kuperman. \u201cA Consensus Statement on Considerations for a Successful CPOE Implementation.\u201d Journal of the American Medical Informatics Association 10.3 (2003): 229-34. Print.\n\nBadger, Stephen, Ryan Bosch, and Praveen Toteja. CEO Leadership: Seven Strategies for Leading Successful EHR Implementations . San Diego, CA: HIMSS 2006 Annual Conference and Exhibit, 2006. Print.\n\nBraude, Robert. \u201cPeople and Organizational Issues in Health Informatics.\u201d Journal of the American Medical Informatics Association 4.2 (1997): 150-51. Print.\n\nChan, Mariachi. \u201cFactors affecting knowledge, attitudes, and skills levels for nursing staff toward the clinical management system in Hong Kong.\u201d Computers, Informatics Nursing 27.1 (2009): 57-65. Print.\n\nHenderson, Richards, Peter Deane, and Johnstone Ward. \u201cOccupational differences in -related anxiety: implications for the implementation of a computerized patient management information system.\u201d Behavior & Information Technology 14.1 (1995): 23-31. Print.\n\nKivuti, Lucy, and Angelina Chepchirchir. \u201cComputerization readiness.\u201d Online Journal of Nursing Informatics (OJNI) 5.1 (2011):1. Web. 19 Feb. 2012.\n\nKrampf, Scholes, and Steve Robinson. \u201cManaging nurses\u2019 attitudes toward computers.\u201d Nursing Management 15.1 (1984): 29-34. Print.\n\nLorenzi, Nancy. \u201cThe Cornerstones of Medical Informatics.\u201d Journal of the American Medical Informatics Association 7.2 (2000): 204. Print.\n\nLorenzi, Nancy, and Riley Robert. \u201cManaging Change: An Overview.\u201d Journal of the American Medical Informatics Association 7.2 (2000): 116-24. Print\n\nMandell, Steven. \u201cResistance to computerization: An examination of the relationship between resistance and the cognitive style of the clinician.\u201d Journal of Medical Systems 11.4 (2011): 311-318. Print.\n\nMcBride, Samuel, and Livingstone Nagle. \u201cAttitudes toward computers: A test of construct validity.\u201d Computers in Nursing 3.1 (1996): 164-170. Print.\n\nNgin, Patel, and Lamina Simms. \u201cComputer use for work accomplishment: A comparison between nurse managers and staff nurses.\u201d The Journal of Nursing Administration 26.3 (1996): 47-55. Print.\n\nParker, Jones, and Phillips Abbott. \u201cThe new millennium brings nursing informatics .\u201d AORN Journal 72.1 (2000): 1011-1017. Print.\n\nSimpson, Gerald, and Kendrick Martins. \u201cNurses\u2019 attitudes toward computerization in clinical practice in British General Hospital. \u201d Computer Nursing Journal 15.1 (1997): 37-42. Print.\n\nVan der Meijden, Miloscav. \u201cDeterminants of Success of Inpatient Clinical Information Systems: A Literature Review.\u201d Journal of the American Medical Informatics Association 10.3 (2003): 235-43. Print.\n\nWilliam, Schecter. \u201cResistance to change and hospital physicians\u2019 use of electronic medical records: A multidimensional perspective.\u201d Journal of the American Society for Information Science and Technology Jan. 2012:1. Print\n",
        "label": "human"
    },
    {
        "input": "VisualDX: Human-Computer Interaction Essay\n\nVisualDX is a type of software developed to assist medical personnel in making a decision on the diagnosis of various ailments and clinical disorders. The initial systems were developed by logical Images. Human-computer interaction is obvious in the application of visuals. This software system may give a diagnosis with reliable precision if the medical personnel\u2019s using it applies its functions appropriately. With the advancement of software engineering, the graphical interface used by visualDX may be able to recognize clinical conditions which can be observed in high-quality images with minimal errors. Often, the HCI of visualDX generates images of the most fitting medical condition. In addition, it generates images of other related conditions and ailments. This enables the clinician to make a proper judgment on whether the application is displaying the most accurate results.\n\nModern versions of visualDX have a vast collection of conditions whose symptoms can be displayed in a graphical form. The software system is able to make a comparison between a large number of conditions in previous records, and the particular condition under scrutiny beyond human recollection. This may often avoid misdiagnosis and therefore, ensure the right prescription by the physician. The use of this system gives fairly consistent results and may arguably be used to improve healthcare delivery with the desirable outcome (Berner, 2007).\n\nThe interface requires the user to feed data into the system. Regulatory procedures and classical methods of diagnosis may not accommodate the use of software and computerized machines to make clinical decisions as this is a sensitive field. This system has yet to be modified to synchronize with the normal working procedures. In my opinion, it would be fair to recommend this system because odds against it are no more than there are in other medical machinery.\n\nResponse One\n\nVisualDX is structured such that the user is guided through the steps of using the software system without having to be a software specialist. This makes the system usable by people with different specializations. The dynamic system may keep track of images and searches so that they can be conveniently accessed again if when necessary. VisualDX is an accommodative system that enables adaptation to a wide range of problems. The modular manner in which most of the system is organized facilitates quick access to the desired interface. The graphical representations by visualDX are of fairly high definition and thus collectively generate a user-friendly interface. This enables the medic or health professional to make an assessment more or less like the classical procedure (Craft 2010).\n\nResponse Two\n\nThe visual interface found in visualDX is particularly useful in dermatological research and diagnosis as visual analysis is of prime importance. This system may require repeated data input with every clinical case. This may make the system prone to inconsistencies but could also avoid misdiagnosis where the system assumes the profile of another different condition of similar visual symptoms. Although this could be a setback to the user, it could be important for the sake of accuracy (Berner, 2007). VisualDX interface supports different conventional image formats adding to its flexibility. Another advantage of the VisualDX is that when results are generated and displayed, ailments with similar visual symptoms are output from the database and displayed enabling balanced decisions.\n\nCould this application be related to dermatological medicine more than any other field?\n\nReferences\n\nBerner, E. S. (2007). Clinical decision support systems: theory and practice (2nd ed.). New York: Springer.\n\nCraft, N. (2010). VisualDx: essential adult dermatology . Philadelphia, Pa. [u.a.: Wolters Kluwer Lippincott Williams & Wilkins.\n",
        "label": "human"
    },
    {
        "input": "Microchip Computer Corporation\u2019s Financial Planning and Budgeting Essay\n\nIntroduction\n\nFinancial planning is an integral part of management. As companies set goals, financial planning provides a means of achieving these goals and reducing risks associated with poor management (Alviniussen & Jankensg, 2009). Microchip Computer Corporation Company has a sales goal of +10% annual growths. From the available data, it is evident that Microchip Computer Corporation had a slight increase in sales in year 2005 as compared to the year 2004. The sales decreased gradually in 2006 and 2007, whereas a considerable increase in net sales records in 2008. In determining the trend of the general growth rates in Microchip Computer Corporation, the essay analyzes the net sales for five years (2004-2008) and determines their year-to-year percentage growth.\n\nMicrochip Computer Corporation\n\nSelected financial data\n\nFiscal year        2008    2007     2006     2005    2004  \nNet sales ($)      8,334   6,141    9,181    11,933  11,062\nPercentage growth  37.71%  -33.11%  -23.06%  7.87%   \u2013     \n\n\nThe year-to-year percentage growth rate\n\n 1. From 2004-2005\n     1. Net sales in 2004=11,062\n     2. Net sales in 2005=11,933\n     3. Increase in net sales from 2004 to 2005 is 871\n     4. Percentage growth is (871/11062)*100=7.87%\n 2. From 2005-2006\n     1. Net sales in 2005=11,933\n     2. Net sales in 2006=9,181\n     3. Increase in net sales from 2005 to 2006 is -2752\n     4. Percentage growth is (-2752/11933)*100=-23.06%\n 3. From 2006-2007\n     1. Net sales in 2006=9,181\n     2. Net sales in 2007=6141\n     3. Increase in net sales from 2006 to 2007 is -3040\n     4. Percentage growth is (-3040/9181)*100=-33.11%\n 4. From 2007-2008\n 5. Net sales in 2007=6,141\n 6. Net sales in 2008=8,334\n 7. Increase in net sales from 2007to 2008 is 2193\n 8. Percentage growth is (2193/6141)*100=35.71%\n\nMicrochip Computer Corporation has an irregular trend in the net sales. Year 2006 and 2007 record very low sales with an upward short in the total net sales in 2008. If the company records a +10% revenue, the target revenue figure is as below:\n\nNet sales in 2008=8,334\n\nIf the company anticipates obtaining a +10% revenue in 2009, the 2009 net sales will be 110% of 2008. Thus, 2009 net sales =110% *8,334= $9,196.4.\n\nIf all factors are held constant, there is a probability that the company recorded a +10% or even more annual revenue in 2009. It is noteworthy that a lower trend in net sales was experienced in two years, 2006, and 2007. Therefore, there is a probability to experience the upward trend for at least two years before experiencing changes (Meigs, Walter, & Robert, 2000). In any case, the management committee would have identified the problem that caused the downward trend in sales, solved the problem, and thus experienced a 35.71% net sales increase in 2008. In fact, the sales in 2009 would increase further than anticipated.\n\nPercentage of sales method\n\nThe percentage of sales method is a tool for financial forecasting (Geoffrey, & Adam, 2002). The method is applied to forecast Micro Chip\u2019s Consolidated Statement of Operations for the period of September 26, 2008 through September 25, 2009. An assumption of a 25% increase in sales, a 15% tax rate, and restructuring costs of 5% of the new sales is applicable in projection of growth. Thus, the financial data is calculated as follows:\n\nNet sales\n\nThe net sales for 2008=8,334\n\nAnticipated increase by 25% gives 125% of 8,334=10,417.5 as net sales for 2009.\n\nTaxes\n\nA 15% tax rate is charged of the new sales figure. Thus tax =15%*10,417.5=1,562.625\n\nNet income\n\nRatio for net income to sales in 2008 is given by net income/Sales = 811/8334=0.097\n\nThus anticipated net income for 2009= 0.097 of 2009 (net sales-tax) =0.097*(10417.5-1562.625) =861.6875\n\nDividends\n\nRation for Dividends to Net Income in 2008 given by dividends/net income.\n\nThus, earnings (loss) per common share given as:\n\nBasic =5.65/811= 0.00697. The anticipated equivalent of 2009 is 0.0067*861.6875=6.003\n\nDilute is =4.64/811= 0.0057. The anticipated equivalent of 2009 is 0.0057*861.6875=4.93\n\nCash, cash equivalent, and short-term investments\n\nRatio for cash to sales in 2008 is given by Cash/Sales = 5,426/8334=0.65\n\nThus anticipated cash for 2009= 0.65of 2009 net sales=0.65*10417.5=6,782.5\n\nCosts\n\nA restructuring cost of 5% of the new sales is anticipated\n\nThus, the cost is 5%*10,417.5=520.875\n\nMicrochips\u2019 Consolidated Statement of Operations\n\nFor the period of September 26, 2008 through September 25, 2009\n\nFiscal years                                               2008     2009     \nNet sales ($)                                                                \nTaxes                                                                        \nNet income(loss) ($)                                       8,334    10,417.5 \nEarnings (loss) per common share                           \u2013        1,562.625\n 1. Basic                                                  811      861.6875 \n 2. Dilute                                                                   \n                                                           5.65     6.003    \nCash dividends per common share                            4.64     4.93     \nShares used for computing earnings (loss) per share(\u2018000)                    \n                                                           143,500  143,500  \n 1. Basic                                                  174,900  174,900  \n 2. Dilute                                                 5,426    6,782.5  \n                                                           6,161    7644.18  \nCash, cash equivalent, and short term investments          300      300      \nTotal assets                                               3,104    3,104    \nLong-term debt                                             \u2013        520.875  \nShareholder\u2019s equity                                                         \nRestructuring costs                                                          \n                                                                             \n\n\nConclusion\n\nIt is noteworthy that the financial statements for the period 2004-2008 do not give details of taxes. Therefore, the resulting irregular trend of sales could arise for such reasons and exclusions of other costs and revenues in the financial statement (Sullivan, & Steven, 2003). It is in 2009 predictions that we consider tax expenses. Various assumptions are applicable in predicting 2009 figures in the financial statement using the percentage of sale method. It is predictable that a number of shares used to compute earnings and losses per share do not change over the two years. It is also predictable change in sales do not affect long-term debts, and thus the figure remains the same (Jordan, 2003). While the figures may have allowance of errors, they reflect true and precise estimates of figures for 2009.\n\nReferences\n\nAlviniussen, A., & Jankensg,.H. (2009). Enterprise risk budgeting: Bringing risk management into the financial planning process. Journal of Applied Finance, 19 (1), 178-192.\n\nGeoffrey, S., & Adam S. (2002). Citizens\u2019 Budget Reports: Improving Performance and Accountability in Government . Web.\n\nJordan, L. G. (2003). Strategic budgeting. The Journal of Government Financial Management, 52 (1), 44-52.\n\nMeigs, K., Walter B., & Robert F. (2000). Financial Accounting (4th ed.). New Jersey: McGraw-Hill.\n\nSullivan, A., & Steven M. (2003). Economics: principles in action . New Jersey: Pearson Prentice Hall.\n",
        "label": "human"
    },
    {
        "input": "Computer-Aided Software Engineering Tools Usage Essay\n\nComputer-aided software engineering, which is abbreviated as CASE, refers to tools employed in the development of information systems (Rock, 1990). In other words, these tools are used to engineer efficient and maintainable software products. First, to use CASE tools effectively, it is paramount to assemble all the necessary tools. Second, it is essential to have a decent layout that necessitates easy access to and use of those tools. Third, handling CASE tools requires a skilled person. CASE tools fall under two distinct categories: upper and lower.\n\nThose which are found in the upper category include specification, requirement, design, and planning. On the other hand, those which fall under the lower category are integration, implementation, and maintenance (Maxim, 2010).\n\nCASE tools are anchored on various building blocks. The first building block is the integration framework. These are advanced tools that enhance communication between CASE tools. Portability services form the second building block. In this case, integration framework and CASE tools are made to be portable meaning they are compatible with various hardware and operating systems. Operating systems, hardware platforms, and environmental architecture form the third, fourth, and fifth building blocks. Operating systems allow for object and database management while the fifth building block necessitates hardware and systems support (Maxim, 2010). On a broader scale, the nomenclature of CASE tools cuts across various components such as business systems planning, support, and project management. Others include framework, programming, re-engineering, simulation tools, as well as integration, testing, analysis, and design (Rock-Evans, 1990).\n\nThe current project seeks to improve the registration process of students. A closer look at the above tools shows that the current project will be dependent on both, upper and lower CASE tools. The specification will be of significant importance because students will be expected to log in to their accounts using unique usernames and passwords. In addition, the integration will be of essential help. It will be useful to integrate data from various sources or databases if the registration system has to work efficiently. In addition, the system has to be flexible to allow for easy maintenance, re-engineering, and most importantly, implementation.\n\nOther tools, like support, will be of an added advantage. Should the end-users (students) of the registration system require assistance, the system must offer alternatives promptly. Support services such as retrieval of login data or change of personal information should be easily accessible. The inclusion of these tools will ensure that the time cycle is reduced and, at the same time, enhances the quality of the system. Another approach that is worth mentioning is system validation. This can be done in four steps. First, there must be reviews of the requirements. This allows for manual evaluation of system requirements. Second, prototyping must be employed. This approach employs executable models of the system to analyze requirements. Third, it will be essential to utilize test-case generation. This strategy necessitates the generation of tests requirements. Finally, automated consistency analysis must be employed to evaluate how consistent the system is and, at the same time, offer suggestions for improvement.\n\nTo ensure that students are enrolled in the correct classrooms, controls such as students\u2019 admission numbers and the generation of unique student registration codes will be required. Students will be expected to enter a unique code sent to their phone numbers and e-mail addresses to validate their identity. Furthermore, it will be important to employ various security controls to reduce misuse of the system. For instance, students will have only three chances of logging in within 24 hours; if all the three have been used in a day, the student will be asked to log in again in 24 hours. In addition, the registration system will only be functional at a stated period. Upon the expiry of the registration window, the system will become non-functional.\n\nTo make the system user friendly, particularly to non-specialized users such as interns, it would be necessary to provide avenues for sharing system data, allow easy access to important tools of the system, necessitate traceable changes to various items in the system, and install version control, as well as, overall configuration management. It will also be helpful to create an automated support system that allows for easy integration of SCI\u2019s and CASE tools into a uniform breakdown structure (Maxim, 2010).\n\nIn summary, this report has defined CASE tools and discussed their relevance to the current project, which aims at developing a registration system. As noted earlier, CASE tools are employed in the development of information systems. They occur in two categories: upper and lower. The current project seeks to improve the registration process of students. A closer look at the above tools indicates that the current project will be dependent on both upper and lower CASE tools. Since there are interns attached to the project, it would be necessary to provide avenues for sharing not only system data and allowing easy access to important tools of the system, but also necessitating traceable changes to various items in the system and configuration management.\n\nReferences\n\nMaxim, B. R. (2010). CASE Tools . Dearborn, Michigan: University of Dearborn Michigan Press.\n\nRock-Evans, R. (1990). Case Analyst Workbenches: A Detailed Product Evaluation . Ovum, London: ACM Digital Library.\n",
        "label": "human"
    },
    {
        "input": "Computerized Physician Order Entry Policy in Healthcare Research Paper\n\nTable of Contents\n 1. Abstract\n 2. Justification\n 3. Hypothesis\n 4. Theoretical Framework\n 5. Implementation Policy Plan\n 6. References\n 7. Interview guide\n\nAbstract\n\nIn order to improve the quality of healthcare, computerized physician order entries (CPOEs) have been invented to replace the traditional handwritten ones. The benefits of CPOEs are many and diverse; but, their implementation and use is yet to be absolute. In this case, adopting the new technology is not due to the ease in which it is implemented; but rather its relative advantage in relation to expected outcomes. Elements such as communication channels, feasible time frame and supportive social structures are important for the implementation of an innovation. Change is very imperative because it is mainly associated with advancement and improved standards of living. The use of CPOE is one such change aimed at improving and advancing the health care delivery system. As a new policy, the Shepherd Center will be targeted as an ideal location for the implementation of this new technology and its benefits will be evaluated. A pre-post evaluation method will be used and Pearson\u2019s Chi-square test will determine the difference before, during and after the implementation of CPOE. The diffusion theory of innovation will be used to discuss adoption of the technology at the center.\n\nThe computerized physician order entry (CPOE) is the use of computers in health care settings to assist in the entry of medication orders from a mobile device or a computer. In addition, the order is documented and captured in a digital, structured, and computable format that is used to improve safety and organization (Centers for Medicare & Medicaid Services, 2010). This system is very effective at the ordering stage when most medication errors and adverse drug effects occur (Reidmann et al., 2011). In comparison to the traditional handwritten orders, the use of CPOE eliminates the issues of illegible handwriting, transcription errors, delayed response time, inaccuracy and failure of completion, and generally improves delivery of health care. Previous studies have indicated that significant reductions in prescribing errors, dosing errors and adverse drug effects (Nuckols et al., 2014; Shamliyan, Duval, Du, & Kane, 2008). The use of CPOE helps to automate dosing and subsequently avoid the errors that occur due to manual calculations (Roberts et al., 2013). This system has not yet been integrated in Shepherd Center, a nursing home in Atlanta, Georgia; therefore, the aim of this paper is to investigate the effectiveness of the policy of computerized orders in comparison to that of handwritten orders among physicians in improving patient quality of care and safety.\n\nJustification\n\nMedication errors are a major source of death and injury in most hospital settings. Shamliyan et al. (2008) cites that at least 500,000 patients in the world will die due to adverse drug effects on annual basis. This results in high health care costs that total up to $ 5.6 million each year. Ordering and transcription from physicians results in 50-61% of all the medication errors. The Institute of Medicine declares that medication errors are a menace to patient safety; thus, an electronic approach is deemed an auspicious solution (Shamliyan et al., 2008). Whereas most hospitals have realized the benefits of this new technology, it is important to note the benefits realized are different depending on implementation design. The Shepherd Center has not yet embraced this new technology, but due to the benefits realized by other nursing homes, there is need to adopt this new technology, and the diffusion of innovation theory will be used to illustrate this (Lee, Hsieh & Hsu, 2011).\n\nHypothesis\n\nThere is no significant difference between computerized physician order entry policy and the traditional handwritten order policy.\n\nTheoretical Framework\n\nRogers (1995, p. 5) defines diffusion as the \u201cprocess by which an innovation is communicated through certain channels over time among the members of a social system.\u201d The rate at which potential adopters choose to accept or reject an innovation is greatly determined by the beliefs they have about the innovation and availability of resources (Mustonen-Ollila & Lyytinen, 2003). This theory entails emulation of pattern of policy from one individual or institution to another. This theory takes more effect when emulation is influenced policy choices of other individuals or institutions within a particular network. Influence occurs through normative pressure, learning, competition, imitation, and coercion (Berry & Berry, 2014). Based on this school of thought, learning and competition is the most influential factor that is prompting adoption of the CPOE policy at the Shepherd Center. Innovations that are deemed to have a higher level of relative advantage have greater trialability, compatibility and observability, and reduced complexity are adopted more readily compared with other innovations (Achugbue, 2014). In an attempt to emerge as an institution with the most efficient healthcare system, adoption of the CPOE, whose use has been proven effective in other hospitals, is paramount at the Shepherd Center.\n\nImplementation Policy Plan\n\nIdentify the problem: This policy arises out of the need to improve patient care quality and safety.\n\nStakeholder involvement and responsibilities: I will be the person in charge of ensuring that the implementation of this new policy succeeds. Other people who will be involved in the implementation of this policy will involve the staff, management, beneficiaries and donors/sponsors. Each person will be given his or her responsibility based on his/her qualifications and expertise.\n\nEvidence that the policy has a high probability of success: The diffusion of innovation theory will be applied to decide whether this policy will be implemented or not. However, based on the discussion earlier on about the benefits of the computerized orders, the diffusion of innovation theory will support the adoption of this new policy.\n\nDraft Policy: A document will be drafted and all the stakeholders will be involved. It will entail a comprehensive layout of the policy, and all the areas that will be involved, available resources, and the activities of all those involved. The draft policy will be reviewed to enable preparation of the final policy that will be approved by the internal committee, and an external committee consisting of experts in the implementation of policies.\n\nApproval of final policy: The final policy will be prepared based on the review of the draft policy; it will incorporate all recommendation and changes suggested in the draft policy.\n\nImplementation: This will be a three-phased activity in that data will be collected prior to training or any form of sensitization to obtain the baseline characteristics that will form a basis for comparison during analysis. Subsequently, data will be obtained half-way the implementation process, and at the end. A pre-post study design will be used (Harris, et al., 2006). Training will take place in all the departments of the hospital, right from the front office, nurse, physician, pharmacist and laboratory technician/phlebotomist. I will liaise with the administration to allow training without interfering the duties. In addition, I will ask the administration to organize a time when staff members are accessible. Training will be done in shifts; in the morning and afternoon to ensure everyone is taught. The training will be conducted for one month by an Information Technology expert. According to Kuo, Wei, Hu & Yang (2013, p. 56-57) and Sanson-Fisher (2004), supportive social structures should be put in place to allow the implementation of a new technology, and the administration will aid in provision of these.\n\nTraining topics will be on medication use and safety, CPOE and its importance in documentation, as well as its integration in the facility. Short-term outcomes will be determined after the first two months of training, and modifications done to ensure the system fully supports integration of the CPOE. Later on, at the end of the entire six months, another evaluation will be conducted to determine the long term outcomes. The interview guide in appendix 1 will be used to obtain qualitative data. Quantitative data to validate information from the subjects and to aid in triangulation will be obtained by measuring the effects, for example, the percentage of reduction in medication errors and adverse drug effects.\n\nEvaluation, Review and Revision of Policy: Both quantitative and qualitative information will be obtained on the effectiveness. The SPSS software will be used to perform analytical functions. Binomial test will be used to evaluate effectiveness of CPOE among the physicians while Pearson\u2019s chi-square test will check for differences in outcomes before, during and after the intervention. A confidence interval of 0.05 will be used. Qualitative data will be transcribed, cleaned and arranged in themes. The results of the evaluation will determine how the policy will be revised in the event the desired outcome will not have been achieved.\n\nReferences\n\nAchugbue, E. (2014). E-business in Education: The Case of Delta State University. In Z. Sun (Ed), Handbook of Research on Demand-Driven Web Services: Theory, Technologies, and Applications (pp. 346-380). Hershey: IGI Global.\n\nBerry, F., & Berry, W. (2014). Innovations and diffusion: Models in policy research. In P. Sabatier & C. Weible (Eds.), Theories of the policy process (3 rd ed.) (pp. 307-314). Philadelphia: Westview Press.\n\nCenters for Medicare & Medicaid Services. (2010). Medicare & Medicaid EHR Incentive Program . Web.\n\nHarris, A., McGregor, J., Perencevich, E., Furuno, J., Zhu, J., Peterson, D., & Finkelstein, J. (2006). The use and interpretation of quasi-experimental studies in medical informatics. Journal of the American Medical Informatics Association, 13 (1), 16-23.\n\nKuo, Wei, Hu & Yang, H. (2013). Applying innovation theory in observing emerging technology acceptance. International Journal of Systems Applications, Engineering & Development, 7 (1), 56-65.\n\nLee, Y.-H., Hsieh, Y.-C., & Hsu, C.-N. (2011). Adding Innovation Diffusion Theory to the Technology Acceptance Model: Supporting Employees\u2019 Intentions to use E-Learning Systems. Educational Technology & Society , 14 (4), 124\u2013137.\n\nMustonen-Ollila, E., & Lyytinen, K. (2003). Why organizations adopt information system process innovations: a longitudinal study using Diffusion of Innovation Theory. Info Systems J, 13, 275-297.\n\nNuckols, T., Smith-Spangler, C., Morton, S., Asch, S., Patel, V., Anderson, L., \u2026 Shekelle, P. (2014). The effectiveness of computerized order entry at reducing preventable adverse drug events and medication errors in hospital settings: a systematic review and meta-analysis. Systematic reviews, 3 (56), 1- 12.\n\nReidmann, D., Jung, M., Hack, W., Stuhlinger, W., Van der Sijs, H., & Ammenwerth, E. (2011). Development of a context model to prioritize drug safety alerts in CPOE systems. International Journal of Medical Informatics, 11 (1), 35-46.\n\nRoberts, D., Noble, B., Wright, M., Nelson, E., Shaft, J., & Rakela, J. (2013). Impact of computerized provider order entry on hospital medication errors. JCOM, 20 (3),109-115.\n\nRogers, E. M. (1995). Diffusion of innovations (4th Ed.). New York: Free Press.\n\nSanson-Fisher, R. (2004). Diffusion of innovation theory for clinical change. MJA, 180 , S55-S56.\n\nShamliyan, T., Duval, S, Du, J., & Kane, R. (2008). Just What the Doctor Ordered. Review of the Evidence of the Impact of Computerized Physician Order Entry System on Medication Errors. Health Services Research, 43 (1), 32-53.\n\nUtley, R. (2011). Theory and Research for Academic Nurse Educators: Application to Practice. Sudbury: Jones and Bartlett Publishers.\n\nInterview guide\n\n 1. What are the benefits of the CPOE policy towards enhancing health care quality and patient safety?\n 2. What is your role in the implementing the CPOE policy?\n 3. What skills do you possess that will enable you to effectively and efficiently use the computerized physician order entries?\n 4. Explain available social structures that will support the implementation of this new technology.\n 5. What are the perceived barriers to implementation of CPOE at this center?",
        "label": "human"
    },
    {
        "input": "Computer Crime in the United Arab Emirates Research Paper\n\nIntroduction\n\nIn the world of technology, the types of crime committed vary greatly, ranging from conventional crime, such as aggravated assault, murder, rape, kidnapping, and carjacking to computer crime. Many policy makers are concerned with combating computer crime given the fact that it is on the increase.\n\nPolicies are being formulated on how to resolve the modern type of crime, which is infringing on the rights of various individuals, organizations, and governments. Computer crime is a new type of offense that is committed with the help of the computer and a network. In this regard, the computer might be used to commit this type of crime or it might be the target of crime (McQuade, 2006). Computer crime is associated with a number of other terminologies, including Net crime, which refers to the exploitation of the internet.\n\nNet crime refers to a scenario in which individuals develop computer programs that are meant to cause harm to other individuals or entities. In their analysis, information communication scholars, such as Halder and Jaishankar (2011), noted that cybercrimes include those offenses that are meted out on other people, particularly to harm them or destroy their public image in society either directly or indirectly, with the help of modern telecommunication network.\n\nModern technology includes the internet (chat and emails) and mobile phones (short messages and MMS). Halder and Jaishankar observed that such types of offenses are dangerous to the survival of an individual, as well as the country\u2019s financial system. In the modern society, crimes related to the computer are enormous (McQuade, 2009). These include cracking, patent violation, child pornography, child sprucing, and privacy issues whereby critical information belonging to an organization or government is lost to fraudsters.\n\nIn the global society, state and non-state actors tend to engage in computer crime with an aim of soliciting or obtaining financial gains through illegal means. This is conducted through espionage and financial theft. These forms of crimes are commonly referred to as transnational organized crimes.\n\nIn the information systems, this act is referred to as cyber warfare since an actor in one jurisdiction aspires to infringe on the rights of another individual in a different jurisdiction. In this regard, actors in the international system have come up with various measures aiming at containing the spread of this type of crime. The international criminal court was established in 2001 to partly with these types of crimes (Fitzgerald, 2007).\n\nLocally, states are also working hard to strengthen local institutions in an attempt to deal with computer theft. This article aims at evaluating some of the laws established in the United Arab Emirates, which are meant to deal with cybercrimes. Moreover, the article analyzes some of the international laws aiming at dealing with computer crime. The article concludes by comparing the copyright laws applied in the United Arab Emirates and those applied at the global level. The article will focus specifically on copyright laws both at a local level in United Arab Emirates and internationally.\n\nProvision of Copyright Law in the United Arab Emirates\n\nThe Federal Law number seven of 2002 in the United Arab Emirates provided some of the regulations that would guide members of the public as regards to copyright. The law was amended in 2006 under the amendment number thirty-two to offer more guidelines on copyright and its related rights (Lessig, 2002).\n\nThe copyright law is a result of various amendments that were instigated since 1972, when the ministers had the right to uphold certain information, especially when they were of great importance as far as national security was concerned. The Federal Law number fifteen paved way for an amendment that gave strict regulations on the spread and publication of a material. Copyright law was mainly controlled by the ministry of the economy implying that the economy minister was in charge of regulating certain materials based on their effects on national security (Fitzgerald, 2008).\n\nThe first article of the amendment delves into the definitions of various aspects related to copyright, but the second article is critical since it gives an individual the power to protect his or her original material through acquisition of patent. The law protects the works of authors from infringement, but only in the United Arab Emirates.\n\nAmong other works protected, books and literal works are the most cosseted. The law also recognizes patents acquired to protect computer software and databases, but the minister must approve such patents. The law protects other original works provided an individual proves that the law was followed when producing the piece of information.\n\nArticle 3 proscribes the protection of other materials or pieces of information obtained through unscrupulous means or techniques. For instance, the law does not protect ideas and procedures that are aimed at benefiting an individual in business or trade. This means that the law does not protect the ideas or pieces of information that would give an individual undue advantage in the market. The law intends to give each person an equal opportunity to compete favorably in the economy.\n\nBased on this, the copyright law in the United Arab Emirates does not offer patent on official documents that are to be utilized publicly. These documents include news of current events, which are released by the media and other works that are already within the public domain (Fitzgerald, 2007). The law would only offer patent on current news under strict conditions, especially if it is proved that only a single organization or individual produced them.\n\nArticle Four states that only the ministry of the economy is in charge of issuing copyright. This implies that any individual with an innovation should report to the ministry in time in order to obtain a patent that would protect his or her intellectual property. In case the rights of an individual are infringed, the ministry would be the reference meaning that the state would take over the case. The works of an individual cannot be protected in case a copyright license is not obtained from the ministry in time.\n\nThe Fifth Article under the second chapter states that the author of any literary work has absolute rights over the information or any piece of information in which a patent is obtained. In this regard, the family member or any relative can inherit a patent in case the author is dead or incapacitated.\n\nIn particular, the law ascertains that the author and his or her descendants would perhaps enjoy everlasting and incontrovertible ethical rights as far as the piece of knowledge is patented (Brisbane, Fitzgerald, & Suzor, 2005). This would be through licensing of the unpublished or published material. In case a different person uses the published or unpublished material, he or she must cite it properly to prove that the ideas therein do not belong to him or her.\n\nThe author has the moral authority to protect his or her works even if it is in already in circulation. In this regard, the information should be utilized in the right way since the author has the legal power to recall it from the public. In case some modifications are carried out on the material, the author has the right to sue anyone attempting to change the information. In this case, the author can seek a legal notice to withdraw the material from the public and seek adequate investigation on the infringement. The author can receive compensation in case his or her works are used without permission in the United Arab Emirates (Fitzgerald, 2008).\n\nHowever, this would only be determined by a competent court. The sixth article is simply a follow up to the fifth article stating that another person cannot attempt to interpret the works of the author without seeking permission or giving proper citations. In the seventh article, the law reiterates that only the author has the power to license the use of the published and unpublished works (Dualeh, 2007). This includes electronic duplication, storage, demonstration, dissemination, and public performance. Moreover, the works of the author cannot be leased, lent, or be accessed electronically without his or her permission.\n\nThe eighth article of the United Arab Emirates copyright law states that the software of any given material can be leased to a third party, but only the author has the right to do this. The works of the author are considered an economic benefit hence any other person should not gain from it economically. Based on this, acquisition of the necessary permit before leasing the piece of information is mandatory for any person wishing to lease the piece of information (Touretzky, 1990).\n\nHowever, the author should not interfere with the process of selling or leasing the piece of information in case the right procedure is followed. The tenth article talks about the pro rata share meaning that the author should receive some monetary benefits whenever his or her piece of work is sold to other people. In this regard, the law gives the author the right to receive any payment in case his or her material is shared with other members of the public.\n\nArticle 11 states that if the agreement in article ten were considered prejudicial, the author or the legally mandated successor would seek redress to establish the amount to be compensated. Under this regulation, the author or the successor would receive additional monetary benefits other than the initially agreed fees (Brenner, 2007).\n\nThe copyright law in the United Arab Emirates contains fifty articles, which are closely related. This means that the rights of the author are clearly stated in order to promote innovation and originality. The minister in charge of economic development is mandated with a responsibility of ensuring that the works of one individual do not benefit fraudsters who are keen on utilizing the works of others without following the law.\n\nThe cases of A & M Records v. Napster Inc and Universal Music Australia Pty Ltd v. Sharman License Holdings Ltd prove that copyright laws can be applied successfully to benefit the author or the owner of the original information. In the two cases, the defendants played a role in reproducing and broadcasting the works of various authors with an aim of gaining profit. The main aim of the copyright law is to enrich the owner of the information (Csonka, 2000).\n\nThis promotes economic stability since the rightful people are allowed to enjoy the results of their skills. Some analysts are of the view that copyright laws do not promote democracy and cultural development since they disrupt the doctrine of accommodation and innovation. An individual has to be prepared to deal with legal suits if he or she attempts to come up with an idea that is already patented. Such scholars argue that a boundary should be drawn in order to promote innovation and uniform economic development.\n\nInternational Copyright Law\n\nIt is unfortunate that no single international law exists on the protection of the rights of authors. However, two treaties exist. These treaties are considered international provisions that protect authors from heinous acts of cyber criminals who always aim at benefiting illegally from the works of others. The international law does not protect any creative work, but these treaties protect any piece of material, as soon as it is fixed in a medium (Walden, 2007).\n\nThese global treaties include the Buenos Aires convention and the Berne treaty on the protection of literary and artistic works. The Buenos Aires global treaty on copyright treaty is simply a mutual recognition of other people\u2019s literary works. In this case, any material would contain the information \u2018all rights reserved\u2019 meaning that the author has absolute rights regarding the usage of the material. This information is usually conveyed in all materials that are not meant for public utilization (Wall, 2007).\n\nRegulations on copyright vary from one country to the other. In the United States for instance, the author and the year of publication should always be cited properly. Other countries are stricter while others are even reluctant to implement the law since no single global law protecting the works of the author exist. Under the Buenos Aires treaty, the works of any author are only given temporary protection, what is usually referred to as the rule of shorter term. Under this policy, the country of origin and the author are usually protected for a specified period.\n\nIn 1952, there was a deliberate attempt to protect authors from cyber criminals. Developing countries complained that the Berne convention favored only developed countries, something that led to the designing of the Universal Copyright Convention. The treaty had a number of provisions, one of them suggesting that each state had to protect the works of authors from other countries, irrespective of the nationality. In case a foreigner needed a license to protect his or her works in a foreign state, the government should treat such an individual in the same way as its own citizen (Williams, 2006).\n\nThis would apply especially to a citizen from a member state. The state has the right to protect the works of an author for at least twenty-five years from the time of registration. If an author dies, his or her works should be protected for at least twenty-five years. However, this is different from the works of art, which are simply protected for ten years.\n\nThe law further states that the author has the right to benefit economically from his or her works through reproduction and broadcasting. In other words, the author is allowed to make some changes to his or her original works without obtaining a license. The law does not apply globally, as developing countries are exempted from certain clauses.\n\nBerne convention is another treaty that is applied in the protection of the works of the author in the global system. It varies greatly from the Universal copyright convention. Berne convention protects the life of the author for at least fifty years after reproduction. However, the scenario is different in the case of cinematographic works since the patent expires after fifty years, irrespective of whether the author is alive or dead. The treaty recognizes some works that do not have authors (Yar, 2006).\n\nFor such works, the expiry of the patent is after fifty years since the information or the works was made public. If the author of the literary works were dead, the member countries would not attempt to protect his or her works. If an author dies before January of each year, the counting would be pushed forward to start in January.\n\nThe member countries have the power to extend the patent beyond fifty years if deemed necessary. However, members cannot shorten the patent of any literary material. If the treaty contradicts national laws of any member country, the laws of the state on the issuance of the patent would be applied to resolve the matter.\n\nApplication of the copyright law at the global level is different from its application at a national level. The universal copyright convention, the Bern Convention, and the Buenos Aires convention are very categorical on the application of copyright laws globally. In a case involving Sony Corporations of America and the Universal City Studios, the Supreme Court in the United States applied international treaties on copyright in interpreting the situation.\n\nIt was determined that the public cannot be denied access to a certain information if the piece of work or technology has various applications or usage. The author or the innovator could have registered a piece of knowledge, but this does not deny the public the right to access it, as far as the application of the technology is not similar to that of the author.\n\nComparison of United Arab Emirates Laws on Infringement and the Global Laws\n\nAs already mentioned in the previous sections, no single law on copyright exists at a global level. This gives individuals an advantage to infringe on the rights of the owner. The three conventions mentioned earlier are focused more on the period in which a certain piece of literature should be protected. For the Universal Copyright Convention, the time limit is twenty-five years while the Berne Convention gives at least fifty years.\n\nThe United Arab Emirates copyright laws do not specify the time in which a technology or a piece of information should be protected. This is mainly because technology is considered wealth. Another person should not benefit from the technology since it is inherited in the same way an individual inherits property. Another difference is that at the global level, protection of the author depends on the goodwill of member states, but in the United Arab Emirates, the law sanctions the state to protect the author.\n\nPunitive measures are usually taken against an individual trying to benefit illegally from the works of another person in the United Arab Emirates. However, the case is different at the global level since various conventions are applied in interpreting the scenario. In some cases, infringement might be allowed with claims that a technology might have different applications. In this regard, the author is not guaranteed of protection given the fact that various governments have different interests regarding the application of technology.\n\nReferences\n\nBrenner, S. (2007). Law in an Era of Smart Technology. Oxford: Oxford University Press.\n\nBrisbane, A., Fitzgerald, B., & Suzor, N. (2005). Legal Issues for the Use of Free and Open Source Software in Government. Melbourne University Law Review, 412(2), 120-174.\n\nCsonka, P. (2000). Internet Crime; the Draft council of Europe convention on cyber-crime: response to the challenge of crime in the age of the internet. Computer Law & Security Report , 1(5), 45-98.\n\nDualeh, A. (2007). Copyright and Knowledge Advancement: A Case Study on UAE Copyright Law . Dubai: QUT.\n\nFitzgerald, B & Bassett, G. (2004). Legal Issues Relating to Free and Open Source Software. Essays in Technology Policy and Law , 1(2), 101-120.\n\nFitzgerald, B (2007). Open Content Licensing: Cultivating the Creative Commons . Sydney: Sydney University Press.\n\nFitzgerald, B. (2007). Internet and E-Commerce Law. Technology Law and Policy , 3(1), 213-214.\n\nFitzgerald, B. (2008). Copyright 2010: The Future of Copyright. European Intellectual Property Review , 43(1), 78-90.\n\nFitzgerald, B. (2008). Legal Framework for e-Research . Sydney: Sydney University Press Sydney.\n\nHalder, D., & Jaishankar, K. (2011). Cyber crime and the Victimization of Women: Laws, Rights, and Regulations. Hershey: IGI Global.\n\nLessig, L. (2002). The Future of Ideas: The Fate of the Commons in a Connected World . Sydney: LBC Information.\n\nMcQuade, S. (2006). Understanding and Managing Cybercrime. Boston, MA: Allyn & Bacon.\n\nMcQuade, S. (2009). The Encyclopedia of Cybercrime. Westport, CT: Greenwood Press.\n\nTouretzky, D. (1990). Common Lisp: A Gentle Introduction to Symbolic Computation . New York, NY: Benjamin/Cummings.\n\nWalden, I. (2007). Computer Crimes and Digital Investigations. Oxford: Oxford University Press.\n\nWall, D.S. (2007). Cybercrimes: The transformation of crime in the information age. Cambridge: Polity.\n\nWilliams, M. (2006). Virtually Criminal: Crime, Deviance and Regulation Online. London: Routledge.\n\nYar, M. (2006) Cybercrime and Society. London: Sage.\n",
        "label": "human"
    },
    {
        "input": "Training Nurses to Work With Computer Technologies and Information Systems Essay\n\nSummary of Educational Need and the Rationale\n\nThe educational need at this stage will be to enhance the ability of the learners to work with computer technologies and information system. The practicing nurses always face numerous challenges in their work that affects their ability to deliver quality work. In order to address these problems, it is always necessary for the nurses to engage in regular training in order to be in a better capacity to understand issues that affect them.\n\nAccording to Billings and Halstead (2009), the field of nursing has faced a massive transformation due to the emerging technologies, and still remains one of the most dynamic professions. In order to understand the changing environmental factors in this field, it is necessary to maintain a regular training which will ensure that the nurses have the knowledge of the current environmental forces. It is through this that they can understand some of the challenges emerging in this profession due to the changing environmental forces. The ability of nurses to work with computer and information systems defines their ability to have easy access to information relevant to their field.\n\nThis knowledge will make it easy for nurses to share their discoveries and receive positive criticism out of their work in order to enhance their knowledge. It is because of these facts that this educational need was considered primary to other educational needs. When the nurses have the necessary computer technology skills, and understand information systems, they will be able to achieve other educational needs with ease.\n\nGoal for the Educational Plan\n\nThe main educational goal for this plan is to teach the nurses how to work with computer technologies and information system. Computer technologies and information system offers a good opportunity to address some of the problem that nurses face when undertaking various tasks. According to DeYoung (2009), information is power. Many institutions are currently working hard to ensure that their employees have access to the relevant information in order to improve their efficiency. Having knowledge in computer technologies and information system puts nurses a step ahead in information sharing. This knowledge will be needed to make them share vital information about new discoveries, best practice in the industry, and some of the common challenges that nurses face.\n\nBy the time this learning process is completed, it is expected that the learners will be able to work with computer technologies and information system in their normal duties. The outcome will be to enhance the ability of the nurses to share related information in nursing among the peers. The knowledge should also make them be able to receive information from doctors and other superiors within the institution on what they are expected to do. This way, there will be a clear flow of information between doctors and nurses, and among nurses. The process of handing over patients will also be enhanced because there will always be sufficient information about the patient.\n\nWhen the nurses\u2019 knowledge on the usage of computer technologies is enhanced, they will be in a better position to address their duties because they will have easy access to the relevant information they need in this field. It is on this basis that it was considered necessary to make it the basic educational need at this stage.\n\nLearning Objectives\n\nIt is important to develop specific objectives for this learning activity that should be achieved based on the broad goal stated above. The objective will define the path that the learning activity should take in order to achieve its objectives. The following are the specific learning objectives for this curriculum.\n\n  * To enhance the ability of the nurses to work with computer technologies and information system.\n  * To change the perception of the nurses towards the usage of computer technologies and information system.\n  * To create a culture of usage of computer technologies and information system among the nurses in this organization.\n  * To enhance information sharing among the nurses within the organization through the use of emerging computer technologies.\n\nIn order to create the lesson plan, it is important to understand the resources and strategies that will be needed at this stage. To develop this lesson plan, it will be necessary to have a number of books that talk about the use of technology in nursing. Books about the emerging trends in nursing and impacts of technology in the field of medicine will also be very relevant. These books and articles will make it possible to understand what is expected of the learners, and how the lesson can be developed in order to address specific issues in this field. Also important at this stage will be the previous lesson plans on related subjects.\n\nIt is important to note that this plan will not attempt to duplicate the previous plans. However, these previous plans will be analyzed to determine their areas of weaknesses and strength. With this knowledge, it becomes easy to come up with a superior plan that avoids the previous flaws, and maximizes on their strength.\n\nAn effective strategy will also be needed when preparing to create the lesson. As Arbon and Ranse (2013) note, when developing a lesson plan, it is always important to start by addressing simple tasks first in order to generate interest of the learners to the subject. Their anticipation should be raised. They should be made to believe that they stand to benefit from this learning activity. For this reason, there will be need to develop concise objectives and benefits that the learners stand to gain by undertaking the classes.\n\nFor this reason, they will have a reason to undertake the classes. They will develop an interest in taking the lessons, and this will boost chances of achieving success in the learning process. The policies within this organization will also be put into consideration to ensure that the learners get enough time to attend the classes without interfering with their duties.\n\nAppropriate Instructional Design Model, Learner Characteristics, and the Learning Theory Used To Develop the Plan\n\nIt is important to come up with an appropriate instructional design that will be able to generate desired success. In this plan, ADDIE model was considered to be the most appropriate instructional plan for this learning process. The model is shown below\n\nADDIE Instructional Design Model\n\nAs shown in this model, the first step is to analyze what the characteristics of the audience are, the scope and timeline of the project, and any budgetary allocations. In this activity, the learners are registered nurses who are already practicing within Gulf Coast Medical Center which is located in Fort Myers, Florida. The nurses are individuals who are ready and willing to learn. The scope of the learning activity will be limited to working with computer technologies and information system.\n\nThe second step will be the design which will basically involve the evaluation and development of the content. It will also involve graphical assets needed in the study, and the e-learning medium, given that the participants will be encouraged to use emerging technologies. The third step involves developing or creation of deliverables using the appropriate tools based on the first two stages above. The fourth stage is the implementation. At this stage, there will be need to ensure that the end-users have the appropriate instructions that will enable them to have access to the deliverables. The last stage will be the evaluation of how effective the process is in achieving the intended objectives.\n\nThis will involve gathering feedback from the end-users about how effective they found the system to be during the learning process. They can propose necessary changes that may be needed to make the process more effective.\n\nGiven the fact that the basis of this learning activity is to enhance the ability of the nurses to work with computer technologies and computer system, APL and Halstead\u2019s Theory of Software Metrics will be important. According to Arbon and Ranse (2013), this theory holds that \u201cThe increasing complexity of computer programs and of the development of systems creates an ever growing need of objective methods for measuring the quality of software\u201d (p. 56).\n\nThis theory emphasizes on the need to maintain continuous learning process when dealing with issues related to technology. Technology is very dynamic, and what is considered to be a breakthrough today may be irrelevant after a short while. The only way of keeping abreast with the changing technological environment is to embrace continuous learning process. It is on this basis that it was considered necessary to introduce this learning process.\n\nContent Outline\n\nAccording to Billings and Halstead (2009), it is always imperative to understand the content of the knowledge that is to be passed to the learners and to develop an outline that shows important areas that will be addressed in the learning process. The following is the brief outline of the content in this learning activity.\n\n 1. The learning goal and objectives\n 2. Understanding the relevance of computer technologies and information system in the field of nursing.\n 3. Understanding the dynamics in the field of computer technologies.\n 4. Analysis of the computer technologies and information system relevant to the field of nursing.\n 5. Determining how nurses can improve their knowledge on computer technologies.\n 6. Using of computer technologies and information system by the nurses to enhance the quality of their services.\n\nThis content outline will define the approach that will be taken in teaching the learners on this topic.\n\nInstructional Methods to Be Used For Delivery of the Content and the Rationale\n\nAccording to DeYoung (2009), identifying appropriate instructional methods to be used in the delivery of the contents is one of the most important steps in the learning process. The instructional method chosen always defines the ability to achieve success in a given educational course. In this learning activity, two instructional methods were considered to be appropriate at different stages. The first method is an actual classroom learning process where the learner and the instructor will be expected to meet face to face.\n\nThis method will be very important during the introductory part of the learning process, and when training the learners on how to use the computer technologies. During the introductory process, it is always necessary to convince the learners of the importance of knowledge that is to be passed to them. Face to face meeting is always effective in convincing people over a given issue. This instructional method is also important when explaining technical concepts in computer technologies. The second approach is the use of online learning. This would effectively determine how well the learners have grasped the learning content.\n\nTypes of Instructional Materials to Be Used\n\nThe instructional instruments that will be used will vary at different stages of learning. The first materials that will be used will be the handouts that shall explain the objective of the topic, what is to be achieved, and the relevant materials that learners should acquire in order to enhance the learning process. The articles will then be used when the learning process is in progress. The articles will expand the knowledge of the learners on the topic. Finally, there will be the use of videos, especially after classes to expound on what has been taught in the class.\n\nAnalysis of Evaluation Methods for the Objective\n\nIt will be important to develop evaluation methods for objectives in order to determine if they have been achieved. In order to determine if the learners have gained knowledge on how to use computers, they will be expected to address different tasks using computer technologies. In order to determine the change in the perception of the learners towards technologies, it will be necessary to evaluate how frequently they use technology in their normal duties. In order to determine if the participants have understood the need to share information, it will be necessary to evaluate how they interact, and the kind of information they share.\n\nReferences\n\nArbon, P. & Ranse, J. (2013). Australasian emergency nurses\u2019 willingness to attend work in a disaster. Australasian Emergency Nursing Journal, 16 (1), 52-57.\n\nBillings, D.M., & Halstead, J.A. (2009). Teaching in nursing a guide for faculty . New York: Cengage.\n\nDeYoung, S. (2009). Teaching strategies for nurse educators . Upper Saddle River, NJ: Prentice Hall.\n",
        "label": "human"
    },
    {
        "input": "Advanced Data & Computer Architecture Term Paper\n\nAbstract\n\nData and information storage, access, manipulation, backing up, and controlled access forms the backbone of any information system. Solid knowledge and understanding of the information architecture, access, storage mechanisms and technologies, internet mechanisms, and systems administration contribute to the complete knowledge of the whole system architecture. This knowledge is vital for procuring hardware and software for a large organization and effective administration of these systems. The discussion provides a detailed view of data storage, access, and internet applications and takes us through systems administration for large organizations. It ends with a discussion on large organizations and their application in large organizations.\n\nFile and Secondary Storage Management Introduction\n\nIntroduction\n\nAn aggregation of software applications, data access control, and manipulation functions and mechanisms in files and secondary storages sum up to file management systems, commonly referred to as FMS. The database and operating systems share file management system functionalities occasionally (Burd, 2008).\n\nComponents of a File System\n\nGraphically, the functions of the operating system and file management systems can be demonstrated based on the layering structure defining both systems, as shown below.\n\nFrom: Systems Architecture\n\nThe file structure represents physical data storage mechanisms and data structures defined by bits and bytes and contiguous memory blocks. The operating systems interfaces device controllers and device driver applications. Data storage, access, and control are achieved through the kernel, which manages the transfer between the memory and other storage locations (Thisted, 1997). The kernel software is modularized with buffers and cache managers, device drivers that manage input and output devices ported to the system, device controllers, and modules that handle interrupts in the whole system architecture (Burd, 2008). Data access and control are defined in the files through logical access mechanisms whose architecture is independent of the physical structure of a file. File contents are defined by different data structures and data types which can be executed through file manipulation mechanisms integrated into the file management system at the design stage.\n\nDirectory Content and Structure\n\nA directory is tabular storage of files and other directories in complex data structures whose information and data are accessed through graphical command lines, as exemplified in the UNIX file system. However, command lines are made transparent to users for other systems since the FMS manages access to the directory (Burd, 2008).\n\nThe hierarchical structure of files and directories has the unique attribute of being assigned unique values whose data structures point to each specific directory through the directory hierarchy (Thisted, 1997. Thus a file access path is specified in UNIX and Windows in different ways. Each of the access mechanisms takes the user to a specific file (Burd, 2008).\n\nStorage Allocation\n\nControlled secondary data and information storage is achieved through input and output mechanisms to files and directories. The systems efficiently identify data and information storage blocks defined by efficient data structures which emphasize smaller units of storage for more efficient space utilization. Thus allocation units vary with varying sizes of allocation (Burd, 2008).\n\nStorage allocation tables define data structures that contain information about allocations, entries, varying allocation units and sizes, access mechanisms that may include sequential and random access mechanisms, among others, such as indexing that makes access to a data item quite efficient.\n\nBlocking and buffering provide a mechanism for accessing and extracting data from physical storage. Blocking factors determine the size of buffering that can be done on any size of the buffer. Buffer copying operations at times intervene where physical blocks of data cannot be copied in their entirety into the buffer locations.\n\nFile Manipulation\n\nAn open service call prepares a file for reading and writing operations through the mechanisms of locating, searching internal table data structures, identifying and ensuring privileged access is provided, identifying buffering areas, and performing file updates. The FMS ensures controlled access and prevention to file data from manipulations from other programs. Once the process is complete, the FMS issues a close file call that ensures the resident program is flushed into secondary storage, the buffer memory is de-allocated, and the table-data structure and file data stamp are both updated (Burd, 2008).\n\nUpdate and deletions of records are achieved through the file management system. Mechanisms are integrated into the FMS to enforce data integrity and access privileges. Microsoft used the FAT file but earlier on developed the NFTS that provided higher speeds operations that were more secure, highly fault-tolerant, and incorporated abilities to handle large file systems (Burd, 2008).\n\nFile Migration, Backup, and Recovery\n\nUtilities, integrated features, and other file protection and recovery mechanisms such as file migration through mechanisms such as undo operations and configurations for automatic backups. On the other hand, file backups are done periodically, constituting full backups, backing up incrementally, and through the differential approach. Recovery is also achieved through utilities incorporated into the FMS through consistency checking and other mechanisms.\n\nFault Tolerance\n\nHardware failure has the potential to cause loss of sensitive data, particularly for large organizations causing undesirable consequences. However, data and information recovery mechanisms are acceptable and reliable. Optical, magnetic, and other devices such as the hard disk are managed through disk mirroring and RAID technologies (Burd, 2008).\n\nMirroring is achieved through data stripping across a number of disks through smaller parallel read operations and RAID redundant read operations. A round-robin approach through regenerations contents to new disks.\n\nStorage Consolidations\n\nLarge organizations find the DAS-Direct attached access inefficient and expensive in a shared environment. Thus, they opt for Network Access Storage, whose architecture is defined by network access and connectivity. The network server concept is hereunder illustrated.\n\nAccess Controls\n\nThe read, write, and file manipulation mechanisms and operations are controlled through the FMS. The FMS and Operating systems structure endeavors to implement and enforce security at different data access levels. Data integrity is enforced to ensure controlled access, authentication, and authorizations mechanisms before service requests are granted by the FMS. Data and sequential access operations are access mechanisms dependent on the physical organization of data. That includes a data structure such as graph directory structures. The directory data structure can be hierarchical, logical, physical directory presentations, and other file control mechanisms (Burd, 2008).\n\nInternet and Distributed Application Services\n\nIntroduction\n\nData and information transfer through the internet architecture follow specific network protocols, operating systems, and network stacks. These enable resource access and interaction through hardware and software application interfaces. The chapter provides a detailed view of network architecture, network resource access approaches, the internet, emerging trends in distribution models, directory devices, and the software architecture of distributed systems.\n\nArchitectures\n\nThe client-server architecture is layered with other variations, such as the three-layered architecture commonly referred to as the three-tier architecture. These divide the application software into the database layer, the business logic layer that defines policies and procedures for business processing, and the data presentation and formatting view (Burd, 2008). The peer-to-peer architecture improves scalability with roles of the client-server and other ported hardware well defined.\n\nNetwork Resource Access\n\nAccess to network resources is provided through the functionality of the operating system that manages user requests through the service layer. The resident operating system, therefore, explicitly distinguishes between foreign applications and remote operating systems. Thus, communication is managed through established protocols.\n\nThe Protocol Stack\n\nThe operating system configures and manages communication through a set of complex software modules and layers. It establishes and manages static connections on remote resources (Burd, 2008). The management of these resources is defined on the premise that resources are dynamic, resource sharing through the networks is possible, and that there is minimized applications and operating system complexity of a local system. Resource sharing is achieved through resource registries, particularly on the p2p architecture.\n\nDynamic resource connections are achieved through various mechanisms, one of them being the domain name system (DNS). This technique is defined by an IP address that incorporates destination address information at its header, which, however, changes dynamically with changing network platforms. A domain name serves for each network connected to the internet consists of two servers, one defining the registry and IP addresses that are mapped into the DNS and the local registry DNS. Standard container objects are defined on LDAP for different destinations such as countries. These objects are uniquely identified based on their domain name attributes (DN). These can be used for administrative purposes across large organizations.\n\nInterprocess Communication\n\nProcesses communicate when they are local to an application or when they are split executing on different computer platforms. These processes coordinate their activities on standards and protocols. A specific focus on the low-level P2P processes across network communications is defined on application, transport, and internet layers. Sockets ported to these devices are uniquely identified through port numbers and IP addresses (Burd, 2008). Data communication takes in either way. The packaging and unpackaging of data packages are achieved through these layers. Remote Procedure call\n\nA machine can invoke a procedure in another machine with a remote procedure call technique through the mechanisms of passing parametric values to the called procedure, holding on until the called procedure completes executing its task, accepting parameters from the process that had been called, and resuming task execution subsequent to the call. At industry levels, tickets and other mechanisms are used to access and pass data from one procedure to the other.\n\nThe Internet\n\nThe internet is a technology framework that provides a mechanism for delivering content and other applications to designated destinations. It consists of interconnected computers that communicate on established protocols. The model is defined by HTTP protocols, Telnet protocols, and mail transfer protocols SMTP, among others. It is an infrastructure that provides teleconferencing services among a myriad of other services (Burd, 2008).\n\nComponents Based Software\n\nThe component-based approach in software design and development provides benefits similar to those provided by complex software applications such as grammar checkers. Decoupling and coupling mechanisms when building or maintaining the components are unique attributes of this approach. Interconnection of companies based on different computers is achieved through a protocol such as those developed by COBRA, Java EE, among others (Burd, 2008).\n\nAdvanced Computer Architecture (n.d) and Burd (2008) both note that software can be viewed as a service that focuses on web-based applications through which users interact with applications and in which services are provided in large chunks. Infrastructure can also be viewed as a service with the advantage of reduced costs and other related issues. Different software vendors and architecture present potential risks that need to be addressed. These include cloud computing architectures and software applications that define the cloud computing frameworks.\n\nSecurity spanning authentication, authorization, verification, and other controls and concerns such as penalties and costs associated with security bridges.\n\nEmerging Distribution Models\n\nHigh-level automation, ubiquitous computing, and decades of unsatisfied needs in various industries have led to new approaches to be adopted by business organizations today. These enterprises include Java enterprise editions, COM, which is a computed object model, SOAP, among other trends.\n\nComponents and Distributed Objects\n\nThese components are autonomous software modules characterized by a clearly defined interface, uniquely identifiable, and executable on a hardware platform. These interfaces are defined by a set of unique services names and can be compiled and run or executed readily (Burd, 2008).\n\nNamed Pipes\n\nData between executing processes occupying a similar memory location are identified as named pipes. These pipes provide communication services for requesting and issuing service requests.\n\nDirectory Services\n\nThis is middleware descriptive of service provided, including directory updates, storage of resource identifiers, provided a response to directory queries, and synchronizes resources. Middleware provides an interface between client-server applications (Burd, 2008).\n\nDistributed Software Architecture\n\nThis architecture provides a link to distributed software components across multiple computer platforms geographically spread across large areas (Burd, 2008).\n\nSystem Administration\n\nIntroduction\n\nThis chapter takes one through the process of determining the requirements and evaluating performance, the physical environment, security, and system administration.\n\nSystem administration\n\nSystem administration identifies strategic planning process for acquiring hardware and software applications, the user audience, determining requirements for system and hardware acquisition. All the integration, availability, training, and physical parameters such as cooling are identified in the process and documented on a proposal. System performance is determined by hardware platforms, resource utilization, physical security, access control mechanisms that were discussed earlier, virus protection mechanisms, firewalls, and disaster prevention and recovery mechanisms (Burd, 2008).\n\nSoftware updates are integral in creating a large organization systems infrastructure. Security service, security audits such as log management audits, password control mechanisms, overall security measures, and benchmarking are core activities in building the infrastructure. Protocols are evaluated prior to the acquisition process that incorporates the identification of new and old software and vendors and standards.\n\nSystem administration may also constitute acquisition, maintenance, and developing software and security policies (Burd, 2008).\n\nApplying the Concepts for Large Organizations\n\nLarge organizations demand software applications and hardware platforms that support their activities on a large scale. That is the case with large organizations such as Atlanta, GA. The software and hardware infrastructure is defined by file access mechanisms that are characterized by high-speed access to information, large backup facilities, fault-tolerant systems, backup methods such as mirroring, and RAID technologies. Storages are consolidated, and access is provided through networked infrastructure on a wide or local area network. Internet access for these systems and institutions provides resource access mechanisms through controlled access protocols. These protocols include web protocols, among others. Software platforms are defined by components that interact and interface with other applications. The infrastructure in these large organizations is interconnected through cables, and security mechanisms span firewalls, system audits, security levels, including privileged controls, among others. Hardware and software acquisitions for large organizations are made through proposals and vendor identification strategically tailored to meet the organizations\u2019 goals.\n\nReferences\n\nAdvanced Computer Architecture. (n.d). The architecture of Parallel Computers. Web.\n\nBurd, S. D. (2008). Systems Architecture . New York. Vikas Publishing House\n\nThisted, R. A. (1997). Computer Architecture. Web.\n",
        "label": "human"
    },
    {
        "input": "Advancements in Computer Science and Their Effects on Wireless Networks Essay\n\nThe most significant technological advancement witnessed in the 20th century was the expansion of World Wide Web in the 1990s. This resulted in the interconnectedness of millions of computers and other pages of information worldwide (Masrek et al. 199). Additionally, it was very cheap to share information across the globe. The introduction of laptops and other portable devices that replaced the desktop computer indicated that there was an influx of mobile wireless connections across the globe (Masrek et al. 199). With time the mobile phones and palmtops were included in the list of easy to move around and accessible networks. The wireless developments in the society have in addition greatly improved from the advent of these technologies.\n\nThe 21st century has witnessed a wide range of usage of wireless devices especially due to portability. This is attributed to the fact that the design of wireless devices does not emphasize on heavy computation and overly secure communication but it is often treated as add-ons (Peng & Sushil 45). Additional limitations such as shared medium have attracted a large number of users to wireless networks. Security procedures such as the jamming attack are difficult to detect and yet very easy to initiate (Peng & Sushil 45).\n\nSmith and Caputi (265) note that wireless networks are very cheap and thus are getting used in many modalities currently. Their uses range from the wireless local area networks to mesh and other sensor networks. As such, their range is wide making the provision of security and trustworthiness a critical issue. Generally, wireless networks are open in nature and are constructed on shared mediums, thus the provision of secure networks is very difficult in such instances (Peng & Sushil 45). An outsider can for instance interrupt a communication from taking place. This can be done by sending constant and other bogus messages that are periodically sent for collision purposes in the network. The bogus messages later lead to increased back off at the node level and other multiple transmissions of data. A jammer for instance can be referred to as a wireless device that produces radio interference attacks on a wireless network (Peng & Sushil 48).\n\nThe main idea behind a jammer is to block wireless connections and keep the medium solely for itself or deform a valid communication that is going on (Peng & Sushil 48). This goal can be successfully accomplished by warding off the traffic source of information from sending out packet data or by thwarting the reception of legitimate data packages. This process, just like hacking, is meant to retrieve information from an authorized user to unauthorized users. There are many jamming approaches and strategies that can be used by an attacker to disrupt communication. The most common is the time-based strategy, where the jamming signal is active and occurs in specified time (Smith & Caputi 268). Other more advanced schemes in jamming use the knowledge of the physical and layer specifications of the target system. Thus jamming is carried out by eliminating some radio frequency signal in the target system (Smith & Caputi 268). Jamming can however be effectively tackled by the PHY-layer communication techniques; which are based on spreading techniques, for instance the Frequency Hopping Spread Spectrum (FHSS). The installation of such systems ensures that the suppleness of the system is maintained (Smith & Caputi 270).\n\nAdvancements in computer science have made it possible for companies such as banks to lose billions through electronic fund transfers, which the culprits use via undetected bank systems. Additionally, through wireless communications, terrorists have been able to carry out their main targets. For instance in the 9/11 attacks, the supposed culprits purchased their air tickets through the undetected online system of the airport. Hacking, the unprofessional act of assessing unauthorized data has been possible through advancements in technology. As such, it has led to many problems for the international community, most notably emanating from the leaked messages from the United States government following their release by Julian Assange. Thus, many computer engineers have proposed the Mobility Oriented Trust system (MOTS) that makes use of the trust table by incorporating a trust node in each cable (Murgolo-Poore et al. 175).\n\nWireless communications where wireless management systems are installed can greatly improve the effectiveness and ease in the application of multiple procedures in an organization. Murgolo-Poore and colleagues add that the output of the various procedures when in good form increases productivity (179). Many organizations have thus transcribed to the wireless connections and the technology is greatly growing, assisted with the advancements in computer science. The concept of trust is very vital in communication and in the development of net work protocol designers, especially where the main intent is to establish a trust relationship in the participating nodes (Murgolo-Poore et al. 175). The latter allows for collaborative use of the system\u2019s metrics. Trust however, as noted, is built if interactions between users have been faithful. In any wireless network, trust is very fundamental and it can be defined as the degree of belief about the behavior of other entities and in most cases it is context \u2013 based. For instance one can be trusted as an expert in car fixing but not in network installations.\n\nWorks Cited\n\nMasrek, Mohamad Noorman, Jamaludin Adnan and Mukhtar Sobariah Awang. \u201cEvaluating academic library portal effectiveness .\u201d Journal of Library Review, 2010, 59.3, 198-212. Print.\n\nMurgolo-Poore Marie E., Pitt Leyland F., Berthon Pierre R. and Prendegast Gerard. \u201cCorporate intelligence dissemination as a consequence of intranet effectiveness: an empirical study.\u201d Public Relations Review , 2003, 29.2, 171-84. Print.\n\nPeng Ning and Sushil Jajodia, \u201cIntrusion detection techniques.\u201d The Internet Encyclopedia . John Wiley & Sons. 2003. Print.\n\nSmith, Brooke and Caputi Peter. \u201cCognitive interference model of computer anxiety: Implications for computer-based assessment.\u201d Behavior & Information Technology , 2001, 20.4, 265 \u2013 273. Print.\n",
        "label": "human"
    },
    {
        "input": "Assessing and Mitigating the Risks to a Hypothetical Computer System Research Paper\n\nThe security of information is very important for the success of any organization and therefore should be given the first priority in the organization\u2019s strategic plans. The computer system of a Hypothetical Government Agency (HGA) faces quite a number of information security threats that need to be brought under control for the agency to operate efficiently (Newman, 2009). Some of the major threats to HGA\u2019s computer system include accidental loss of information, virus contamination, theft, unauthorized access and natural disasters. This paper will highlight some of the Biometric solutions the agency has put in place to address computer security issues and finally assess the role of Biometric systems in an Information Technology environment.\n\nPayroll fraud and errors is one of the major security issues that occur due to inadequate information security system (Vacca, 2009). This threat can be solved by initiating an automated payroll process that is in line with both Government and HGA\u2019s information security policies. The time and attendance data should be concealed as much as possible to reduce chances of manipulation. The other security threat is unauthorized execution that can be controlled if the system administrator is the only person who can control privilege to server programs (Vacca, 2009). The Local Area Network should not remain operational past the normal working hours by installing a limited configuration to the system. Apart from configuring clerk and supervisory functions, there should be constant managerial reviews and auditing to check for any unauthorized execution (Newman, 2009). Errors experienced during entry of data can be minimized by entering time and attendance sheets in duplicate.\n\nAccidental corruption in information systems can be controlled by having a special program that limits access to the system server (Kizza, 2008). The loss of time and attendance data is prevented by having backups of the server disks incase of a disaster or any accidental loss. The time and attendance data can also be left to be online for three years before the data is taken and stored in the archives. HGA has policies in place that are supposed to ensure there is continuity in all the major operations of the agency. One of the major security threats in HGA\u2019s computer system is constant interruption of its operations by various reasons (Kizza, 2008). The threat of interruption of operations is controlled by developing contingency plans that are supposed to ensure that adverse circumstance does not interfere with continuity of operations (Newman, 2009).\n\nIn conclusion, it is important to note that biometric systems are very important in any Information Technology environment. Information systems have network-related and virus threats that require biometric measures in order to protect the system and ensure security of information (Kizza, 2008). The risk assessment team reports all the threats found in the system so that all fraud vulnerabilities are mitigated as soon as possible. Authentication mechanisms are very key to solving the problem of payroll fraud through time and attendance data. All the paper work procedures and information handling procedures must comply with HGA\u2019s policies to safeguard the system against all forms of corruption (Vacca, 2009). Regular auditing and protection of the system from external networks are some of the biometric solutions to information security issues. Information security is essential for the successful operation of any organization.\n\nReferences\n\nKizza, J. M. (2008). A guide to computer network security. New York, NY: Springer.\n\nNewman, R. C. (2009). Computer security: Protecting digital resources. New York, NY: Jones & Bartlett Learning.\n\nVacca, J. R. (2009). Computer and information security handbook . New York, NY: Morgan Kaufmann.\n",
        "label": "human"
    },
    {
        "input": "Choosing an Appropriate Computer System for the Home Use Essay\n\nExecutive Summary\n\nThe report introduced the topic of personal computers. It looked at the history of how personal computers have evolved to become one of the most adopted gargets in businesses and the personal lives of many individuals. The study entailed both primary and secondary research that aimed at enlightening the researcher about the most vital hardware and software of a personal computer. A survey to determine the prices of the various requirements of a PC was conducted. The results were displayed in tabular form before appropriate recommendations were given on the best personal computer to be purchased.\n\nIntroduction\n\nThe contemporary world has witnessed a great deal of advancement in technology and particularly in Information Technology. This particular advancement has resulted in the adoption of Information Technology by many organizations to enhance their performance. Information technology has been widely adopted across all fields ranging from schools, hospitals, businesses, government offices, and households. Information technology is associated with the current enhancement in communication within and across borders. On the other hand, the widespread of information technology has led to an increase in the production of computers which include computer hardware and software to address the ever-rising technological requirements. Many firms have ventured into this new business of manufacturing computers and computer accessories. Some of the major companies that are renowned in the manufacture of computers and computer accessories include Dell, HP, Compaq, and IBM.\n\nThe widespread adoption of computers greatly enhanced in 2001. During 2001 more than 125million personal computers were manufactured in contrast to 48 thousand that were produced in 1977. The widespread intensified further such that bb the end of the year 2002, more than 500 million personal computers manufactured and sold worldwide, it is estimated than 75% those computers that were produced were used for office work, while the rest which comprised of 25 percent were for personal use. Among the computers manufactured, 81.5 % were desktops, 16.4% laptops while the rest comprised 2.1 % being servers. The United States of America received 394 million computers which accounted for 38.8% of those computers shipped. Europe received 25% while 11.7% of the shipped computers were delivered to the Asia-pacific region. In 2008, the number of personal computers that were in use globally had increased up to one billion. It is estimated that by the year 2014 the number of computers that will be in use will have doubled from one billion to two billion. The major consumers of the manufactured computers are Japan, the United States of America, and Western Europe which accounts for 58 percent of all the produced personal computers.\n\nBackground Information\n\nIn the early 1970s, personal computers were referred to as microcomputers and were often sold in kit form and limited quantities. They were only sold to only hobbyists and technicians. They contained very little programming that possessed only the toggle switches to input instructions and the processed information was output by front panel lamps. They required hardware such as computer terminals, keyboards, hard disks, and printers to be used practically. Micral N is known as the earliest commercial personal computer designed. It was designed with a microprocessor, the Intel 8008. It was manufactured in 1972 and 90,000 pieces of these computers were sold. In the early 1970 and 1980, most computers were manufactured and sold for household use. They used software that was meant for personal productivity, programming, and games. As time passed personal computers become more common. They lost their technical distinction and business computers that had color, sound and graphics became more common. Business people and game systems users started using the same computers that shared similar processors and operating systems. The nature of personal computers changed to resemble those computers that were tailored for businesses that even possessed local area networks to enhance communications.\n\nA personal computer is a general\u2013purpose computer. It is often a desktop computer whose size, specifications, and sales price are designed to meet the requirements of an end-user. A personal computer is designed with hardware that is necessary for personal use. Similarly, such a computer is also installed with application programs that are meant to meet the requirements of the operator. Among the common application software include emails, games, Ms. Office applications, emails, and digital media among others.\n\nProblem state statement\n\nThe high rise in the level of information technology has resulted in many organizations venturing into the business of manufacturing computers and computer accessories. This has resulted in a big challenge of monitoring the quality of manufactured products where some of the computer hardware and software that are currently produced are of very low quality. In addition, many computer users lack adequate information about the necessary hardware and software that their personal computers should be equipped to enhance their usage. It is from these perspectives that this report investigates the recommended requirements of a personal computer in terms of necessary quality hardware and software for a personal computer.\n\nPurpose of the Study\n\nThe findings of this report will help in informing the reader about the hardware and software that are required in a personal computer. It will also advocate the right specifications in terms of speed and size that are most suitable for a personal computer.\n\nMethodology\n\nThis report entailed the use of both secondary and primary research. The researcher reviewed relevant books, journals, and articles about the requirements of a personal computer. To supplement the study researcher the researcher also interviewed several computer technicians to help him identify the best computer brands that are available. In addition, the researcher visited various shops that specialized in the sale of computers, computer software, and accessories and tried to find out the prices at which they were selling various personal computer products.\n\nLiterature Review\n\nA personal computer is composed of two parts, hardware, and software. The hardware represents the physical components of a computer. They include the computer case, mouse, keyboard, CPU, Memory, motherboard, disks, and printer (Messmer, 2001).\n\nA personal computer case is a thin sheet that encloses some hardware of the computer such as motherboard, hard disk, processor, and memory. The cases are often offered in two forms namely desktop and the tower. Today the tower is more commonly adapted than the tower. The tower is placed on the floor next to or under the desk. On the other hand, the desktop is placed on the desk and the monitor is placed on top of it. The towers are mostly offered in two shapes which are the ATX 12 which can accommodate the wide motherboard and the ATX 8.5 wide which can accommodate the smaller motherboard. The motherboard and power supply are mounted on the floor and mainly at the rear end. The drives which include the hard disk and the CD/DVD are usually mounted at the enclosures that are known as the drive bays which are located at front of the case (Anthony, 2000).\n\nThe motherboard is a major component of a PC where many of the circuits in a personal computer are located. It contains all the components and circuits that operate the personal computer. The motherboards have the following components CPU, BIOS RTC and Chip Set. The bios act like a nonvolatile memory that keeps the computer configuration information. It contains all the necessary codes that are required by the computer to communicate with the mouse, keyboard, communication devices, and disk drives. When the computer is switched on, the BIOS gives the codes that are required to set up the necessary functions so that the PC can be in a position to operate. The RTC which is referred to as the real-time clock is used to keep time, day, and date in a 24-hour format just like any other watch. The PC uses this clock to indicate the time when files are created or modified ( Messmer, 2001).\n\nA power supply is another important component of a PC. It is contained at the back corner of a PC case and next to the motherboard. It converts the direct voltage that is supplied through the grid system into a direct voltage that is used by other components in the PC. It contains a 20 conductor cable that carries +5vdc, -5vdc +12vdc, -12vdc and is usually grounded to the motherboard. It also has another pair of cables with four conductors and two 4-pin connectors that carry +5vdc, +12vdc, and ground to the hard disk, floppy disk, or CD/DVD. Po supplies are rated at 200-500 watts.\n\nThe hard disk is installed in one of the drive bays that are present in the personal computer. It is usually powered by a 4 conductor cable that originates from the power supply. Data to and fro the motherboard is usually transmitted by a 40-pin integrated drive electronic cable. Data in the disk is stored magnetically on multiple and stationary disks that are stacked on top of each other. Small arm-like objects with magnetic pickups move hurriedly back and forth across the bottom and top surface of each disk drive. The sensors float a few microns over the rotating disk surface and can read and write at a very high speed. Most of the common hard disks are rated at 5400 or 7200 revolutions per minute. The rate of data transfer from them to the motherboard is approximately 66 Mbytes/sec. They exist in various sizes that range from 40GB, 80GB, and 120GB among others ( Messmer, 2001).\n\nThe floppy disk is installed in one of the external bays in front of the personal computer case and is usually secured in position by screws. It is generally powered by a cable with a 4-pin connector that originates from the power supply. It transmits data to and fro the motherboard with a help of a 34 pin ribbon cable. It keeps data magnetically on a removable floppy disk. A pickup arm-like feature in the drive floats over the disk surface. The arm-like feature usually moves hurriedly back and forth across the disk surface as a small magnetic sensor at the rear of the arm writes and reads data on the rotating surface. Floppy disks hold 1.44 Mbytes, but these days floppy disks are rarely used as they have been surpassed by CD/DVD and flask disk (Davidson, 2007).\n\nCompact Disk Drive often referred to as CDD is installed in one of the external drive bays located in front of the PC case. It is usually secured in position by machines screws. Some manufactures of personal PC provide rail-like system systems that are mounted on the CDD. This makes the drive to be easily removed from the PC without having to remove any screw. Data is usually stored optically on the surface of the disk. A laser is attached to an arm-like object that moves forth and back across close to the disk surface and sends light towards the disk surface that is covered with a slim layer of aluminum. The CDD is currently the most common removable storage media for personal computers and can accommodate up to 700 Mbytes of data. It derives its power from the power supply through a 4-pin cable. A 40-pi IDE cable carries data from and to the motherboard. There are generally two Compact Disks Drives available which are CD-ROM which stands for read-only memory. This is the earlier model of CDD. It can only read CDs. It generally reads standard CD and most CD-R type disks. Often it is also able to read CD-RW disk type. The other type is CD-RW. This is a Rewritable. It can read and write CD-R and CD-RW disks types. It can also read standard CD-disks type (Davidson, 2007).\n\nThe Digital Versatile Disk drive is often referred to as DVD. It is installed in one of the external drive bays on a personal computer. It is generally secured in position with the help of machine screws. In some machines, they have special rail-like systems where the DVD are mounted so that they can remove them more easily without removing any screw. The drive is manufactured in a manner to enable it to access data kept in a DVD. A laser moves to and fro close to the disk surface and gets data at a very fast speed. It delivers its power through a 4-pin cable that connects the CDD to the power supply. It uses a 40-pin integrated Drive Electronic cable to transmit data from and to the motherboard. These two categories of DVD namely DVD-ROM and DVD-RAM. DVD reads DVDs and CDs while Random Access Memory reads and writes DVDs and can equally read CDs.\n\nThe monitors are personal computers output hardware. They are generally grouped into two Cathode Ray Tubes often referred to as CRT and Liquid Crystal Display commonly referred to as LCD. The CRT is the earlier version of monitors. They are generally big and heavy. They are available in screen sizes that range from 14 to 21. Data is commonly transmitted to the display by a cable that has a 15-pin shell connector that plugs into a connector on the video card which plugs into the personal expansion slot. The LCD is less bulky and heavy than CRT. The LCDs are connected to the personal computers in the same way the CRT is connected. They are more convenient to use than the CRT (Davidson, 2007).\n\nThe keyboard is another important component of the personal computer. Keyboards are used to input data into the computer. They occur as either PS2 OR USB. Some keyboards offer more functions such as volume control for speakers or web browser controls. There are wireless keyboards that use infrared rays to communicate.\n\nThe mouse IS also important hardware in a computer system. It also occurs either informs of a USB or PS2. There is also a wireless mouse that uses infrared rays to communicate.\n\nPersonal computers are designed with audio output devices. Personal computers use speakers and headphones as their major audio outputs. Similarly, others have monitors that have speakers mounted on their sides. Some passive speakers are plugged into personal computers and are powered directly by the output signal. On the other hand, active speakers which are powered from the sound card using battery or rectified AC house power are often used by personal computers and especially where amplification of the sound is needed. The printer is another important output device that is used to output softcopy into hardcopy.\n\nSoftware\n\nPersonal computers are also installed with various computer software programs. The operating system is a major program that is necessary for the operation of a PC. The operating system supports the system files that are necessary for the booting process of the computer. There are various types of operating programs such as XP, Window 2000, Vista, and Linux among others. Other application programs that are common in personal computers include Microsoft word, database, spreadsheet, PowerPoint, publisher, and PageMaker. Other application packages are installed depending on the interests or usages of the owner (Tanenbaum, 2005).\n\nUses of the Personal Computer\n\nThe personal computer has various uses such as research work, playing computer games, entertainment, and communication. The PC should be connected with the internet to enable the user being in a position to access various learning materials over the net with the help of the search engines such as goggles. The MS Office program installed in the system assists the user in his daily activities. For instance, he/she can use the Microsoft word application to type his mails or write personal notes. Similarly, he/she can use the spreadsheet provided by the MS Office to do some personal calculations. The spreadsheet can be very essential when the user is conducting quantitative research. He can use the application to analyze the data collected into meaningful information which can be outputted graphically and even printed and accessed in form of a hardcopy (Anthony, 2000).\n\nThe personal computer is also used for entertainment. For it to be very effective for entertainment, the system should be fitted with a window media player to enable it to play music accordingly. The PC should also be fitted with an active speaker which will enhance the music played by the computer by amplifying it accordingly. Similarly, the PC is still be used to watch movies and soap operas. The LCD is the best monitor to use since it is more portable than the CRT (Anthony, 2000).\n\nThe personal computer also assists in enhancing the communications of the user greatly. Since most computer systems are connected to the internet, the user uses the computer to chat with his friends, send emails, or communicate with his family members through chat, emails, Facebook, or Twitter.\n\nFindings of the primary study\n\nThe following table illustrates the prices of personal computers from among the renowned computer brands that are readily available in the market worldwide. The illustrated computers have the same specifications, but the difference in the prices is a result of the market performances of the brands.\n\nSpecifications\n\nModel  Processor Speed memory (GHZ)  Hard disk (GHZ)  CDD     LCD          Cost\nHp     3.0                           120              DVD-RW  17 SIZE LCD  $530\nDell   3.0                           120              DVD-RW  17 SIZE LCD  $500\nIBM    3.0                           120              DVD-RW  17 SIZE LCD  $515\n\n\nRecommendation\n\nThe best computer to buy as a personal computer should be the hp dual core which has a speed of 3.0 GHZ. It should also have a memory of 1GB, a DVD-RW with an LCD display. The market price of this computer is $ 530.The computer should also be purchased together with a printer which will also be a HP model Photosmart Plus B210a E at a rate of $ 200.The printer will be used to print documents and photographs of the user. In addition, the computer should be installed with XP software which will cost $ 100. The XP was preferred as the best operating system since it is user friendly and supports many programs. The computer will also be installed with MS Office applications which will cost $ 100. In addition, the personal computer should be installed with a Kaspersky Anti-Virus 2012 that will cost $ 50. This software will help to protect the system from being infected by virus which can delete the files saved in the system or crush the system all together. In addition, the personal computer should be connected to the wireless internet at a cost of $ 500. The internet will help the user to search for materials while conducting his/her research. Similarly, the internet will assist the user to communicate with his friends or colleagues through the chats, emails, face book or twitter. The computer should also be accompanied with an active speaker that will cost $ 50. An LCD monitor that will be used to output the PC information will also be purchased. The LCD that will be purchased will be of size 17 and it will cost $150.An LCD monitor was preferred over a CRT because it is more portable than the CRT which is regarded as bulky and heavy. Since the PC purchased will be used for the entertainment, active speakers that amplify sound will be purchased to amplify the music appropriately. Joy sticks and appropriate software for playing the computer games will also be considered. The choice to purchase a HP computer was arrived at because Hp is a renowned brand of high quality. Its price is relatively affordable because it ranges relatively to the price of the other popular brands.\n\nReference List\n\nAnthony, J., 2000 Computer Hardware . Web.\n\nDavidson, P., 2007. The Architecture of Computer Hardware, Systems Software, & Networking . New York: Prentice Hall.\n\nMessmer, H.P., 2001. The Indispensable PC Hardware Book . New York: Prentice Hall.\n\n.Tanenbaum, A.S., 2005. Operating Systems Design and Implementation . London: Oxford University Press.\n",
        "label": "human"
    },
    {
        "input": "Computer Components in the Future Research Paper\n\nTechnological innovation is always at the forefront of computer component technology with advances in production and development resulting in faster, better and lighter products (Bursky, 26). These changes can be seen with the improved memory capacities inherent in today\u2019s hard disk systems, the increased processing power of processors, new innovations in disk drive technology which enable greater media storage capacities as well as new technologies enabling better component cooling (Bursky, 26). It must be noted though that not all technological innovations are actually inherently adoptable by the general population. For example, during the late 1990\u2019s and early 2001 one of the latest innovations in external storage methods was the development of the ZIP drive. Back then it was thought of as a revolutionary concept in external storage however it was never truly adapted by the general population due to subsequent advances in driver technology which enabled people to burn information onto CDs. Nicholas Carr in his article, \u201cIT doesn\u2019t matter\u201d, which examines the use of technologies and their implications on society explains that technologies and their widespread use only become cheaper once they reach their build out completion. The term \u201cbuild out completion\u201d refers to a point in technological development wherein a type of technology has already reached commercial viability and can be effectively replicated and mass produced. Carr explains that so long as certain forms of technology have not reached a point of build out completion they will most likely never be adopted due to their prohibitive costs and the uncertainty attached to the technology itself. This particular lesson can be seen in the case of ZIP drive technology wherein the uncertainty behind its use led to it never being adequately adopted by the general population. It is based on this that it can be assumed that not all technological innovations will actually be adopted by the general population and this includes several of the new innovations currently being released in the market today. For example, the advent of 3D computer screens is heralded by many as a possible new standard to computer viewing yet industry data shows that not only is its usage unwieldy for the average user it serves no purpose for normal computer tasks such as using word processing software or utilizing the internet. When trying to determine what the future holds for computer component technology the manufacturing process itself should also be taken into consideration. Lately various consumer report groups have stated that certain PC component shave increasingly been found to actually be designed to eventually break down due to inferior materials being used. Such a case is actually quite true in the case of certain components whose operational lives are limited to only a few years and are not meant to last more than 4 years of continuous usage at most. The reason behind this lies with changes in the method of production wherein components are no longer being built to last but rather are being built with the current pace of innovations and consumer demands for cheap components. While current trends in various computer innovations may seem like the future of computer component technology it should not be assumed that they will actually attain the status of general utilization since various factors such as consumer adaptability and their inherent build out completion need to be taken into account.\n\nFuture Computer Components and their Durability\n\nWhen examining computer components made recently with those constructed 12 years ago it can be seen that older parts are bulkier, slower, and of course less advanced as compared to recent creations yet for some reason older computer parts seem to have a longer operational timeline compared to some of the newer parts created. Operational timelines refer to the length of time a particular component is expected to work under normal operational conditions. On average this can range from 3 to 5 years depending on the rate of usage before the parts begin to cease functioning. Yet an examination of various studies examining the durability of various computer parts constructed in 1998 show that components used back then apparently still work all the way till 2012. While it can be argued that those particular components do not endure the same type of \u201cpunishment\u201d components today undergo the fact remains that when comparing the operational timeline of computer components constructed within the past year they are apparently getting lower and lower with some components lasting only 2 to 3 years before problems begin to occur. This rather strange phenomenon brings up a rather intriguing question: if advances in technology are suppose to make components better why are they breaking down sooner as compared to parts constructed in previous generations? An examination of previous computer parts show that a majority of the components are far heavier as compared to recently constructed parts. While it may be true that as technologies improve components get lighter further examination reveals that older model components seem to have a far sturdier basis for construction as compared to recently created parts. This can range from using heavier plastics, stronger silicon and using more metal in the construction of the parts themselves. This translates into greater durability over the long term as compared to parts that would use cheaper forms of metal and lighter materials. In comparison computer components today are made of far lighter materials which do translate into better heat and electrical conductivity however this also makes them far more prone to break down as compared to parts constructed out of heavier materials. The fact is computer components created 10 years ago were actually created to last for a significantly longer time as compared to parts today. In fact various studies examining advances in computer component technology reveal that several parts manufacturers actually build their components to eventually break down. The reasoning behind this lies with the fact that with the current rate of technological innovation building parts to last does not make as much sense as it used to since components are replaced with newer models and types on an almost yearly basis. More durable components translate into greater production costs which results in higher component prices. With competition in the component industry being determined by who can produce the latest type of product at the lowest price it is not competitively feasible to sell components at a higher cost on the basis that they are more durable. In effect durability no longer holds as much sway as it used to due to the current fast paced advances in technological innovation which almost ensures that public companies or private individuals replace their various computer components before they break down. Another factor to take into consideration is the fact that a majority of consumer buying behavior is geared more towards acquiring the latest parts rather than the most durable. Not only that, various studies examining consumer buying behaviors show that more consumers buy components on the basis of their low prices as compared to determining whether a particular component is durable or not. This in effect encourages companies to adopt a production strategy that focuses more on producing cheap components while at the same time sacrificing their durability. It is based on this that it can be expected that computer components in the future will be manufactured in such a way that manufacturers will intend for them to fail after a certain degree of usage. The inherent problem with this possible future situation is that while it inherently benefits companies due to a continuous stream of income it does not benefit certain segments of the population who cannot afford continuous component changes due to a limited component life spans.\n\nSwitching from Air Cooled to Water Cooled Systems to Mineral Oil Cooled Systems\n\nNearly 85% of all computer systems in the world utilize traditional air cooling technologies in order to control the high temperatures created by either the processor, the north and south bridge in the motherboard, the video card and various other components within a computer that generate heat through constant usage (EDN, 16). This process usually involves a metal plate directly being attached to a particular component with a set of metal attachments being connected directly to fans from which the heat is dissipated via cool air being circulated directly to the component through the fan system (EDN, 16). Auxiliary fans are also usually connected to various PC casings from which cool air from the exterior of the casing is circulated towards the inside with warm air being removed via an alternative fan system. This particular setup has been utilized for the past 17 years in a majority of computer systems today and continues to remain one of the dominant methods of cooling PC components (Goldsborough, 30). Unfortunately due to the increased temperatures produced by various parts this particular system has begun to reach its limitations in terms of effective usability. While fan systems are effective in bringing cold air into various systems they are actually ineffective in keeping temperatures low within casings over a prolonged period of time. In fact, over a certain period of time fan systems fail to keep temperatures within nominal levels for proper operation and this often leads to parts breaking down earlier than they should due to the high temperatures within the casing that the fans are unable to effectively control. This often leads to burned out components, sudden computer shut downs and a variety of other results normally associated with such ineffective systems for cooling. One method that companies have utilized in order to resolve this particular issue has been to constantly keep the areas where a number of systems are located under a particular temperature through various air conditioning units. While this particular method of resolving growing PC temperatures is effective in the short term the fact remains that in the long term such a method of temperature control is costly and as such an alternative means of resolving this particular problem is needed. In the past 5 years one of the growing alternatives to resolving the heating problem of PC\u2019s has been to utilize liquid cooling systems as a replacement for traditional fan cooling systems (Goldsborough, 30). Liquid cooling systems utilize a series of tubes containing a mixture of distilled water and various coolants which circulate towards a cooling plate directly attached to the component producing heat (Upadhya and Rebarber, 22). Heat exchange occurs when cool water from the system hits the heat sink and absorbs the heat transferring it away from the system via the serious of tubes towards a radiator that expels the heat and cools down the water (Upadhya and Rebarber, 22). Unlike conventional fan systems liquid cooling can reduce heat up to 60 percent better and produces far less sound resulting in better component longevity and better long term performance from a PC (Upadhya and Rebarber, 22). It must be noted though that liquid cooling systems utilize more electricity compared to traditional fan cooling systems due to the use of both a pump and a radiator in order to dissipate the heat that and such systems are also more maintenance oriented due to the necessity of checking the coolant levels in order to ensure that there is enough liquid coolant to make the system work popular. Unfortunately, the utilization of liquid cooling systems over most of the current consumer market is still isolated to a few select groups such as gamers, graphics artists and variety of other users that utilize systems with high core temperatures. One of the reasons could be the inherent price of liquid cooling systems which are considerably higher than normal fan cooling systems. Nicholas Carr in his article, \u201cIT doesn\u2019t matter\u201d, which examines the use of technologies and their implications on society, explains that technologies and their widespread use only become cheaper once they reach their build out completion. The term \u201cbuild out completion\u201d refers to a point in technological development wherein a type of technology has already reached commercial viability and can be effectively replicated and mass produced. Carr explains that so long as certain forms of technology have not reached a point of build out completion they will most likely never be adopted due to their prohibitive costs and the uncertainty attached to the technology itself. Based on this it can be assumed that another reason why the use of liquid cooling technologies hasn\u2019t achieved widespread use is due to the fact that it hasn\u2019t reached its build out completion thus creating uncertainty in the technology itself. An examination of current methods of liquid cooling show that while the technology itself has taken great strides in innovation, effective systems are still rather unwieldy for the average computer user. This as a result has created a certain level of consumer uncertainty in the product itself despite the fact that it is a far better alternative compared to fan cooling systems. Based on this it can be assumed that general adoption of liquid cooling systems will come in future only after the technology itself has reached sufficient build out completion that it could be utilized easily by average computer user and will be at a sufficiently lower cost. It must be noted though that the general utilization of liquid cooling systems is only one possibility that could occur for computer component cooling technologies in the future; recently the use of mineral oil cooling systems has taken been gaining a substantial following resulting in it being a possible contender in the future as a primary cooling technology. Mineral oil cool utilizes a different heat exchange principal compared to either fan cooling technologies or liquid cooling; it involves pouring a substantial amount of mineral oil into a water tight casing in order to immerse all the computer components. After which the oil is subsequently pumped through a series of tubes into an external radiator in order to dissipate the heat accumulated through the oil. The technical aspect behind this particular system is actually rather simple; heat from the various computer components is transferred directly to the mineral oil and is then subsequently cooled down by the external radiator. This actually ensures the parts rarely accumulate significant heat, it must also be noted that due to the special qualities of mineral oil it does not cause electrical shortages in the equipment unlike when the components are submerged in liquids such as water. Similar to the case of liquid cooling systems mineral oil technologies are far from their build out completion stage and as such it really cannot be said whether mineral oil systems or liquid cooling systems will become the dominant form of component cooling in the future. It will all depend on which system is the most feasible to commercially utilize in the immediate future and whether it can be adapted for general usage.\n\nCPU\u2019s made out of Diamond\n\nIn relation to the topic mentioned earlier involving heat dissipation technologies one of the latest breakthroughs in semi-conductor research is the utilization of diamond based semi-conductors as a replacement for current silicon based chipsets. What must be understood is that as consumers demand more performance and processing power from CPUs companies in turn have developed smaller transistors within processors in order to provide consumers with the processing power they need (Merritt, 12). Unfortunately, as CPUs get increasing smaller, more sophisticated and possess more processing power the end result is greater difficulties in terms of thermal dissipation. On average a single processor consumes nearly a hundred watts of electricity in order to maintain proper operational standards, as the number of processes increase so to does the level of power utilized (Merritt, 12). Unfortunately dissipating heat from an area that is smaller than one square centimeter presents itself as a significant problem for various chip manufacturers since the amount of heat produced tends to reach significant levels after a certain period of time (Davis, 37 \u2013 38). With the release of core i7 processors by Intel as well as the latest Intel Sandy Bridge processor the end result are chips which require increasing sophisticated methods of cooling which conventional fan systems are hard pressed to provide. While alternatives do exist such as the use of liquid cooling systems or mineral oil systems these are still far from their build out completion stage and as such are not generally used by most consumers (Oskin, 70-72). This presents itself as a significant problem for processor manufacturers such as Intel since recent consumer trends show that on average consumers demand higher processing power nearly every 2 years due to the demands of ever increasingly sophisticated software systems. Unfortunately silicon based processors show signs of thermal stress after they reach temperatures of 100 degrees Celsius or more (Davis, 37 \u2013 38). If consumer demands are to be met companies would need to increase the capacity of their current processors which would in turn increase the amount of heat produced which would very likely cause the processors to literally melt as a result of increased thermal stress. It must be noted that as technological innovations continue to grow companies increasingly find that the traditional materials and components they utilize have actually reached the limits of their usability and can no longer provide the necessary structural infrastructure needed for their products. It is this particular situation that current processor manufacturers find themselves in (Oskin, 70-72). One possible alternative to utilizing silicon that processor manufacturers say shows potential is the use of diamond based semi-conductors. It is a well known fact that diamonds are in fact the hardest substance known to man and due to their inherent qualities possess useful properties that can be utilized in the production of robust processors. On average diamonds possess the ability to conduct heat better than silicon, they have an inherently high breakdown voltage and it must be noted that they also possess a distinctly high carrier mobility. Not only that, as mentioned earlier silicon based processors have a tendency to show severe thermal stress when temperatures reach 100 degrees Celsius, diamonds on the other hand can endure several times that particular temperature and still have little ill effect. It must be noted that though that various critics are skeptical over the use of diamond based CPU\u2019s due to the fact that a diamond is based from carbon and as such acts as an insulator rather than a semi-conductor. One way around this that researchers have discovered is to dope the diamond in boron resulting in it turning into a p-type semi-conductor. Not only that, by reversing the charge of the boron this in effect enabled the creation of an n-type semiconductor. It must be noted that both p-type and n-type semi-conductors are needed to create a transistor and as such the ability to create both through a diamond doped in boron indicates the definite possibility of effectively creating diamond based processors in the future. Unfortunately one of the inherent problems in utilizing diamonds as processors are their inherent cost. Due to the relative rarity of diamonds and the fact that they are coveted by the jewelry industry their utilization as a replacement for current semiconductor technology seems infeasible due to the fact they would not be cost effective for both consumers and manufacturers. Researchers found one way around this by effectively creating a process the creates artificial diamond \u201csheets\u201d that are purer and on par in terms of overall hardness compared to real diamonds. The only difference is with this particular process the diamond like material can be molded into various different shapes depending on the use required and it is actually cheaper to create as compared to various semi-conductor sheets used in today\u2019s processor industry. Based on this innovation various experts agree that it might be possible for processors to achieve speeds equivalent to 81 GHz without the processors subsequently melting from the sheer amount of heat produced by such an activity. It must be noted though that the utilization of this particular type of technology is still in the testing phase and as such will not come into effect for another 10 years at the very least.\n\nBlu-ray Disks and the Future of Disk Drives\n\nDisk drives have been an ubiquitous part of most computer setups for the past 18 years or so making it one of the most well known and used components in any computer. As of late though, the advances in driver technology have slowly updated the technology from initially allowing consumers to burn information onto a CD-rom disk to eventually allowing them to watch and burn DVDs. The latest incarnation of the technological advances in driver technology have resulted in the creation of the Blu-ray disk format enabling consumers to store up to 30 to 40 gigabytes of data onto a single disk (Digtial Content, 39). The creation of Blu-ray disk technology actually follows a distinct trend in driver technology wherein every few years drivers and disks are created that have higher storage capacities, faster speeds, better video resolution quality and an assortment of added functions that surpasses the previous generation. Initially CD-rom disks had a capacity of 700 mb this changed when DVD based technologies and drivers came into the picture resulting in storage capacities reaching 4 to 6 gigabytes depending on the type of DVD bought. Today, Blu-ray disk technology enables storage capacities of up to 30 gigabytes which far surpasses the storage capabilities of DVD based drivers and disks. Based on this trend in advances for this particular aspect of computer component technology it can be assumed that Blu-ray disk technology will not be the final version of disk drive technology rather it is merely another evolutionary level (Perenson, 94). In fact disk production companies such as Philips, Sony, Maxwell, TDK, Optware and Panasonic have already announced a new potentially new form of media consumption in the form of the HVD disk which is slated for release in the next few years. HVD or Holographic Versatile Disk utilizes a new form of holographic embedding technology enabling data to be stored holographically onto the surface of a disk thus enabling greater storage capacity on a smaller surface area (Digtial Content, 39). In fact estimates show that a single HVD disk has the capacity to hold up to 6 terabytes of data, greatly exceeding the 30 gigabytes of data most Blu-ray disks can hold. It must be noted though that one of the more unfortunate aspects of disk drive technology is the fact that when new drivers and types of disk come out it becomes a necessity to transfer data from the older version of the technology to the newer type which is an arduous process at best (Medford, 8). In recent years the degree of technological innovation has advanced to such an extent that newer versions of disk drivers and disks come out nearly every 2 or 3 years resulting in a painful cycle for consumers as they migrate their data from one storage medium to the next (Digtial Content, 39). Based on this it can be seen that disk drive technology has an inherent weakness connected to the migration of consumer data from one version of the technology to the next. In fact it can even be said that no matter how far this particular technology progresses in terms of storage capacity and sharper video playback it will still have the same data migration problems that consumers now face. At times it must even be questioned whether disk drive technology is even necessary at all. For example, at the present solid state devices such as USBs are one of the dominant forms of external storage due to the ease of data transfer that they provide (Digtial Content, 39). While such devices are no where near the capacities of the future HVD format the fact remains that they provide a far easier method of data transfer as compared to the disks (Medford, 8). Another factor that must be taken into consideration is whether regular consumers really need disk formats that can store 6 terabytes of data. While it may be true that in the current information age the amount of media consumed by the average person reaches several hundred gigabytes the fact remains that it doesn\u2019t go above 500 gigabytes. In fact consumers that are able to consume storage capacities of 1 terabyte and up are in the relative minority compared to most computer users today. Another factor that must be taken into consideration is the fact that with the current popularity of cloud computing especially with the release of Apple\u2019s iCloud network this has in effect made data storage more of a problem for companies rather then regular consumers. It is based on this that it can be stated that the development of increased storage for regular consumers should follow a slower rate of consumption in order to lessen the frustration of continuous changes to new media storage formats. Unfortunately, based on current trends in technology releases companies seem to be more inclined to release new media storage formats without taking into account actual consumer necessity behind the release. What is currently occurring is case where consumers seem to irrationally inclined to follow new media storage formats such as the Blu-ray disk without taking into account the fact that their current method of storage is perfectly fine. Companies take advantage of this by continuously releasing new storage formats since they know consumers will follow the new formats and port their data over to the new storage devices. This particular situation benefits companies more so than it does actual consumers and as such, based on current trend and consumer behavior, the future of disk drive technology seems to be destined for the continuous release of ever increasing file storage methods which consumers will unlikely need but will buy nonetheless.\n\nChanges in Display Technology: is 3D the future of Digital Display Technology?\n\nFor many computer users, classic CRT display technology has been around for 25 years and was once one of the most used types of monitors in the computer component industry. Yet with the development of cheap LCD technologies within the past 7 years or so CRT screens are beginning to be phased out in favor of cheaper and more cost effect LCD screens. It must be noted that unlike other forms of component technology monitors tend to be rather slower in technological innovation. While it may be true that in the market today there are several brands and types of LCD screens ranging from small 22 inch screens to massive 41 inch LCD monstrosities the fact remains that most of them adhere to the same basic design principles with certain additions added in by manufacturers to differentiate them from the rest (Kubota and Yazawa, 942 \u2013 949). In fact within the past 5 years the technology has only improved slightly with the creation of High definition screens and LED display systems but the basic design and components utilized are still roughly the same. It must be noted that unlike the developments seen in processor technology, disk drives and pc cooling, enhancements in display technology only benefits the visual aspect of a user\u2019s experience. It does little to improve PC performance, longevity, and in fact a large percentage of current consumers tend to stick to the classic LCD models and types developed 5 years ago as compared to the newer high definition LED screens used today. The reason behind this is rather simple; most people are unwilling to pay higher prices for a technology that can be obtained at a lower cost with little discernable change in PC performance (Kubota and Yazawa, 942 \u2013 949). In fact a majority of most PCs are used mainly for work and as such unless a person is in the media industry high definition screens aren\u2019t really a necessity. While it may be true that as of late there has been significant developments in display technology as it can be seen with the creation of 3d screens for computer users it must still be questioned whether the adoption of such technology for general use will happen within the immediate future. 3D vision technology has been largely advertised by most companies as the latest wave in innovations in display technology. In fact companies such as ATI and NVIDIA have attained significant profits in selling video cards that are 3d capable, yet when examining the percentage of actual use most laptops and PCs today still utilize classic LCD technology that was available 5 years ago. Comparatively processors, disk drivers, memory sticks and even disk drivers today have changed drastically from way they were since then. The reason behind this is rather simple, LCD technology already reached its build out completion years ago as such the technology has proven to be stable, most consumers prefer to use it and its cheaper as compared to utilizing some of the latest screens available today. In fact various projects show that the consumer market that will avail of the latest developments in screen technology will be isolated towards gamers, media related junkies, and media corporations. For a vast majority of other computer users LCD technology will still be utilized for quite some time due to its stability and lower price. What must be understood is that when examining the current trend in 3d vision technology it seems to be more inclined as a creative gimmick rather than serving an actual use. While it maybe be true that it makes games seem more realistic it is not necessarily an integral and necessary part of a user\u2019s computer experience. In fact, the 3d capabilities of a screen can be taken away and programs on PC will run with no difference whatsoever in performance or display. A majority of programs today don\u2019t require 3d vision to be utilized and its use is isolated towards only a certain segment of the computer user population. It must also be noted that in order to actually use a 3d capable monitor a user would need to wear a special set of 3d glasses in order to see the \u201c3d\u201d effect. One problem with this method of usage is that various studies show that not all users are actually comfortable with utilizing 3d vision technology. Cases of eye strain, blurred vision and distinct feelings of discomfort have been noted in certain computer users which call into question the ability of the technology to appeal to a large segment of the population. Not only that it must be noted that 3d vision screens are on average several times the price of normal LCD screens and require special 3d capable video cards to actually work. This increases their overall cost to consumers and actually discourages them from buying the screens in the first place. Display technologies should serve to provide a discernable utility to consumers, while 3d vision may seem nice it doesn\u2019t really serve any actually positive purpose beyond making games look better. Other potential technologies that could be applicable to future display technology is the use of holograms as a replacement for solid screen devices however after extensive research on the applicability of the technology in the near future for general use it has been shown that even marginal commercial use is still 10 years away at best.\n\nHeat Sinks, RAM memory and its Future Capacity\n\nWhen various people think of PC components they usually bring up topics such as screen resolution, hard disk space, disk drivers and the capacity of their video cards yet they always seem to forget about RAM memory. The reason behind this is actually quite simple, the inherent popular culture by the general population often interacts directly with output devices such as monitors and input devices such as keyboards, the mouse and the disk drive. It is due to this that they often take notice of the factors that directly affect their interaction with various PC systems and such the amount of memory on their hard drive in order to store their files, the quality of the screen resolution on their monitor, the type of disk drive they have as well as the capacity of their video card so that they can play crisp video files. RAM memory is often relegated into the capacity of being a secondary aspect of the average home computer setup yet is an integral component in any PC. Lately RAM memory has been generating a greater degree of interest as the amount of tech savvy enthusiasts grows resulting in more interest in individual computer parts. It was actually due to this growing interest that various PC enthusiasts discovered the inherent limitations of RAM memory due to their increasing capacity. Similar to the case of processors, as the amount of RAM memory grew per individual stick the end result was a greater degree of heat produced which in turn affected the performance of the RAM in the long term (Deen, 48). Prolonged operating conditions actually resulted in slower computer responses as the memory struggled with increasing temperatures affecting its ability to actually work. While this is not the case for all computers it has been noted in enough cases that the RAM production industry has in effect released a stop gap measure to resolve it (Deen, 48). Depending on the manufacturer of origin, certain memory sticks actually came with heat sinks included to help dissipate the heat away from the sticks and into the air within the casing. It must be questioned though whether this particular addition to RAM memory will become an industry standard in the coming years. Heat sink technology works by drawing heat away from a particular device through either a copper or aluminum tube and theoretically dissipates it into the air within the casing. While various experts and industry personnel may say that it is effective in dissipating heat logic dictates that it seems to be a rather inadequate method of heat dissipation. For one thing, the technology works through the process of inherent temperature changes within particular \u201czones\u201d (Deen, 48). The high temperatures from the memory stick are drawn to the lower temperatures in the surrounding air through the copper or aluminum heat sink, in effect lessening the temperature burden on the memory stick. What must be understood though is that this works only if the there is a distinct temperature change between the two zones. As operational times increase so to does the ambient temperature within a particular PC case, while there are casings with sufficient temperature control mechanisms in the form of interior fans that help to regulate the temperature the fact remains that not all casings have this since this entails a significant additional cost to computer users. As a result ambient temperatures within particular case models can increase to such a degree that in effect it reduced the efficacy of heat sinks resulting in a gradual deterioration of performance. It must also be noted that as mentioned earlier liquid cooling systems or mineral oil cooling systems have been gaining significant amounts of consumer interested and could be eventually used as the primary cooling methods of PCs in years to come (Deen, 48). It has already been proven that both cooling mechanisms are far more effective in cooling memory sticks as compared to heat sinks and as such it cannot really be said that heat sinks incorporated into memory sticks will become industry standards. While various manufacturers are advocating that they should be (they get to charge higher prices for memory sticks with heat sinks) the fact remains that based on future methods of cooling such as the use of mineral oil and liquid cooling it cannot be said that the current trend in the utilization of heat sinks in memory sticks will continue into the future especially when liquid cooling or mineral oil technology reaches their respective build out completion times and become commercially viable.\n\nThe Push towards Miniaturization and Holograms\n\nOne of the most recent trends in the development of PC components has been a distinct push towards miniaturization with various components decreasing in size and weight as consumers demand more portability in the devices they use. This has given rise to products such Intel\u2019s Atom processor, more efficient miniature laptop batteries and a host of other innovations all aimed at making PC components smaller and thus more easily carried by the average consumer (Murphy, 113). In fact it can even be said that this apparent push towards miniaturization is a trend that will continue far into the future with holographic technology taking precedent in future portable devices such as net books and laptops. The reason why holographic technology is being stated is due to the fact that one of the latest developments in holographic technology has been the utilization of credit card sized keyboard that uses infrared technology to identify the positioning of a user\u2019s fingers to in effect create the simulation of interaction between a user and a holographic image (Issei et al., 32 \u2013 34). While this particular type of technology is still a decade or two away from actual commercialization it must be noted that this in effect could become the future of all input devices (Gomes, 40). Holograms can be described as 3d images created through various output projection sources to create the illusion of volume. For example one of the current applications of the technology has been the creation of \u201cvocaloid\u201d concerts in Japan wherein a projected holographic image is created on stage to simulate an actual person singing in a concert. Unfortunately, while input technologies involving infrared light can be currently utilized holographic technology is still within its infancy. The fact is creating an effective 3d hologram requires a significant amount of energy as well as a self contained projection apparatus that can project the necessary image onto a black screen or onto a particular space template. With the current push towards miniaturization it cannot be feasibly stated that holographic technology can utilized as a portable medium within the next few years. Various studies examining the rate of development of holographic technology specifically state that it will require at least another 20 years before the technology becomes applicable for commercial purposes. One of the reasons behind this is connected to the fact that the development of holographic display technology is mostly being conducted by research labs in various universities and not by any significant commercial company that produces various forms of display technology. Companies such as AOC, VGA, LG and Asus are focusing their efforts more on traditional methods of display technology such as LCD screens rather than conceptual technologies that have yet to actually attain proper conceptualization (Gonsalves, 6). What must be understood is that companies are more or less profit oriented and as such they will not expend resources on developing a product that is still in the theoretical stage (Gonsalves, 6). While it may be true that holograms may be the wave of the future for display utilized in computers the fact remains that until the technology actually proves itself commercially feasible it is unlikely that companies will allocate any amount of resources towards the development of the technology.\n\nWorks Cited\n\nBursky, Dave. \u201cStunning Advances To Captivate ISSCC Attendees.\u201d Electronic Design 53.2 (2005): 26.\n\nDavis, Sam. \u201cHigh Efficiency Challenges Power-Management Design.\u201d Electronic Design 56.5 (2008): 37-40.\n\nDeen, Mueez. \u201cMemory fuels performance engine.\u201d Electronic Engineering Times (01921541) 1509 (2008): 48.\n\nDigtial Content.\u201dA Word on Storage.\u201d Digital Content Producer 33.3 (2008): 39.\n\nGOLDSBOROUGH, REID. \u201cPC a Little Sluggish? It Might Be Time for a New One \u2013 Or Not.\u201d Community College Week. 2008: 30.\n\nGomes, Lee. \u201cKEYS TO THE KEYBOARD.\u201d Forbes 184.4 (2009): 40.\n\nGonsalves, Antone. \u201cNvidia shaves costs of graphics processing.\u201d Electronic Engineering Times (01921541) 1509 (2008): 6.\n\nIssei Masaie, et al. \u201cDesign and development of a card-sized virtual keyboard using permanent magnets and hall sensors.\u201d Electronics & Communications in Japan 92.3 (2009): 32-37.\n\nKubota, S., A. Taguchi, and K. Yazawa. \u201cThermal challenges deriving from the advances of display technologies.\u201d Microelectronics Journal 39.7 (2008): 942-949.\n\nMedford, Cassimir. \u201cMusic Labels, SanDisk in CD Rewind.\u201d Red Herring (2008): 8.\n\nMerritt, Rick. \u201cSERVER MAKERS GET GOOGLED.\u201d Electronic Engineering Times (01921541) 1553 (2008): 22.\n\nMurphy, David. \u201cUpgrade to Gigabit Networking for Faster Transfers.\u201d PC World 27.12 (2009): 113-114.\n\nOSKIN, MARK. \u201cThe Revolution Inside the Box.\u201d Communications of the ACM 51.7 (2008): 70-78.\n\nPerenson, Melissa J. \u201cBlu-ray on the PC: A Slow Start.\u201d PC World 27.4 (2009): 94.\n\nEDN.\u201dPoint cooling advances for hot ICs.\u201d EDN 54.5 (2009): 16.\n\nUpadhya, Girish, and Fred Rebarber. \u201cLiquid Cooling Helps High-End Gamer PCs \u2018Chill Out.\u2019.\u201d Canadian Electronics 23.3 (2008): 22.\n",
        "label": "human"
    },
    {
        "input": "Computer Hardware: Structure, Purpose, Pros and Cons Essay\n\nTom\u2019s hardware guide offers a comprehensive online publication that reviews developed computer hardware and computing technologies. This means that it is an important start for all information technology professional resources. In the article titled Wi-Fi security: cracking WPA with CPUs, GPUs and the cloud, a posting by Andrew Ku, argues that despite the convenience associated with wireless Wi-Fi networks, security is a major problem ranging from the cracking of passwords at the desktop level to the cloud (Ku 2011). The Wi-Fi is a potential for security breaches. The increasing perception that information is secure when online facilitates increased the use of online applications. However, instances of computer security vulnerabilities are imposing significant constraints to the user confidence regarding the use of online applications and wireless networks. The article aims at exploring the vulnerabilities that wireless networks and applications that are executed over the cloud. The article also discusses the First Line of defense security measures that can be implemented when adopting wireless security strategies (Ku 2011). The article also suggests that WEP encryption is obsolete as hackers can bypass the encryption methods that are mostly deployed by wireless networks. Strategies that can be used in securing a WPA network are also discussed in the article. This essay offers a comprehensive discussion of what was learn from the Tom\u2019s hardware website with respect to the core aspects of the module. The main focus of the post is with respect to the security issues of web 2.0 technologies.\n\nThe articles found in the Toms hardware website offers comprehensive review regarding the security of web 2.0 technologies are vital in the current data communications field. A significant characteristic of the Web 2.0 platform is that mobile users are the ones who undertake actions such as generation and uploading of content to the web sites (Ku 2011). This is increasing evident as large enterprises are embarking on the adoption of Web 2.0 tools, which include blogs and RSS. With such features, the Web 2.0 is vulnerable to exploitation by malicious users, implying that organizations have to implement appropriate mobile security strategies (Ku 2011). One of the most significant mobile threats associated with web 2.0 technologies is cross-site scripting, which allows malicious users and hackers to inject client-side script into web content that has already been accessed by other users. Basically, cross-side scripting provides a framework through hackers can evade the access controls. Cross-site scripting accounts for approximately 80 per cent of Web 2.0 threats; as a result, large enterprises should deploy appropriate strategies to combat this threat. In addition, the detection of attacks initiated by cross-side scripting is normally difficult, and is used by malicious users to maximize the effects of the attacks. XSS uses the Browser Exploitation Framework to establish an attack on the user environment and the web content (Ku 2011).\n\nThe second mobile threat that Web 2.0 technologies is susceptible to is SQL injection attacks, which primarily entail the use of a code injection technique in order to take advantage of a security vulnerability associated with the Web 2.0 technologies. Web 2.0 is susceptible to injection attacks due to the fact that users can generate and upload web contents to a web site. This in itself is vulnerability, through which malicious users can initiate an SQL injection attack. Other injection attacks can be initiated in the form of Java Script Injection and XML injection. Because Web 2.0 technologies significantly depend on client side code, hackers make use of client-side input validation in order to evade the access controls.\n\nThe third issue associated with mobile security in Web 2.0 technology is information leakage that is initiated by the user generated content. Hackers exploit this feature of the Web 2.0 technologies to upload and run their malicious code on the web site. This could result to a large enterprise hosting an inappropriate content, which could not only result to cases of data breaches, but also affect the brand. Information leakage has significant effects on the operations of a company and normally serves as a threat to data integrity and confidentiality (Ku 2011).\n\nInsufficient anti-automation also makes the initiation of attacks on Web 2.0 applications easy. This is facilitated by the programmatic interfaces of most of the Web 2.0 applications. Inadequate anti-automation can foster the automated retrieval of information and the automated opening of accounts in order to facilitate access to the web content. Such threats can be curbed by the use of Captchas. Information leakage is also another mobile security issue associated with Web 2.0 technologies. The aspect of mobility of Web 2.0 technologies facilitates content sharing, which can initiate a vulnerability that malicious users can exploit in order to gain access to the system. It is arguably evident that the internet revolutionized the way businesses are conducted and how people undertake their work. The Web 2.0 is an important aspect of the internet that played a significant role in enhancing business functionality. A significant limitation is that with its increased usage implies increased risk; as such, they offer opportunities through which malicious users can inject and run malicious code in web content (Ku 2011).\n\nThere are also various links within the website that are helpful to follow up. Some of them include For IT Pros, Charts, Brands and Articles that offer comprehensive information concerning hardware and a review of the computing technologies. Other salient information concerning the website is that it can be used a starting point for conducting hardware reviews basing on performances, benchmarks and costs for people intending to acquire computer hardware. Altogether, the site is an important tool for an IT professional owing to the diverse pool of articles that covers almost all domains of information technology ranging from computer hardware, computing technology, emerging computing trends and so on.\n\nReference\n\nKu, A. (2011). Wi-Fi Security: Cracking WPA With CPUs, GPUs, And The Cloud . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Science: \u201cDICOM & HL7\u201d Report (Assessment)\n\nIntroduction\n\nPianykh, \u201cDigital Imaging and Communications in Medicine (DICOM) is an application with the prime function of communication through effective and efficient handling, storage, retrieval, and printing information in image form. It\u2019s mainly applied in medical fields such as in Hospitals(3).\n\nIn the description, DICOM is operated and communicated through network protocol that requires an internet provider (IP) for systems to communicate. DICOM\u2019s communication interface is in form of a file. The data is collected from patients and stored in two or more entries which are exchanged to obtain images in form of a format recognized by DICOM. DICOM application processes data to yield information. The information is then archived and communicated through external system hardware to print or scan images into pictures. DICOM machines are connected with remote workstations such as manufacturer\u2019s hardware across networks. Information formats can also be transferred across networks managed by a different server thereby ensuring smooth flow of data processing and management.\n\nTechnology\n\nInformation Technology in DICOM\n\nThis information technology function involves an exchange of objects through templates to obtain images. Sources of such objects and images are derived from patients\u2019 data arranged in structured reports and formats recognized by the system. The file formats contain templates that are unique and are capable of identifying the type of image registered. For example, it can distinguish images of ultrasound devices from other image sources. The integration of medical disciplines such as pathology and Radiation Therapy are all a result of the use of visible light which facilitates scanning and radiograph functions. It stores information based on records of patients treated and images of patients taken during the treatment process. Image communication has also been fast and accurate through the use of high-speed visible light in changing information to images.\n\nTransport Technology in DICOM\n\nIn the transport of information, DICOM recognizes the receiver\u2019s needs such as understanding the type of information required. The device then sends the images or objects as programmed. This creates some form of interaction between the sender and the receiver of the information from one destination to the other. Such an interaction is known as Association establishment. It involves encoding and sending relevant information through the syntax mechanism of transfer. The syntax transfer is very fast. It\u2019s used to detect the compatibility of device functions. The technique is also used to determine the magnitude of errors resulting from exchanging image or object information.\n\nCharacteristics\n\nMedia manipulation and exchange\n\nMedia objects and images can be stored in formats that can easily be manipulated by the system administrators. Such images and objects are manipulated by the exchange in such a way that they can be recognized and interpreted by other devices that function through DICOM file formats. For example, ultrasound devices can be used to obtain information from hardware storage objects like CDs. Doctors are able to access information from CDs about patients referred from remote hospitals.\n\nManagement of information\n\nDICOM is characterized by the services it offers in the management of information. It stores all the information through services offered by system administrators. Penalty boxes are used to store temporary and lost information for easy retrieving. The penalty boxes resemble the normal recycle bins or trash and handle temporary deleted information. The receiver of information is capable of removing and adding any image through a service known as DICOM Storage Commitment. This service transfers all the functions required to be done by the receiver with coded information format. The system is so defined that executive functions are contained in the local storage disks.\n\nObject exchange\n\nDICOM is the center of object exchange; collected reports from patients are changed to images through the information modification technique. This new information technology is essential in archiving pictures sourced from coded information. Radiology is an example of a mechanism used to enhance communication in medical operations.\n\nThe characteristic information pattern\n\nDICOM involves three main components or patterns in the collection of information. In the context of observation, there is the observer who is a person or the observer of the device, procedures to follow in the course of the observation and subjects or recipients i.e. observable units of study.\n\nImage Quality\n\nIt ensures that the images displayed are consistent with the coded patient\u2019s information. Radiologists are able to interpret the image the same way the physicians will interpret the same image displayed. Image quality and visibility are the characteristic factors that ensure all image presentations and manipulation remain constant throughout the operation.\n\nStructured Reporting characteristic\n\nThis is an application that enables structural exchange among similar objects and images. It interprets structured message reports about information of important images or objects and is capable of changing the structure based on the availability of the information received by the system.DICOM is made up of coded structures. According to David \u201cIt\u2019s an item of code sequence\u201d (42).\n\nMechanisms and Security\n\nSecurity mechanisms enable safe and protected environments.Information access is controlled by rules and regulations.The application is password protected and the user must provide the password recognized by the system to have an access to objects and images. \u2018Log in\u2019 is prompted before access is permitted.\n\nAnother security requirement is the electronic signature.System administrators ensure no alteration of information by any user through the use of electronic signature. Therefore users with signatures have an access and can change any information as required anytime (Pianykh 247).\n\nUses of DICOM\n\nStorage\n\nIt\u2019s a service used to store and send structured information.Images obtained from the storage sources are transferred from one workstation to another within or across networks.\n\nStorage Confirmation function\n\nThe service ensures permanent storage of images.It automatically creates a backup of images.Its controlled by safe commands for example the user must confirm that he wants to delete the image or information from the local disk either permanently or temporarily.\n\nRetrieve\n\nThe service can be used to retrieve lost files of objects and images from the system.It can be used to search files in different locations too.\n\nPrinting\n\nThis service is used to print hard copies of images from printers. Other peripherals are interconnected and programmed to perform DICOM functions.\n\nHL7\n\nHealth Level Seven (HL7) is an organization based on scientific functions of evaluating health information through electronic sharing and exchange. It also controls methods and procedures of retrieving, managing and delivery process.\n\nIt\u2019s descriptive in that it incorporates high standards of efficiency and enhances effectiveness in healthcare organization.This increases the efficiency in delivery of healthcare- based information as required by the Standards Operating Procedures (SOPs).\n\nTechnology Level and characteristics of HL7\n\nHL7 involves the use of computer systems linked to several hospitals and other healthcare organizations. Its major roles include encoding information, building structures and identifying language used in clinical reports and documents.\n\nComponents of such clinical reports include patients billing documents and other reference records such as patients\u2019 follow-up documents. The interface links the users, the patients and the system. The three components should be capable of communicating freely with other functional parts within the healthcare organization.\n\nMethodology in HL7 operation is based on standards, principles and control of the data flow and uniform communication of information through systems.This strategy enables hospitals to exchange and share information easily across geographical locations.Such standards are based on concept development, documentation processes and levels of standard application and the ease with which messages can be sent, received and retrieved as a function of operation.Identification of HL7 is thus guided by the factors discussed above.\n\nUses\n\nBenson has discussed the benefits in the application of HL7 as:\n\n 1. HL7 simplifies the inter-relation of data types and this ensures smooth hospital workflows.\n 2. It\u2019s fast in the retrieval process of patients\u2019 information.It also fastens billing processes and maintains information over long period of time through storage.\n 3. Administrative functions are supported through messaging. This facilitates financial and other organization operations through electronic messaging.\n 4. It identifies the information model required and uses the relevant data to connect to the relevant fields.This reduces costs of carrying out the information requirements and increases efficiency and accuracy.\n 5. HL7 incorporates functional concepts that match the system\u2019s operation.This ensures reliability and understanding of how clinical organizations work and relate to each other as a result of this technology\u201d (26-45).\n\nConclusion\n\nDigital Imaging and Communications in Medicine (DICOM) and Health Language Level Seven (HL7) are related in that they both show certain levels of technology application in Health information, management and organization of healthcare centers. High speed communication through networks is a vital area of concern in the study of the two health standards.Imaging technology and communication has really enhanced the diagnosis of diseases.Such information is then evaluated and shared electronically within healthcare organizations.\n\nThe benefits of the two levels of clinical information technology help in the storage and retrieval of patients\u2019 information.Management of health care organizations has also been made simple through internet connectivity of different organizations across geographical locations.\n\nThis technology has resulted to more innovations within the field of medicine. Recent researches show that visible light is becoming a crucial component in the Radio Therapy and CT scanning operations in the diagnosis and treatment of diseases through radioactivity.\n\nStallings says high speed networks have improved performance and quality in its application in different fields including medicine, in the diagnosis of diseases.Therefore DICOM and HL7 has been helpful in other medical fields such endoscopy and dermatology and entirely in the identification of patients medical profiles.This has made patient attendance in healthcares smooth and efficient.Time and material resources such as money has also been saved quite a lot.\n\nTechnology has enhanced interpretation and treatment of diseases. Doctors feel comfortable in undertaking their professional duties without much stress.\n\nWorks Cited\n\nBenson, Tim. Principles of Heath Interoperability HL7 and SNOMED : Springer, 2010.\n\nClunie, David; DICOM Structured Reporting; Pixel Med Publishing: Bangor, PA, 2001.\n\nPianykh, Oleg S. Digital Imaging and Communications in Medicine (DICOM); A Practical Introduction and Survival Guide: Springer; 2008.\n\nStallings, William. High-Speed Networks and Internets: Performance and Quality Of Service. Prentice-Hall: November 2001.\n",
        "label": "human"
    },
    {
        "input": "Computer Technology: Databases Essay\n\nDatabases are components of Information Systems that are used when the Information Systems have large amounts of a, especially when the interfaces in the Information System are interactive, and when users can access the system concurrently. The use of databases in such a scenario, therefore, ensures that information is available to users in real-time, and that system performance bottlenecks are minimized due to appropriate data management. There is a variety of databases from which an organization can choose. The choice of the database application depends on the volume of data that a particular Management Information System will be handling. For Management Information Systems handling large volumes, it is advisable to use heavy database applications like Microsoft SQL or Oracle. While for Management Information Systems that handle small volumes of data, light databases like MySQL are appropriate. This paper is an analysis of the database applications used in my school.\n\nIn my school, we perform tasks that do not require the use of an Information System using the Microsoft Access database. These tasks include records of student information, student courses, and even their performance in Random Assessment Tests and Continuous Assessment Tests. With this database; MS Access, we can query the database for specific student information. For instance, if a teacher wants to know the details of the students who have failed their CATs, the database is queried to provide this information (\u201cA Beginners\u2019 Guide\u201d, N.D.). With the database, we also generate reports to heads of departments containing specific information. For instance, the head of the Computer Science department may want to know the number of students who are taking databases and their names. This can be generated in the form of a report by Microsoft Access.\n\nIn addition to Microsoft Access, the school uses JPAMS in preparing student grades, and their transcripts. This is actually what the school uses the database mainly for. The database is very instrumental in enabling the school to produce student transcripts in time. It also comes in handy for teachers who may want to check how certain students, or even classes, have been performing. The JAMS database is also used to capture student attendance so that it may be used in grading students, in case the school opts to implement that policy. The database is holistic in the sense that, it captures all the information that may be of interest to teachers as they do their evaluation of how their students have performed throughout the semester. For instance, a teacher may need to know issues related to the discipline of students in a certain semester or even in a certain year. The JAMS software is very instrumental in such situations because it can generate discipline-related reports. An example of these reports is the discipline frequency report, which gives discipline-related information with details like the time it occurred (Allen, 2009). Another similar report is the court report, which gives performance-related information mapped against discipline-related information.\n\nAs evidenced in the discussion above, my school mainly uses two databases. These are Microsoft Access and the JAMS. Microsoft Access is used to capture general everyday information like student details and performance in CATs, while JPAMS is used to capture students\u2019 performances and other relevant information. The latter is any information that is deemed to affect student performance. It is therefore evident that the use of databases in the organization is rewarding since it makes tasks easy and provides timely information when the same is needed.\n\nWorks Cited\n\nAllen, V. (2009). JPAMS Newsletter. Web.\n\nMicrosoft Access 2003. (N.D.). A Beginners\u2019 Guide. Web.\n",
        "label": "human"
    },
    {
        "input": "Global Climate and Computer Science Term Paper\n\nTable of Contents\n 1. Introduction\n 2. Case analysis\n 3. Conclusion\n 4. References\n\nIntroduction\n\nGlobal climate can be referred to as the information of humidity, atmospheric pressure, temperature, wind, atmospheric particle count, rainfall and other meteorological elements in the world recorded over a long period of time (Thornthwaite, 1948). Biologists and astronauts have argued that climate change assumptions rather than facts, and computer modeling rather than real-world observations, underpin political attempts to combat climate change. Existing data recorded in the past, both ecological and geographical, indicate that climate change is an environmental, social and economic challenge on a global scale (IPCC, 2007).\n\nScientists who address the natural, economic, and sociological characteristics of climate change are mainly concerned about the direction or the timing of changes. In an attempt to discover the role technology can play in the research of climate change, several approaches have been recommended by the UN\u2019s Intergovernmental Panel on Climate Change (IPCC). One of the greatest challenges that the computer scientists are facing is coming up with technological solutions to climate change. The risks and vulnerability of developing countries (especially in Africa) by uncertain and complex climate change disasters is largely attributed to lack of modern equipment and human resource.\n\nCase analysis\n\nThe UN\u2019s Intergovernmental Panel on climate change (2007) revealed that the global average surface air temperature has increased considerably since 1970. It is estimated that the change in the normal temperature, of the Earth\u2019s surface is mainly based on measurement from thousands of weather stations, ships and buoys around the world, as well as from satellites. The amount of rainfall across the globe is not distributed evenly.\n\nThe normal distribution of rainfall across regions is primarily influenced by atmospheric circulation patterns, the availability of moisture, and surface terrain effects. Various researchers have argued that the several elements of climate have been exacerbated by human induced actions such as: the widespread use of land, the broad scale deforestation, the major technological and socioeconomic shifts with reduced reliance on organic fuel, and accelerated uptake of fossil fuels.\n\nClimate resilient development has received increasing recognition in the discourse of poverty and environmental problems. This was largely upheld by the United Nations Framework Convention on Climate Change (UNFCCC), which aims at stabilization of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system (IPCC, 2007). The approaches that have been used in the past to tackle climate change have evolved over the past regimes from political science arena, to the conference of parties (COP) negotiations and finally targets and timetables approach. The most recent and innovative approach of use of technology was last considered during the Kyoto protocol.\n\nUse of technology in addressing climate change will not only provide reliable information to all the stakeholders but also create early warning for vulnerability and conditions creating risks. By use of software and other equipment\u2019s such as Geographic information systems, ERDAS Imagine, photogrammetry and seismogrammetry represents the power of computer science towards finding solutions to climate change. The availability of real-time and future information convinces that technology can indeed be used in analyzing the complexities and interdependencies of any one intervention required.\n\nConclusion\n\nModern technology can enhance proper adaptive capacity in combating to climate change. Use of modern equipment\u2019s to detect the major elements of weather such as rainfall and temperature, will allow proper preparedness, planning and readiness to the possible impacts of climate change. However, technology can not operate in vacuum and this calls for clear policy guidelines to support and build the required framework for technological approach. Its is evident that technological approach paint the bigger picture of climate change and provide estimates for the likely consequences of different future scenarios of human development.\n\nReferences\n\nIntergovernmental Panel on Climate Change, 2007, Summary for Policy Makers, in Climate Change 2007: Synthesis Report, WHO/UNEP.\n\nThornthwaite, C. (1948), An Approach toward a rational classification of climate. Geographical Review 38 (1): 55\u201394.\n",
        "label": "human"
    },
    {
        "input": "How to Build a Desktop Personal Computer Essay\n\nPersonal Computer (PC) is defined by the American heritage dictionary as a small computer which is designed and intended for use by an individual at home or at work for light-computing tasks(\u201c\u201dPersonal computer\u201d). The PC gained widespread adoption from the early 1980s with the introduction of versions of Apple computers notably the Apple II and later IBM PC by IBM which enabled many people to own computers in their homes. Technological advancement and competition in the computer hardware industry has resulted in low-cost microprocessors and compacted computer parts. Today there are a host of computer parts manufacturers that produce compatible components that work together when correctly assembled. In this process essay it is shown that is it easy for anyone with little computer knowledge to assemble and own a computer of his/her own specifications.\n\nThe first step in assembling a PC will involve gathering the components. Some of the basic ones are: motherboard, Computer case, power supply unit, processor and its cooler, memory, hard disk drive, optical drives (DVD/CD/Blue ray drive), video graphics card, keyboard, mouse, monitor and operating system. In addition, tools such as a set non-magnetic screw drivers, thermal paste ,manuals and a pair of pliers are also required. The choice of many of these components is up to the owner and their prices vary depending on the manufacturer. However, emphasis should be on their compatibility ,purpose and availability (\u201cHow to assemble a computer\u201d). The motherboard forms one of the core components as everything is plugged onto it and its choice will depend on whether one is building an AMD or an Intel system. It also determines the type of memory used, maximum speed and future upgradability (Hutcheson 3). The processor will determine the speed of the system but the choice between the two major types-Intel and AMD- remains a matter of taste. The case should be suitable as far as air circulation for CPU cooling is concerned. For those that come with a power supply unit, it is important to ensure that it is compatible with the motherboard. Hard Disk Drive is for storage but is important to go for the most modern ones with high speed (RPM). Video card is required in case the motherboard lacks an integrated one or higher resolution is required. Keyboard, mouse and monitor are required for configuring the system after the assembly. The thermal paste serves to help in dissipating heat generated by the CPU and should be changed after a period of 6 months (Gupta n.pag). Manual are critical for this process especially for beginners for reference purposes.\n\nAfter gathering all the components, the assembler should be aware of a few safety precautions before the actual assembly begins. As static electricity can damage any electronic device, it is important for one to discharge static electricity from the body prior to handling any device. This can be by wearing an anti-static wrist band or simply by touching the casing with both hands(Hutcheson 4). In addition, excessive force should not be applied during installing of the components to avoid any damages (Hutcheson 4). It is also important to work on a spacious table top in a well-lit room. Before commencing the assembly process, the components should be unpacked from their wrappings and laid neatly on the working area.\n\nWith all the parts ready and the necessary safety precautions in mind, the final activity is the actual assembly. The first step involves opening the case and studying its layout particularly where the motherboard is laid. The casing cover is can be removed by a sliding mechanism or unscrewing the screw on its back panel. With the case open, the motherboard is then mounted on its inner side while ensuring that its integrated ports are well within the rectangular cut-out of the back panel. This may involve securing it on the stand-offs on its perimeter or using screws. The processor is then carefully inserted on the motherboard processor socket. It is important to consult the motherboard manual for the correct alignment and locking mechanism. With the processor well secured, the thermal paste is applied to cover the whole of its top. This is followed by placing the heat sink on top the processor and locking it into place. The locking may involve a lever like pin but it is important to consult the manual. If the heat sink lacks an attached fan, the fan can be fixed with the help of its manual. After inserting the processor and its coolers, the power supply unit (PSU) can be inserted (in case the case does not come bundled with one) in its position such that the power ports are visible from the back panel and held using screws. The PSU is then connected to its corresponding 24 or 20-pin connector on the motherboard. The square-like power cable for the processor from the PSU should also be connected on the appropriate port on the motherboard. After installing the PSU, the memory modules can then be inserted on their slot on the motherboard by unlocking the end clips and gently pushing them down until the clips snap up. Afterwards, the hard disk drive is inserted into its bay area and held using screw. It is then connected to the power supply and its motherboard data cable. The type of connection to the motherboard will depend on whether it is IDE or SATA-based in addition to the number installed. The optical drives such as DVD/CD/Blue Ray drives are fixed by first removing the front panel plates and secured with screws inside their bay area. They should also be connected to the power supply and onto the corresponding data cable from the motherboard. I case of more than one non-SATA optical drives, the jumper setting should be set as required with the help of a manual. The last step will involve connecting the wires of the power switch, the reset switch, the hard drive LED, internal speakers and other inputs such as front panel USB and audio panel to their appropriate motherboard ports. It is very important to consult the motherboard manual on this as some of these have look-alike connectors and it may be easy to make a wrong connection. However, this should not pose a serious problem as most motherboards bear appropriate abbreviations near the correct ports. The graphic card if required or due to lack of an integrated one on the motherboard can be installed by fixing it firmly on a PCI slot after removing the cover plates at the back panel. With everything in place, it is important to recheck every connection before closing the case cover and powering the computer after connecting the system unit to the monitor , keyboard ,mouse and to a power outlet. A successful assembly will result in the system booting up although not to completion because of lack of the operating system. After installing an operation system, a window system will require that installation of device drivers using the motherboard driver CD. On the other hand, a system that does not boot up or results in smoke or a burning smell will require a recheck of the connections and /or verification of the individual components.\n\nAssembling a computer is a pretty simple affair nowadays. It is possible to build a computer system that lives up to ones preference and taste. The process involves sourcing the required components and connecting them together on a motherboard contained in a case. With just the accompanying manuals any person with little computer knowledge can build a customized system that best serves his/her interests.\n\nWorks Cited\n\nGupta, Ankur. \u201cHow to assemble and build a PC\u201d. DigitGeek.com . DigitGeek, 2008. Web.\n\n\u201cHow to assemble a computer\u201d. Liutilities.com . Uniblue, 2007.Web.\n\nHutcheson, Mike. \u201cHow to build a computer\u201d. Squidoo.com . Squidoo, LLC.n.d. Web. 2011.\n\n\u201dPersonal Computer\u201d. thefreedictionary.com . Farlex, inc, 2011.Web.\n",
        "label": "human"
    },
    {
        "input": "Majoring in Computer Science: Key Aspects Term Paper\n\nComputer Science, abbreviated as CS, is the study of the fundamentals of information and computation procedures and of the hands-on methods for the execution and application in computer systems. Computer scientists formulate algorithmic techniques that generate, define, and transform information and devise appropriate concepts to represent complex systems. Computer science has several branches and sub branches, these include computational complexity theory that focuses on computational techniques, computer graphics that focuses on particular aspects of computational methods. Yet, others focus on the application of computational methods, these include programming languages and human-computer interaction (University of Cambridge, para. 3).\n\nMany people tend to confuse computer science with other careers or courses that deal with computers such as IT, or think that it follows on their interaction with computers, which usually involves activities such as gaming, internet access, and document processing. CS offers a deeper comprehension of the inner workings of the programs that enable us to use various computer applications and programs, and using that knowledge to design new applications or improve the current ones.\n\nPreparations Required of a Person Intending to Major in Computer Science\n\nBefore majoring in Computer Science, a student needs to prepare adequately in order to have a background on the topics and concepts covered. The first preparation expected of a student is to undertake a general reading in order to have a broad background and loose understanding of topics and issues covered in this Major. This will also increase interest towards having a deeper understanding of these issues. Since the study of Computer Science considerably depends on mathematical techniques, a person should develop a mathematical background, both technical and on a recreational level such as games and puzzles. Students are also advised to have basic knowledge of coding and cryptography, fields that have a strong link to this Major. For general reading, students can refer to various journals such as New Scientist that has articles relating to advances in computer science (Williams College, para 1).\n\nApart from reading widely, students are expected to have good study skills as they will have to manage their own studies, schedule their time, arrange classes, and still find time to attend to other non-academic activities such as recreational activities. Computing equipment is an integral element in understanding computational techniques, there fore, students are advised to have, or plan to purchase a computer, as this will assist in completing class projects and in undertaking personal studies and projects (My Majors, para. 4). Students will also be expected to have a fair typing speed as this will improve their working speed, this can be achieved by using the various training programs available at low prices.\n\nCourses Required of a Computer Science Major\n\nThe compulsory and elective courses required for a Computer Science major differ significantly among various universities, however, certain courses are common. The courses include computer based courses and non-computer courses, and students must be careful that they do not fail non-CS courses such as business courses or language development courses just because they are boring. This is because these courses will count to the final grade, besides, they may become useful in future.\n\nThe most common courses expected of a CS major are listed below (Williams College, para. 5):\n\n  * Algorithms\n  * Artificial Intelligence\n  * Calculus\n  * Compiler Design\n  * Computer Architecture\n  * Computer Graphics\n  * Computer Organization\n  * Computer Science Theory\n  * Computer Theory\n  * Data Logic\n  * Data Management\n  * Data Structures and Advanced\n  * Programming\n  * Design Physics\n  * Device Utilization\n  * Discrete Mathematics\n  * Distributed Systems\n  * Electronic Design\n  * Files and Databases\n  * Information Management\n  * Introduction to Calculus\n  * Introduction to Computer Science\n  * Logic Design\n  * Machine Language\n  * Network Fundamentals\n  * Operating Systems\n  * Programming Languages\n  * Statistics\n  * Theory of Computation\n\nSome of these courses are elective while others are compulsory, again, this varies from institution to institution. In addition to the computer courses, students will be expected to take non-CS courses (Spolsky, para. 6). These courses offer the student an all-rounded perspective and can be of importance in both work and non-work environments and include Business Development Skills, Entrepreneurship Skills, and Communication Skills. However, other courses assist the students in the various computational techniques and include the various level of Calculus, Discrete Mathematics, Algorithms, Theory of Computation, and Computer Graphics.\n\nList of Potential Jobs in Computer Science\n\nA Computer Science major can land a vast range of jobs depending on the courses taken during the study. Some majors are more marketable than others, therefore, students must choose their courses wisely, probably at the advice of human resource experts. However, students should not only look into being employed by various firms, rather, they can opt for other prospects such as self-employment or establish consultancy firms. According to Spolsky (2005), a major in Computer Science can land the following jobs:\n\n  * Application Developer\n  * Business Analyst, IT\n  * Computer / Network Support Technician\n  * Computer and Software Sales\n  * Computer Game Developer\n  * Computer Graphics Design\n  * Computer Hardware Technician\n  * Computer Networking/IT Systems Engineer\n  * Computer System Developer\n  * Database Administrator\n  * Information Technology Consultant\n  * Information Technology Director\n  * Information Technology Project Manager\n  * Information Technology Specialist\n  * Network Administrator, LAN / WAN\n  * Network Engineer, IT\n  * Network Manager\n  * Programmer\n  * Programmer Analyst\n  * Senior Software Engineer\n  * Software Architect\n  * Software Developer\n  * Software Development Manager\n  * System Manager\n  * Systems Administrator\n  * Systems Analyst\n  * Web Developer\n\nWorks Cited\n\nMy Majors. Computer Science Major. No Date. Web.\n\nSpolsky, J. Advice for Computer Science College Students. 2005. Web.\n\nUniversity of Cambridge. Preparing to study Computer Science. 2010. Web.\n\nWilliams College. Computer Science: Major Requirements. Web.\n",
        "label": "human"
    },
    {
        "input": "Networking Concepts for Computer Science Students Essay\n\nFirewalls and routers\n\nRouters and firewalls are network connectivity devices that handle data packets on the network. The router will direct these data packets to their addressed destination on an internal as well as external scale. This is within or without the local area network (LAN). The firewall, on the other hand, is a hardware or software that secures a network against external threats. On their own, firewalls have no inbuilt intelligence as far as identifying or recognizing intrusions is concerned. These must therefore be used in conjunction with an intrusion detection system (IDS). It is therefore desirable that a competent network administrator should include all of these in their network management policy.\n\nSubnetting\n\nA subnet defines a physical part within the transmission control protocol and internet protocol (TCP/IP) environment. This process makes use of IP address that represents a distinct network ID. Normally one network ID by the InterNIC is issued to an organization (Holliday, 2003). When the network is divided into subnets, each segment should use a diverse subnet ID. Each segment therefore has a distinctive subnet formed by dividing into two parts the host ID bits. One part identifies the segment as a unique network, while the other identifies the hosts. This process is called subnetting.\n\nBenefits of Subnetting\n\nOrganizations apply subnetting to create multiple physical segments across one network. This enables one to:\n\n  * Blend various technologies including Ethernet and token ring\n  * Overcome the current technology limitations like those placed on the maximum number of hosts per segment\n  * Reduce network overcrowding by re-directing traffic and minimizing broadcasting\n\nThe IP addressing system used for subnets is known as subnetting. Before one implements subnetting, he needs to resolve the current requirements. One should define the required host addresses corresponding to the physical segments available.\n\nEach TCP/IP host must have a minimum of one IP address. Based on these a single subnet mask is sufficient for the whole network. A unique subnet ID must be defined for every physical part and thus a range of host IDs for each segment must be defined.\n\nIP Addressing\n\nAll networks must have a way of uniquely identifying individual components in a way that the identifier may be in a name or number format. In the case of the TCP/IP protocol, a unique number called the IP address is used to recognize each host. The IP address defines a host position on the network (Day, 2008). The IP address is unique and has a standardized format. Every IP address identifies the network ID (network number) and Host ID (host number). The network ID represents the resources positioned on the same physical section of the network. The host ID represents a TCP/IP host like a workstation, server, and router located on the same segment. The IP address is 32 bit long and is subdivided into octets; a group of 8 bits separated by periods. The internet community derived and categorized the IP address into five classes to cater for different sizes of networks. The classes A, B and C are widely used in this classification. These classes of addresses identify the bits that represent the network ID and those which represent the host ID. It also gives the total figure of networks and hosts on each network. Class A network addresses cover networks having many hosts. This allocates 126 networks and roughly 17 million hosts for each network. Class B network addresses cover varied sized networks and thus can support 16,384 networks with about 65,000 hosts on each network. Class C addresses are suitable for local area networks (LANs), which are relatively small with approximately 2 million networks and 254 hosts per network.\n\nAddress Resolution Protocol (ARP)\n\nWhen two machines communicate, an IP address identifies the destination machine. However, transmission of data must take place at the physical and data link layers. For this purpose, the physical address of the destination machine must be used. The address resolution process involves comparing IP and hardware addresses. Address resolution protocol (ARP) obtains the hardware address of broadcast based hosts on networks. ARP acquires the hardware address of the target host or gateway by using a local IP address broadcast of the destination (Athenaeum & Wetherall, 2005).\n\nImmediately the hardware\u2019s address is obtained they are stored as an entry within the ARP cache together with the IP addresses. This ARP cache is continuously scrutinized for the IP and hardware address mapping prior to starting an ARP request transmission. Prior to communication taking place between two hosts, the IP address of each host must be determined and mapped to the host\u2019s bandwidth address. An ARP request and reply constitutes the address resolution process with the following steps:\n\n  * An ARP request is started anytime a host attempts to communicate with another host. In case an IP resolves that the IP address is local, it checks for hardware address in its cache for the destination address. If no match is found ARP constructs a request basing on the queries like, \u201cwho is the IP address; which is your IP address, \u201d etcetera.\n  * The source host\u2019s IP and hardware addresses form part of the request. The ARP request is transmitted over the network so that all the hosts on the same network can receive and process it.\n  * Every host on the network gets the broadcast and compares it to its own IP address so that where there is no match, the request is ignored.\n  * The destination host identifies the request\u2019s IP address it has received and matches it to its own address. It then sends an ARP reply consisting of hardware address directly to the source host.\n  * The destination\u2019s host ARP cache is then updated with the IP and hardware addresses which are matching with those of the source host. After the reply, the source host establishes the communication.\n\nAn ARP broadcast can generate considerable traffic on a network hence reducing network performance. To optimize this process, the results of an ARP broadcast are held in a cache for two minutes, and if the entry is not used within the two minutes, the entry is held for a further ten minutes within the cache before it is deleted. Entries in the ARP cache are automatically timed out in case hardware addresses change or if a network card is replaced.\n\nIP Routing\n\nRouting involves choosing a path to use in sending packets over a network. Routing takes place at an IP router or host when it transmits IP packets. A routing table stored within the memory of the host or router is consulted during the routing process. The table consists of entries with the router IP addresses to additional networks which are used during communication. A configured router is only able to send packets to certain networks. During an inter-host communication, IP initially determines whether the location of the host is local or remote. If it is remote, the IP then verifies the routing table for a path to the remote network or host. The IP may use an IP address for the router to transfer a packet over the network. The routing table is referenced continuously for the addresses of the remote network or host. If the path is unavailable, an error message is send back to the source.\n\nStatic and Dynamic IP Routing\n\nStatic routers make use of routing tables which are constructed and updated manually. When a route is changed, static routers have no ability to share this information amongst themselves nor do they exchange routing information with dynamic routers.\n\nDynamic routing, on the other hand, is a function accomplished by the routing information protocol (RIP). Similarly, open shortest path first (OSPF) may also be used. Dynamic routers periodically share routing information during dynamic routing process.\n\nTCP/IP Services\n\nEmail\n\nWhat we call E-mailing or electronic mailing is the transmission of messages pr a message over a communication network. Most e-mail systems consist of a basic text editor used to compose, edit and send the messages. The messages are addressed using specific email address, which must be unique to the recipient. Electronic mail boxes store the sent messages until the intended recipient retrieves them. All online services and internet service providers (ISPs) offer gateway functions as well as e-mail although most of them support the exchange of mail with users on other systems\n\nSimple Mail Transfer Protocol (SMTP)\n\nSimple Mail Transfer Protocol (SMTP) specifies how a mail is delivered from one system to the other. It is a relatively straight forward protocol that makes the connections from the sender\u2019s server to that of the recipient and then transfers the message.\n\nSMTP is used for:\n\n  * Delivering messages from the email client to the SMTP server\n  * Transferring messages between various SMTP servers\n 1. \n\nSMTP is not used for transferring the message from SMTP server from a recipient to its client email because it requires both the source and destination to be connected to the internet.\n\nPost Office Protocol 3 (POP3)\n\nPost Office Protocol 3 (POP3) is a typical messaging protocol that is used in receiving e-mail. This protocol, which operates in the client server setup, ensures that e-mail is delivered and stored by an internet server. One can then proceed to check their mail box and download the mail. The internet message access protocol (IMAP) is an alternative protocol to POP3, which is used to view e-mail at the server as though it were on the client computer. Both POP3 and IMAP protocols are involved in receiving e-mails. However, they differ from SMTP used to transfer email across the internet.\n\nHypertext Transfer Protocol (HTP)\n\nThis protocol defines the basis of the web, where most information is stored in a hierarchical or two dimensional sequences. However, the hypertext formatted documents will allow information to be accessed in any order and from any direction by using links from one document to the other. These links are embedded into the documents, and contain the uniform resource locator (URL) address, to another location. This is then used as the addressing system on the internet. This URL is unique because it contains all the information required to locate any internet resource. Typically the URL address consists of five parts; the left most part defines the name of the protocol and a fully qualified domain name (FQDN) of the server contains the resources in question. The third portion of the URL corresponds to the port address followed by the directory path. Then the directory path to the resource is followed by the file name with the suitable extension.\n\nFile Transfer Protocol (FTP)\n\nThis is a connection oriented protocol which is considered useful especially when transferring files between different operating systems. This protocol may be used to relocate files in both directions between an FTP server and client. An FTP site can be password restricted meaning that one would require a username and a password to access it. The FTP operation involves logging on to the distant computer, browsing directories and then transferring the required files. Browser software such as internet explorer makes this procedure much simpler by automatically logging one on to the FTP server, provided anonymous connections can be allowed. FTP also supports the process of copying files to a server from the client computer thus it is more suitable for the transfer of files than HTTP.\n\nTelnet\n\nAnother way of accessing information from servers is to log on to the remote computer using telnet. This service provider terminal emulation software supports a remote connection to another computer. However telnet has a disadvantage where one must know how to issue commands to the computer they are logged in. Moreover, the computer must be in a position to grant access to the server and should be able to run the telnet demon service.\n\nSimple Network Management Protocol (SNMP)\n\nThis is a constituent of TCP/IP originally developed to monitor and troubleshoot routers and bridges. SNMP provides the ability to monitor and communicate status information between terminal servers, wiring hubs, routers and gateways, and computers running Windows NT and LAN manager servers. In its structure, the SNMP has two components agents and management systems on a distributed architecture.\n\nPorts\n\nApplication processes using TCP/IP for transport have unique identification numbers known as ports. These ports specify the communication path between a client and the server part of the application where all applications, on servers, have pre-assigned port numbers. This assignment is carried out by the internet assigned numbers authority (IANA) ranging between 0 and 1023.\n\nThe Table 1 below denotes some of the well known port numbers with their corresponding services. These port assignments are documented in RFC 1700.\n\nTable 1: Ports numbers and corresponding process names\n\nPort number  Process name\n20           FTP \u2013 Data  \n21           FTP         \n23           Telnet      \n25           SMTP        \n53           Domain      \n67           UDP         \n80           HTTP        \n110          POP3        \n\n\nUser datagram protocol (UDP) and TCP indicate source and destination as port numbers within packet headers. The network operating system software broadcasts data from various applications to the network, and recaptures incoming packets from the network. It then matches them to their corresponding packet IP addresses. \u201cFirewalls are devices configured on the network to distinguish between packets, based on the source, and destination port numbers, services in this case, associate with the transport protocol ports using sockets\u201d (Day, 2008, p.67). A socket is a software oriented set up identified as the transport point on a particular network. In summary the 256 values, categorized in the documentation for the port numbers and their associated services, are outlined below:\n\n  * 0 to 63 reserved for network wide standard functions\n  * 64 to 127 which covers the host specific functions\n  * 128 to 239 reserved for future use while 240 to 255 was for any experimental functions\n\nEthernet\n\nEthernet is a term used to refer to a variety of local area network (LAN) technologies. Ethernet contains various wiring and signal standards that focus on the physical layer. This standard was developed, by Xerox PARC, in 1975 (Blanc, 1974). Currently Ethernet is a pattern describing a connection scheme for computers and data systems through a shared cable. The Ethernet specification covers similar functions as the open standards interconnection (OSI), physical and data link layers of data communication.\n\nSome of the typical features of Ethernet include:\n\n  * The use of linear bus or star topology\n  * The signal mode baseboard\n  * Access method carrier that senses multiple access with collision detection CSMA/ CD\n  * The transfer speed that ranges between 10Mbps to 100Mbps\n  * The Cable that is used is thicknet, thinnet or unshielded twisted pair (UTP)\n  * The maximum frame size is 1518 bytes\n  * The media is passive drawing power from the computer and therefore will not stop working unless the media is improperly terminated or physically cut.\n\nEthernet arranges data in frames, which defines a data unit transmitted individually The frame itself has a length of between 64 and 1518 bytes, including the typical Ethernet frame, which is 18 bytes long. Generally, the data portion in this frame is 46 to1500 bytes long.\n\nEthernet was formerly based on the concept of computers exchanging data using coaxial as the media. This coaxial would traverse every building to interconnect the computers. In order to manage collisions on the medium a scheme carrier, that senses multiple access with collision detection (CSMA/CD), was introduced. \u201cThis scheme carrier was preferred because it was simpler than the token competing technologies which the computers used special kind of transceiver called attachment unit interfaces (AUIs) to provide connection to the cable\u201d (Day, 2008, p.5). The Ethernet cable length was considered a limitation at that time. It was not possible to build very large networks using Ethernet and thus Ethernet repeaters were developed. The invention of Ethernet repeaters greatly improved signal strength over long distance transmissions.\n\nEthernet Standards\n\n10 Base T\n\nThis Ethernet standard implies a transmission speed of 10Mbps on base band when utilizing the twisted pair cable. It is an Ethernet standard that uses unshielded twisted pair (UTP) for connection purposes. The 10Base T is physically wired as a star but the logical topology is a bus. Usually, the hub of a 10Base T network can be used as a multi-port repeater and is often situated at the end of a cable linked to the hub. Every computer is connected using two pairs of wire, with one pair in use to receive data, while the other for transmitting data.\n\nOther variants of the Ethernet standards are 10Base 5 and 10Base F, which are implemented over the Cat5 and fiber optic cable respectively. The 10Base 5 standard is a standard Ethernet implementation that uses a thick coaxial. Regardless of the physical star topology, Ethernet uses repeaters which enforce a half duplex and CSMA/CD schemes. The repeater, which is normally limited in capacity, also has the signal enforcing collisions function that deals with packet collision. \u201cThe total through-put by the repeater depends on a single link and hence a uniform operating speed\u201d (Day, 2008, p.5). Connectivity devices like repeaters define Ethernet parts, but they forward all data to other connected devices. This becomes problematic because the entire network becomes one collision domain. Therefore, order to address this issue, switching and bridging can be implemented to enable communication at the data link layer.\n\n\u201cThe bridge, at this level, discovers the port address and forwards network traffic addressed to these ports\u201d (Halsal, 1989, p.21). The use of bridges further enhances mixing of speeds and results in the interconnection of more segments. As a result of this, fast Ethernet was introduced. This was introduced due to increased demands for greater bandwidth because of faster server processors, innovative applications and more challenging environments that required superior network data transfer rates than those provided by the existing LANs. As networks grow, there are more users catered for through server application approaches. As a result of this, there is increased network traffic. One inevitable effect is that the average file server is strained in the through-put typical of today\u2019s LANs. Current data demanding applications, which include voice and video with network server backups, demand reduced latency with enhanced data transmission speeds and reliability.\n\nThe popularity of 10Mbps LANs and their expanse makes them a suitable springboard for faster networking technologies. Some of the key features of the fast Ethernet will include:\n\n  * Its basing on CSMA/CD protocol that defines the traditional Ethernet access technology. However, this standard decreases the duration of time that each bit is broadcasted by a factor of ten. This raises the packet speed from 10Mbps to 100Mbps while requiring minimal changes to the network system. The challenging factor for implementing this technology is the collision detection function. While the bandwidth increases ten fold, the collision gap shrinks to one tenth.\n  * Data is transmitted between Ethernet and fast Ethernet, devoid of protocol conversion, because fast Ethernet includes the old error control functions, the frame format and length.\n  * These standards can use twisted pair and fiber optic as media.\n  * Fast Ethernet can be categorized into 100Base TX; 100BaseT4 and 100Base FX.\n\nOther technologies that have competed Ethernet can also be mentioned here. The most common of these is the token ring.\n\nToken Ring\n\nThis was developed in 1984 to cover the complete collection of IBM computers. The objective of developing token ring was to allow for the use of twisted pair cable to connect a computer to a LAN using a wall socket. The wiring structure for this scheme is centralized. The features of the token ring scheme include:\n\n  * Star ring wired network topology.\n  * The ring is logically implemented on a central hub\n  * Token passing is utilized as the access method\n  * Can use shielded and unshielded twisted pair as well as fiber optic cabling\n  * Have transfer rates that range from 4 to 16 Mbps. The 16Mbps token ring reduces delay by placing the token back on the ring immediately after the data frame has been transmitted. Token ring switches, which support full duplex, may support speeds of up to 32Mbps by simultaneously transmitting and receiving.\n  * Baseband transmission with a maximum cable segment length of computers within a space of between 45 and 200 meters.\n  * A frame based technology with a maximum frame size of approximately 5,000 bytes.\n\nFiber distributed data interface (FDDI)\n\nThe fiber distributed data interface (FDDI) is a 100Mbps network that uses token passing and fiber optic media. It was released in 1986 and was used as; a metropolitan area network (MAN), campus area network (CAN) and local area network (LAN) technology. It provides a high speed backbone and is either a physical star or ring where the logical layout represents a ring. Its media access control scheme is token passing. A copper distributed data interface (CDDI) is often used as a migration path to FDDI. The system can use existing twisted pair cabling thus functioning in the same manner as FDDI. The specification of fiber distributed data interface (FDDI), is similar to 802.5 (token ring) but it supports higher bandwidth and maximum segment distance of 100 kilometers. The FDDI \u2013 2 provides sound and video handling and can use dual counter rotating rings to protect against media failure. Incase a failure occurs in such a setup the node, known as dual attached stations (DASs) on either side of the break, re-establishes a ring by using a back up ring.\n\nFrame relay\n\nThis is a high speed transmission scheme that uses wide area network (WAN) protocol. A Frame relays is implemented at the physical and data link layers with reference to the open systems interconnection (OSI) model. It can suitably work on integrated service digital network (ISDN) interfaces. Moreover, it is a packet switching technique that is still in use over a variety of other network interfaces (Stallings, 2006). In line with this set up, some users are permitted to use the available bandwidth during idle periods. As a packet switching technology, frame relay makes use of two techniques. The variable length packet technique, that supports a more efficient and elastic data transfer process and the statistical multiplexing technique, when implementing controls network access. This allows for more flexible and efficient use of the available bandwidth. Today most local area networks (LANs) support the packet switching technique.\n\nAsynchronous transfer mode (ATM)\n\nAsynchronous transfer mode (ATM), unlike the frame relay, is a cell relay technique. The ATM scheme uses tiny packets of fixed size called cells to transmit data, video or voice applications. The networks rely on an already existing link between a transmitter and a receiver. These networks will achieve very high speeds of data transmission typically between 155Mbps to 622Mbps. An ATM network implements cell switching and multiplexing technologies. This is mandatory in order to maximize the benefits of circuit switching. Circuit switching supports guaranteed capacity and constant transmission delay. Alternatively, packet switching has more flexibility and efficiency for irregular traffic than any other network. ATM networks can also implement scalable bandwidth and this makes them more efficient than time division multiplexing, which is an example of synchronous technologies. Therefore, TM can be used with varied media such as coaxial, twisted pair and fiber optics cable which are intended for other communication systems.\n\nIntegrated services digital network (ISDN)\n\nThis is a service offered by telephone carriers to transmit digital data. In ISDN, digital signals are transmitted over the existing telephone network. The signals include text, voice, music, data, graphics and video, which are digitized and then transmitted over existing telephone wires (Stallings, 2006). This is currently used to implement telecommuting, high-speed file transfer, and video conferencing among other applications.\n\nSynchronous optical network (SONET)\n\nSynchronous optical network (SONET) wide area network oriented technology that is implemented using fiber optic as the key medium. The network transmits voice, data and video at high speeds. The synchronous transport signals (STS) and the synchronous digital hierarchy, are the American and European equivalent versions of this technology respectively.\n\nCable modem\n\nThis technology is implemented using the cable modem, which acts like a bridge, and is able to support bi-directional data communication. This is implemented by use of radio frequency implemented over hybrid fiber-coaxial (HFC) and RFoG infrastructures. The cable modem is suitable for implementing broadband internet access because it supports high bandwidth infrastructures. Cable modem also comes in handy with the advent of protocols such as voice over internet protocol (VoIP), because it can be implemented as a telephone within such as setup\n\nVGAny LAN\n\nThis technology combines Ethernet and token ring. It is also known as 100 VGAny LAN, 100Base VG and has the following specifications:\n\n  * 100Mbps bandwidth\n  * Can be used to implement a cascaded star topology using category 3,4 and 5 twisted pair and fiber optic cable\n  * The demand priority access method supports two priority levels low and high\n  * Can be used to implement an option to filter individually addressed frames at the hub thereby enhancing privacy\n\nWireless Local Area Networks (IEEE.802.11)\n\nA WLAN uses wireless transmission media, where 802.11 is used as a standard to implement wireless local area networks (LANs). The WLANs are applied in LAN extensions, building interconnections, nomadic access and on demand networks. They are also suitable for connecting devices on large open areas. In most cases, WLAN is not known to work in isolation and therefore it will be linked to a wired LAN at some point hence becoming a LAN extension. The typical WLAN setup will include a control module, which serves as the interface to the wireless LAN and which can either be a bridge or router linking the wireless LAN to a backbone (Mittag, 2007). The control module works by polling and token passing in regulating access to the network. User modules on the other hand could be hubs interconnecting a wired LAN, workstations or a server. Whenever various wireless devices are placed in the same range grouping, connoting a single control module, this is called a single cell wireless LAN. For the cross building interconnect, a point to point wireless link can be implemented between buildings. Typically, bridges or routers could be used to connect devices in this case. The nomadic access approach involves a LAN hub and a number of mobile data terminal equipment such as laptops. Both devices must be within operation range for transmission to succeed. An adhoc network is a WLAN that is peer to peer in nature and which is established to meet an immediate need. WLANs requirements will include the following:\n\n  * A medium access scheme needed to optimize the use of available medium.\n  * A substantial number of nodes\n  * A connection to a wired LAN backbone\n  * Restricted surface area of a diameter that ranges between 100 and 300 meters.\n  * Available battery power source to power the mobile nodes.\n  * Well defined transmission robustness and security to deter eavesdropping\n  * Dynamic configuration to manage MAC addressing and network management\n  * Roaming capabilities to enable mobile user modules to move from one cell to the other.\n\nWLAN is generally categorized based on the transmission technologies used. The three main categories are:\n\n  * Infrared LANs whose expanse is limited to a single room\n  * Spread spectrum LANs which will generally require no licensing to operate\n  * Narrow band microwave which operates at frequencies that may require licensing.\n\nThe following table 2 summarizes the wireless technologies available today.\n\nTable 2: Wireless technologies\n\n                      Infrared                Spread spectrum                    Radio               \nDiffused infrared     Directed beam infrared  Frequency hopping                  Direct sequence       Narrowband microwave\nData rate (Mbps)      1 to 4                  1 to 10                            1 to 3                2 to 50               10 to 20\nMobility              Stationary /mobile      Stationary with line of site(LOS)  Mobile                Stationary / mobile \nRange (meters)        15 to 60                25                                 30 to 100             30 to 250             10 to 40\nDetectability         Negligible              Little                             Some                \n                                              902 to 928 MHz                     902 to 928 MHz      \nWavelength            \uf06c: 800 to 900nm         2.4 to 2.4835 GHz                  5.2 to 5.775 MHz    \n                                              5.725 to 5.85 GHz                  18.825 to 19.205 MHz\nModulation technique  ASK                     FSK                                QPSK                  FS / QPSK           \nRadiation             \u2013                       <1W                                25mW                \nAccess method         CSMA                    CSMA/ Token ring                   CSMA                  ALOHA, CSMA         \nLicense required      NO                      NO                                 YES                 \n\n\nThe spread spectrum is becoming the most commonly used and dependable form of encoding for wireless communication. As such, the use of the spread spectrum will ultimately improve reception while reducing the jamming and interception incidences. The basic idea used in spread spectrum involves modulating a signal in order to boost the signal bandwidth to be transmitted. There are basically three approaches worth mentioning for the spread spectrum. Frequency hopping spread spectrum: This is where the signal is broadcasted over various radio frequencies hopping between these frequencies at some defined interval of time.\n\nDirect sequence spread spectrum: This is where every signal bit is translated into numerous bits for the transmission. This process is accomplished by use of a spreading code.\n\nCode division multiple access: This is where several users make use of the same bandwidth with limited interference.\n\n802.11 Architecture\n\nA basic service set (BSS) constitutes the basic unit within a WLAN. This BSS involves various stations which run the same MAC protocol and share the same medium of transmission. The BSS can be standalone or interconnected to a distribution system (DS), via an access point (AP). The access point here acts as a bridge and the BSS is what is commonly known as a cell.\n\n802.11 Architecture terminologies:\n\n  * Access point (AP): This is a station or node on a WLAN that offers access to the distribution system.\n  * Basic service set (BSS): These are a number of stations under a single operational command.\n  * Distribution system (DS): This is the interconnection that exists between BSS and integrated LANs.\n  * Extended service set (ESS): Consists of integrated LANs and BSSs that appear singularly at the logical link control (LLC) layer.\n  * MAC protocol data unit (MPDU): This is a data unit transmitted between two devices.\n  * MAC service data unit (MSDU): The unit of information shared between two users.\n\n802.11 Services\n\nA total of nine services are provided by the WLAN. These services are implemented at an access point or another special purpose device. These services are summarized in the table below.\n\nTable 3: Wireless LAN services\n\nSERVICE            SUPPORTS             PROVIDER           \nPrivacy            Access and security  Station            \nIntegration        MSDU delivery        Distribution system\nAuthentication     Access and security  Station            \nDistribution       MSDU delivery        Distribution system\nMSDU delivery      MSDU delivery        Station            \nDisassociation     MSDU delivery        Distribution system\nDe-authentication  Access and security  Station            \nRe- association    MSDU delivery        Distribution system\nAssociation        MSDU delivery        Distribution system\n\n  * Association: This service defines the connection between a station and the access point device.\n  * Re-association: This is the transfer of an association between two access points.\n  * Disassociation: The notification between two access points that a connection is about to be ended.\n  * Authentication: Set up the distinctiveness of each stations to the other\n  * De-authentication: The termination of an authentication service.\n  * Privacy: Established to stop the messages contents from being read by an unauthorized receiver.\n\nWLAN Switches\n\n\u201cThese devices are used to implement a link to the access points through a wired connection. The switches act like gateways to the wired network\u201d (Halsal, 1989, p.21). Initially, in WLAN deployment, all the access points were autonomous. However, a centralized architecture has gained popularity providing the administrator with a structured method of network management. In WLAN, a controller carries out management, configuration and control of the network. A most recent innovation in WLAN is the Fit Access Points (FitAps). This setup supports encryption while establishing the desired exchange. This is then supported by the new on-market chipsets, which support WPA2. Within FitAps, there is also dynamic host configuration protocol (DHCP) relay. Additionally, other functions like VLAN tagging, which relies on service set identifiers (SSID), are implemented in the FitAps.\n\nNetwork Address Translation (NAT)\n\nThis process involves the dynamic address modification of IP packet headers which is carried out within a routing device. \u201cThe one to one network address translation process focuses on the IP addresses, the header checksum and other checksums which have the IP address of the packet that needs to be changed\u201d (Holliday, 2003, p.1). Alternatively, the one to many NAT alters TCP/IP port information in the outgoing packets while maintaining a translation table. In this case returned packets can be correctly translated back using the information in the translation table. The one to many NAT is also referred to as network address and port translation (NAPT). The NAT process often occurs in the router. A number of ways are available for port translation and the full cone NAT is most common of these and supports a one to one transmission mode.\n\nAt this juncture, the internal address is matched to an external one and as such, any external host is capable of sending packets through the internal address port as well as the external address of the port. For the restricted cone translation, once the internal and external addresses of the ports have been mapped, an external host sends packets to internal address only if the internal address had initially communicated to the external address. For the port restricted NAT, once the internal and external addresses have been mapped, an external host can transmit packets to the internal address by broadcasting them to the external port address if the internal port address had beforehand send a packet via the external host port. (Stallings, 2006, p.89)\n\nPublic and Private Addressing\n\nPrivate networks that do not have a link to the internet can use any host addresses. Public addressing does not permit two network nodes to have the same IP address. Private addresses are available to tackle the decreasing public IP addresses and thus they can be used for different nodes directly interconnected to each other.\n\nDomain Name Service (DNS)\n\nThe TCP/IP protocol uses the binary version of the IP address for locating hosts on a network. The dotted decimal notation is used for configuration purposes but it is not particularly intuitive for humans to remember. This led to a unique friendly name being assigned to each host on a TCP/IP network. This consists of two types of names:\n\n  * Host name: The administrator assigns an alias to a computer. Originally, a local file was held on each host to provide a lookup table to match host names with corresponding IP addresses.\n  * Fully qualified domain names (FQDNs) are used to provide a unique identity for the host to avoid duplicate host names.\n 1. \n\nFully qualified domain names must adhere to the following rules:\n\n  * The host name must be unique within the domain.\n  * \u201cThe total length of the fully qualified domain name must be 255 characters or less with each node (part of the name defined by a period) having not more than 64 characters\u201d (Holliday, 2003, p.1).\n  * The FQDNs supports alphanumeric and hyphen characters only.\n\nFQDN and their corresponding IP addresses are held on a domain name service (DNS) server, although a local host file can be used. Each domain must provide an authoritative DNS server to hold information relating to that domain. Host names and FQDNs are used for PING and other TCP/IP utilities instead of the IP addresses. To make use of these friendly names, there has to be a system for resolving a host name to its IP address and also ensuring the names are unique. Prior to domain name services, a host file called HOSTs was used. Resolving host names to IP addresses involved the InterNIC, which is the central authority maintaining a text file of host names and IP address mappings. Whenever a site required to add a new internet based host, the site administrator would dispatch an email to InterNIC giving the host name to IP address mapping; a process that was carried out manually. Downloading and copying the latest HOSTs file and installing it on each host was the task of the network administrator. Each host then performed name resolution by looking up a host name within a copy of the HOSTs file and locating matching address. Maintaining completeness and accuracy of the file became too difficult as hosts increased leading to the development of DNS.\n\nThe DNS has a hierarchical and distributed structure for name resolution to IP addresses. DNS uses a distributed database system which contains information related to domains and hosts found in those domains. Information is distributed amongst name servers that hold a portion of the database. Maintenance of the system is delegated and the loss of one DNS server does not prevent name resolution from being performed because of the distributed nature of the system. The DNS system has its own network of servers that are consulted in turn until the correct resolution is returned for every request. The hierarchical structure of the domain name system (DNS) is such that at the top, there are nine root servers (A to I).\n\nImmediately, underneath the root lies the top level domain labeled by the type of organization. In some countries, the top level domains are organized using the ISO country codes which include uk for United Kingdom, nl for Netherlands and de for Germany. Beneath this level is the second level domain that covers companies and governments with extensions such as com. or gov. By tracing records from the root, and traversing the hierarchy, one can find information about a particular domain.\n\nThe root servers have complete information about the top level domain servers. In turn, these servers have information relating to servers for second level domains. Records within the DNS tell them where the missing information can be found. FQDN reflect the hierarchy from most specific (host), to the least specific (a top level domain). The user types in a uniform resource location, which is an address the browser, will ask the DNS client software to determine the IP address. The local DNS client software then requests the DNS server for the resolution of the submitted address to an IP address. The request is transmitted to the root domain where the server provides the IP address requested. This address is then returned and stored in a local cache and the IP address is also retransmitted back to the DNS client software. This address is then forwarded to the browser which establishes the connection and opens the corresponding web page. This process only takes a few seconds.\n\nNetwork security management\n\nComputer system security remains very important in order to guard the integrity of the information stored. The file system has the mechanism needed for storing and accessing data and programs within the computer system. Resident file information system is vital and needs to be monitored in order to detect unauthorized and unexpected changes thereby providing protection for the system against intrusion. The most effective process to detect host-based intrusions is by noting changes to the file system. In any network platform, the process of monitoring such changes becomes quite a challenge for the administrator. Online threats remain a reality for today\u2019s businesses, especially those relying on the internet. Active and passive attack incidents are escalating every day and network administrators are having a daunting task of detecting, controlling, or minimizing the effects of such attacks. One of the common methods to secure the facility includes the common access control and auditing procedures. Perimeter systems, that are sensitive to intrusions, can be set up to boost physical security. An intrusion detection system (IDS) is one of the tools in the organization\u2019s network security armory that also includes a firewall and an antivirus. The IDS will compliment a firewall to ensure desirable network security for any organization.\n\nIn today\u2019s enterprise networks and the internet, there is a gap between the theoretical understanding of information security best practices and the reality of implementation. Much of this is due to inability of network management staff to communicate this approach such that non-technical influencers and decision makers can grant executive sponsorship. The approach of layered defense in depth policy procedures and tools is well accepted as best practice for information security. The pressure for return on security investment (ROSI) has further exacerbated the difficulty of implementing technology. Since 9-11, security has been elevated to an area of critical liability for many business continuity providers. Based on this approach, service and business continuity providers such as Cisco have questioned how to protect the integrity of mission critical operations without limiting the flow of business. Corporate information security officers have for a long time now been asking this question.\n\nIntrusion detection systems (IDS)\n\nThese form part of the amalgamation of tools known as the intrusion detection system (IDS). The IDS constitutes an application software and associated hardware that is capable of monitoring network activities. This is for the sake of detecting malicious activities or any other violations relating to policy and procedures. The system actively monitors and reports back to the network administrator. Intrusion detection systems will address intrusion prevention, which is the process of attempting to stop likely incidences after performing an intrusion. Generally, IDS, besides establishing a record of intrusion activities and generating appropriate notification to the administrator, these systems can also rebuff threats causing the intended threats to fail. IDS will broadly cover two main categories.\n\nThe network intrusion detection system (NIDS), which consists of an infrastructure of hardware as well as software, can identify intrusions through the examination of host activities as well as network traffic. \u201cThis is made possible through established connections to a network hub or switch or otherwise a configuration that can enable network tapping or the establishment of port mirrors\u201d (Stallings, 2006, p.93). Often, the administrator will establish network borders using sensors or set up choke points. These are employed to capture traffic on the network. Snort is an example of an NIDS used to capture and analyze individual packet content in order to establish malicious traffic.\n\nThe host based intrusion detection system (HIDS) defines the other intrusion detection system type involving an agent. Here the agent will analyze system calls as well as application logs.\n\nReferences\n\nAthenaeum, A., & Wetherall, D.J. (2005). Computer networks (5th ed.). Upper Saddle River, New Jersey: Prentice Hall.\n\nBlanc, R.P. (1974). Review of computer networking technology. National bureau of statistics, 1-136.\n\nDay, J. (2008). Patterns in network architecture. Upper Saddle River, New Jersey: Prentice hall.\n\nHalsal, F. (1989). Data communications, computer networks and OSI (2nd ed.). Boston MA: Addison Wesley.\n\nHolliday, M.A. (2003). Animation of computer networking concepts. Journal on Educational Resources in Computing , 3(2), 1.\n\nMittag, L. (2007). Fundamentals of 802.11 protocols . Web.\n\nStallings, W. (2006). Data and computer communications (8th ed.). Upper Saddle River, New Jersey: Prentice hall.\n",
        "label": "human"
    },
    {
        "input": "The Microcomputer: Medical Application Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Research overview\n 3. Medical background of electrocardiogram (ECG)\n 4. Uses of ECG HCG- 801\n 5. Features of ECG HCG- 801 monitor\n 6. Conclusion\n 7. Works cited\n\nIntroduction\n\nIt is imperative to note that people have employed computers in nearly all areas of operations. They have enhanced research, fastened trade, innovations and inventions and generally have improved the living standards. In addition, computers have brought novel technologies in medicine and research. Their penetration in almost all sectors is due to application and wider utilization of microchips and software that manufacturers devise daily. Therefore, this research will focus on medical or biological application of micro-computers and will back the information from articles. The research will entail a brief description of the work presented in the article, give a medical background of the application, display the role of this micro- computer, and demonstrate how medics employ computers in medicine. Moreover, it will grant information about the microcomputer like the name of the chip, manufactory, board memory, and description of major components (MIT).\n\nResearch overview\n\nThe research is going to focus on the ECG monitor. In years, microchips have persistently transformed electronics enabling the fresh products to flow in the market for instance the portable computers and digital players. Technology Labs have worked hard to produce low power chips to treat diseases like heart problems, monitor the progression of a patient, or predict seizures in epileptic patients. Such kind of implantable gadgets have transformed medicine by reducing medical expenses. The microelectronics systems are applicable in medicine to dispose blood pressure sensors, which nowadays are affordable, compared to earlier years (MIT).\n\nMedical background of electrocardiogram (ECG)\n\nECG is a graphic machine that keeps track of electric current generated in heart muscles. Computerized ECG\u2019s came to operation towards the end of 1960s. They involved analog processing and had transistor speakers to give audio signals and alerts. The main function was to give information concerning the heart conditions and provide perfect details on the performance of the heart. The medics put the electrocardiograms produced by electrodes at various locations on the body. The purpose was to shed off the small heart electric current recorded. The standardized ECGs enable doctors to compare cardiac performance of a person under study. Normal ECD displays typical upward and downward deflections reflecting the interchanging contractions of aria and the ventricles of the heart (University of Pennsylvania).\n\nUses of ECG HCG- 801\n\nThe advantage of making gadgets like \u201cOMRON portable ECG monitor HCG- 801\u201d manufactured by Omron is to provide a chance of low- power interfacing biomedical sensors, signal and energy processing. The ECG monitor worn by the long-sufferers as they perform their daily duties enables the physician to have a clear picture of the heart health. The data from this chip is almost perfect and it is crucial in revealing any other problems that a patient may be developing in the course of diagnosis. Therefore, the patient is saved from time-consuming lab tests and latent costs that could arise if ECG was not present. The doctors regularly advice patients who suffer from heart problems and others that may suspect they have heart related issues, to have ECG monitors put in their bodies for a few days to observe their heart performances. In addition, ECG is used in gather information concerning the electrical health of the heart. By so doing, it helps to spot the future heart problems venerable to the patient. Therefore, doctors are capable of seeing the heartbeat shape and variations from the recorded data a day after the patient show heart complications. The medics use the gathered data to correlate the eightfold increase of deaths associated with cardiac death that occurs shortly after the ninety days of patients found with heart problems. The portable ECG available is also used to sense symptoms like heart pain and palpitations. Patients may use it to detect the shortness of the breath which otherwise would be hard to record using the conventional devices in laboratories. The advantage of the portable ECG 801 is that, it is usable whether one is at home or at work and therefore reliable to all patients (University of Pennsylvania).\n\nFeatures of ECG HCG- 801 monitor\n\nThe \u201cOMRON portable ECG HCG- 801 monitor\u201d has a recordings memory of about thirty seconds. The recorded data is later used to know when the heart symptoms occurred and at what rate. The patient can then present the recordings to the doctor for examination and from the results and from the doctor is able to performing corrections on the diagnosis procedures. ECG HCG- 801 monitor is easy to work with because it displays the results clearly on an elevated-resolution screen. The HCG- 801 screen backlight is adjustable hence; the patient adjusts screen brightness to desirable levels. Data and messages concerning the heartbeat and pulse normally become perfectly displayed and well organized. This Portable HCG-801 has a big SD memory that holds three hundred measurements and a universal service bus (USB) port to transfer data into any large computer or personal computers for data storage and backup purposes.\n\nConclusion\n\nHaving learnt about the microcomputers in this research it is notable that gadgets like ECG HCG-801 are helpful to many people. The research has revealed the advantages and uses of ECG monitors, for example measuring heartbeat rate, palpitations, and checking the patient\u2019s breath while at home or at work. The research has looked at main features of HCG-801 and there is no doubt that computer technology is applicable and useful in medicine.\n\nWorks cited\n\nMassachusetts Institute of Technology, MIT. Revolutionizing medicine, one chip at a time. 2010. Web.\n\nUniversity of Pennsylvania. Mini-Project I: Cardiac Monitor VI. 1999. Web.\n",
        "label": "human"
    },
    {
        "input": "The Reduction in Computer Performance Report\n\nThe issue at hand pertains to a slowing down in the performance of a user\u2019s machine. In addition to this, there is a warning message about a defect on the hard disk. There are a number of likely causes for the reduction in machine performance which include: the hard disk is badly fragmented or the storage space is running low. In addition to this, the warning message indicates that the hard disk may contain system errors or even bad sectors which are responsible for the degraded performance. If not dealt with, these defects may result in a complete crash of the hard disk.\n\nSince it is evident that the hard disk has some defects, it is necessary to run some maintenance procedure to deal with the issue. The Check Disk (Chkdsk) utility available in Windows XP enables one to monitor the health of the hard disk. The chkdsk helps to verify the integrity of the file system by examining the hard drive for file system errors and for physical defects. Since a thorough check disk utility is to be run, the options \u201cautomatically fix-system errors\u201d and \u201cscan for and attempt recovery of bad sectors\u201d will be checked. By doing this, the scan will be thorough and lengthy which will ensure that errors are fixed.\n\nFragmentation of data may also cause the computer to be slow and hence exhibit reduced performance. Severe fragmentation may result in a computer crash. It is therefore necessary to run Defragmentation on the hard disk. Badly fragmented hard disks can affect the performance of the computer and one uses the disk defragmenter utility to reorganize the files on disk. By running this utility, the hard drive will be analyzed and a map of the data will be given. From this an alert is issued of whether the disk needs to be defragmented. One defragmentation takes place a performance boost will be noticeable.\n\nThe degradation in system performance may be the result of an accumulation of unnecessary files. To deal with this, one can run the Disk Cleanup utility. This utility assists to deal with temporary files that Windows may have collected over time. This utility will analyze the disk and display actions that can be undertaken to recover disk space. From this utility, programs that are not used can be uninstalled or old system restore points deleted. This will result in space being freed up and hence improve system performance.\n",
        "label": "human"
    },
    {
        "input": "Trusted Computer System Evaluation Criteria Essay\n\nThe Trusted Computer System Evaluation Criteria is a standard established by the US Department of Defense that outlines the fundamental requirements in order to evaluate the efficiency of the computer security controls that have been integrated onto the computer system. The fundamental role of the TSEC was to assess, catalog and facilitate the selection of computer systems that are to be used for effective processing, data storage and the retrieval of sensitive information (Daly, 2004). The Common Criteria for Information Technology Evaluation is a framework through which users of a computer system can specify the functional and assurance security requirements, after which vendors can facilitate the implementation of computer security basing the claims of the users. The common criteria offer assurance that facilitates specification of user requirements in terms of functional and assurance, vendors\u2019 implementation of their requirements and standard evaluation in order to ensure that a security product meets the international computer security standards. This paper discusses the impacts associated with the transition from the Trusted Computer System Evaluation Criteria to the International Common Criteria for Information security evaluation. The paper provides an overview of the concepts of security assurance and trusted systems, an evaluation of the ways of providing security assurance throughout the life cycle, an overview of the validation and verification, and the evaluation methodology and clarification techniques deployed in both criteria for security evaluation.\n\nSecurity assurance is one of the core objectives and requirements of the Trusted System Evaluation criteria, which stipulates that a secure computer system should have hardware and software mechanisms, which can be evaluated independently in order to foster adequate assurance that the computer system ensures minimum security requirements. In addition, the concept of security assurance should provide a guarantee that independent portion of the computer system works as it is required. Security assurance guarantees the protection of data and any other resources that it hosts and it controls. The basic argument is that that the hardware or software entity in itself is a resource, and should have appropriate security mechanism (Herrmann, 2003). In order to facilitate the realization of these objectives, there are two principal kinds of security assurance that are required; they include assurance mechanisms and Continuous Protection Assurance. Assurance Mechanisms involve operational and life-cycle assurance; while the Continuous Protection Assurance involve the trusted mechanisms that are used in the implementation of the basic security requirements and ensure that these requirements are not subjected to unauthorized alterations. Trusted system on the hand refers to a system that can be depended upon to undertake its specified functionality and ensure the outlines security policies (Lehtinen, 2006). The underlying argument is that the failure of a trusted system is bound to result to a breaking of a particular security policy. Basically, a trusted system can be perceived to be a reference monitor and play an integral role in monitoring all the access control decisions. The relationship is that security assurance results to a trusted system, with the outcome being an integration of computer hardware and software, and any other middle ware that can be used in the enforcing of particular security policies. In order to avoid failure of the trusted system, higher levels of system assurance are required in order to guarantee the effectiveness of the trusted system. An empirical analysis of the above implies that the TSEC utilized six evaluation methodologies, while the Common Criteria utilized seven evaluation methodologies (Merkow, 2004).\n\nUnder the Trusted Systems Evaluation Criteria, life-cycle assurance normally entails the carrying out a security testing, the specification of the design and its respective verification, configuration management and then finally the Trusted System Distribution. One of the TSEC requirements is that security implementation should take place throughout the lifecycle of system development. Security testing is used to determine whether a system has the capability of protecting its data and resources without impairing its overall functionality. Therefore, security testing aims at assessing the ways in which a system ensures confidentiality, data integrity, user authentication, system availability, user authorization and non-repudiation. Specification of the design is done in accordance with the functional and user security requirements. Trusted systems have to integrate the functional and user requirements with the security policies in order guarantee the core objectives of information security. Design specification is an important process during the outlining of design requirements during the implementation of a security system (Lehtinen, 2006). Verification simply entails confirmation that the security system is functioning in accordance with the expected requirements, in the sense that it should meet the minimum security requirements in order for the system to be deemed effective in terms of enforcing security. Configuration management aims at ensuring that there is consistency with respect to security performance. These normally include keeping track of any needed changes and constant adjustment of the security baselines in accordance with the nature of the security threats available. Two core processes are undertaken during configuration management, they include revision control and baselines establishment. Trusted System Distribution on the other hand aims at offering guaranteeing the security of a trusted system prior to its installation. According to the TSE requirements, it is important that the security properties of a trusted system be intact prior to its installation for the user. In essence, the installed system should be an exact copy of the system that was evaluated against the requirements of the TSEC. Basically, the lifecycle of a security system implementation entails the definition of security requirements, design and the implementation. Assurance justification and the design implementation requirement are vital in ensuring that the implemented system meets the security evaluation criteria under the TSEC and the Common Criteria, with the Common Criteria having more evaluation frameworks compared to the TSEC (Daly, 2004).\n\nValidation and verification are vital in ascertaining the effectiveness of a security system. Validation and verification are used in checking out that a security system meets the design specifications and that its functionality is not impaired as required. Validation and verification are significant elements of the Quality Management System. Specifically, validation can be perceived as a quality control strategy used in the evaluation of whether as security system has complied with the international security standards, regulations and specifications. Verification is usually an internal process and takes place during all the phases of the security system development and implementation. Validation on the other hand can be viewed to be a quality assurance process that has the principal objective of analyzing the performance of a security system and also aims at guaranteeing high levels of security assurance, in the sense that the implemented security system meets all the requirements in order for the security system to be deemed effective. Basically, it evaluates the fitness of purpose and facilitates the acceptance of a security product by the end users. Validation entails developing the right security system; while verification involves building the security system in a right manner, implying that it takes into account that the specifications are implemented as required, which means that it is a match against the needs of the users.\n\nThere are various evaluation methodology and certification techniques that can be used to determine the levels of security assurance of an information system. The main objective of evaluation methodologies is to determine the vulnerability of a system, which normally includes an assessment of the breaking of the security policies and controls, which may in turn result to a violation of the security policies (Daly, 2004). Formal verification is one methodology that can be deployed in order to ensure that a security system meets some certain constraints. It normally entails the establishment of preconditions and post conditions of the implemented system. In order for the system to be deemed effective, the post conditions must meet all the constraints. Penetration Testing is also another technique that can be used to determine is a security system meets some minimum constraints. It normally entails the hypothesis stating of the characteristics of the system and the state that is likely to impose the system to vulnerability. The result is normally a state that has been compromised. Tests are carried in order to see if the system will become compromised, therefore making the system vulnerable (Herrmann, 2003).\n\nIn conclusion, the transition from the transition from the Trusted Systems Evaluation Criteria to the International Common Criteria resulted to more secure systems owing to the fact that the Common Criteria has more evaluation frameworks compared to the TSEC.\n\nReferences\n\nDaly, C. (2004). A Trust Framework for the DoD Network-Centric Enterprise Services (NCES) Environment. New York: IBM Corp.\n\nHerrmann, D. (2003). Using the common criteria for IT security evaluation. New York: Auerbach.\n\nLehtinen, R. (2006). Computer security basics. New York: O\u2019Reilly Media, Inc.\n\nMerkow, M. (2004). Computer security assurance using the common criteria. New York: Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics and Audio Data Retrieval Research Paper\n\nTable of Contents\n 1. Abstract\n 2. Introduction\n 3. Literature Review\n 4. Conclusion\n 5. Reference List\n\nAbstract\n\nThe dynamic nature of crime in modern society has transformed the way in which criminal activities are executed and has led to increment in cyber and computer related criminal activities. Due to the soaring increases in cyber crimes, various states have put more emphasis on computer forensics as a means of protecting and curbing such crimes.\n\nWhile computer forensics have played a major role in reducing cases of cyber terrorism and corporate crimes in most countries, the process have faced major challenges which emanate from data accuracy and associated costs of storage, searching and indexing as well as the efficiency of data retrieved using various techniques.\n\nThe study addresses one of challenges faced by computer forensic experts, retrieval of audio information. While corporate entities and other organizations struggle to identify the relevant data that needs to be preserved and the associated costs, the technology used by forensic experts to retrieve such information are often inaccurate and inefficient which minimizes the strength of such evidence in court.\n\nIntroduction\n\nAdvanced technology in the modern society has contributed to the increase in computer and computer supported criminal activities due to the soaring increases in the number of internet users across the world and computerization of business processes which has created opportunities for computer criminals and terrorists to execute crimes.\n\nNumerous studies have revealed that crimes such as cyber attacks, hacking, and other computer based criminal activities have been costing business organizations and governments a considerable amount of money each year which has prompted the development of Computer forensics to preserve, identify, extract, and document computer evidence. Computer forensics can be defined as the process through which information is extracted from a computer crime scene while guaranteeing its accuracy and reliability through retrieval and storage as data or magnetically encoded information.\n\nLiterature Review\n\nAudio files present a major challenge for computer forensics during the criminal justice process. Data presented in form of email, instant messaging, faxes, text messages, data derived from business computer applications as well as voice messages sent through network avenues and digital devices are commonly used in business organizations due to their ability to cut down costs (Vacca, 2005). However, on occurrence of computer related crimes, such voice based files may be difficult and expensive for computer forensic experts to retrieve.\n\nAlthough numerous systems have been put in place to facilitate storage and indexing of data within organizations, these systems are very expensive which prompts companies to outsource the role to other companies (Caloyannides, 2009).\n\nFurthermore, despite the fact that the tools for searching and storing data are often effective and accurate, with audio data, such levels of accuracy and efficiency have not yet been achieved. Indeed, the three current means of searching audio data; phonetic search, transcribing by hand, and automatic transcription (Ewechia, 2011) have been found lacking to some extent.\n\nPhonetic search technology extracts audio information through wave patterns and often results in false hits due to the wide variations in people\u2019s mode of speech, accent, pronunciation and dialectics. In addition, this method does not have the ability to transcribe audio messages into texts and hence solely relies on the hearing ability of experts and other concerned stakeholders (Ewechia, 2011).\n\nManual transcription of audio data which facilitates in conversion of audio messages to text is effective but its time consuming since it depends upon the listener to transcribe the words as they are hear which makes the process more labor extensive hence expensive. Machine transcription, which is an automated means converting audio data to text, is a faster means of retrieving audio data but it suffers from accuracy issues emanating from factors such as differing pronunciation and clarity of recordings (Ewechia, 2011).\n\nNew federal rules of civil procedures have been put in place to ensure that companies identify key communications and data sources which should then be saved for future references. As requirement for retention of data increases, identifying the type of data to be preserved has become a major challenge for organizations which impacts on future data availability.\n\nIn addition, computer forensic experts are expected to prove beyond reasonable doubt that that the information they have extracted through these methods is exactly as it was on the computer or other digital device in order to guarantee accuracy and reliability (Lucas & Moeller, 2004). Failure to guarantee such aspects in audio data reduce the strength of evidence hence decreasing the likelihood of success in a court of law.\n\nConclusion\n\nComputer forensics has become an increasingly important component in the fight against crimes. This is primarily due to its ability to retrieve, and present the data required for criminal investigations in a clear and precise form. The data retrieved through computer forensic technology has played a major role in availing evidence which has provided lead to many cases and has also prevented cases of false incrimination.\n\nDespite the challenges that the process faces, the technology continues to evolve and advance as time progresses. We can only anticipate a future where more advanced methods of data retrieval will be developed in order to guarantee accuracy and validity of such data.\n\nReference List\n\nCaloyannides, A. M. (2004). Privacy Protection and Computer Forensics . Massachusetts: Artech House.\n\nEwechia, (2011). Audio Files Present Challenges for Computer Forensics and E-Discovery . Web.\n\nMoeller, B., & Lucas, J. (2004). The Effective Incident Response Team . New York: Addison-Wesley.\n\nVacca, R. J., (2005). Computer Forensics: Computer Crime Scene Investigation Vo. 1 . New York: Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensic Timeline Visualization Tool Report\n\nIntroduction\n\nThe article is about research on a computer forensic tool that was carried out by Jens Olsson and Martin Boldt at Blekinge Institute of Technology. The two scientists came up with a prototype of an existing computer forensic utility that combines the output of other forensic utilities and displays it against a given timeline. In computer forensics, timeline analysis is time-consuming because each process investigated differs from the other. Data that is not relevant in one process may be crucial in another process in forensic investigations. This is the reason for manual investigations of timeline in forensic analysis. This process is time-consuming. There is a need for the development of a forensic tool that analyzes and filters the necessary information then displays the outcome in a timeline that is easily understood.\n\nThe necessity to save time in computer forensic investigations is the basis of the tool that Olssen and Boldt came up with. It is called CyberForensics TimeLab (CFTL). The tool forms a kind of directory for all evidence gathered about time. The outcome is then plotted on a graph of the timeline. In doing so, forensic investigators analyze the timeline easily and quickly as compared to the existing forensic tools. An existing tool that is similar to the CFTL is the Forensic toolkit (FTK) that was created by Access Data. It analyzes different data just like the CFTL but does not display all the timestamp data in a way that makes work easier for the investigator.\n\nOlssen and Boldt tested the CFTL by giving it to several users then comparing its functionality to FKL. The results were positive and overwhelming. 12 tests subjects used the tool CFTL and FKL to solve a hypothetical forensic case. The case was solved in 14 minutes using CFTL and in 45 minutes using FKL.\n\nCase Project\n\nQuestions that the woman should be asked are time and access-related. First of all the investigator should determine the location that the files found on her computer were meant to be and who has access to it. This can help in narrowing down other possible suspects in the case. The first question the woman should be asked is about other individuals with access to her computer. She should clearly define people with regular access to her computer and specifically those who have accessed it within the last month. This information helps in relating the list of individuals with access to the main source of files to the ones with access to her computer.\n\nThe investigator should then determine how the woman can benefit from accessing the files found on her computer. Are they of any importance to her? If she is fired, who benefits the most? Is it an individual within the company or the company itself? The answers to these questions help the investigator to determine if the woman has been framed or not. The investigator should then determine the woman\u2019s history with computers and the level of her computer knowledge. Does she have any certification in computers? How knowledgeable is she in computer use? Finally, the investigator should determine the number of times she logs into her computer in a day and specific times she has logged into it for the past week. The investigator should then check the computer logs and information about the files. The date and time that the files were created and last modified. The user logged in at the time the files were created is also important for the investigation.\n\nRational Vs Holistic\n\nPeople who tend for the rational approach are managers whose guidance stems from the statements of finance. They believe that positive outcomes ought to be achieved by well-calculated means with great precision. All their actions and decisions are based on facts that can be quantified and observed. These managers are viewed by their subjects as plain and uninspiring. According to the subordinates of managers observing rational approach to decision making, they are autocratic.\n\nOn the other hand, individuals who tend for the holistic approach are leaders driven by several factors that include relationships. These leaders make their subordinates optimistic hence a general improvement in the performance of the organization. They consider the underlying factors while making their decisions. These leaders take into consideration not only the relationships within the workplace but also social resources that are complex. The employees in this case perceive their managers to be visionary.\n\nStudies have shown that holistic managers are more effective in decision-making as compared to rational managers. Results of these studies show that the performance of their organization is improved as workers are more optimistic than those under rational management.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics: Data Acquisition Report\n\nTable of Contents\n 1. Introduction\n 2. Computer forensics training\n 3. Return on investment (ROI)\n 4. Reference\n\nIntroduction\n\nData acquisition is a branch of computer forensics concerned with the retrieval of data originally located on a suspect medium such as a hard drive. These data could be images or files. The types of data acquisition are either live or static. This acquisition can fall in any one of the four methods highlighted below:\n\n  * The bitstream disk to image, which can be used to create copies, which are bit for bit replicas. This method can enable one to make more than one copy during the process.\n  * The bitstream disk to disk is used where bitstream disk to the image is not possible.\n  * While considering the files of interest only, the logical or sparse acquisition will suffice. However, this option remains suitable for large disks and can collect pieces of deleted data.\n\nFor the 2GB hard disk in question, a lossless compression may be suitable. However, for effectiveness, this will be combined with a digital signature verification process. For such a case, the contents will be copied as an accurate image to a file preferably to another disk using third-party tools such as ProDiscover or EnCase. However, this depends on other factors that would otherwise disqualify the bitstream disk to the image file method.\n\nAnother hard disk is used as the target medium onto which a copy of the suspect hard disk is made. EnCase and SnapCopy is examples of software that can be employed for this process. The acquisition precedes validation of the data. Windows has no inbuilt validation algorithms that are typically hashing algorithms. Third-party utilities are used for such validation processes. Alternatively, Linux validation can be used with the dcfldd with a verify file option to compare the image to the original.\n\nComputer forensics training\n\nCYber DEfense Trainer (CYDEST) describes a virtual environment addressing computer forensics as well as network defense. Considering that training on an actual platform may be costly and remote, CYDEST provides an opportunity for network administrators as well as digital forensics investigators to run real-life scenarios in a virtual environment. This setup achieves realism through \u201csupport for highly realistic \u2018invasive\u2019 training scenarios which include ongoing attacks and live forensics as well as an automated evaluation of student\u2019s performance\u201d (Brueckner et al., 2008, p.105). CYDEST relies on virtualization.\n\nVirtualization for this case is capable of achieving rich scenarios at random, which are suitable for training purposes. This virtualization is hosted with Xen, which is a hypervisor \u201crunning directly on the hardware as an operating system control program\u201d (Brueckner et al., 2008, p.105). Xen will support one or more machines as specially privileged guests. This platform also supports the Linux operating system as a privileged guest. In a CYDEST session, the hosts seen by the student are unprivileged. Xen can also allow other architectures to enable hosts on the network to run licensed software such as windows.\n\nCYDEST in its assessment employs passive and active observation. The former will cover reports involving a student\u2019s responses to direct queries. Active observation on the other hand is whereby a student\u2019s actions are monitored and both the direct and indirect results are analyzed. \u201cCYDEST is web-based and can be accessed over the internet or locally\u201d (Brueckner et al., 2008, p.106). Some shortcomings as far as CYDEST is concerned to relate to the complexity of some of its components. It is not uncommon that a virtual host may unexpectedly crash in which case the system reverts to a predefined baseline. It can be also noted that the student exercises are not repeatable.\n\nReturn on investment (ROI)\n\nDetermining the return on investment (ROI) is the single most important aspect of any investment today. This stands true for real estate, stocks, or new business ventures just to mention a few. Estimating a return on investment (ROI) helps the business planner to choose from among several investment options. During an IT project, training remains one of the relevant aspects for successful project delivery. This could be team training for those directly involved in the project or user training for the eventual beneficiaries of the project deliverable. User or staff training will result in improved performance and productivity.\n\nIt will be noted though that there are no cash flows representing training and therefore net present value as an ROI method is unsuitable to use here. While considering employees as organizational assets, their work can be determined by their remuneration package. Their salaries would translate to how much worth the employee is to the organization. However, this cannot be directly measured in terms of cash flows for the net present value method in determining the return on investment. A return based on their salaries would be a more appropriate method to determine the return on investment as the use of the net present value (NPV) method is unsuitable.\n\nAn alternative to NPV would be the use of the annual percentage yield (APY). For such a method, the cost associated with the investment will first be determined; the returns will then be calculated or estimated. The next undertaking would be to define a timeline for the returns and based on this calculation of the annualized return of investment or the annual percentage yield.\n\nReference\n\nBrueckner, S, Guaspari, D, Adelstein, F, & Weeks, J. (2008). Automated computer forensics training in a virtualized environment. Digital investigation , 5(1), 105- 111.\n",
        "label": "human"
    },
    {
        "input": "Computer Network Security Legal Framework Coursework\n\nIntroduction to Computer Network Legal Framework\n\nAs the current society advances through massive usage of computer networks, various legal issues and regulations have been developed to ensure sustained protection of sensitive information and intellectual property among organizations. According to Lloyd (2008: 118), cyberspace remains an eminent issue in the current society dominated by massive increase in IT, where statutory amendments and rational precedents need to be reconciled with the existing laws.\n\nParticularly, computer network security issues are becoming focal considerations with regard to the computer abuse in cyberspaces, where accurate legal and ethical formalities needs to be incorporated to protect the information and data relayed in networked environments. This paper will discuss the legal framework governing computer networks in UK, with regard to the legal obligations facing Hayes International operations.\n\nData protection refers to the measures and strategies incorporated in the information system to safeguard data within a networked environment. Data protection is a very complex strategy incorporating all processes and structures installed within networks to prevent any data damage or leakage within the network. With the introduction of cloud computing, the need of data protection has been rising significantly within computer networks to facilitate the protection of Intellectual Property among the users.\n\nBrief History of Computer Network Legal Framework\n\nConsidering the reportedly increasing cyber crimes and fraud, the need of information security has been necessitated. Security education has been one of the major areas of specialization among scholars in order to enhance development of adequate information systems security. In the year 1985, fraudulence over the computer through internet was detected. As a revealed by Barry (2009: 541), Computer Fraud and Abuse Act was developed ; in which individuals who were found conning others or intimidating other online were to be prosecuted. The government intervened and instructed on development of systems to investigate on the crimes committed over the internet through the use of computer forensics science.\n\nNotably, the law required the establishment of accurate evidence of any information which could be considered as involving fraudulence to be used in the court for prosecutions of the accused. As reported by Michele and Stokes (2010: 571), any individual found guilty of the offense was to be fined according to magnitude of the fraud case, or be imprisoned for two years. According to Data Protection Act of 1998, unauthorized access to private competitor information in a networked environment is illegal and the firm regulating the online network should be liable for the offense.\n\nOn this basis, Hayes International Company should reinforce its network security in order to facilitate privacy for the sensitive information relayed across its networks by its clients. This is an obligatory legal duty for Hayes Company to ensure safeguard its clients\u2019 data.\n\nThe Legal Framework of Computer Networks in UK with Regard to Hayes International Company\n\nAs revealed in the 2002 Social Networks Act, various security analyzers suggested the incorporation of extra elements in the information security systems including authenticity and possession. In this case, confidentiality has been upheld in the currently used security system since the encryption of personal credit cards and other confidential information have been limited to appear in very few places. This has greatly reduced fraud since various personal codes have not been publicly displayed. For instance, (2000: 22) reports on how online transactions using credit cards would only involve the displaying of the card data to limited number of places.\n\nThis would reduce the chances of the number being tracked reducing the chances of confidentiality breaching. In this regard therefore, confidentiality in information security systems has been uplifted which have increased privacy among individuals while making online transactions. On this basis, Hayes International Company should consider updating its network system, by having specific data for its clients be displayed at limited places with an aim of reducing the chances of any data leakages.\n\nSince Hayes International Company owns a number of leading online retailers from the global society, its concerns on the data security for its clients should be a priority. According to UK Copyright Law, comprehensive protection of Intellectual Property requires businesses to honor and respect patent and copyright policies provided by data producers or developers. Since it has become easy and faster to copy digital information, the law prohibits not only the interference of any information on a networked environment, but also the tendency of making it available to the public without the consent of the owner.\n\nOn this basis, the Hayes International Company should ensure that the information relayed across its networks for its clients is not interfered with or tampered at the slightest point. Further, the information control center of the company should ensure no leakages of the information relayed, as it may interfere with the performance of its clients. By so doing, Lloyd (2008: 123) reveals how coherence and consistency in the network system would be achieved, which will enhance the ultimate achievement of the goals and objectives set by all parties within the system.\n\nIn the year 2005, information security integrity was established. This meant that, any data encrypted online should not be modified or changed without being detected. With regard to Michele and Stokes (2009: 577), this strategy has been enhanced through the involvement of ACID classic models which provide unique codes to the original data provided in which its modification amounts to its rejection in the process of decrypting the data. One of the most important aspects of this strategy is that, organizations which have adopted the strategy would be having more security of their information since it would not be easy for interrupters to make any alterations and modifications.\n\nIn order to ensure guaranteed security for its clients\u2019 data, Hayes International Company ought to conform to high-tech information system capable of ensuring un-interruptible information system in cases of data leakages. As noted by Farr and Oakley (2010: 89), this is a legal obligation entrusted to all data companies, since they are eligibly liable for any inconveniences arising out of data leakages within their network system. As a central point of data processing for its clients, Hayes International Company should establish computer software capable of monitoring all the transactions carried out by its clients in order to produce evidence for its clients in cases of any suspected crime of fraud.\n\nAuthenticity is another element that has been of late introduced in the information security as a result of development of e-commerce and international transactions. In this case, the parties involved in any transaction ought to provide genuine information which must be validated after the parties involved come into consensus. As held by Barry (2009: 544), this particular reinforcement of the information security systems in the year 2005 was found to impact positively on the development of international business transactions since there were low chances of fraud and other internet evils.\n\nIn order to facilitate efficiency among retailers and their clients, Hayes International Company need to establish a system capable of validating any transaction, only when the parties involved come into consensus. This strategy conforms to the 1998 Cyber Crime Act in the sense that, all online transactions ought to be facilitated sequentially with an aim of reducing cases of fraud among online businesses. In order to facilitate the policies presented in this Act, Hayes International Company should establish validation strategy in its systems in order to reduce cyber crimes among international businesses.\n\nAccording to Brazell (2008: 56), the reinforcement of information security has been accompanied with \u2018non-repudiation\u2019, where each party involved in any transaction ought to fulfill the obligations of the contract. This acts as a guarantee for the clients to indulge into the transaction. This has enhanced lower chances of any act of fraud since each party can not breach the contact without a mutual consensus with the other.\n\nIn this respect therefore, information security has been reinforced to a great extent. Having full understanding of the virtual nature of many online businesses, Hayes International Company should ensure the establishment of non-repudiation strategy in order to curb any chances of fraud cases among the clients involved in the online transactions. Since the company is in control of the network system for all transactions, it would be quite reliable to have its businesses come into contract fulfillment before facilitating the transactions made between the parties.\n\nFurther, the introduction of cryptographic technology in information security has been one of the most reliable strategies. As noted by Lloyd (2008: 127), cryptographic strategies are meant to secure data and information displayed online by converting the data into unreadable form by other people apart from the user alone. Though various information infrastructures apart from the computer are being developed, the computer remains the main infrastructure among the others as it ultimately controls a number of processes.\n\nSince this technology has been proved to be quite reliable, Hayes International Company should ultimately consider it as an ultimate reinforcement for the security of its clients\u2019 data. Particularly, the law provides for network businesses to employ the best strategies possible to enhance the highest level of security among its online clients who might be relying on such networks.\n\nAccording to Computer Fraud and Abuse Act of 2001, it is illegal to access and retrieve information of another individual or organization without prior permission from the owner. Since Hayes International Company deals with facilitating communications between businesses and their clients, it is unlawful for its employees to access any information relayed within its systems belonging to such businesses. As reported by Macdonald and Rowland (2000: 17), in cases of abuse of the Act, the individual or organization alleged will be liable to pay $ 10,000 to the plaintiff. In this case, the Act acts as a safeguarding strategy to confidential information and data among organizations to enhance efficiency and coherence in the achievement of the organizations\u2019 goals and objectives.\n\nAs reflected in Cornell and O\u2019Connor (2006: 97), this Fraud Act of 2001 inhibits intentional alteration of computer programs within a networked environment which may cause damages resulting into threatening or loss of people\u2019s lives. More so, if the alteration of the programs results in losses of data stored within a networked environment, the individual or organization responsible is held guilty and liable for compensating the victim of the information loss amount not less that $ 5,000.\n\nIn the former case where the alteration of the programs involved results into loss of lives, the individual or organization responsible is held guilty of an offense, and liable to pay a fine not less than $ 9,000. On this basis, Hayes International should ideally control the programs facilitating the process of relaying information of its online clients in order to avoid any dangers resulting into either data of lives loss. By having the law restricts program alterations; it makes it possible to reduce the risks associated with information system\u2019s security within a networked environment.\n\nFurther, according to the Computer Fraud and Abuse Act the action of knowingly defrauding or trafficking of sensitive information like passwords in a networked environment without permission is considered as illegal. As held by Bainbridge (2007: 39), if an individual or organization gets involved in an act of trafficking encrypted data belonging to other individuals or organizations trough using software to transform the encrypted information into readable form without prior permission from the authorities is considered as guilt.\n\nIn this case, Hayes International Company should ultimately safeguard the information relayed within its networks without getting involved in act of trafficking any of such sensitive information. More specifically, the organization should have its staff be aware of the legal obligations associated with such actions and consider respecting and safeguarding intellectual property belonging to other organizations or individuals.\n\nIn response to the invading viruses, the software security law restricts the introduction of any malware to another computer in a networked environment intentionally. Since most of the known malware softwares like viruses have been known to interfere with the stored information in a computer, it is considered as an offense to infect other computers within the networked environment. In order to reduce the rates of viruses infection within the system, Linux and UNIX security wares have been necessitated since the invasion of viruses in the computer files goes to the UNIX and Linux systems and interfere with the entire computer system (Farr and Oakley 2010: 87).\n\nIn this respect therefore, the development of encrypted viruses by the Hayes International should be facilitated in order to enhance reliable security against malware infections within the networked environment.\n\nConclusion\n\nAs it has been revealed, the current advancement of information system requires a regulation and control measures in order to enhance efficiency and consistency through the use of computers. Particularly, the legal system has been found quite significant to reinforce the information systems regulation in order to protect intellectual property among organizations, which may be considered as the central nerve for their profitability or performance.\n\nOn this basis, Hayes international has an obligatory role of safeguarding its network system with an aim of upholding Intellectual Property for its clients. Quite importantly, its conformity to the legal requirements in computer networks forms a basis for its prosperity. As it has been revealed, the 1998 Cyber Crime Act and 2001 Computer Fraud and Abuse Act have been quite important in governing computer processes in networked systems in UK.\n\nSince the law stipulates on misuse of information systems to interfere with other people or organization\u2019s sensitive information, coherence and consistency in the information system has largely been achieved. More so, any computer program which may pose danger to other people\u2019s lives within a networked environment has largely been addressed in the Computer Law. As a result, the information system\u2019s law is not only concerned with data security, but also on human welfare.\n\nReference List\n\nBainbridge, D. (2007) Introduction to Information Technology Law, 6 th Edition. London: Longman Publishers, 24-51.\n\nBarry, S. (2009) Contemporary Legal Issues Facing Information Technology. Computer, Internet and Electronic Commerce Terms, (February Issue): 543-571.\n\nBrazell, L. (2008) Information Technology Law and Regulation. Oxford: Oxford University Press, 37-81.\n\nCornell, D. and O\u2019Connor, M. (2006) EU Communications Law. Manchester: Sweet & Maxwell Publisher, 95-132.\n\nFarr, S and Oakley, V. (2010) Internet Law and Regulation. London: McMillan Publishers, 57-93.\n\nLloyd, I. (2008) Information Technology Law. Oxford: Oxford University Press, 103- 141.\n\nMacdonald, E and Rowland, D. (2000) Information Technology Law, 3 rd Edition. England: Routledge Publishers, 5-38.\n\nMichele, R. and Stokes, S. (2009) Information Technology Law. Computer and Telecommunications Law Review, (Issue): 357 \u2013 404.\n",
        "label": "human"
    },
    {
        "input": "Computer Sciences Technology: E-Commerce Coursework\n\nIntroduction\n\nE-commerce is synonymous with e-business and entails the buying and selling of goods and services on the internet especially the World Wide Web. E-commerce constitutes several components necessary for it to run. E-commerce is based on several critical components. These components are in four broad categories that enable e-commerce to run on the web. These categories are the website, the merchant account, payment transaction software, a secure server connection, and a shopping cart.\n\nGlobalization\n\nGlobalization has ensured a borderless world. Globalization encompasses the greater movement of people, services, capital among others. This movement is not new but has been there for ages however, recent advancements in technology and the decline in trade barriers have led to the increased speed of exchange. Globalization has offered new markets and widened existing ones which translate into bigger profits and more wealth for individuals and countries. The benefits of globalization far much outweigh the problems it presents. It has led to faster growth of economies for those countries engaging in the international economy. Open economies have grown at a faster rate in comparison to closed economies.\n\nCheap imports make the variety of goods available to consumers and have helped in the betterment and improvement of locally produced goods as a result of the competition. Economic gains of globalization have led to the realization of better health and the provision of clean water increases the life expectancy of people in the process. This has seen global life expectancy double in less than a hundred years.\n\nInternational political and economic wrangles between countries have been easily abated due to the spirit of interdependence fuelled by globalization. This has in turn led to an increase in international trade as trading partners bolster peace amongst themselves. Perhaps the most important factor that has facilitated globalization and subsequent increase in international trade is the improved technology. Technology advancement has caused a reduction in costs and prices thus changing the world\u2019s mode of communication.\n\nASPs (Internet-based application service providers)\n\nThis term refers to firms that offer services online (or via the internet). It also refers to those companies that supply software applications and software-related services online. In addition, the ASP makes available this application to online customers via the internet and bills the users. ASPs have several advantages over the traditional approaches and these include:\n\n 1. ASPs use a pay-as-you-go model that is less expensive when compared to lump-sum payments that are available to large organizations with high-frequency requirements.\n 2. ASPs have less expensive start-up costs because an individual organization only pays for internet usage.\n 3. ASPs are easy to set up as all a company needs is a browser to start enjoying the services.\n 4. The need for additional IT personnel which tend to be very expensive is eliminated by ASPs.\n 5. ASP also does away with the need for focused (IT) applications. This is because ASP can support more than one application at the same time.\n\nASPs rise was motivated by the need to reduce the high costs of specialized software. Nowadays, the feature has helped lower this cost making it possible for small companies to acquire the software (Botha 2004 p.32).\n\nOnline banking\n\nOnline baking is a term coined to refer to the payment and performing of transactions over the internet via the bank and other secure websites. This has ensured that you only need an internet connection to access bank services from anywhere and at any time. Online buyers and suppliers can easily access their accounts around the clock from the confines of their homes. People no longer have to do banking on their own and can easily transfer funds from one account to another. Online banking only requires you to have a personal computer and a modem and you will be able to connect to your account. Several methods are available for you to bank online.these include:\n\nInternet-based\n\nThis method allows a person to access his or her account via home-based internet.\n\nBank software\n\nHere, your bank issues you with software that you install into your personal computer which allows you a secure connection to the bank\u2019s network and access your account.\n\nOnline banks offer a variety of services to their customers. These services are:\n\n 1. Checking account balances\n 2. Online account transfers\n 3. Loan status\n 4. Investments tracking\n 5. Online loan applications\n 6. Paying for bills\n 7. Account information downloads\n\nOnline banking offers several services to customers such as access from any location on the globe, you can work offline, better control of your money and 24 hours account access.\n\nE-cash\n\nElectronic cash is similar to physical cash in that it is portable, transferable, and is anonymous. In addition, electronic cash has the other property of being obtained fraudulently in a similar manner to physical cash (Varadharajan & Mu 1999 p.54). Just as physical cash operates in a system involving three entities \u2013merchant, customer, and bank, so does electronic cash. However, instead of coins and notes used in physical cash, electronic cash uses re-usable bit strings. E-cash has security properties that enable identity detection of double-spending, has the capability of conducting payments for transactions with minimal chances of tracing and is difficult to forge.\n\nElectronic cash circulates in three different transaction processes that occur during withdrawing, paying, and in deposits. A withdrawal transaction is carried out when a customer identifies himself or herself and authenticates an account with the bank and is then allowed to withdraw physical money from the bank. In carrying out a purchase, a customer with a given bank presents the electronic cash to a merchant while paying for items.\n\nThe merchant in turn receives physical money by depositing the electronic cash with the bank and getting hard cash in return. E-cash has the advantage of carrying out payments minus a third party i.e. without involving the bank (Varadharajan & Mu 1999 p.55). E-cash needs to satisfy several other properties such as being independent (Opplinger 2002 p. 255). Banks are given the mandate to mint digital coins. These digitally minted coins contain a digitally signed serial number and a private key making each coin unique.\n\nE-checks\n\nAccording to Bhasker (2009 p.279), an electronic check is all electronic equipment that can pay and collect deposits and functions from devices such as computers, phones, and ATMs. Electronic checks are providing a very rapid and safe mode of settling financial accounts among and between trading partners. Their processing does not require a pre-arrangement but works over both the public and individual networks through interconnection with the existing bank\u2019s system of clearing and settlement. Electronic checks help businesses reduce fraud, save time and money and at the same time improve customer service.\n\nElectronic checks have become a vital part of global payments have helped reduce the labor-intensive paper checks and are now an acceptable means of payment just like credit cards. Their other benefits include their ability to increase control with their flexible choice express solutions for customized guarantees and easily verifiable programs. With e-checks, it is possible to reduce losses due to their ability to search multiple databases across nations. They also can eliminate errors as they pose accurate information about customers.\n\nWith modern technology, electronic checks are made to resemble paper checks apart from the fact that they are electronically operated and use digital signatures during the signing and in the endorsement. Authentication is also carried out using digital certification for the payer, the payer\u2019s bank, and the payer\u2019s bank account. The delivery of electronic checks is through the mail system or is delivered directly to the bank.\n\nE-wallets\n\nE-wallets function similarly to physical wallets. Electronic wallets allow customers to keep electronic cash and other information in a single but safe way. E-cash is site-specific and can only work for a specific site. E-wallets store e-cash. There are several types of e-wallets: server-side e-wallets that are operational among the vendor side, client-side e-wallets operational within the customer\u2019s side and on his or her personal computer, and an e-wallet that is a hybrid between the first two (Zhou 2004 p.75).\n\nE-wallets enable customers to use cyber coins in which they take money from their online accounts and deposit it in the wallet, a process termed binding (Liflander 2000 p.110). The customer then can pay for the goods from the e-cash in the wallet to the merchant. It is important to note that no money moves online apart from information. Further, no amount of money leaves the banking system making it hard for fraudsters. Most e-wallets contain both software and information components. The software component provides the identification and security such as the actual transaction number. E-wallets support other different types of payment modes such as credit cards, debit cards, and electronic checks. It also contains shipping information.\n\nIntermediaries\n\nElectronic (intermediaries) are a distinctive form of e-commerce. It is an electronic bazaar that enables competent buyers to post their demands and requests to either purchase or sell a product online. This feature enables manufacturers to connect with their prospects and customers across the globe by simply using a computer. This feature has the advantage of reducing costs that would have otherwise been incurred during transactions.\n\nIntermediaries enable (small and medium) exporters lacking the ability to engage in (international) trade to get the market for their products. Intermediaries connect between exporters and customers in different geographical regions of the world. They enable effective and efficient trading between an exporter and different customers and suppliers. The intermediary analyses a customer\u2019s preferences for products and also suppliers and industries that distribute the particular goods. This feature of e-commerce helps exporters identify and in turn take advantage of any opportunities that may be prevailing. Because intermediaries have a global network and a lot of experience in conducting international trade transactions, they can access, collect and analyze information rapidly and efficiently (kosari, Banan, Tork and Broumandnia 2010 p.67).\n\nThe intermediaries further give updates on the course that businesses are taking, conditions in the market among others. They also offer legal advice to customers locally thus enabling exporters to take full advantage. This feature greatly reduces the hustle that online buyers and suppliers have to go through. It also becomes difficult for exporters who may want to monitor or even safeguard against some opportunistic behavior shown by foreign buyers (Kosari et al 2010 p. 68).\n\nPublic Key Encryption\n\nThe public domain field of cryptography was steered by Whitfield Diffie and Martin Hellman in the 1970s (Botha 2004 p.52). This was a work of research in which these two researchers tried to crack a way in which ciphers would be made asymmetrical rather than symmetrical. The two scientists wanted the two keys to be so different that it would be different for anyone to compute either. One of these keys is made available in the public domain and is available to users. It can also be published in telephone books and on business cards while at the same time maintaining the due security provided that the second key is kept a secret. This new method of protecting data received acceptance and became known as public-key cryptography.\n\nThe public key (encryption) employs a dissimilar key to code (and to decode) the message. The codes however fit together like pieces of a puzzle with the coding parts being made available to the public while the decoding part remains private.\n\nThis pair of the key is used in two different ways:\n\n 1. Authenticating the message through coding using the private key.\n 2. Message is coded using a (public) key and can only be decoded using a (private) key thus observing privacy.\n\nSecure shopping\n\nBotha (2004) sites several features of the internet that make it insecure for online shopping. The internet is an open system making communication paths inherently insecure. This is translated to mean that private information transmitted over the internet might be changed by cyber attackers. The lack of a governing body makes online shopping risky and when coupled with a lack of software standards, shopping becomes even riskier. Most online shopping sites are unsecured due to the high costs incurred making shoppers do so at their own risk (Botha 2004 p. 37). There is also the risk of hacking, errors, emotional injury, denial of service, and theft which have resulted in promoters of e-commerce establishing ways for secure shopping.\n\nWebsite security has been beefed up in a bid to protect organizations and customers from abuse by both the authorized and the unauthorized persons with access to corporate websites. It is however important to know that total internet security cannot be guaranteed although stringent measures continue to be undertaken. These include digital signatures, certificates and certificate authorities, secure socket layers (SSL), secure electronic transactions (SET), firewalls, and virtual private networks (Botha 2004 p.50).\n\nTo a larger extent, secure shopping is guaranteed through encryption that tries to keep documents and communications confidential. This step has been achieved by the use of digital certificates that are encrypted through public keys and that try to verify where a certain message has come from. Certificate authorities have been set up that is trusted by a large populace and that bind public key pair to a given identity. These measures have ensured secure shopping free of theft and hacking.\n\nOnline auction sites\n\nOnline auction sites operate the same way as traditional auction sites. However, the online auction is more effective in that they allow auctioneers do away with several costs such as those for hiring auction location, employees, and transportation of the objects from place to place. Online auctions are very efficient allowing sellers to reach millions of bidders while only paying a small commission. On the other hand, buyers can access whatever they want from wherever they are.\n\nThe auctioneer changes their role to market makers at the same time. This form of market is motivated by liquidations, online items, and the fact that it is possible to sell anything. There are diverse modes of auction sites including reverse auction also called a tendering system that operates in large organizations where many suppliers bid. Bidding is done by submitting proposals for tenders by the prospective suppliers. Business-to-business auction sites connect several large companies allowing them to buy inventory at competitive prices.\n\nOnline auction sites provide customers with a wide range of items. Customers can bid on new and used items with the stock cutting across different forms of merchandise. Among the largest online auction sites is eBay that was started way back in 1995 by Pierre Omidyar. Amazon, uBid, e Bid, and overstock are also large bargain online auction sites. In this auction site, the position of an auctioneer is taken by special software while server databases store the information about items traded on the site. Bids are accepted and winners are notified by server-side scripts (Parsons & Oja 2010 p.388).\n\nDebit card protection\n\nDebit cards also termed ATM cards have gained a lot of popularity in the recent past. Debit cards allow their holders to pay for goods and services efficiently and effectively. With the proliferation of cybercrime, users of debit cards are faced with security needs. This has resulted in cardholders preferring to use their cards only for standard ATM withdrawals. This is driven by the fear that online thieves might knock out bank balances. Banks have therefore resorted to initiating safety measures to compel their customer to use the cards (Gurusamy n. d. p.30).\n\nMajor companies in the USA like MasterCard international have declared that their debit cardholder shall not be held liable for unauthorized spending of up to $50. As a means of ensuring card security, debit card makers have initiated several measures such as the guard PIN which is a safe Personal identification number that is to be memorized and written down but to be kept a secret. Cardholders are advised to understand their cards and only use them when necessary as any spending is deducted from the main account.\n\nWhen using a debit card to pay for goods or services, a person is advised to keep safe the receipt as they contain the last four digits of the card. This is in case of thieves crack the code and empty your account (Gurusamy n. d. p.31). Loss of the card must be reported immediately to the bank for blocking and always check for the bank balance and report any unauthorized charges promptly.\n\nSummary\n\nA large percentage of e-commerce is carried out over the internet although there is the physical transportation of the goods. An intermediary acts as a connection between a customer and a supplier. Electronic payment methods exist and are used to boost e-commerce. E-cash is similar to hard cash although it is minted on bit string and stored in an online bank and e-wallets.\n\nReference List\n\nBhasker, 2009. Electronic commerce . New York, Tata McGraw-hill.\n\nBotha, J. 2004. E-commerce. South Africa, Juta and Company Ltd.\n\nGurusamy, n.d. Financial Services, 2E. New York, Tata McGraw-Hill.\n\nKosari et al, 2010. The Electronic Intermediary and Effects on This for Developing E-Commerce, International Journal of Computer and Network Security vol 2, no 8.\n\nLiflander, R., 2000. The Everything Online Business Book: Use The Internet To Build Your Business. London, Everything books.\n\nOpplinger, R, 2002. Security Technologies for the World Wide Web . London, Artech house.\n\nParsons, J, J, Oja, D., 2010. New Perspectives on Computer Concepts 2011 . London, Cengage learning.\n\nVaradharajan, V, & Mu, Y, 1999. Information and Communication Security: Second International Conference, ICICS\u201999, Sydney, Australia: Proceedings, Springer.\n\nZhou, X., 2004. Web Information Systems-WISE 2004: 5th International Conference On Web Information System Engineering . Brisbane Australia, Springer.\n",
        "label": "human"
    },
    {
        "input": "Computer Sciences Technology: Influencing Policy Letter Essay (Critical Writing)\n\nI would like to bring to your attention the fact that the Electronic Privacy Act of 1986 (ECPA) is outdated and needs to be revised. The Act contains good policies, but it has been twenty-four years since it was enacted, and since then, there have been major developments in communication technology. The ECPA does not protect all forms of communications and consumer records. Due to the provisions of this act, service providers are compelled to surrender consumer data from their servers to government agencies on demand. The government agency demanding the consumer data only requires certification in writing that the information is for an investigation of foreign counterintelligence. The certification does not need any judicial review to be effective, which exposes the provision to abuse by agencies.\n\nThe types of crime on which surveillance can be used are therefore increased. Although it may be argued that this provision is aimed at protecting the wider population, it leaves the privacy of the consumers in danger of being tampered with by anyone who can get a written statement certifying that the information is relevant. This provision can also be abused by employers. For example, once an employer has a suspicion that an employee is not acting in the company\u2019s interest, he/she can gain access to communications at the workplace, even though workplace communications are theoretically protected. Although it may be argued that the employer has a right to protect the interest of the organization, this leaves the employee\u2019s privacy unprotected, since it is easy for an employer to get permission to monitor communications within the company.\n\nThe Electronic Privacy Act also does not protect email in temporary storage. Virtually all electronic mail is stored temporarily in transit at least once. This means that there is no privacy enacted at all for electronic mail. Although online email services make communication with loved ones easy, emails disclose a lot of information about a person\u2019s interests, habits, beliefs, and concerns. Every citizen who uses email is therefore exposed and can have their privacy invaded. Under the current electronic privacy law, if anybody leaves a message in their webmail account for more than a hundred and eighty days, the government can demand access to such a message without a search warrant. The contents of such a message would have been accessed by third parties for whom the message was not meant, mainly the government officials charged with the duty of carrying out such access. It may be argued that any private email that is useful would not remain unopened for such a long time, but this does not preserve any kind of privacy for people.\n\nIt is clear that the Electronic Communication Act is outdated and should be revised to account for the recent developments in technology. Upon revision, the ECPA will keep up with the current developments, such as protecting electronic mail that is in temporary storage. The regulation should also ensure that employees\u2019 privacy is well guarded and employers should not infringe their privacy without the permission of the employee. The permission to conduct surveillance should only be given to specific individuals of legitimate authority, and not just to anyone who can get a letter stating that they have permission; this way, it will reduce the number of crimes committed by people who can access others\u2019 information with the pretext that they have permission. If all these precautions are enacted, there will be more consumer privacy and hence a heightened and healthier use of technology by American citizens.\n\nSincerely,\n\nIn Young, Song.\n\nConcerned citizen.\n",
        "label": "human"
    },
    {
        "input": "Computer Sciences Technology: Smart Clothes Research Paper\n\nTable of Contents\n 1. Abstract\n 2. Introduction\n 3. History of Smart Clothes\n 4. What is Smart Clothing?\n 5. Smart Clothes for Monitoring Health\n 6. Smart Clothes and Entertainment\n 7. Smart Clothes in the Future\n 8. Conclusion\n 9. Works Cited\n\nAbstract\n\nThis paper looks at what is meant by the term smart clothes. It gives the history of smart clothes by looking at the relationship between fashion and technology. In this paper we find that the smart clothes are dated back to the early 20th century and they can be attributed to the works of artists and scientists. Smart clothes differ from other fashions in that they act as clothing as well as protective devices. They are embodied with electronic sensors for carrying out different functions. This paper relates the functions of smart clothes in monitoring the health of patients and shows how they can be used for entertainment. It concludes by forecasting about the future of smart clothes which seems to be promising in the midst of internet technology.\n\nIntroduction\n\nClothes are worn for different purposes; the main purpose behind the introduction of cloths was to protect the body from harmful rays of the sun. They are also worn to prevent private parts from being exposed to everyone. There are many different types of clothing worn for different purposes. Some are worn occasional while others form part of the daily life. The type of clothes worn differ according to seasons of the year, for instance during winter, people wear heavy clothing to keep themselves warm whereas during summer people wear light cloths to help their bodies cool. We are used to three types of attire; Business casual, Professional Dress, and campus casual.\n\nBusiness Casual is not common in a professional business or an office setting but can be found in a semi-conservative workplace. Professional dress code is the most conservative type of business wear. It is common in finance office, accounting office or other conservative business environments. Sometimes employees are forced to wear professional because of the conservative employers who have hired them (Heathfield 1). Campus casual refer to the clothing that one puts on during his/her leisure time. It comprise of jeans, sneakers, tee shirts among others. Campus casual can be worn under certain circumstances in the work place for example in casual networking events.\n\nWith advancement in technology, a new fashion has emerged that is neither used because of the occasional nor can it be grouped into a specific type of dressings. This new fashion is referred to as \u201csmart clothes\u201d. They are cloths embodied with electronics and different sensors for protection purposes. This paper will look at the history of smart clothes, look at some of the functions of smart clothes with more emphasis on the medical field, and conclude by giving a thought on the future possibilities of this new fashion.\n\nHistory of Smart Clothes\n\nArt that is considered as something worn on the body has had an elusive reputation for a long period of time. Furthermore, works that are wearable contribute to an unknown history of projects that are not necessarily linked or thought to form part of a cohesive practice. There are many similarities between garments and art that are considered to be inclined to commercial infrastructure and cultural literature. With the introduction of wearable technology (smart clothes), many scientists working with wearable have begun to achieve critical mass attention and their activities have crystallized into a type of creative practice that merits analysis and an expanded discourse.\n\nFashions and garment are two terms that are used interchangeably by many people (Christiane 164). The notion of fashion is bound up with the advent of modernism, but modernism itself is a cultural enunciation brought about by technological advancements apparent in industrial production, urbanized society, and mass marketing. This makes fashion, technology, and modernisms terms that are inexorably bound up together (Sung 60).\n\nThe history behind smart clothes is dated back to the late 19th century and early 20th century when different authors and artists started writing books and magazines relating fashions with modernism. Throughout the 20th century, many debates were held by many artists discussing the relationship between garments and technology. Some of the clothes were designed during that time were treasured by descendants of the wearers. For instance, in 1930s, there was a development of elastic thread resulting from research which led to the rapid changes in women\u2019s undergarments and eventually in clothes themselves (Siegfried and Silvia 8).\n\nMore developments in garments became evident after the second world war when they runway to retail reflected ideas called from science fiction, and delineation of futurist lifestyles, as well as industrialization. Today, garments are design to reflect their technological culture, or their culture attraction to technology. Wearable computing or what is referred to as smart clothes is thought to have begun with Edward in 1961. He used pocket-sized analog computer Thorps to predict results in roulette games in Las Vegas. In fact, smart clothes erupted in the 1990s due to the influence of multiple forces such as technology, inspiration from literature, and mass media.\n\nSo it is not surprising that practices involving wearable technology have received little attention all over the world, or even the attention that exists has frequently been directed toward notions about skin and the naked body, and not what is conceived around it.\n\nWhat is Smart Clothing?\n\nSmart clothing is a form of clothing designed according to technology and it acts like an active device. These are clothes that are able to detect the wearers temperature and to regulate it. For instance, if they detect a hot temperature they release chilled water to keep the body cool. Smart clothes are embedded with electronics to help them function properly. Even though these clothes were introduced quite some time back, they have not been produced in mass quantities and are only available to specific groups of people, for example the military. Many researches have been carried out and scientists are still researching on how smart cloths can be developed to include the properties possessed by computers (Anissimov 1).\n\nFor instance, they are intended to have the ability of storing information the same way computers do and be able to manipulate this information as intended. They are also intended to display images such as videos and be used together with internet devices. Other functions of smart clothes include; detection of harmful chemicals that might be in the air, reaction to a bullet, generate power, record what the wearer says and store the information, among others.\n\nNormally, we are used to soft clothing which is flexible and light, however, the smart cloths are hardened by the electronics making them thick and rigid. One does not feel comfortable in this kind of clothes but has to wear them because of their functions.\n\nFor instance, the military soldiers do not have a choice when it comes to smart cloths because they have to wear them for protection purposes. The distinction between doctrine and percepts is matched by a distinction between choice and selection: virtue alone is good and choice worthy, but among indifferent matters some could be selected in preference to others. Smart clothes, for instance, are in themselves worthless; but there could be good in the selection of smart clothes. Critics say that a selection could be good only if what was selected was good (Lev 2).\n\nSmart Clothes for Monitoring Health\n\nSmart clothes are made with flexible sensors that are able to sense when one is feeling sick and relay the information to the appropriate people. There was a case cited of an old woman who just woke up in the morning and went about her daily chores. But while she was working, she felt an excruciating pain run through her chest. Not sure of what was happening to her; she tried to reach the telephone to ask for help. However, the pain was too sharp and she fell on the floor even before she could get hold of the telephone; she was suffering from heart attack (ICT Results, \u201cSmart Clothes: Textiles That Track Your Health\u201d 3). As she lay helplessly on the floor, an ambulance arrived and took her to the hospital and the doctors were able to revive her. Were it not for the smart clothes she was wearing, she could have died alone in her house.\n\nThis is just one illustration of the importance of smart clothes in the medical field. Smart clothes can be used to monitor patients suffering from heart diseases, and respiratory problems. They are embodied with sensors that are able to detect changes in the body temperature, perspiration, and blood conditions and the same information is transmitted to the hospitals. With smart clothes, a doctor can tell when a patient is in a critical condition and when she is all right even without face to face interaction with the patient (Curone 4). If well monitored, they can reduce mortality rate especially among people who live by themselves.\n\nHowever, this is an emerging technology and is yet to be exploited. In the countries where they are available, they are normally sold at very high prices making it difficult for the common man to acquire them. But with the government intervention and support, they are some hope and very soon we might own a pair of the smart clothes (Marco 2).\n\nSmart clothes that are designed for monitoring health conditions use different methods; they can use yarn dipped in and covered by nanotubes that are able to conduct electricity. This is a strong material and is comfortable on the body since it is flexible and has the ability to detect changes in body temperature. One of its main uses could be to sense changes in blood, and transmit a signal that the wearer is bleeding (University of Michigan 7).\n\nThere is another type of textile that was developed by BIOTEX that is woven into clothes to detect changes in health through body fluid analysis for PH or oxygen levels. These electronic textiles have sensors that receive the fluid, analyze it, and change color depending on the results of the analysis. The change in color is used to denote a health condition (ICT Results, \u201cSmart Clothes for Better Healthcare\u201d 4). There is another type of smart cloth that is almost similar to the one made by BIOTEX; these clothes were made by Health Ware for monitoring respiratory movement, pulse rate, skin temperature, oxygen saturation in the blood.\n\nThese clothes give information to the Portable Patient Unit (PPU), which compiles the information into the single unit and then sends it to a central server through a GPRS enabled mobile connection for medical staff to monitor. Another advantage of the smart clothes developed by Health ware is that they have sensors that are transparent, so that the people wearing them are not in a position to see all the wires covering their body (University of South Australia 8).\n\nThis creates more comfort, and the wearer\u2019s feel relaxed while wearing them, more so when they know that doctors are constantly monitoring their health. University of Bolton in England developed a special kind of smart clothes that uses an antennae system to pick up changes in heat. This type is used for detecting breast cancer by picking up abnormal changes in heat that are often associated with cancer. However, Busari (3) notes that, this system cannot be depended on fully for detecting cancer because there are other reasons that may cause abnormal heat changes and not necessarily related to cancer.\n\nSmart Clothes and Entertainment\n\nThere is a certain type of smart cloth system that has the ability to download data from the clothes and transmit it to a computer located in a special wardrobe. This system was developed by researchers from the University of South Australia. By using this system, user may wear the clothes that have the capacity to record various data, and then hang the clothes for the data to be downloaded to the computer in the wardrobe.\n\nWhile this is going on, the clothes are recharged for next wearing. Even when washing these clothes, the electronics embedded in them do not get damaged. A wearer can even change into another set of clothing and continue the process of downloading data. They have sensors that can light up to indicate rain or have a built in MP3 player. They can also record speeches made by the wearer which is then downloaded and can be used in the computer for future reference.\n\nThese clothes can be worn to certain occasions where the user wants to record all the proceedings of that occasion. They might soon be used in classroom for taking notes. Instead of writing notes from the teachers\u2019 dictation, these clothes have the capacity to record such data which can later be downloaded and stored in the computer.\n\nSmart Clothes in the Future\n\nThere are numerous possibilities of smart clothes in the future. Many researches are directed to the medical application although other possibilities are arising. With the ability of smart clothes to light up, sense changes in body temperature, play music, download and store data, and perform other similar functions, the new fashion will change many people\u2019s social life. The military may use smart clothes to monitor the conditions of soldiers in combat as well as for protecting themselves against their enemies. These clothes have the ability to detect bullets and prevent solders from being harmed by them.\n\nBy using smart clothes, mothers will be in a position to find their lost children by tracking the location of their clothes. However, the potential applications of these clothes are only limited by imagination and it is up to scientist to prove their actual application (Edward 20).\n\nWhen they will finally be accepted as a new fashion, smart clothes will act as an excellent concept. However, some people are frightened by the idea that their clothes could soon be able to monitor everything they do or what affect them. While this remains to be true, there need to be some limitations on exactly how much the wearer is being monitored, although the helpfulness of these clothes still takes priority. There is another group of people who are concerned that their bodies will be covered with electronics devices and are afraid of the injury that may result from electrocution.\n\nThis implies that, a lot of public awareness has to be done if people are going to accept these clothes (Lou 1). They need to be educated on how the smart clothes work and the benefits they stand to gain while wearing them. Currently, the electronics can be washed while still in the clothes without being damaged. This means that, there is no cause of worry as to whether one can be harmed by these electronics. Smart clothes are receiving much attention currently and a lot of scientists are devoting their time to study more about them and develop this new venture further.\n\nConclusion\n\nWith the advancement in technology, everything is becoming computerized. Internet technology has swept many traditional practices and almost everyone is relying on it. Industrialization has also played a very big part in the current developments and we are yet to see and witnesses more changes. One of the areas that are receiving international recognition is changes in fashions. Things are changing with time attracting the attention of both artists and scientist. The most recent development in fashion is the so called \u201csmart clothes\u201d. The development towards these clothes started in early 20th century when artist started relating fashion with technology.\n\nWith this, scientist have taken front sit in advancing this new concept and with it emerged the smart clothes. These are clothes embodied with different sensors for detecting changes in the body and from the surroundings. They can detect changes in the body temperature, respiration, blood circulation, among other things. They have been proved to assist doctors in monitoring their patient and with them many lives have been saved. The future of this new development is promising and more people are getting enlightened on the importance of wearing smart clothes.\n\nWorks Cited\n\nAnissimov, Michael. What is smart clothing? 2010. Web.\n\nBusari, Samuel. Futuristic fashions will fight our health scares , 2008. Web.\n\nChristiane, Paul. Digital Art Thames & Hudson . London and New York, 2003, p. 164.\n\nCurone, David. Heart Rate and Accelerometer Data Fusion for Activity Assessment of Rescuers during Emergency Interventions. IEEE transactions on information technology in biomedicine, vol. 14, no. 3.\n\nEdward A. Shanken, \u201cThe House That Jack Built: Jack Burnham\u2019s Concept of \u2018Software\u2019 as a Metaphor for Art,\u201d L.E.A. Archives, Vol. 6, No. 10 (1998).\n\nHeathfield, Susan M. Dress for Work Success: A Business Casual Dress Code , 2010. Web.\n\nICT Results. \u201cSmart Clothes: Textiles That Track Your Health.\u201d ScienceDaily , 2008. Web.\n\nICT Results. \u201cSmart Clothes for Better Healthcare.\u201d Science Daily , 2009. Web.\n\nLev, Manovich. Fashion Sites , 2001. Web.\n\nLou, E. Smart Garment to Help Children Improve Posture. Proceedings of the 28th IEEE EMBS Annual International Conference . New York City, USA, 2006.\n\nMarco Di Rienzo. Textile Technology for the Vital Signs Monitoring in Telemedicine and Extreme Environment. IEEE transactions on information technology in biomedicine, vol. 14, no. 3 , 2010.\n\nSung Bok Kim, \u201cIs Fashion Art?\u201d Fashion Theory Vol. 2, No. 1 (1998), pp. 51-72.\n\nSiegfried Zielinski and Silvia M. Wagnermaier, \u201cDepth of Subject and Diversity of Method: An Introduction to Variantology,\u201d in Variantology 1: On Deep Time Relations of Arts, Sciences, and Technologies, Siegfried Zielinski and Silvia M. Wagnermaier, eds., Kunstwissenschaftliche Bibliothek Vol. 31 (Walther K\u00f6nig: K\u00f6ln, 2005), pp. 7-8.\n\nUniversity of Michigan. Clothing With A Brain: \u2018Smart Fabrics\u2019 That Monitor Health. ScienceDaily . 2008. Web.\n\nUniversity of South Australia. \u201cSmart Suit Doesn\u2019t Miss A Beat.\u201d ScienceDaily , 2007. Web.\n",
        "label": "human"
    },
    {
        "input": "Dependability of Computer Systems Essay (Critical Writing)\n\nIntroduction\n\nThe advancement in technology has heralded the onset of a whole new era in global networking. So much so that the safe keeping and continued existence of the discrete data has become an issue of global interest. Business institutions and well off individuals alike, are investing heavily in the security of their systems. They are buying sophisticated systems whose services may come in handy when there is need to operate their systems; be it in aviation, nuclear operations, and radiotherapy services or even in securing delicate information in sensitive databases. This creates a need for the service to be delivered. When the system can justifiably be trusted, it creates dependability.\n\nDependability entails the ability of computer systems to incorporate features that comprise its consistency, accessibility, data protection, its life and the cost of running and maintaining it. Failure of these systems leads to loss of delicate data and may expose the data to unwarranted parties. The main aim in this project is to highlight and expound on the primary ideas behind its operation.\n\nAfter going through observations made over time in regard to their dependability on computer systems, people have tried to define dependability. From this in depth look at dependability then commences. It is rated according to the threats it faces, its attributes and how it ensures users of its dependability. Means of dependability are also looked into, by terms of how fast the system responds to queries, and the probability of generating results. Dependability merges all this tasks within a single frame work. Therefore the main purpose of this study is to present an on-point and brief outline of the model, the methodology and equipment that have evolved over time in the field of dependable computing.\n\nThe ideas behind dependability are founded on three basic parts that involve the threat to the system, the attributes of the system and the technique in play through which the systems achieves dependability.\n\nReliability\n\nReliability can be described in a number of ways:\n\nIt can be defined as, the idea that something is fit for a purpose with respect to time; the capacity of a device or system to perform as designed; the resistance to failure of a device or system; the ability of a device or system to perform a required function under stated conditions for a specified period of time; the probability that a functional unit will perform its required function for a specified interval under stated conditions or the ability of something to fair well (fail without catastrophic consequences) (Clyde & Moss, 2008).\n\nIn the dependability on computer systems, reliability architects rely a great deal on statistics, probability and the theory of reliability. A lot of business computer systems are employed in this type of engineering. They encompass \u201creliability forecast, Weibull analysis, thermal management, reliability testing and accelerated life testing\u201d (Clyde & Moss, 2008). Since there is a big amount of reliability methods, their cost, and the changeable degrees required for diverse situations, a majority of projects build up a reliability-program-plan to identify the duties that will be carried out for that particular system.\n\nThe purpose of reliability in computer dependability is to come up with a reliability requirement for an item for consumption; establishment of a satisfactory reliability system, and performing of suitable analysis and tasks to make certain that end results meet necessary requirements. These particular tasks are controlled by a reliability manager, who is supposed to be a holder of an accredited reliability engineering degree. Additionally the manager has to have added reliability-specific edification and training. This kind of engineering is intimately linked with maintainability and logistics. Loads of problems emanating from other fields for example security can also be handled using this kind of engineering methods.\n\nAvailability\n\nIn dependability of computers, availability can be described as follows: the extent at which a system is, in a specific operable and committed state when starting a mission, \u201c(often described as a mission capable rate). Mathematically, this is expressed as 1-minus unavailability; the ratio of (a) the total time a functional unit is capable of being used during a given interval, to (b) the length of the interval\u201d (Blanchard, 2000).\n\nAn example for this is when \u201ca component capable of being used 100 hours per week (168 hours) would have an availability of 100/168\u201d (Blanchard, 2000).Though, distinctive availability values are shown in decimals i.e. 0.9998. In elevated availability functions, a metric, equivalent to the numeral of 9 that follows decimal points, is used. \u201cIn this kind of structure, 5 nines are equal to (0.99999) availability\u201d (Blanchard, 2000).\n\nAvailability is well recognized in the writings of stochastic modeling and the finest maintenance. Barlow and Proschan (2001) for example describe availability of a fixable system as \u201cthe probability that the system is operating at a specified time\u201d. On the other hand Blanchard (2000) defines it as \u201ca measure of the degree of a system which is in the operable and committable state at the start of a mission when the mission is called for at an unknown random point in time.\u201d\n\nAvailability actions are categorized by the time gap of interest or the devices for the systems downtime. \u201cWhen the time gap of interest becomes the main concern, \u201cwe consider instantaneous, limiting, average, and limiting average availability. The second primary classification for availability is contingent on the various mechanisms for downtime such as the inherent availability, achieved availability, and operational availability\u201d. (Blanchard, 2000)\n\nSecurity\n\nComputer dependability has a division known as computer security. This is a division of computer system technology dealing in information securities that are related to computers and networking. The purpose of computer-security includes the safeguarding of information from \u201ctheft, corruption, or natural disaster, while allowing the information and property to remain accessible and productive to its intended users\u201d (Michael, 2005).\n\nComputer-system security is the combined procedures and mechanisms by which responsive and important information and services are safeguarded from publication, interference or disintegration by illegal activities or unreliable individuals and unintended events correspondingly. The tactics and methodologies employed security-wise, often vary depending on the computer technology. This is because of its rather indefinable purpose of preventing unnecessary computer behavior, instead of allowing required computer behavior\n\nExpertise on computer dependability security is founded upon logic. Because security is not essentially the most important goal of the majority of computer applications, planning a system with the issue of security in mind, frequently inflict restrictions on that program\u2019s performance.\n\nThe following are four moves towards security in computers, sometimes a mixture of approaches come in handy:\n\nTrust all the software to abide by a security policy but the software is not trustworthy (this is computer insecurity). Trust all the software to abide by a security policy and the software is validated as trustworthy (by tedious branch and path analysis for example). Trust no software but enforce a security policy with mechanisms that are not trustworthy (again this is computer insecurity). Trust no software but enforce a security policy with trustworthy hardware mechanisms (Michael, 2005).\n\nComputers are made up of software performing on top of hardware. A computer system is a merge of these two components providing precise functionality, to comprise either a clearly stated or completely accepted along security policies.\n\nUndoubtedly, quoting the Department of Defense Trusted Computer System Evaluation Criteria (TCSEC) archaic though that may be \u2014the inclusion of specially designed hardware features, to include such approaches as tagged architectures and (particularly addresses stack smashing attacks of recent notoriety), constraint of carrying out text to the precise memory sections and/or register groups in computers and computer systems (Michael, 2005).\n\nSafety\n\nSafety is the condition of being safeguarded against \u201cphysical, social, spiritual, financial, political, emotional, occupational, psychological, educational or other types or consequences of failure, damage, error, accidents, harm or any other event which could be considered non-desirable\u201d (Wilsons, 2000). This can also be described as the control of identified risks to attain a satisfactory level of risk. It can possibly take the structure of being guarded from the occurrence or from disclosure to something that brings about health or cost-effective losses. This can comprise the safeguarding of people or property.\n\nComputer system safety or reliability is an engineering regulation. Continuous modification in computer knowledge, environmental regulation and community safety concerns make the scrutiny of compound safety critical structures even more demanding.\n\nA common myth, for example in the midst of computer system engineers concerning structure control systems, is that, the subject of safety can be enthusiastically deduced. As a matter of fact, safety concerns have been exposed one after another, by many practitioners who cannot be assumed by one person over a small period of time. Information on literature, the standards and routine in any given field is an important part of the safety engineering in computer systems.\n\nA mixture of theory and track-record of performance is involved and track-record points out a number of theory areas that are pertinent. In the US for example, \u201cpersons with a state license in professional systems engineering are expected to be competent in this regard, the foregoing notwithstanding, but most engineers have no need of the license for their work\u201d (Bowen, 2003).\n\nSafety is frequently viewed as a small part in a collection of associated disciplines these are: \u201cquality, reliability, availability, maintainability and safety. (Availability is sometimes not mentioned; on the principle that it is a simple function of reliability and maintainability)\u201d (Wilsons, 2000). This subject tends to establish the worth of any work, and insufficiency in some of these areas. \u201cThis is considered to bring about a cost, beyond the cost of addressing the area in the first place\u201d (Wilsons, 200o); high-quality managing is then anticipated to reduce total costs.\n\nPer-formability\n\nPer-formability, at an initial impression, comes out like a simple gauge of performance. One may define it simply as an ability to execute a task or perform. In real sense, performance only constitutes about half any per-formability evaluation. This is actually a compound measure of a system\u2019s dependability on overall performance. The measure is one of the fundamental assessment methods for degradable systems (extremely dependable structure s that can go through a refined degradation in performance when malfunctions are detected and still allows continuous usual operation). A good example of such a system that can be degradable is a spaceships control system having 3 central processing units (CPUs).\n\nA malfunction in the system could be disastrous, perhaps even causing the loss of life. \u201cThus, the system is designed to degrade upon failure of CPU \u201c1\u201d, i.e. CPUs \u201c2\u201d and \u201c3\u201d will drop their lower priority work in order to complete high priority work that the failed CPU would have done\u201d (Carter, 2009). The majority of functions in per-formability currently deal with degradable computer systems like the one mentioned. On the other hand, the idea is applicable to degradable structures in a diverse area starting from economics to biology.\n\nPerformance can be considered as the QOS (quality of service) as long as the structure/system is exact. Performance modeling on the other hand entails representations of the probabilistic nature of consumer demands and predicts the system\u2019s performance ability. This is done under the supposition that the system-structure remains steady. Dependability is an \u201call-encompassing definition for reliability, availability, safety and security.\n\nThus it is what allows reliance to be justifiably placed on the service the system delivers. Dependability modeling deals with the representation of changes in the structure of the system being modeled\u201d. (Ireson, 2006) These are usually due to errors, and how these changes influence the ease of use of the system. Per-formability modeling, then, considers the result of system changes and the effect on the general performance of the whole structure\n\nAt one time, the majority of modeling work separated performance and dependability. At first, dependability of structures might have been satisfactory, and then the execution was optimized. This brought about systems with good performance especially when they were fully functional, but a severe decline in executing tasks when, unavoidably, malfunction occurred. Fundamentally, \u201cthe system was either \u2018on\u2019 and running perfectly or \u2018off\u2019 when it crashed. Improvements on this, lead to the design of degradable systems\u201d. (Lardner, 2002) Since degradable structures are designed to carry on with operations even after some component have failed (albeit at decreased levels performance), their ability to execute tasks cannot be precisely assessed without considering the impact of structural/system changes, breakdown & repair.\n\nInitially scrutiny of the structures from a pure performance point of view was most likely optimistic because it disregarded the failure-repair performance of the systems. On the other hand, dependability scrutiny tended to be conservative; this is because performance considerations were not well thought-out. Therefore, it was fundamental that processes for the joint assessment of performance and dependability be increased.\n\nMaintainability\n\n\u201cThis is the measure of an equipment or systems ease and rapidness to restore its initial operational status as a result of a failure\u201d (Randell, 2001). As shown by Randell again this \u201ccharacterizes an equipment design to installation, availability of skilled personnel, and environment under which physical maintenance should be performed and how adequate the procedure of maintenance performs\u201d. Maintainability is articulated as the likelihood that an item is retained in or otherwise reinstated to that definite condition over a said instance, when maintainability is being carried out accordingly to the set method and resources.\n\nMaintainability merit is normally the MTTR (Mean Time to Repair) and a perimeter for the maximum repair time. Often it is expressed as; M(t) = 1-exp(-t/MTTR) = 1 \u2013 exp(-mt). M = constant maintenance rate, MTTR \u2013 Mean time To Repair (Laprie, 2005).\n\nMTTR is a mathematical average implying a system repair speed and visualization is easier than the probable value. Maintainability concern is to accomplish shorter repair times for maintaining high availability so as to minimize cost control of downtime productive equipment at the time availability is critical. A 90% possibility that in less than or up to 8 hours, maintenance repair will have been completed with 24 hours as the maximum repair time could be a good maintainability goal.\n\nEvaluation of dependability\n\nComputing systems operate on five basic properties. These are: Functionality, usability, performance, their cost (both purchasing cost and operational cost) and their dependability. The ability of a computer system to deliver the appropriate service and gain the trust of an end-user is referred to as dependability. Service delivery refers to the efficiency of the system as per the view of the user. The function of the system refers to its intended use, as is prescribed by a particular system\u2019s specification. When the system carries out the intended service it was meant for without a hitch, and delivering on time, it\u2019s referred to as correct service.\n\nWhen the system fails to deliver a service it was made for, or shows some deviation from rendering the correct service it\u2019s referred to as system failure. This is also called an outage. When this happens and the system is restored to performing its original service, it is called service restoration. Reflecting on the understanding of the definition of system failure, dependability can be defined as \u201cthe ability of a system to avoid failures that are more frequent or more severe, and outage durations that are longer, than is acceptable to the user(s)\u201d(Jones, 2000).\n\nSystem failure may be associated with its non-complicity to a given specification or if a program\u2019s function is not fully described. This part of a system that may in future cause system breakdown is called an Error. The assumed cause of an error is referred to as a Fault. When a fault in a system produces leads to an error, then the fault is active, if it is in an inactive state then it is dormant. There are various ways through which a system can fail. The failure modes can be ranked according to how severe their exposure to the system can be. The positions, with which they are categorized, are classified into three categories. These are; \u201cthe failure domain, the perception of a failure by the end-user and the consequences of these failures to the environment\u201d (Laprie, 2005).\n\nAn interactive set of components forms a system set while component states encompass the system state. When there is an error it causes a malfunction in the state of the components but it will not lead to system failure unless it encroaches up to the service interface of the system. One of the ways used to classify errors is categorizing them according to the component failure they cause.\n\nA system comprises of a set of interrelating components; consequently a system state is a set of its constituent\u2019s states. A fault initially causes inaccuracy inside the state of a component, but the whole structures malfunction will not come about as long as the fault does not get to the service interface of the whole structure. A suitable categorization of errors is described in terms of component malfunction that they bring about.\n\nUsing the expressions in the figure above, \u201cvalue vs. timing errors; consistent vs. inconsistent (\u2018Byzantine\u2019) errors when the output goes to two or more components; errors of different severities: minor vs. ordinary vs. catastrophic errors\u201d (Schneider, 2000). A mistake is noticed if its existence in the particular system is specified by an error note or signal. Errors/faults that are in the system but not identified are dormant errors. Faults and their causes are very varied. Their categorization as per the 6 chief criteria is shown in Figure 3 below.\n\nIt could be argued that introducing phenomenological causes in the classification criteria of faults may lead recursively to questions such as \u2018why do programmers make mistakes?\u2019, \u2018why do integrated circuits fail?\u2019 Fault is a concept that serves to stop recursion. Hence the definition given: adjudged or hypothesized cause of an error. This cause may vary depending upon the viewpoint that is chosen: fault tolerance mechanisms, maintenance engineer, repair shop, developer, semiconductor physicist, etc (Schneider, 2000).\n\nElementary fault classes\n\nConclusion\n\nThe main concept of dependability is its nature to incorporate which in turn \u201cpermits the putting on board classical notions of \u201creliability, availability, safety, security, maintainability, that are then seen as attributes of dependability.\n\nThe fault-error-failure model is central to the understanding and mastering of the various threats that may affect a system\u201d (Newman, 2003). This facilitates unified presentations of the threats, at the same time preserves their specificities through a range of fault classes which can be definite. Another important aspect is the employment of a fully generalized idea of failure as is contrary to one which is limited in some way to scrupulous types, reasons or outcomes of failure. The model presented as a way for achieving dependability is exceptionally valuable. It is because the means conforms to the attributes of dependability. This is again with regards to the design of whichever system has to carry out trade-offs owing to the fact that the attributes are inclined to clash with each other.\n\nComputers and digital devices are increasingly being used in significant applications where failure can bring about huge economic impacts. There has been a lot of research and presentations describing techniques of attaining required high dependability. To achieve the desired dependability, computer systems are required to be safe, secure, readily available, well maintained, and reliable, have enhanced performance and robust against a variety of faults that includes malicious mistakes from information attacks. \u201cIn the current culture of high tech tightly-coupled systems which encompass much of the nationalized critical communications, computer malfunction, can be catastrophic to a nation\u2019s, organization\u2019s or individual\u2019s economic and safety interests\u201d (Mellon, 2005).\n\n\u201cUnderstanding how and why computers fail, as well as what can be done to prevent or tolerate failure, is the main thrust to achieving required dependability. These failures includes malfunction due to human operation, as well as design errors\u201d (Mellon, 2005). The aftermath is usually massive, as it embodies record loss, lack of reliability, privacy, \u201clife loss, or even income loss exceeding thousands of dollars in a minute\u201d (Mellon, 2005). By system structures that are more dependable, such effects can be avoided.\n\nReferences\n\nAlgirdas, A. (2000). Fundamental concepts of computer systems dependability . Los Angeles: Magnus university press.\n\nBarlow, A and Proschan, N. (2001). Availability of fixable systems. Computer systems , 43, (7): 172-184.\n\nBlanchard, D. (2000). Availability of computer systems . New York, NY: McGraw-hill.\n\nBowen, S. (2003). Computer safety. IBM Systems journal , 36 (2): 284-300.\n\nCarter, W. (2009). Dependability on computer systems. Computer systems , 12 (1): 76-85.\n\nClyde, F., & Moss, Y. (2008) Reliability Engineering and Management. London: Mc-Graw-hill.\n\nIreson, W. (2006). Reliability engineering and management . London: McGraw-hill.\n\nJones, A. (2000).The challenge of building survivable information-intensive systems. IEEE Computer , 33 (8):39-43\n\nLaprie, J. (2005). System Failures. International journal of computer technology , 92 (3): 343-346.\n\nLardner, D. (2002). Degradable computer systems . P. Morrison and sons Ltd.\n\nMellon, C. (2005). Dependable systems . London: McGraw Hill\n\nMichael, R. (2005). Computer software and hardware security. New York, NY: CRC Press.\n\nNewman, B. (2003). Fault error failure model. Sweden: Chalmers university press.\n\nRandell, B. (2001). Computing science. UK: Newcastle University press.\n\nSchneider, F. (2000). Faults in computer systems . National academy press.\n\nWilsons, T. (2000). Safety. London: Collins & associates.\n",
        "label": "human"
    },
    {
        "input": "Human-Computer Interaction and Communication Essay\n\nTable of Contents\n 1. Abstract\n 2. Problem background\n 3. HCI vs. iphone4\n 4. Principals of good HCI design left out\n 5. Conclusion\n 6. References\n\nAbstract\n\nHuman-computer interaction is about the connection between systems and the people that use them concerning their place of use. This paper seeks to look into a system that has been introduced into society but failed due to a bad implementation of the human-computer interaction process. As a case for this paper, it will dwell much on iPhone 4, its introduction, and the matters that have led to the much criticism it has fetched about the human-computer interaction systems.\n\nProblem background\n\nAny system is always led by huge expectations before its introduction into the market. This is an issue that results from various advertisements and other marketing strategies (Card, 1983). The iphone4 was long anticipated to be a hit in the market though it came up with much negative feedback from its users who categorized it as a flop. The main reason for this negative outlook was the design factors that made the phone users encounter some problems in usage and handling, calling call dropping, branded features among other factors as they will be discussed in the paper.\n\nHCI vs. iphone4\n\nHuman-computer interaction falls into various categories which can be classified as technological and cognitive features, in the technology part, the design of the software and the hardware are some of the crucial factors to be considered. At the same time, it has the cognitive function which is more on how knowledge is passed by the machine to the user. In this field, the focus is on how people and machines interact and in the end, the person gets what he required from the machine. As the person is interacting with the gadgets or the computers, he should be favored by the design in that the system should be in such a way that it supports human usability.\n\nIt should also be clear that the system should be in such a way that the person using it should have easy access to information. This means that the computer system or the gadget under discussion should be designed in such a way that it should aid the person in the access of information (Moggridge, 2007). This can be through normal information retrieval procedures like browsing the internet.\n\nIphone4, upon its release, got a lot of criticism in various fields. In such a short while after acquiring the phone, many people started complaining of the poor reception that the phone had and that it dropped calls. In many common phone designs today, the reception of the network depends upon the placement of the antenna. In this particular phone, the antenna had been dedicated to one part thus was always blocked once a person tried to make or receive a call which resulted in interrupted reception. This has also been seen to have the extra risk of increasing the closeness of radiation to the body as the phone tries to communicate with the network masts.\n\nThis is a very serious flaw in the design of the phone which does not take care of the security of the user. The user context is one of the core HCI components and thus the design failure was a poor consideration of the HCI design.\n\nThe name of the phone, the iphone4, implies that it is a 4th generation phone while in an actual sense, it is a 3rd generation gadget (3G). This means that it cannot use wireless standards which use 4G. In the HCI components core structure, the particular computer system comes in. while buying this phone, people would have been duped into believing that it was a 4th generation gadget that they were buying whilst they were buying a gadget that would only operate on 3rd generation networks. This is a serious design and software flaw that again goes against the HCI considerations.\n\nThe phone was meant to be designed in such a way that the stainless steel band around it would improve wifi. This proved not to be the case since during the testing of the product; the wifi connection was not working very well. For a person to get a good connection, it is almost a requirement that there is a wireless router nearby otherwise the connection will be bad. Looking again at the core components of good HCI design is a consideration of the environment in which the computer system is to be used. The environment has to be favorable to the user particularly for the intentions that the system was made for (Saffer, 2006).\n\nA mobile phone has features that enable any particular user to use the gadget irrespective of his or her location including while in transit. The iPhone was integrated with the wifi feature as part of the whole system but not on conditions that some other devices were to be nearby. It hence follows that the understated performance of the wifi is a gross inconsistency with the considerations of a good HCI system. The main deterrent of a proper HCI conducive environment was the questionable environment question (Dawes, 2007).\n\nThe design of the phone was aimed at making it a slim product that was easy to handle. The thin nature of the phone has a shape that makes it so easy to slip out a user\u2019s hand. Looking at the essence of slimness, there is a stylish behavior that comes with it. The nature of the phone to easily slip out of the hands and fall and the other factor in design where the phone is made of glass makes people worry much.\n\nThe glass cover on the phone was designed to be three times stronger than plastic but on testing, it cracked almost entirely on being dropped four times. This then makes the phone users buy an extra protective case to make sure that it gets protection upon slipping. Looking at both cases, the design of the phone beats the slimness logic as the users cannot use it in that condition comfortably. The HCI component of the computer system at hand is then surpassed and also the component of usability.\n\nLastly on the design issues, the phone designers did not consider the information age of the users. Most of the users of the phone buy it considering the internet connectivity and searching of the required material over the net. Considering this matter, the phone was integrated with the Bing search engine whilst most of the users are conversant with Google. This was an outright in consideration of the users of the phone beating the components of good HCI design\n\nPrincipals of good HCI design left out\n\nIn Human-Computer Interaction, there is a basic goal of improving the interactions between human beings and computer systems. In the long run, the field aims at reducing the barriers that do exist between human beings and machines. Professionals in this field have had a gross consideration of the effects in both the user interfaces in graphics and also the web. When a system has been designed such that this is not put into consideration or even after being put into consideration is somehow not fulfilled, then this system will have failed in the implementation of the HCI techniques (Jones & Marsden, 2006).\n\nThe analysis of Apple\u2019s iphone4 has shown that most of the key components of HCI were closely related to the huge public outcry of the phone buyers in a negative way. This is clear evidence that the failure of the gadget to hit the market as was calculated was due to the failure in the implementation of the HCI design components.\n\nConclusion\n\nThe case of the launch of the iphone4 into the market is a good indicator of a system that has failed to a bad design of the human-computer interaction. The four core functionalities which were the major causes are the user, the task, context, and the particular system (Myers, 1998). The user was the iPhone buyer who did not get the product that he was eyeing. The task is using the phone through calling, web browsing, and other tasks which had failures in most cases. The environmental context was affected through the probability of breakage through falling and environmental cases. The particular system was the phone itself which has been seen to have quite a lot of problems.\n\nReferences\n\nCard, S. (1983) The Psychology of Human-Computer Interaction. Erlbaum, Hillsdale.\n\nDawes, B. (2007) \u201cAnalog in, digital out: Brendan Dawes on interaction design\u201d Behaviour & Information Technology 3: 25-76.\n\nJones, M. & Marsden, G. (2006). \u201cMobile Interaction\u201d Design International Journal of Human-Computer Interaction 7(1): 123-135.\n\nMoggridge, B (2007) \u201cDesigning Interactions\u201d ACM Transactions on Computer-Human Interaction3 (2): 78-98.\n\nMyers, B (1998) \u201cA brief history of human-computer interaction technology\u201d. Interactions 5(2):44\u201354.\n\nSaffer, D. (2006) \u201cDesigning for Interaction\u201d Human-Computer Interaction 23-65.\n",
        "label": "human"
    },
    {
        "input": "Personal Computer Evolution Overview Essay\n\nA personal computer is a machine that performs many functions such as manipulation of data, calculations, and acts as a spring of information. In most cases, a personal computer like its name performs individual tasks. The first computers, also known as mainframe computers or ENIAC, appeared huge and occupied even the whole room. However, several centuries later, technological advancements led to the development of microprocessors that reduced the size of mainframe computers into household and business items. Historically, the question of who invented the computer has not been easy to answer. This is because the computer is a complex machine with many parts. Additionally, not one inventor was involved in the development of the personal computer. History asserts that different inventors invented different parts of the personal computer before the actual amalgamation. So far, the evolution of personal computers has been a startling story; a story so hilarious and entertaining; a story of invention and development of technology (Allan 2-3).\n\nTo date, the computer industry has registered numerous astounding successes not comparable to any other industry. Almost 2000 years ago, experts engaged in themselves in technical expertise that saw the development of the first computer through huge. However, today 2000 years later, we now have a portable digital machine that can store massive data and process others within seconds. It is important to note that the first evolution of a personal computer occurred in the first century. However, these computers were not electronic. The first electronic computers came into the market 50 years ago. Over the last forty years, the world has witnessed the evolution of personal computers from mainframe computers into portable microcomputers. Astonishingly, these machines have changed the lifestyle of many people from America to Asia, from Europe to America, and from Australia to South America. Today, at least every one out of two homes in North America possesses a personal computer. Amazingly, the effectiveness of any business organization depends on the use of personal computers. From banking to the stock market to manufacturing, a personal computer is a vital item for business success. Contemporary society relies heavily on computers for day-to-day activities, processes, and practices (Sherman 3-13).\n\nHistorical writers claim that the abacus, a wooden rack with double parallel wires and bead strung hang on them, was the foundation of modern-day computers. In those times, scientific programmers had to commit to memory the conventions of programming in order to place the bead at its rightful position. In that case, the beads assisted users to carry out calculations. Later on, the experts of that time managed to develop an Astrolabe to perform navigation. In the seventeenth century, the computer industry started shaping up. Precisely, in 1642, an engineer by the name of Blaise Pascal became the first person to assemble the world\u2019s foremost digital personal computer that performed arithmetic that involves additions. Thirty years later, Gottfried Leibniz invented the computer but did not assemble it. However, in 1694, this particular computer became a real item used for personal purposes. The computer could perform additional arithmetic calculations such as addition, multiplication, and addition. Nevertheless, many people did not use the two computers developed by Leibniz and Pascal as they considered them bizarre. One hundred years later, Charles Thomas successfully developed the foremost perfunctory calculator, which had the capability to add, divide, subtract, and multiply digits. These mechanical calculators acted as the springboard towards the development of new and enhanced desktop calculators, and by 1890, there were many desktop calculators in the market. These desktop calculators performed distinctive tasks as compared to mechanical calculators. For example, they could stockpile and perform habitual reentry of precedent results, accrue fractional outcomes, and even print them. However, these operations could not go through the manual installation. Nevertheless, these desktop calculators assisted people in the commercial sector to perform various tasks but not those working in the field of science (A Brief History of the abacus, p.1).\n\nCoincidentally, as Thomas Charles was busy developing a desktop calculator, a mathematics professor at Cambridge University, Charles Babbage was also busy developing the existing computers to give them a new look. His work of developing a complex calculator started in 1812 when it came to his realization that the pre-existing long computers, principally those that generate mathematical tables, were in actuality a succession of conventional actions that are continually recurring. Thus, he suggested that one can manage to perform such operations automatically. Based on this assumption, Charles Babbage embarked on the journey of producing an automatic mechanical calculating machine, and towards the end of 1822, he had managed to stage a working model as an exhibit of his work. He named this model a different engine. However, Charles Babbage was not a rich man. The desire to manufacture this model of calculators forced him to seek financial support from the British Government in order to fabricate a different engine. One year later, in 1823 to be precise, Babbage managed to fabricate a steam motorized and entirely automatic difference engine. The difference engine was a complex machine controlled by a predetermined instruction program that could perform varied tasks such as printing the ensuing tables.\n\nAlthough the different engine was the grand progress of the century, characteristically, it had imperfect malleability and applicability. Nevertheless, this did not deter Babbage from researching ways to improve the difference engine. However, in 1833, stopped the development of the different engines and instead, sought to develop another machine. His main idea was to develop an entirely program-driven, routine perfunctory digital workstation called an Analytical Engine. A number of personal computer evolution history materials quickly assert that this was the prime idea behind the development of the modern digital computers, microcomputers. Among the very many features of the Analytical Engine is an identical decimal computer that could work under the instruction of decimal digits, and a memory of one thousand digits. Thus, for the purpose of future operations, the in-built functions of the engine had to be complex and included sophisticated functions that could perform conditional credit transfer hence, tolerating the execution of instructions in multiple ways. Nevertheless, it took a century to develop this particular engine.\n\nIn the period between the first century and 1970s, personal computers were outsized, pricey systems owned by gigantic conglomerates, government bureaus, and research institutions such as universities and colleges. Unlike today, the end-user of mainframe computers could not intermingle with the machine. Nevertheless, these computers had the capacity to set up errands for the mainframe on off-line equipment, for example, card punches. During ancient times, computer experts piled and processed numerous assignments in batch mode. Thereafter, the users would then come for the results (Wulforst 12-56).\n\nNevertheless, users had to wait for long hours or even days for the experts to complete particular tasks. This is because of the slowness of the mainframe computers to process information and deliver the output. Perhaps as a relief to many users, engineers managed to develop the first commercial computer in the 1960s. These computers appeared complicated as compared to earlier mainframe computers. Unlike the mainframe computer, these minicomputers interacted with other machines via a time-sharing system and manifold computer terminals, which allowed many users to use a single processor to perform multiple tasks. Consequently, many business firms and organizations benefitted from this development. Slowly by slowly, the huge computers were with time reduced into smaller items with advanced features.\n\nThe successive evolution of personal computers that took very many years to complete resulted in the present personal computer, the microcomputer. This would not have been a reality had it not been scientific research in the field of microelectronics. Scientists researching the standardization of computers came up with two paramount technical innovations. In 1959, scientists came up with an integrated circuit (IC), setting up the stage for further computer development and hence, evolution. The development process did not continue at this stage. Several years later, in 1971 to be exact, scientists developed the first microprocessor. These two technical innovations became the driving gear of personal computer evolution. For instance, the development of an integrated circuit allowed the miniaturization of memory circuits of the computer for easier data retrieval. On the other hand, it was also a sigh of relief to computer users when the first microprocessor came into the market. This is because the microprocessor abridged the size of the central processing unit to an equivalent size of a solitary silicon chip.\n\nPrimarily, a microprocessor is an electronic device, which merges thousands of transistors into a miniature silicon chip. The person behind the development of the microprocessor is Ted Hoff, at that time worked for Intel Corporation in the United States. Up to this particular stage, we now almost have a microcomputer characterized by a condensed central processing unit that is acting on a single silicon chip called a microprocessor. It is important to note that the central processing unit of the computer is actually the heart of the computer. In particular, it is the one responsible for computations, logical operations, encloses in service commands, and administers data flows. Thus, there was still immense prospective to develop another system, which will operate as an absolute microcomputer.\n\nIn 1974, something peculiar happened in the computer industry. This year, engineers under the umbrella of Micro Instrumentation Telemetry Systems (MITS) managed to assemble the world\u2019s foremost desktop-size classification mainly for individual purposes. The news of the development of this system came as a surprise to many people who eventually demanded to acquire the new system for personal use. This demand heightened further when the editor of a trendy technology periodical challenged the inventors to vend a mail-order computer kit via the periodical. The inventors of this desktop-size system named it the Altair computer and anybody who wished to acquire an Altair computer had to pay four hundred dollars. To the dismay of the inventors, many people made offers to acquire Altair computers. The demand heightened further until there were no more computers to buy. However, numerous entrepreneurial companies who had acquired the art of manufacturing Altair computers developed additional computers that responded to the ever-rising demand. In 1977, Tandy Corporation established new models of computers with two foremost attractive features. These new models of computers from Tandy Corporation came with a keyboard in addition to a cathode-ray display terminal. Consequently, these computers became the most fashionable simply because a user can program them. Additionally, these computers had the capability to store data using another device known as a cassette tape (Hewlett Packard 1-5).\n\nPersonal computer evolution did not stop here. The new phase of yet personal computers was on the way when Stephen Wozniak and Stephen Jobs established their own computer fabrication firm known as Apple computers. The two engineers cum programmers had managed to develop a homespun microprocessor workstation slat-Apple I while working at Intel Corporation in 1976. Since the two were now working in their own firm, they managed to develop Apple II. The Apple Computer Incorporation was now selling Apple I and Apple II electronic devices, the latter selling at $1,290. Apple II also came with a keyboard and complete color graphics among other distinctive features. Furthermore, the new computers from Apple Computer Incorporation had advanced features that none of the previous computers had. For example, Apple microcomputers are characterized by a long-drawn-out memory, low-priced disk-drive programs, and data storage. They also had some advanced features such as color graphics; made available by the Apple II electronic device. Later on, Apple Computer Incorporation sold millions of microcomputers that making it one of the biggest manufacturing companies not only in the United States but also worldwide. Driven by the hunger for success just like Apple Incorporation, several computer-manufacturing firms emerged in order to compete with Apple Computer Incorporation. In fact, before the end of the 1970s, there was a potential market for microcomputers as the sales increased each year.\n\nAmong the several manufacturing firms that opted to compete with Apple Incorporation was IBM. However, the first computer manufactured by IBM came into the market in 1981. Popularly known as IBM PC, the model appeared less complicated, mainly fabricated from ancient technologies. Nevertheless, according to researchers, the model was a landmark in the mushrooming meadow of computer manufacturing. Although the IBM personal computer did not make use of the recent technology, it showed that the microcomputer industry was new above contemporary vogue. In fact, many businesspersons preferred this model as compared to other models. Among the very many features of an IBM personal computer was a 16-bit microprocessor. This microprocessor was the fundamental basis for future developments, for example, powerful micros and operating systems. Notably, the availability of this microprocessor to other manufacturing firms initiated the standardization of the computer industry.\n\nIn the 1980s, a major development occurred in the computer industry evolving the personal computer further. For example, manufacturing firms managed to develop a potent 32-bit computer proficient in running complex multi-user operating systems at elevated velocities. In fact, the 32-bit computer separated minicomputers from microcomputers. The new microcomputer characterized with sufficient computing power was now capable to serve even big organizations without any quandary. Additionally, the manufacturers developed trouble-free, comprehensible means for managing the operations of a microcomputer. For example, the conservative operating system replaced the graphical user interface to allow computers to choose icons, either graphic signs or computer functions, from the monitor as a substitute to typed commands. Later on, an engineer by the name of Douglas Engelbart developed a computer mouse whose expediency modernized personal computing. There were also major developments that occurred in the 1980s for example, the establishment of voice-controlled systems and the use of lexis and syntax of verbal communication to run microcomputers (Sherman 34-76).\n\nThe next model of computers appeared in the 1990s. However, in order to come up with such models, vast intelligence and fortitude were principal prerequisites. Consequently, the knowledge on computer usage increased and many people began to realize the importance of computers in their lives. Additionally, many people realized that just like somebody attending driving lessons, the operation of a computer also required some knowledge and skills. Nonetheless, in those times, it was not easy to figure out the exact roles of computers in society. This is because different types of computers performed different functions; an operation that confused many computer users of that time. The emerging computers and the increasing knowledge on computers made it possible for the execution of varied tasks, which could take several pages to name every single one. For instance, it is paramount to note that the novel computers that came into the market in the 1940s were simple and their purpose undoubtedly spelled out. Precisely, these computers performed mathematical functions as the main fundamental task. As compared to manual calculations, these computers were far much better. Nevertheless, the successive evolution of computers influenced the improvisation of loads of usage styles and computer types, which lucidly delineated the purpose of the personal computer (Allan 526-528).\n\nAs from 1990, a particular computer had to fall into one group of computers comprising mainframes, minicomputers, or microcomputers (personal computers). Perhaps to explain the features and characteristics of computers in each group, the mainframes as explained earlier, were huge but with the potential to amass substantial volumes of data in form of numbers and lexis. Noticeably, these were the world\u2019s first categories of computers, which came into the limelight in the 1940s. In most cases, not very many people owned mainframe computers but they had suitors from the banking firms, manufacturing and service industries, and educational or government institutions. This is because these computers were costly hence, many individuals could afford them. Moreover, the lifetime of mainframe computers was between five and ten years. Larry Wulforst asserts that these types of computers required immense expertise to run and maintain. There is a great contract between these computers and the other remaining groups. For example, they produced a roaring sound when in operation due to their vast engine (126).\n\nThe next group comprises minicomputers or network computers. The inventors of these computers wanted to enhance communication via computers. Indeed, this type of the computer revolutionalized the communication industry from the time of their invention to date since they interlinked very well. For instance, in order to counter the nuclear threats and assails from enemies abroad, the United States government designed standard networking computers in the 1960s aimed at enhancing communication among soldier camps. Further research on these computers saw the design of the internet, which is a fundamental tool in modern communication systems. In fact, according to statistics released in the 1990s, these computers had spread to every corner of the world. Primarily, networking computers performed the function of traffic managers and controlled the whole internet. Today, so many people all over the world access internet either at home or in cyber cafes courtesy of these computers. With the emergence of optical fiber transmission cables and the standardization of other technologies, the internet has become one of the fundamental tools of communication Campbell-Kelly and Aspray 297).\n\nThe last types of computers are those classified as personal computers. It is important to note that successive evolution of personal computers beginning from an abacus resulted in what we now call a personal computer. These computers are easy to use and can perform multiple tasks such as word processing and communication via links concurrently. Comparatively small in size, these computers also have additional features and sell at a cheaper price. In conclusion, the evolution of personal computers has been a long journey that has taken over 2000 years. The present personal computers comprise a central processing unit, a keyboard, a mouse, and a video monitor all working as a single unit. Each of these parts performs different tasks even as the CPU remains the heart and brainpower of the entire system (Palferman and Swade 48).\n\nReferences\n\nA Brief History of the Abacus. 2010. Web.\n\nAllan, Roy. A History of the Personal Computer: The People and the Technology . London: Allan Publishing. 2001. Print.\n\nCampbell-Kelly, Martin, William Aspray. Computer, A History of the Information Machine . New York: Basic Books. 1996. Print.\n\nHewlett Packard. 9100A desktop calculator. 1968. Web.\n\nPalferman, Jon, and Swade Doron. The Dream Machine. London: BBC Books. 1991. Print.\n\nSherman, Josepha. The History of the Personal Computer . New York: Franklin Watts. 2003. Print.\n\nWulforst, Harry. Breakthrough to the Computer Age. New York: Charles Scribner\u2019s Sons. 1982. Print.\n",
        "label": "human"
    },
    {
        "input": "The Qatar Independence Schools\u2019 Computer Network Security Control Term Paper\n\nThe integration of new technologies into a school setting can be seen as an inevitable process. In that regard, the educational institutions that start to integrate technologies, not only to gain a competitive advantage in the education market, they also allow increasing the efficiency of the learning process, and reduce costs. Computer networks and electronic resources are among the necessary technological assets that attract new students, and allow schools to reduce paper costs. Other advantages can be seen through the possibility to integrate multimedia resources into the curriculum. Qatar Independence School has set on the path of integrating Information Technology (IT) into the school context. Having already fully equipped computer labs, the school intends to expand and integrate the labs into a unified system connected to the internet, with a digital attendance record, and touch screen test system. In addition to computers in the labs in the proposed system, wireless access will be provided through WIFI hotspot zones. With security aspects being the main focus of such a system, the present project documents the process of designing and implementing security control through fingerprint facilities in the computer network in Qatar Independence Schools.\n\nRequirements and Technologies\n\nPrimary Research\n\nIn order to identify the requirements for the project, a qualitative research was conducted, in which a series of unstructured interviews with various IT analysts and representatives of computer security field were held in order to identify feasible requirements for the proposed system. The basic outline of the interviews can be seen in Appendix D. Accordingly, eight interviews were conducted during the span of 10 days, during qualitative data were collected from the main participants of the study. The result of the interviews mainly outlined several common themes and patterns in terms of the design of the proposed security system. These themes and patterns can be seen through the following:\n\n  * Most respondent s agreed that third party separate solutions will be a cheaper and more feasible acquisition.\n  * The authentication procedure using fingerprints should be differentiated through different purposes and different terminals.\n  * Passwords and other knowledge-based security measures are not sufficient means to manage security in applications.\n\nOne of the main arguments supporting fingerprint authentication was through evident through flexibility and speed. One area of arguments justifying the use of a fingerprint identification system is being fast in input. Many experts cited the example a comparison between the time required for a password input and a fingerprint scan. Considering the fact that Qatar Independence School has 320 students and 45 instructors, the time delays for inputs can pose a real issue in terms of efficiency and flexibility. Categorizing the essential part of the system all respondents s acknowledged that fingerprint authentication facility can be seen as one of the most significant parts of the project. Answering the question on the main part of the system, John Summers, 36, independent IT analyst stated,\n\nThe main part in fingerprint identification is the scanner and the database with the templates. Everything else is a matter of providing an infrastructure to communicate between those two parts. Fingerprint authentication is as simple as that.\n\nNevertheless, there were other parts as well which presence is vital for the system to function properly. Those parts included a database server, scalable and capable to manage many queries efficiently, a server that can serve as the core of the whole system, and wired and wireless infrastructure that will connect all the parts of the system together. In that regard, one essential theme that emerged during the interviews is the compatibility between the different parts of the system. An example of the latter can be seen through installing a fingerprint authentication scanner which is not capable of integrating with Microsoft\u2019s Active Directory. Accordingly, the compatibility between the SQL server, the server, and the fingerprint hardware should be also paid attention to. Finally, the need to implement other means of security in addition to the fingerprint authentication can be dictated by the fact of using a network in general and a wireless network in particular. Answering the question on using other means of security, Rolanda Bayley, 29, an system administrator, stated,\n\nFingerprint authentication are necessary to protect access to desktop and laptops, but how about the wireless network. Using powerful access points to cover the school territory, users from outside might infiltrate the network. In this case, it is not about using free internet. It\u2019s about protecting the databases of the school.\n\nIn terms of touch screen tests, the main analogy was drawn between the required devices and the products of Apple\u2019s IPad. Nevertheless, all respondents agreed that the price of the device is a major factor for such purpose. The main direction suggestion was using low cost touch screen tablets, which are Wi-Fi certified and capable to integrate into the computer network of the school. Other usages identified by respondents included the following aspects:\n\n  * E-reading device for textbooks, electronic articles and lecture notes.\n  * Playing multimedia materials.\n  * Time management tool.\n  * A tool for taking notes.\n\nSecondary Research\n\nA review of literature on the technologies to be used was conducted for the purposes of the present project. In that regard, the main parts of the system can be divided between authentication terminals, a database, a server, and wired and wireless infrastructure. The computer lab desktop terminals will not be discussed within the scope of this project limiting the description to the processes and the interactions of the security system. The overall design of the system can be seen through Figure 1. In that regard, the technologies used include the following aspects.\n\nFingerprint identification\n\nThe technology behind fingerprint identification can be seen through two simple processes scanning and matching. The fact that human identifiers such as fingerprints have distinctive anatomical characteristics enables biometric recognition, which will provide better security and higher efficiency. In that regard, such identifiers cannot be \u201cmisplaced, forged, or shared, [and] they are considered more reliable for person recognition than traditional token- (e.g., keys or ID cards) or knowledge- (e.g., password or PIN) based methods\u201d (Maltoni, 2009). Two modes exist for the operation of fingerprint authentication facilities. The first mode is the enrolment mode, i.e. the mode in which the fingerprint is acquired, and then its feature extracted and stored in a template in the database (Seshadri and Avulapati, 2010). The second mode is the authentication mode, i.e., the mode in which the acquired fingerprint will be matched with existing templates, and according to which scores, certain security policies will be applied (Seshadri and Avulapati, 2010). It should be noted that the first phase is a one-time phase that will be performed for students and staff in Qatar Independence School, while the second phase is a continuous day-to-day security procedure. The connection of the fingerprint authentication system to the server can be seen through Appendix E.\n\nIn order to mitigate the risks of potential failure of the fingerprint authentication system, the computers will be capable of authorisation using passwords. Such possibility will be saved for system administrators for the case of emergencies and computer check ups. It should be mentioned that the labs in this context are reserved for desktop PCs, which will be connected wirelessly to the network. Although being stationary, as opposed to touch screen tablets, the utilization of wireless connection will allow expanding the number of computer labs without substantial investments in the wired infrastructure. Accordingly, fingerprint authentication devices will be attached to the desktops in computer labs (see Figure 1).\n\nFigure 1 \u2013 an Example of a desktop finger print authentication.\n\nDatabase\n\nThe database contains profiles of each student along with staff personnel with a fingerprint identification template matched to it. Microsoft SQL Server 2008 will be used to create, store and manage data contained in the profiles. Such data will be stored in databases which are scalable and can be even shared between several organizations (Harbaugh, 2010). In that regard, considering the fact that the Qatar Independence Schools are considering increasing the number of students enrolled, SQL Server can serve as platform capable of handling massive databases, integrating all the data and managing all user queries. Accordingly, combining SQL Server and Windows Server will enable the school to create a centralized data centre, easily upgradable into a more powerful structure if necessary (Microsoft, 2010).\n\nServer\n\nThe core of the system can be seen through Microsoft Server 2003. One of the main tasks of the server can be seen through providing network access for connected terminals (Boswell, 2003). An essential element of the implemented technology is the usage of security policies. The security policy can be defined as a collection of rules and instructions that control computer systems and their components, network cohesion, secret and confidential information, etc. In the context of the present project, security policies can be seen as elements of a template to which fingerprints can be attached. Using user and group policies the administration of the school can limit the rights of these users and groups to perform certain actions (TechNet, 2005). The assignment of permissions can be implemented through either groups or roles in Microsoft Server 2003. The logical roles that can be assigned might include teachers, IT staff, students, administration, etc. The roles can be seen as a collection of tasks supported by an application (MSDN, 2008a). The stages of forming such policy can be seen through either assigning users directly to roles, or collecting users in groups and then assign them to roles (MSDN, 2008a). The information of group policies can be stored in group policy containers (GPC) in the corresponding template, which can be seen as an area in Active Directory (AD). Utilizing authentication software compatible with AD will allow managing policies and groups, loading profiles and accounts, linking passwords, and others, to be performed in seamless manner within a single system.\n\nWi-Fi\n\nWi-Fi technology enables the connection of different devices wirelessly over the air. In that regard, such technology will make the organization of the network structure in the school easier, and at the same time will eliminate the necessity for the devices to be connected to the network in specific places only (Figure 2). The principle of work can be seen through putting several access points throughout the school territory, which will enable all Wi-Fi certified devices to connect to the internet. The certification aspect will ensure reliable connections and security over the air for the devices connected to the network (Wi-Fi Alliance, 2006). The distinction between finger print authentication and standard methods of security over Wi-Fi can be seen in that the standard methods, e.g. Wi-Fi Protected Access (WPA) security (Wi-Fi Alliance, 2004), can be seen through different application. WPA can be used along with MAC filtering to control access of unauthorised devices, while fingerprint authorisation will link any processes, activities, achievement , etc, of the student to his/her unique profile, controlled by fingerprints.\n\nTouch Screen Tablets\n\nThe definition of tablet PCs can be seen as \u201ca portable computer equipped with a touch screen and special pen that allows users to \u2018handwrite\u2019 on the screen\u201d (Xiang et al., 2009). Several other variations might exist, including internet tablets, such as Apple\u2019s iPad. The main differences can be seen in the system of control and their usage as handwriting devices. Nevertheless, the main principles of such device are the same, a touch screen, light weight, and connectivity. The benefits of touch screen tablets might conform to the findings of the primary research, where their use include such aspects as recording presentation slides with notes and audio recording, assessment through receiving, marking, and returning assignments, and using as an interactive teaching aid (Xiang et al., 2009). A study conducted to review the technological aspects of using wireless tablet PCs revealed that students are \u201coverwhelmingly in favour of introducing Tablet PCs to lecture rooms and distance education\u201d (Xiang et al., 2009). The alternatives to Apple\u2019s IPad, which are either available or ready to be available, include options such as Knowledge Now (KNO), an education oriented low-cost tablet PC, and Marvell prototype (Tsotsis, 2010, Charbax, 2010). Other options can be seen through various Android-based tablet PCs, available from different vendors (Light in the Box, 2010). The connection of the tablet PCs within the system can be seen through Appendix E.\n\nMinimum Requirements\n\nThe main software requirements for the project are mainly composed of Microsoft products family. The following family of software products were selected for the system:\n\nFor the server:\n\n  * Microsoft Server 2003 Standard Edition\n\nFor Laptops and Desktop:\n\n  * Microsoft Windows XP\n\nFor the database server\n\n  * Microsoft SL Server 2008\n\nFor the Tablet PCs there are no software requirements of the tablets themselves as they come pre-installed with a proprietary operating system (OS). Such system might be based on Android OS, Apple iOS, or Windows Mobile, or other tablets that might be used. Additionally, the choice of the tablet might require installation of software to sync the tablets with Desktops. In that regard, such aspect also depends on the choice of the tablets. Android-based tablets might be recommended in that regard.\n\nThe main hardware requirements for the project can be seen through the following aspect:\n\n  * Finger print scanners for desktop use (installed on PCs).\n  * Fingerprint scanner for attendance records (installed on school and/or class entrances).\n  * Wi-Fi Access points \u2013 devices for converting wireless and wired communication through transmitting radio signals (Mitchell, 2010).\n  * Touch screen tablet PCs.\n\nAccordingly, the hardware requirements might include the minimum specifications for the operation of the Microsoft and SQL servers. According to the official requirements of Microsoft SQL Server 2008, the minimum hardware requirements include:\n\n  * Minimum: 1.0 GHz Pentium III-compatible processor or faster.\n  * Minimum: 512 MB for SQL Server Express with Tools, and SQL Server Express with Advanced Services (MSDN, 2008b).\n\nMicrosoft Server 2003 requires the following hardware specifications:\n\n  * Processor with minimum speed of 550 megahertz (MHz)\n  * 256 megabytes (MB) of RAM (Microsoft, 2005).\n\nAdditionally, each of the desktops in computer labs should be equipped with a certified wireless adapter that will connect the computer to the network through wireless access points.\n\nRisk Assessment\n\nRisks in IT projects cannot be excluded. In that regard, many of the risks associated with the current projects are foreseeable, and accordingly, can be identified and planned. The risks can be categorised into several types, which include risks external to project management and the organization, governmental regulatory changes, industry specific procedures, change-driven factors, emergencies, and others (Murch, 2001). The main risks associated with the current project can be seen through the following table:\n\nType of risk                                                                     Description                                                                                                                                                                                   Risk level  Risk management plan                                                                                                                               \nThe end of the funding of the project                                            Such risk is concerned with the possibility that the investors might end the funding of the project due to                                                                                    Low         Constant confirmation of beach phase before proceeding along with reviews with the board.                                                          \nRegulatory risks, the introduction of new standards regarding radio frequencies  Radio frequencies are governed by government standards, for which changes might permissions might be needed for operations outside of classrooms                                              Low         Evaluating regulatory standards prior to conducting the planning process and making sure certified equipment are purchased.                        \nSudden rise of equipment prices                                                  There might be differences in time when the budge will be established and when the equipment will be actually purchased, which might lead to a threat of exceeding the costs of the project.  Low         Including estimated and forecast price in the plan. Conducting continuous reviews of the requirements of the project.                              \nUnderestimation of project requirements                                          Changes in the requirement during the project\u2019s implementation.                                                                                                                               Medium      Reviews and regular meetings should eliminate the possibility of sudden changes in the requirements.                                               \nLoss of information due to hardware failures                                     Plans, programs, designs, might be lost during implementation to a failure in hardware or software.                                                                                           Medium      Backing up information during all the phases of the project. Using the cloud to save information from loss due to software and/or hardware failure.\n\n\nTable 1 \u2013Risk Assessment\n\nQuality Management\n\nThe quality in the context of project management can be defined as \u201cthe degree to which a set of inherent characteristics fulfil requirements\u201d (Phillips, 2010). In that regard, it can be stated that the quality implied in the context of project management is about the quality of the deliverables and the processes. For each of the deliverables in the projects there should be metrics used to measure the good of the service or the product. The value of the deliverables in this project can be seen through such factors as the value of the implementation, i.e. cost-benefit analysis, value of the service, the value of the experience, the value of the longevity, and the value of the reliability (Phillips, 2010).\n\nExperiments that will test the functionality and the service delivery of the project should be planned. Such experiments might include such aspects as authentication, breaking through the network, the stability of connection, and testing variations in the policies and rules for different users and groups. The schedules for testing should be organised in order to monitor the quality should correspond to each stage of implementation on a prototype of the system. The tasks to be tested and their schedules can be seen through the following table.\n\nThe element to be tested  The performed task                                                                                  The Schedule       \nAuthentication enrolment  Creation of different templates of fingerprints and assigning different roles and responsibilities  5-8 December       \nMatching fingerprints     Testing identification of users from different groups and categories.                               8-9 December 2010  \nPolicies and rules        Testing performing task and actions not assigned to the group.                                      13-15 December 2010\nWireless Security         Performing different leak tests on different terminals in the system.                               15-17 December     \n\n\nTable 2: the Testing Schedule\n\nGantt chart\n\nFigure 1: Gantt chart for the Project\n\nFigure 2: Slack Time\n\nReferences\n\nBOSWELL, W. 2003. Inside Windows Server 2003, Boston, Addison-Wesley.\n\nCHARBAX. 2010. Marvell announces $99 Moby Tablet to Revolutionize Education. Web.\n\nHARBAUGH, L. G. 2010. Review: Microsoft SQL Server 2008 R2. Web.\n\nLIGHT IN THE BOX. 2010. Android Tablet PC. Web.\n\nMALTONI, D. 2009. Handbook of fingerprint recognition, New York, Springer.\n\nMICROSOFT. 2005. Windows Server 2003, Enterprise Edition: System requirements. Web.\n\nMICROSOFT. 2010. SQL Server 2008 \u2013 Your Data, Any Place Any Time. Web.\n\nMITCHELL, B. 2010. Wireless Access Point. Web.\n\nMSDN. 2008a. Dynamic Groups in Windows Server 2003 Authorization Manager. Web.\n\nMSDN. 2008b. Hardware and Software Requirements for Installing SQL Server 2008. Web.\n\nMURCH, R. 2001. Project management : best practices for IT professionals, Upper Saddle River, NJ, Prentice Hall.\n\nPHILLIPS, J. 2010. IT project management : on track from start to finish . New York: McGraw-Hill.\n\nSESHADRI, R. & AVULAPATI, Y. K. 2010. High Performance Fingerprint Identification System. International Journal of Computer Science and Information Security. Web.\n\nTECHNET. 2005. Local Users and Groups overview. Web.\n\nTSOTSIS, A. 2010. Kno To Build A Single Screen, Education-Focused Tablet. Web.\n\nWI-FI ALLIANCE. 2004. WPA Deployment Guidelines for Public Access Wi-Fi\u00ae Networks. Web.\n\nWI-FI ALLIANCE. 2006. Wi-Fi CERTIFIED. Web.\n\nXIANG, W., GOH, S., PATHER, S., MAXWELL, A., WANG, H. & KU, H. 2009. USE OF WIRELESS TABLET PCS AS AN EFFECTIVE LEARNING AND TEACHING ENHANCEMENT TOOL. Web.\n\nAppendices\n\nAppendix A \u2013The Overall Design of the Project\n\nAppendix B: The Implementation of Wi-Fi\n\nAppendix C: The Project\u2019s List of Activities for the Phase of Analysis and Design\n\nAppendix D: Interview Questions Outline\n\n 1. Describe the implementation of a fingerprint facility in a school.\n      + The parts of the system.\n      + The main precautions.\n 2. How is it justified?\n      + Factors for\n      + Factors against\n 3. Other means of security.\n 4. Touch screen tests.\n      + Typical scenario.\n      + Other usages of the devices.\n\nAppendix E: Tablet PCs and Finger Print Facilities\n",
        "label": "human"
    },
    {
        "input": "Threats to Computer Users Report (Assessment)\n\nIntroduction\n\nIt is commonplace that modern users of computers encounter a myriad of challenges in the course of their endeavors. This calls for increased vigilance and awareness by these users, to protect the confidentiality and their data and personal information. It is noteworthy that organizations are also susceptible to such attacks, highlighting the necessity of intervention and protective measures. In view of this, several organizations have introduced security training as a mandatory segment of their orientation procedures (Newman, 2009).\n\nPhishing Tricks\n\nPhishing infers a structure of social engineering that deceives credulous computer users to offer private information to third parties feigning legitimacy. The information varies greatly, and can include basic details; including a person\u2019s complete name and address. Some request for social insurance details. A majority of the Phishing frauds involve fiscal resources, thus ask for bank account and credit card details. Initially, swindles were limited to select groups of computer users. Presently, they are extensive and have copious delivery techniques. Most of the rip offs propagate through e-Mail, and assume the individuality legitimate brands or depository institutions. Other vectors used for attacks gaining popularity include Instant Messaging services.\n\nA fraudulent message delivered through e-mail ensures that unwary users receive a specially crafted correspondence from what appears as a bank or any other credible on-line service. These statements often refer to procedural concerns with the recipient\u2019s account, thereby requiring them to provide the necessary updates. This is achieved by following an attached link for prompt admittance (Stewart, Tittel & Cha, 2005). In most cases, the links lead to duplicates of authentic sites and require the unsuspecting clients to fill certain forms, disclosing their personal information in the process.\n\nMitigating vulnerability\n\nEnsuring they respond to personalized correspondence only.\n\nClients should avoid providing personal information and responding to forms included in e-mails.\n\nThey should also make certain to be over safe networks whenever they reveal their fiscal details, including credit card information.\n\nSubscribers should check into their virtual accounts regularly to ensure their integrity.\n\nExperts also recommend that clients install protective tool bars in their browsers that can offer protection from Phishing sites. Most importantly, clients should ensure their browser applications have updated defense patches, which are renewed at regular intervals.\n\nNetwork Scans and Attacks\n\nScans happen with the aim of determining open ports or service areas. The vulnerability of services running on a system is directly proportional to the amount of open ports. Vulnerable systems are often exploited for different reasons. Some include; crashing the running service and rendering it inoperable; unlocking a gateway with system administrative rights and connect to the attacker; carrying out functions embedded in its payload by launching scripts or programs; incorporating the attacked system into a network of distributed denial of service, embedded on a website of the attackers choosing. This makes it a functionless system. Lastly, they carry out espionage missions by recording and relaying confidential or significant information to the sender (Stewart, Tittel & Cha, 2005).\n\nMitigating vulnerability\n\nComputer users should obtain and use firewall products with their computer systems, whether they are software or hardware based.\n\nIt is advisable to have up to date operating systems, which function properly and have all their security patches in place.\n\nLastly, users should disable all unnecessary services within their systems.\n\nEavesdropping\n\nThis threat entails spying on other persons while they relay personal information over the internet. It is notable that this vice often targets persons revealing fiscal or other personal information. Prime targets are users of systems located in public places, since these persons cannot monitor the individuals standing at their backs; neither can they prevent strangers from looking at the keyboard or monitor. The availability of minute monitoring devices has propagated this offense, because they can be mounted on a targets body (Stewart, Tittel & Cha, 2005). The powerful zoom technology allows the offenders to monitor activities on the keyboard and monitor of the targets computer from a distance.\n\nMitigating vulnerability\n\nPersons should avoid personal computing in public places and areas that are easily accessible.\n\nComputer users should embrace the use of password enabled screen savers.\n\nPersons connected to a network should log out whenever they break from their engagements on the computer.\n\nUsing privacy screens for monitors also helps, persons viewing from an angle wider than 30 degrees will be obstructed.\n\nMost importantly, users should avoid revealing private information in public places. They should note down private details whenever they wish to communicate classified details.\n\nComputer Theft\n\nIt is notable that present day personal computers are smaller than they were several years back and store more information than the earlier models. Laptops, net books and tablets are in the mainstream, while PDA\u2019s and cell phones constitute technological waves. This implies that confidential information can be stored different locations within the computer and carried from one place to another with the user. On many occasions, the data includes commonly saved data files and private information. The latter often exist within the cache files of internet browser applications. On many occasions, they include mail inbox details and other customized settings governing third party applications. This implies that well-informed thieves may access crucial information stored in the device they stole (Stewart, Tittel & Cha, 2005).\n\nMitigating vulnerability\n\nPeople should be conversant about the location of their devices.\n\nPeople should install tracking devices in the machines for activation in case of a loss.\n\nThe use of security cables for small computers slows down the activities of thieves.\n\nKeeping the appliances in hidden spots reduces risks of theft\n\nEnsuring that all computers have Boot level passwords\n\nUsing software that encrypts data present in the hard disk\n\nEnsuring crucial information is removed from the computer at regular intervals.\n\nViruses, Worms and Trojans\n\nAn increase in the use of internet applications, including peer file sharing increase the replication and spreading rate of malware to minutes or hours. Advanced programming techniques and the advent of scripted utilities further increase the danger posed by these programs. Some malicious activities include; obliterating operating system and personal records; recording personal information and monitoring system traffic flow (Stewart, Tittel & Cha, 2005).\n\nMitigating vulnerability\n\nEnsuring the antivirus commences operation automatically upon system boot.\n\nScanning through all incoming mail attachments before accessing them\n\nConducting downloads from reputable sites only\n\nUpdating the operating system regularly by installing vendor availed patches\n\nSpyware and Adware\n\nAdware programs spread with browsers as part of their scripting codes, appearing as download links or vending sites. Most spyware applications embed themselves on these programs and other peer applications. Free programs, including screen savers and other utilities also propagate these spyware applications (Newman, 2009). Most of them are installed without prior consent, mostly hidden in end user agreements. Some of their operations are enumerated below.\n\nMitigating vulnerability:\n\nAbstain from free downloads, especially unknown plug-ins and other system utilities.\n\nUse licensed antivirus software an regularly scan for spyware.\n\nObtaining and using popup killers with browser packages.\n\nRestricting the use of unnecessary applications and cookies\n\nAvoiding peer-to-peer networks\n\nScanning mail attachments before opening them\n\nEnsuring the operating system is patched and up to date\n\nRefraining from accessing SPAM and mail from pornographic and other un-trusted sites\n\nDownloading updates of the operating system and patching whenever necessary.\n\nSocial Engineering\n\nIt refers to manipulating trust to obtain confidential information from others as part of an espionage mission. This makes the threat more pronounced internally, since interested parties in the organizations can trick subordinates to reveal confidential information. Perpetrators gather information about their target from various sources, including dumpster diving and corporate websites. While the vice is not rampant, an occurrence often has grave repercussions (Newman, 2009).\n\nMitigating vulnerability:\n\nAsk for return contacts to verify the identity of callers\n\nDeny all requests incase of intimidation\n\nShred papers with confidential information\n\nErase magnetic media after use or use physical destruction when erasing fails\n\nReferences\n\nStewart, J. Tittel, E & Cha, M. (2005). Certified Information Systems Security Professional Study Guide . California, CA: John Wiley and Sons.\n\nNewman, R. (2009). Computer security: Protecting Digital Resources . Massachusetts, MA: Jones & Bartlett Learning.\n",
        "label": "human"
    },
    {
        "input": "Career Options for a Computer Programmer Essay\n\nThis report discusses career options for a computer programmer. The report extensively looks into the typical job titles and roles or duties performed by people who hold the job titles. Further, the report discusses the personal skills necessary for success in this job category. Most organizations require new employees to have some level of experience. This report looks into this requirement and explores the way by which an individual without experience can enter the industry or gain entry into a company. Money is the greatest motivator for employees. Apart from many other numerous reasons why people choose careers, the earning power in a profession plays a critical role in career choice. Therefore, this report will consider the attractiveness of remuneration perks given to computer programmers.\n\nInformation technology is a diverse field. The diversity manifests in the form of myriad career options that one can pursue in IT. There are different specializations in IT in line with specific information technology needs in organizations. One field in IT that is widely respected is computer programming. People who do computer programming are called computer programmers. In this age, most organizations require the services of a computer programmer. Computer programmers go by different names depending on the focus in a given organization. For instance, an organization that focuses on software development call computer programmers, software engineers or software developers. Some organizations focus on system development and thus computer programmers in the organization go by titles such as system developer, system administrator, system designer, and system analyst or system engineer. In some organizations, computer programmers are known to handle the technical aspect of computer systems. Therefore, they go by titles such as technical assistant, system technician or generally IT officer.\n\nIrrespective of the title used computer programmers are individuals who have the technical know-how in computer program creation. Computer programs or software are encoded instructions that a computer executes. This field is highly specialized and requires a proper understanding of logical sequencing. Therefore, the first duty a computer programmer can have is software development. However, given software development means one can also troubleshoot and correct says instructions on software, computer programmers also offer support and maintain systems.\n\nFor one to launch self into a computer programming career, one has to take professional courses in computer programming. Most companies only take in people with experience. Therefore, it is normally recommended that students take up apprenticeship or internship positions while still learning. Volunteering also helps to offer one enough experience. Career progression is guaranteed by further education and accumulated work experience. Some programmers move from the level of being mere technicians to becoming managers or administrators of other programmers. Moreover, some computer programmers specialize in teaching computer languages to students who later qualify as computer programmers. Despite what the computer programmer actually does, he or she must have the capacity to design and implement software solutions.\n\nIn some organizations, computer programmers are involved in the testing of computer programs, operating systems and other advanced software. In order to implement an IT solution, many times organizations source ready-made software from the market but they can also develop software in-house. When the software has been bought or when the system has been developed fully, a computer programmer has to test it. Once the system or software has been installed and is running, the computer programmer\u2019s focus is on offering support in maintaining the system.\n\nThe work performed by software engineers is complex and technical and the knowledge of computer science is highly appreciated in the working environment. The computer programmer has to have a number of technical skills or capacities. The first technical capacity required of a computer programmer is the ability to accurately interface with the user\u2019s requirements. The computer programmer is responsible for translating the user\u2019s requirement into a specific and well-written command that the system can accept. This capacity is achieved through familiarizing with the external interfaces and ways of ensuring that functional designs capture different user specifications.\n\nThe second technical skill requirement is designing. The computer programmer is responsible for creating and designing the system by selecting major components. He or she investigates the possible ways of designing the system from the requirements gathered. Thirdly, the system engineer ensures that the new system being designed will meet the user\u2019s requirements; he or she analyzes and predicts input, output, memory, software and hardware aspect of design to ensure they meet the required development standards.\n\nThirdly, the computer programmer has to understand software validation and verification of data. Computer programmers plan and subsequently implement verification for the systems to ensure that they meet the specific requirements. He or she develops a test plan and test data which is used to validate the system to ensure it works to its specification.\n\nLong gone are the days when computer programmers operated as an indifferent tech whiz. System development and implementation in modern organizations is an interactive process approached as projects. For each project, a project team composed of various professionals is formed. Therefore, apart from technical skills, soft or personal skills are critical for success in this area.\n\nTo succeed in the current job market, a computer programmer should be able to communicate well with the client and the team that he or she is working with. Efficient and accurate verbal and written communication is necessary for giving information. Secondly, the person should have needed to have strong analytical and judgmental skills. This is an important aspect because it helps in analyzing the processes involved throughout the development of a system to ensure every step is considered based on the requirements.\n\nGood communication, analytical, problem-solving skills and good judgment should be complemented by strong leadership or management skills. The system engineer should display assertiveness; confidence and hard work to enable him or her to realize the best of what he or she is doing. Good leadership goes hand in hand with being a good team player.\n",
        "label": "human"
    },
    {
        "input": "Computer Control System in a Laboratory Setting Essay\n\nTable of Contents\n 1. Introduction\n 2. Advantages\n 3. Design\n 4. Conclusion\n 5. References\n\nIntroduction\n\nComputer control system is an approach that is used to control and monitor specific parameters of substances within laboratory settings remotely. The elements of a control system include two important parts; data acquisition instruments that enable monitoring of desired parameters and microcontrollers intelligent instruments that are essential for controlling the desired parameters (Hebert, 2007). In this paper we are going to briefly discuss the functioning system of one control system used in regulating temperatures for liquid substances among other variables in a laboratory setting; the iControl system.\n\nAdvantages\n\nThe advantages of using iControl systems within laboratory settings are many and numerous; most important is the fact it enables control and monitoring of key variables in a laboratory environment that might pose risk where hazardous substances are involved (Hebert, 2007). In addition, the iControl system enables monitoring and control of parameters over long duration of time thereby saving on costs associated with personnel that would have been required to achieve the same objective (Hebert, 2007). Another advantage includes the consistency of data collection and overall data quality that is detailed and well organized since it is computer generated (Hebert, 2007). Finally, automatic remote control and monitoring of parameters saves on time and effort that would be required by personnel to physically implement desired changes (Hebert, 2007).\n\nDesign\n\nIControl is an advanced microcontroller system that is installed in a PC and capable of controlling and monitoring laboratory parameters wirelessly on a 400 MHz signal. In this case the iControl system is designed to control and monitor temperature variables of a hazardous liquid within a laboratory environment as well as other parameters such as smoke and light. The iControl software is run by Labview program that enables the system to function at the desired level, monitor parameters and control processes (Sparkfun.com, 2010). The essential components of an iControl design system are two microcontrollers, sensors, peltier heater/cooler, H-bridge, and Analog to Digital (ADC) converters.\n\nThe type of microcontrollers used for this project is the ATMEGA328 model manufactured by Atmel, each with its own function; one is for data acquisition and the other is for adjusting desired parameters to function appropriately (Hudson, 2006). There are three sensors installed in the iControll system to measure the key parameters by detecting changes on specific variables of interest which in this case includes temperature, smoke and light intensity. The temperature variable is measured by LM334 sensor model which is essentially a Zener diode that is configured to operate at temperature range of -40 0 C and 100 0 C (Hudson, 2006).\n\nThe fire detection component is another type of a sensor that detects the presence of smoke to trigger a fire alarm through the ADC relay component then to a buzzer that alerts personnel through sound. Finally, the light parameter is monitored by a third sensor, a LDR component that functions in the same way as the smoke sensor (Projects.net, 2010). The ADC component is for converting the sensors output data which is inform of analog to digital format that can be analyzed by the Labview program. The peltier heating and cooling element functions by initiating cooling or heating processes achieved through a H-bridge driver based on the prevailing temperature conditions and the desired temperature level (PeltierInfo.com, 2010).\n\nOther components of an iControl system include control module and control loop that are used to transmit control commands, RF communication system, LCD and a buzzer. The RF wireless communication system is an Amplitude Shift Keying device that is set at 433 MHz which is able to transmit and receive data (JayCar.com, 2010). All the processes of the iControl system are facilitated by the Labview software program that enables the actual monitoring and control of laboratory parameters remotely.\n\nConclusion\n\nThe iControl system project was successfully completed and a trial operation set up to determine how well it will function under laboratory settings. The Labview program was able to accurately capture and record the temperature, light and smoke variables as desired. To determine the effectiveness of the iControl at specific temperatures, the monitoring and control system were calibrated to maintain the temperature of the liquid at 35 0 C, beyond this temperature the peltier component switched from heating to cooling mode in order to maintain the temperature at 35 0 C. The LED output signals of smoke and light functioned by lighting when tested by introducing smoke and light in the laboratory environment. Finally, the process of data capture and transmission of information was determined to occur instantly routed by the two transmitters as desired. As such the iControl system project was certified as successful having been tested and determined to function as designed in a way that would control liquid temperatures and monitor the effects of smoke and light intensity within laboratory settings.\n\nReferences\n\nHebert, D. (2007). The Skinny on PC Based Control Systems . Web.\n\nHudson, J. (2006). Microcontroller Interfacing Circuits . Web.\n\nJayCar.com. (2010). Remote Keyless-Entry Transmitter and Receiver . Web.\n\nPeltierInfo.com. (2010). Thermoelectric Modules: Thermoelectric Cooling Solutions . Web.\n\nProjects.net. (2010). Interfacing with Microcontroller 4-Bit Mode . Web.\n\nSparkfun.com. (2010). Semiconductors: Precision Temperature Sensors . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Network: Electronic Mail Server Essay\n\nProject Proposal\n\nThis project is exclusively developed to describe and demonstrate the operation and significance of an electronic mail server. This project proposal, together with drafts and diagrams of related documents, will be extensively used by a networking management team of an organization or computer network practitioners, in order to determine whether to approve work that is developed on this project. Furthermore, a clear as well as precise plan assists in setting expectations which will be used at later stages for evaluating the success of the project. This project mainly focuses on the technology underlying the electronic mail servers and basically what are electronic mails (LearntheNet.com, 2010).\n\nBackground\n\nElectronic mail, or e-mails, as they are known to its many fans, has been in use for more than two decades. Earlier, in 1990, it was mainly used in academia. Throughout the 1990s, e-mails became known to the masses at large and developed rapidly and exponentially till the extent where the number of e-mails sent out per day is nowadays vastly more than the number of paper letters (Vleck, 2008). In practice, the major problem of sending messages to various different email addresses is typically solved by entering a list of multiple addresses separated by comma within the \u201cTo\u201d, \u201cBCC\u201d, or \u201cCC\u201d fields of the message. However, there are two significant reasons why some SMTP mail servers programs do not support this technique:\n\n 1. They are not Spam programs. Adopting this technique for sending messages is frequently exercised while sending unsolicited and malicious commercial e-mails.\n 2. This method does not involve quick message delivery whenever the SMTP server is loaded across a user\u2019s computer (LearntheNet.com, 2010) (Vleck, 2008).\n\nHowever, if the user enters multiple addresses within the \u2018CC\u2019 (carbon copy) or \u2018BCC\u2019 (blind carbon copy) fields of their e-mail message, the mail server program will send the e-mail message only to the first 50 consecutive e-mail addresses, thereby ignoring the remaining addresses. Gradually, when experience was gained, even more elaborate systems were proposed. During 1982, the ARPANET email proposals were issued as RFC 821 and RFC 822 which are specifications for transmission protocol and message format respectively. However, minor revisions like RFC 2822 and RFC 2821 have become Internet standards. Nevertheless, everyone still relate to Internet email as RFC 822 (Tanenbaum, 2005) (Kawamata & Jiro, 2001).\n\nIndeed, in 1984, CCITT enlisted its X.400 recommendations. Two decades later, email systems on the basis of RFC 822 have been widely used, whereas those based on X.400 have vanished. The reason behind RFC\u2019s success is not its good features, but because X.400 was so complex and poorly designed that nobody was able to implement it well. Additionally, given an option between simple minded and working RFC 822 based email system and a non-working X.400 email system, most of the enterprises chose the former (Vleck, 2008).\n\nServer Problems to be addressed\n\nSome of the complaints are as follows:\n\nMessages had no internal format or structure, thereby making computer processing difficult. For instance, if a forwarded message was added to the body of another message, it was difficult to extract the forwarded part from the received message. 2. Sending a message to a specific group of persons was cumbersome. This facility is often needed by managers in order to send out memos to all of their subordinates. 3. The sender would never know if the message reached the destination or no. 4. The user interface was weakly integrated wherein the transmission system required users to first edit the file, leave the editor, and the invoke the file transfer program. 5. If a network user was planning to be away for several weeks on business and wanted every inbound e-mail to be managed by his secretary, this was extremely difficult to arrange. 6. Lastly, it was not very possible to create messages containing a combination of text, images, voice, videos, facsimile, and send them to the recipient (Tanenbaum, 2005). Indeed, when such a message is directed towards the ISP\u2019s server, it creates copies of the original e-mail message corresponding to the number of addresses entered in the aforementioned fields. Afterwards, the server then sends every copy of the original message to the server; the ultimate outcome is precisely similar to the event as if a different message was being sent to every address on the list. Rather than resolving the problem, it has only been worsened. Furthermore, the server which must deliver those messages ends up producing congestion across the Internet, as a result of which its performance degrades. And no administrator acknowledges or appreciates such things, and almost all ISPs immediately close the accounts of the network users who attempt to send a huge number of messages at the same time (LearntheNet.com, 2010) (Kawamata & Jiro, 2001).\n\nCC and To Fields : In cases when several addresses are being entered within the \u2018To\u2019 & \u2018Cs\u2019 fields, every recipient would be able to view every other recipient\u2019s address from a user\u2019s list. Then, the message would be seen very unprofessionally created (LearntheNet.com, 2010). Consequently, the marketing effect created by such a sending technique is highly unfavorable because of two main reasons:\n\nFirstly, if the message to be sent is not personalized; that is, it is not addressed personally to the recipient. 2. Secondly, if the user was so unprofessional or careless to expose their entire mailing list to everyone\u2019s sight; hence, to what extent can a user expect that the customers would be motivated to buy his goods or services and reveal their confidential and sensitive information, such as credit card data, user names, passwords, etc, to him? (LearntheNet.com, 2010)\n\nBCC Field : The BCC field holds its significance in the fact that when a user enters recipient email addresses within the Bcc field of the message, the recipient would not be able to see the addresses in the list in a similar fashion as they are inserted into the \u2018CC\u2019 as well as the To\u2019 fields. Furthermore, to make a particular message reach its target address and make the server accept it, it is extremely essential to enter a certain value within the \u2018To\u2019 field. This value, in turn, will be the same for every recipient and therefore, if a user enters, for example, [email protected] , each message recipient will be able to see that address wherein his /her personal address must be (LearntheNet.com, 2010) (Oricode, 2010).\n\nWhenever the server, at which the mailbox indicated by the corresponding address in the sender\u2019s list is situated, accepts a message like this one, it may recognize it as Spam. Moreover, automated mechanisms are employed for blacklisting people who intend to send out Spam mails; of course everything relies on the software loaded at the server end and the administrator end. A message created by a user contains adequate information about them. Deactivating the account with the respective ISP is one of the steps which could be applied (Tanenbaum, 2005). Email, similar to most other types of communication, is equipped with its own styles and conventions. In particular, it is extremely informal and also contains a relatively low threshold of use. Moreover, the reason why it is worth confronting and solving the problem mentioned above is that people who would not think of writing a letter or even calling up to a very important person do not show any hesitation to send an informal and sloppily crafted e-mail. E-mail can be exchanged with people all around the world. Not only that, it also provides a economical, fast and a convenient way to sends out messages to friends, colleagues, and family (LearntheNet.com, 2010).\n\nThis project will produce a detailed description of what electronic mails are, their significance, features of emails, architecture as well as services. Apart from this, the project also throws light on the user agents involved, format of an email, and the various protocols involved in the development of an electronic mail (Oricode, 2010). However, this project will not talk about the message format of an email in depth. The message format is covered superficially in this project report.\n\nClear diagrammatic illustrations of the working of emails, how they are created and sent by the sender and viewed and read by the recipient are exclusively elaborated in a section dedicated to it. This project also provide and overview of the how Windows Server 2003 can be used as a messaging solution and the supporting technologies to aid the working of Electronic mails. Furthermore, this project can be tested on technologies such as Outlook Express and Microsoft Outlook where the client is configured for providing the best outcome and email service. The research paper deals with the various electronic mail server requirements along with a list of most efficient mail servers (Oricode, 2010) (The FreeBSD Documentation Project, 2010). The above mentioned bottlenecks or problems is accurately addressed by this project, by providing appropriate solutions and remedies that are derived exclusively from a number of diverse technologies, protocols, design and structures. The following plan has been used to manage and evaluate the project. Moreover, the primary assumptions affecting the project plan are also documented below:\n\n  * What is an Electronic mail?\n  * Email programs\n  * Server Requirements\n  * Architecture and Services\n  * Messaging Protocols\n  * Message Formats\n  * Windows Server 2003 as a Messaging solution\n  * Supporting technologies\n  * Internet Black and White Lists\n\nMoreover, the project also deals with how envelopes and message are distinguished into paper mail and electronic mail. A diagrammatic representation of how current technologies have been deployed and put to use for the development of electronic mails systems and their enhanced features to meet all possible requirements of the user and to offer them an excellent experience of emailing and communication.\n\nThe project report also presents the numerous benefits electronic mails have on user all across the globe backed by the efficiency in operation of the electronic mail servers. Significantly, the format of the email messages is also spoken of in this project report. These message formats have been evolved along with the evolution of electronic mails, and thus becomes much more beneficial for a user to precisely specify how the e-mail should look when received by the recipient, that is, the structure of the body of the email, how easily it may be read by the receiver, and finally, to whom all the message should be reached. This project is tested on the client side components such as Microsoft Outlook and Outlook Express. Supporting technologies which enhance the electronic mailing services within Windows Server 2003 is also elaborated in this documentation. Additionally, it also talk about its benefits in an organization that requires additional functionality as well as collaboration capabilities who aim at utilizing these supporting technologies.\n\nA brief but a descriptive illustration of the concept of Internet black and white lists is also presented in this project report. The problem of back lists and white lists are addressed by presenting an effective solution that can be comprehended by users of all types. Apart from this, the research paper examines the major causes of failure encountered by enterprise email systems, together with offering preventative measures in order to reduce the percentage of unplanned email outages (IBM, 2006). These failures are directly related to Storage Area Network, Hardware server, Database corruption, Connectivity losses, and Natural disasters. This research paper also talks about the various security measures to be taken by users and network administrators while using the e-mail services. Techniques to be adopted by users to avoid misuse of any of their confidential information, attacks and viruses the emails are prone to, and security measures users must undertake, are covered in this research paper. The project is developed to be used by a computer user who uses emails to send mails and messages to numerous recipients across the world, network administrators who will deploy the project at the company servers or on standalone PCs, and any networking personnel who are capable of dealing with and expertise in electronic mail servers.\n\nExecutive Summary\n\nElectronic mails or e-mails have lately become the most permeate kind of business communication. Emails have been enforcing a deeper impact on very aspect of enterprises, small or big, thereby enhancing the quality of communication amongst employees, management executives, clients, vendors, stakeholders, as well as business partners. E-mail systems continue to degrade in performance, even when large-scale enterprise investments are made in replication, tape back up systems, etc. Furthermore, as it is known that man-made as well as natural disasters may result in e-mail outages, new research studies have shown that e-mail systems are usually turned down by failures in technologies (IBM, 2006).\n\nThis research paper not only gives an in-depth study of what email servers and systems are, but also covers their features, and underlying protocols that supports their operations. The paper also studies the major causes of email system failures at enterprise level and also provides an efficient guidance in order to reduce the probability of the occurrences of such unintentional e-mail outages. The research paper aims at addressing the practical problem of sending out messages to various e-mail addresses by entering a list of email addresses separated by comma, in the \u2018To\u2019, \u2018CC\u2019 and \u2018BCC\u2019 fields of an email message. The very first email systems consisted of file transfer protocols, which required the convention that the first line of the message should contain the recipient\u2019s address. However, as time passed, the limitations of this approach became even more obvious (IBM, 2006). Therefore, this research study was written to provide an in-depth study of the developments of the e-mail systems, requirements of an e-mail server, and the protocols and technologies on which an e-mail server depends.\n\nIntroduction to Electronic mail systems: The Internet electronic mail system is stated as one of the most essential and important resources in the Internet services. With the e-mail system, one can exchange electronic mail with people across the globe. Moreover, it offers an economical fast and a convenient way to send messages to colleagues, friends, and family.\n\nSpeed: E-mails are much faster as compared to traditional mails. An e-mail message is capable of traveling around the world within minutes.\n\nCost: When an Internet user pays a service provider for a connection to the Internet, no extra charge is applied for sending and receiving e-mails. Moreover, an Internet user does not have to pay extra charges even if they send a long message to a recipient across the globe. Exchanging e-mails can possible save you money over long distance calls as well. Convenience: One does not need to buy and stick stamps for e-mails and drop the emails into one\u2019s mailbox. Only an e-mail program is required to help a user send, receive, and manage their emails (Lewis, 2008) (Kawamata & Jiro, 2001) (The FreeBSD Documentation Project, 2010). An electronic mail server is a computer which moves and stores mails across networks and over the Internet. In other words, in order to enable a computer to become an electronic mail server, the computer should have the \u2018mail server\u2019 software installed in it. Moreover, a mail server is capable of sending as well as receiving mails.\n\nFor sending mails, the mail server software employs SMTP, short for, Simple Mail Transfer Protocol (Lewis, 2008). The working of SMTP is explained below:\n\n 1. The mail client that is the software used for composing or creating the email, such as Outlook Express, sends the mail to the mail server which is already configured within the software package of the mail client. The SMTP server configuration stores the mail server\u2019s address.\n 2. The SMTP server analyzes the \u201cTo\u201d, \u201cCc\u201d, and \u201cBcc\u201d field addresses in order to find out which server the email is supposed to reach.\n 3. Eventually, the SMTP server sends the e-mail to the destination mail server.\n\nDue to the latest trend of spam mails, certain SMTP servers are getting increasingly sophisticated. Indeed, some SMTP servers examine the \u2018From\u2019 field address for making sure that the email address contained in it is a legitimate one for that particular domain prior to sending the email (Lewis, 2008).\n\nSMTP server contains three unique kinds of authentication techniques:\n\nAnonymous Access: This entails that no username or password is needed\n\nBasic Authentication: A user account and password is needed, and is sent out as plain text across the network. Moreover, it is suggested that TSL, Transport Layer Security, encryption be utilized with basic authentication for avoiding unauthorized identification of user names as well as passwords.\n\nIntegrated Windows Authentication: This feature entails that the user account and password extracted from the Active Directory is utilized during the authentication processes. At the receiver end, in order to receive the email, the mail server software uses either POP3 (Post Office Protocol 3) or IMAP (Internet Message Access Protocol). Working of POP3 is as follows:\n\n 1. The POP3 server receives the e-mail originating from an SMTP server\n 2. Then, the POP3 server analyzes the \u201cTo\u201d, \u201cCc\u201d, and \u201cBcc\u201d field addresses in order to find out which server the email is supposed to reach.\n 3. The POP3 server then sends the mail to the \u201cinbox\u201d or \u201cmailbox\u201d for that address, only if the recipient email address is legitimate for that domain. However, if it is not a legitimate address, the POP3 server sends back an error message to the SMTP server.\n 4. Finally, when the user opens their mail client such as Outlook Express, the in-built software package checks the POP3 server for new email. Then, the address of the mail server may be seen within the configuration of POP3 server (Lewis, 2008).\n\nPOP3 allows three distinct types of authentication services in order to verify user connections at the POP3 services.\n\n 1. Local Window Account Authentication: This method is used if the server is not included as a part of the Active Directory, or if the user accounts are required to be stored at the server at which the POP3 services reside\n 2. Encrypted Password File Authentication: This method is used whenever the server is not utilizing Active Directory, or if the user accounts are required to be stored at the server at which the POP3 services reside\n 3. Active Directory Integrated Authentication: The e-mail server is a domain controller or a member server that is included in the Active Directory domain. Furthermore, IMTP is the latest protocol being used to receive emails, and is used by several mail server software packages in place of POP3 for receiving mails.\n\nA newer and broadly-used function of certain mail server packages is the Mail Filtering feature. These are the mail servers that function as \u2018spam detectors\u2019, which analyze the inbound mail and inspects whether or not the mail is \u2018spam\u2019. Significantly, the aspects for determining whether the mail is \u201cspam\u201d are based on the package. Common factors are: keywords, letter combinations, the originating domain, etc (Lewis, 2008).\n\nE-mail Programs: Commonly, there are certain popular email programs including Eudora Light, Outlook Express, Netscape Mail, etc. Al of these programs contains almost the same features, and allows a user to create, send, receive as well as organize their messages (Kawamata & Jiro, 2001).\n\nHow e-mails get around the Internet\n\nArchitectures and services\n\nThis section provides an overview of what exactly e-mail systems can do and the ways in which they are organized. E-mail systems typically comprises of two sub-systems: the user agents and the message transfer agents. The user agents allow people to read as well as send e-mails. The message transfer agents are responsible for moving the email messages from origin to target address. The user agents are nothing but local programs which offer a menu-based, command-based, or graphical method in order to interact with the email system. On the other hand, the message transfer agents are typically system daemons, which are processes running in the background. Their main role is to move emails through the system (Tanenbaum, 2005) (The FreeBSD Documentation Project, 2010) (AOM Software, 2010).\n\nTypically, e-mail servers or systems perform five fundamental functions.\n\n 1. Composition: It refers to the process of creation of messages and answers for those messages. Even though any general text editor may be utilized for the message body, the system itself is capable of providing assistance for addressing and the several header fields that are attached to every message. For instance, while answering a message, the email system may extract the sender\u2019s address from the incoming email and can automatically insert it at a proper place within the reply.\n 2. Transfer: It refers to the process of moving messages from the sender/ originator to the receiver. In large parts, this needs a connection to be established with the destination or some intermediate machine, producing the output of the message, and releasing the connection. Furthermore, the email system must perform this work automatically, without disturbing the user (Tanenbaum, 2005).\n 3. Reporting: Reporting deals with informing the sender of the message as to what happened to the message. Questions such as Was it rejected? Was it delivered? Was is lost?, etc are answered by the reporting process. Moreover, numerous applications are there wherein confirmation of delivery of the message is not only important but also necessary and may have legal significance.\n 4. Displaying: This function deals with displaying the incoming messages that is needed so that user can read their emails. Most of the times conversations are required or a special viewer should be invoked, for instance, if the message is a digitized voice or a PostScript file. Simple conversations as well as formatting are also sometimes attempted.\n 5. Disposition: Disposition is the last step and is related to what the receiver does with the message after he/she receives it. In this, possibilities include discarding it after reading, discarding it before reading, saving it, so on and so forth. Moreover, it must be possible to retrieve and re-read the saved messages, or forward them, or process them in other manners (Tanenbaum, 2005).\n\nIn addition to these fundamental services, certain email systems, particularly internal corporate ones, offer a variety of advanced features. Whenever a person is moving are they are away from their workplace for a period of time, they may wish to forward their emails; hence the system must be capable of doing this automatically. Indeed, almost every system allows its users to create mailboxes in order to store incoming emails. Commands are needed for creation and destruction of mailboxes, inspecting the contents of mailboxes, inserting and deleting messages from mailboxes, etc. Furthermore, corporate managers frequently find it necessary to send messages to each other and to each of their subordinates, clients, customers, or suppliers. As a result, a technique of mailing lists was developed, which is nothing but a list of email addresses. Whenever a message is sent out to the mailing list, identical copies of it are delivered to every recipient on the list. In essence, other advanced features include carbon copies (CC), blind carbon copies (BCC), secret or encrypted email, high priority email, and the power for secretaries to read as well as answer their bosses\u2019 emails (Oricode, 2010) (Vleck, 2008) (Tanenbaum, 2005) (The FreeBSD Documentation Project, 2010). Emails are now being widely used within organizations and industries for intra-company communications. For this, it allows for far flung employees to communicate and cooperate on complex projects, even across many time zones. Furthermore, by eliminating most cues that are associated with age, rank, sex, etc, email debates are likely to focus on thoughts and ideas, and not on corporate status (Oricode, 2010) (AOM Software, 2010).\n\nParticularly, a primary idea fundamental to email systems is the distinction between the \u2018 envelope \u2019 and its contents. The envelope is used to encapsulate the message. The envelope contains all the important information required for transporting the message, like the destination address, security level, priority, all of which are unique from the message itself. The envelope is used by the message transfer agents for routing, similar to the working of a post office.\n\nEvery email is divided into 2 sections, namely the header section and the body section. The header part defines the control information for the user agents, such as the source and destination of the message, the time it was mailed, the person who created the message, etc (Tanenbaum, 2005). Next, the body of the message is totally for the human recipient and contains the main content of the message that may be plain text, or Multipurpose Internet Mail Extensions (MIME). MIME permits a sender\u2019s mail server to accurately encode 8-bit binary files by using a mail system which supports only plain text. Moreover, a MIME enabled mail server first encodes binary data as text and the recipient\u2019s mail server then decodes the text back to a binary file (Oricode, 2010).\n\nMessage formats\n\nRFC 822: Email messages contain a primitive envelope, a number of header files, a blank line, and a message body. Every header field logically comprises of a line of ASCII text that contains the field name, a colon and a value. The primary header fields referred to the message transport is as follows:\n\nTo: this field indicates the DNS address of the primary receiver/ recipient. This can have multiple recipients also (Tanenbaum, 2005).\n\nCc (Carbon copy): This field specifies the email address of a secondary recipient (Tanenbaum, 2005). However, with respect to delivery, there is no difference between the primary and the secondary recipient.\n\nBcc (Blind carbon copy): This filed is similar to the Cc field, with only difference of the line being eliminated from every copy of original message being sent to the primary receiver and the secondary receiver (Tanenbaum, 2005). Moreover, this field enables users to send out copies of the original message to third party recipients without the primary as well as secondary recipients knowing about the action.\n\nFrom: Indicates who created the message.\n\nSender: Indicates who sent the message; contains the e-mail address belonging to the actual sender. The \u2018Sender:\u2019 field is different from the \u2018From:\u2019 field (Tanenbaum, 2005). Received: The line that contains this field is included by the message transfer agent during the transfer route to the destination. This line stores the agent\u2019s identity, the date and the time when the message was received, together with other additional information which may be used for detecting bugs within the routing system.\n\nReturn-Path: The final message transfer agent adds this field and indicates how the recipient can get back to the sender. Theoretically, this data may be derived from every \u2018Received:\u2019 header; however it is hardly filled in and only holds the sender\u2019s address (Oricode, 2010) (Tanenbaum, 2005).\n\nServer Requirements\n\nA standalone computer that is running as a mail server must have ample disk space in order to store the messages. In cases where the computer is running GUI-intensive mail server software, such as an amount of commercial products, then the computer must have high memory and a high speed processor. For instance, the system requirements list of Microsoft Exchange Server indicates that the machine must have a 733 mHz processor, minimum of 1 GB disk space, and 512 MB RAM (Lewis, 2008) (AOM Software, 2010).\n\nCost: An issue\n\nCost could be an issue for a small business or a home user. In this case, there exist several open-source solutions which cost little to no money. For instance, the Apache servers consist of mail servers which are free of cost, and they also run on almost every common platform such as Windows, Linux, and UNIX. However, the disadvantage of these \u201ccheap solutions\u201d lies is their configuration is not as easy and do not have additional features in comparison to their commercial counterparts (Lewis, 2008) (Louwrens, 2003).\n\nWindows Server 2003 as an efficient messaging solution\n\nBoth Exchange Server and Windows Server 2003 offer POP3 functionality to their users. Nonetheless, in Windows Server 2003 just the fundamental POP3 functions are provided to the user and administrator. This involves only a limited amount of messaging experience with availability of administrative features. Therefore, POP3 services are stated to be most ideal for small-scale organizations that require only the basic administration and messaging functionality. Additionally Exchange Server offers an enhanced collaboration and messaging environment to the administrator and the user. It also supports several Internet protocols. Microsoft introduced the E-mail services in Windows Server 2003. As aforementioned, the basic configuration needed for loading and running the email services includes the services of SMTP and POP3 (Louwrens, 2003) (AOM Software, 2010).\n\nSupporting Technologies\n\nSeveral supporting technologies are available in Microsoft technologies that enhance the emailing services in Windows Server 2003. Organizations that require additional collaboration capabilities and functionality utilize these technologies.\n\n 1. Free/ Busy information: The publishing of \u2018Free/busy\u2019 notification on a website is possible with Microsoft Outlook (Louwrens, 2003).\n 2. Public Key Infrastructure: For enabling secure mail communication with an organization, a PKI (Public Key Infrastructure) should be implemented. PKI certificates to digitally sign the mail messages prior to sending, is assigned to users. Mail communications among organizations need more complex security, so a third party certification authority is recommended. S/MIME is used at the client side in order to ensure that the mail is digitally signed as well as encrypted before sending.\n 3. Additional feature such as Windows share-point services, scalability features, and real-time communication server is also employed (Louwrens, 2003).\n\nInternet Black Lists and White lists\n\nTwo of the most damaging and least effective techniques for combating spam are the white lists and the black lists. Commonly, these lists intend to harm innocent computer users and avoid crucial business emails from being delivered. If the user is sending emails from an email server residing on their computer, and the IP address is resides on one of the lists, then that may affect the user in two opposing ways:\n\n 1. The messages will not be delivered if the email server of a recipient verifies the IP addresses of inbound connection with those contained in the white and black lists\n 2. If the message successfully reaches the recipient, then the recipient is able to run anti-spam software which employs block lists in order to group the messages into spam mails. The sender\u2019s email may be seen residing in a folder of spam mails or be permanently discarded and never be accessed by the recipient.\n\nBlack Lists\n\nA black list is nothing but a list of spam users containing addresses as well as domains having spam email servers which the sender is aware of (Oricode, 2010). This list is used for blocking every email that arrives from particular servers across the Internet which is can be used to send spam mails. Examples of black lists are SpamCop and Open Relay Database. Almost every anti-spam product has its own black list (Oricode, 2010).\n\nWhite Lists\n\nThese are exactly opposite in nature and functionality to black lists. White lists are a list of known, authorized and trusted email addresses as well as domains which are invariably allowed to send out emails, irrespective of the content in it. These lists are used to complying the senders to authenticate their identity before the delivery of emails, to the recipient (Oricode, 2010).\n\nConclusion\n\nTherefore, the research study provided in this paper addresses every aspect related to the functioning of electronic mail systems. The topics covered in this paper clearly represent the in-depth study backed by the underlying technology and protocols, employed by the email systems. It also talked about how email systems are beneficial by presenting a detailed study of its operations and configuration on different clients. The research paper also allows its readers to gain knowledge of the requirements of an email server, what are the precise uses of the fields in the email message format, and what they should do if cost is a major issue while using mail servers in their business or home and personal use. The paper also accurately addresses the problem of messages can be sent to several recipient at the same time through the use of \u201cTo\u201d, \u201cCc\u201d and \u201cBcc\u201d fields. The paper advices users that this problem can be solved by not sending out email messages to recipients to whom the messages are not personally addressed. Moreover, the paper also talked about the additional technologies that support electronic mail systems and servers, and the concept of black lists and white lists that are used for sending out emails to the concerned recipient by having knowledge of their identity. To conclude, e-mail is short for electronic mail or message that is sent between two devices. Besides computers, e-mails can be sent as well as received from portable and mobile devices like PDAs, cell phones, etc. With e-mails it is possible to communicate through personal and work-related mails having attachments for documents, photos, audio and video. Electronic mail systems are the simplest form of communication in today\u2019s world and almost every person having access to the Internet depends on e-mail systems to send messages and stay in touch with the family, friends and business partners, across the world.\n\nReferences\n\nAOM Software. (2010). Category Communications / E-Mail Clients . Web.\n\nIBM. (2006). Why E-mail Fails: A Survey of E-mail Outages . IBM Global Technology Services. Web.\n\nKawamata & Jiro. (2001). Electronic mail server in which electronic mail is processed . United States Patent. Web.\n\nLearntheNet.com. (2010). How E-mail Works . LearntheNet.com. Web.\n\nLewis, J. (2008). Mail Servers . Scribd.com.\n\nLouwrens, F. (2003). Implementing the Windows Server 2003 E-Mail Services . Inobits Consulting.\n\nOricode. (2010). Sending to Multiple E-mail Addresses . Oricode. Web.\n\nTanenbaum, A. (2005). Computer Networks (4th ed.). New Jersey, USA: Pearson Education.\n\nThe FreeBSD Documentation Project. (2010). Electronic Mail . The FreeBSD Documentation Project. Web.\n\nVleck, T. (2008). The History of Electronic Mail . Web.\n",
        "label": "human"
    },
    {
        "input": "Current Laws and Acts That Pertain to Computer Security Research Paper\n\nAbstract\n\nSince the advent of networking of computer systems through the intranet and internet, security of data and information has always been threatened by unauthorized access, use and modification of data. A weak computer security cannot only affect government and state security but could also cause the collapse of the economy. As new threats continually crop up and devised by skilled computer hackers and individuals who wanted to earn or simply disrupt a specific computer network, the government should double its time in passing new laws that would deter attack on computer networks.\n\nIt occurs for a number of times that government prosecutors find it difficult to prosecute apprehended computer offenders due to lack of appropriate laws to cover specific acts. It is only after the fact of commission that the government can react and pass laws that would address such act. The government is merely reactive to circumstances when passing computer security laws but it can be proactive and pass laws that would cover possible computer violations and intrusions.\n\nThis paper will present the laws and acts enacted by Congress that would penalize cyber crimes and strengthen the computer networking system. Although the focus would be current laws involving computer security, previous laws shall also be cited to provide a historical perspective on the development of the laws and acts. Moreover, later laws are passed to cover issues that were not addressed by the previous laws. Cases of computer breach will also be cited to show how they affect legislation.\n\nIntroduction\n\nThe computer system is in constant threat from various sources, individuals, groups and even other governments. The computer system can be attacked internally, by unauthorized users who may be employees of the company, and externally (by hackers who want to steal information or simply to disrupt the operating system or program). With the rise of the internet, voluminous valuable data and information pass through international boundaries that involve commercial, banking and financial transactions. Intelligence and defense information accessed or damaged by unauthorized persons can disrupt the stability of a country.\n\nThe internet has become part of everyday life such as email messaging and online purchases (Smith, Moteff, Kruger, Seifert, Figliola & Tehan 2005). Retail purchases in November 2004 were done online with 69 percent using broadband and 31 percent through dial-up (Nielsen/ /NetRatings, 2005, as cited in Smith, Moteff, Kruger, et al., 2005). Out of the total retail sales of $938.5 billion for the fourth quarter of 2004, $18.4 billion comes from e-commerce retail sales (U.S. Census Bureau, 2004, as cited in Smith, Moteff, Kruger, et al., 2005).\n\nComputer security is associated with vulnerability of a computer while connected with a network of computers (Kinkus, 2002). Computer security has three areas of concern (referred to as CIA) that should be addressed: a) confidentiality (access only by authorized users), b) integrity (protection of information from unauthorized changes which are not detected by authorized users, also refers to privacy), and c) authentication (verification of users) or availability (access to information by authorized users (Kinkus, 2002). Privacy of user information is the most important of the technical areas (Kinkus, 2002) since personal data must not be shared unless the user consents thereto.\n\nPieces of information of the user can be taken from various sources that can give a holistic search habit of the user (Kinkus, 2002). The user must have complete control of the information provided, the purposes it is used and who can use it (Kinkus, 2002). Breaching these technical concerns is considered a crime in several jurisdictions and referred to as cyber crime.\n\nContext of the Problem\n\nCyber crime refers to activities committed using a computer intended to harm a computer and network (McConnell International 2000). Computer crimes have gained international attention but laws against such acts are unenforceable in other countries (McConnell International 2000). The absence of legal protection can only mean that establishments have to implement technical protection to hinder unauthorized access or prevent destruction of information (McConnell International 2000).\n\nThe commission of cyber crimes continue to increase but victims of illegal access prefer not to report them since it would expose their technical weakness, the possibility of commission of \u201ccopycat crimes,\u201d and loss confidence by the user to the system (McConnell International 2000, p. 1). It is incumbent upon the government to provide sufficient protection to public and private computer network and system to avert huge financial losses and damage through appropriate regulation and passage of laws.\n\nProblem Statement\n\nThe rise of the internet paves the way for a new mode by which to communicate and conduct commercial transactions. Valuable information stored electronically is also transmitted through this technology. Along with this development, individuals with criminal minds find this technology lucrative to prey upon. They continually seek ways to commit offenses either to earn or simply damage a system or information. They look for weaknesses in the computer system so that they can break into them. The government continually addresses the problem of computer security through the passage of laws that would penalize certain internet activities and regulate the system through guidelines and standards. But cyber crimes are not deterred by the laws and still occur. The government should pass laws that would totally eliminate the commission of internet crimes.\n\nHypothesis\n\nThe laws and Acts passed by the government have sufficiently provided security to computer systems and maintain privacy of information against unauthorized intrusion, access and damage.\n\nResearch Questions\n\n  * What are the cyber crimes that affect computer security?\n  * What are the laws and Acts passed by the government to bolster computer security and protect information against illegal access and damage?\n  * How much damage and loss do cyber crimes have upon the computer network and resources?\n  * Did the laws and Acts deter the commission of cyber crimes?\n\nTerms and Definitions\n\nAct \u2013 statute passed either by the Federal Congress or State Congress. All statutes generally fall under the term law.\n\nArtefact \u2013 same as artifact. The term used by social constructionists when referring to a technological device.\n\nCALEA \u2013 Communications Assistance for Law Enforcement Act of 1994.\n\nClosure \u2013 in SCOT, it is a stage wherein the meanings attributed to an artefact stabilize and further innovation to the device ceases.\n\nComputer \u2013 a machine consisting of hardware, software, peripherals and accessories. It needs a software consisting of programs and applications in order to function as intended.\n\nComputer Security \u2013 refers to the implementation of standards and guidelines, the technical and software applications that would protect the computer system, as well as the information stored and transferred electronically from one computer to another.\n\nComputer System \u2013 comprises the hardware, software, and interconnection that enable transfer of information and communication through electronic gateway.\n\nCOPPA \u2013 Children\u2019s Online Privacy Protection Act.\n\nCyber crime \u2013 crime committed upon a computer system or database with the use of computer.\n\nECPA \u2013 Electronic Communications Privacy Act of 1986.\n\nFACT \u2013 Fair and Accurate Credit Transactions Act.\n\nFCC \u2013 Federal Communications Commission.\n\nFCRA \u2013 Fair Credit Reporting Act.\n\nFederal law \u2013 statute or Act promulgated by the federal Congress.\n\nFISMA \u2013 Federal Information Security Act.\n\nGLBA \u2013 Gramm-Leach-Bliley Act.\n\nInternet \u2013 interconnectivity of computer systems around the globe.\n\nIntranet \u2013 networking a series of computers within a closed system or a single organization. A firewall protects the system from outside access.\n\nLaw \u2013 generic term that includes statutes, Acts, presidential issuances, etc. passed by federal or state governments and other government institutions authorized to pass such issuances.\n\nNIIPA \u2013 National Information Infrastructure Protection Act of 1996.\n\nRelevant social group \u2013 a group of users in society that exerts some influence upon the development of technology and ascribes meaning to the artefact.\n\nSCOT \u2013 Social Construction of Technology.\n\nLimitations\n\nThe materials included in this paper are sourced out from internet websites that provided commentaries on computer security laws, copies of the laws themselves and news items. All the laws are public documents and are readily available for public use on the net. Since the laws and articles that would be available on libraries can also be located on the internet, this student availed of the latter mode to search for data. No statistical correlation is included in the paper except the presentation of figures that correspond to damage or loss caused by cyber crimes.\n\nDelimitations\n\nThe laws and Acts passed by the State Congress vary as there are a number of states in the US. Such laws and Acts have different contents, and requirements. Therefore, they are excluded in the discussion of this paper. State laws also have different definitions of specific acts as well as conditional requirements for the laws apply, therefore, they are intentionally not discussed in the paper.\n\nAssumptions\n\nFrom the numerous laws and issuances passed by the federal government and institutions, this student assumes that they have adequately addressed the need to protect the computer systems. The government is doing its utmost in order to maintain the integrity of the computer infrastructure and protect valuable information from passing to unscrupulous individuals preying upon any weakness in the computer system.\n\nTheoretical Support\n\nPrivacy is a socially constructed value that should be upheld for being the foundation of other rights of an individual such as freedom, rights to property, right to associate, etc. (Levine, 2003). Privacy extends to computer systems that stored personal information (in digital form). Banks, hospitals, and other commercial firms possess personal information of who availed of medical, financial or banking services. Technology develops within society to meet specific needs of individuals in the community.\n\nThe Social Construction of Technology or SCOT is a theoretical framework that views a social group as an active participant in the construction of technology (Bijker, 1995, as cited in Engel, 2006). SCOT is the first constructivist outlook that views development in technology as a \u201csocial process that shapes society and is shaped by society\u201d (Engel, 2006, p. 2). Technology develops in response to a perceived need of society. The users in that society react to the technological development or innovation. SCOT is also utilized in exploring the issues concerning anonymity of users, online payment, security and privacy (Phillips, 1998).\n\nThe users as relevant social groups are not passive end-users but participate actively to further innovate technology (Engel, 2006). Different social groups give different meanings to an artefact (i.e. technological device) that allows for the different forms of the device (Bijker, 1995, as cited in Engel, 2006). It is when a dominant meaning prevails that flexibility of forms slows down until a closure occurs (Bijker, 1995, as cited in Engel, 2006). The users are capable of influencing the development of the technology through the different meanings attributed to it that gives different forms and thereby contribute to the construction of the technology (Engel, 2006).\n\nThe computer and internet technology can have different meanings for various users (Engel, 2006). They can use the technology according to the meaning they ascribe to the technology. Thus, one group may use it for social networking, another for remote teleconferencing, or for banking services. However, a group of users can attribute a meaning to the device which is to inflict damage or gain profit.\n\nThere is a constant shaping between society and technology (Bijker, 1995). The computer system developed as a stand-alone machine. Later, the computer was able to connect with other computers through a network of cables within a closed system. The internet allowed interconnection with other computer systems across boundaries to other organizations. In all the stages of these developments, the users exert some influence (Engel, 2006). The meanings of a technological artefact in a developed country may not differ from the meanings of the relevant social groups in developing countries since the former can transfer the meanings together with the artefact (Engel, 2006).\n\nThere has been continuous innovation being introduced into the artefact with the two-way influence of technology and society. At present, there is no stabilization of the meaning or closure since user groups continually introduce changes into the device. The user group that causes damage or loss to the computer system continually challenges the security setup of the computer and find new modes by which to break into it.\n\nThe government as another user group, has to pass laws and Acts that would criminalize activities that infiltrate the computer system since it violates privacy and confidentially, as well as profit from illegal activities. The laws also impose a certain fine for the damages caused to institutions infiltrated. As new acts are perpetrated against computer security, the government must cope up with new laws that would properly define such acts so that the offender can be prosecuted. The offender that causes the damage and loss should not only be sanctioned with fines but be penalized under the criminal justice system since the extent of the damage is widespread with pecuniary loss reaching billions of dollars.\n\nThe government also prescribes standards and guidelines for organizations that store information and offer online services to the public to strengthen their computer security and which should be regulated by government agencies to ensure compliance. With the interplay of the various user groups in society, the consumers, the organizations offering financial or banking services, organizations that hold personal information (e.g. hospitals), the software programmers, the hackers and offenders, and the government, the artefact changes in order to make computer security invulnerable to cyber attacks. While the programmers make new programs to hinder existing threats, on their own because the product software can be sold to users or through the prodding of an existing client that used the software company\u2019s application to run the information management of the client, the government must seek ways through standards, regulation and laws so that computer security can be strengthened.\n\nSince computer systems within the US can be accessed via the internet by offenders in other jurisdictions, social construction occurs on a global level. That is why great powers as well as established international organizations encourage all countries to codify their laws to cover cyber crimes so that prosecution would be facilitated on all fronts, locally and internationally. On the global scale, the user groups would include the states and countries, international bodies, and international corporations.\n\nSignificance of the Study\n\nThis study is a great contribution to existing studies that explore the effectiveness of laws passed to address the problem on computer security. There is no study identified that addresses the effectiveness of the laws in deterring cyber crime. Thus, this paper can provide the groundwork for future studies concerning this area of research.\n\nResearch Design and Methodology\n\nThis paper used the quantitative research design and methodology in exploring the impact of the laws and Acts passed on deterring cyber crimes. The research design is non-experimental wherein no variables are manipulated but only establishes the relationship between the variables (Belli, 2008). The variables \u2013 the laws and cyber crimes \u2013 are analyzed as they exist because they cannot be manipulated (Belli, 2008). Literature, laws and Acts, and available statistics are included in the research to determine if the laws are able to maintain the integrity of the computer systems and information and the newer offenses not yet addressed by legislation.\n\nOrganization of Study\n\nData will be gathered from available literature on the internet on the kinds of cyber crimes already addressed by law. Also to be explored are the activities that affect computer security but cannot be prosecuted criminally because they are not defined as crimes by existing law. The extent of damage, frequency of commission of cyber crimes and cost of loss will be correlated with the laws already passed in order to determine if specific crimes are deterred.\n\nTypes of Cyber Crimes: Damage, Loss and Prosecution\n\nThe Federal Bureau of Investigation has a four-fold mission to counter cyber crime, namely: a) \u201cto stop those behind the most serious computer intrusions and the spread of malicious code,\u201d b) \u201cto identify and thwart online sexual predators who use the Internet to meet and exploit children and to produce, possess, or share child pornography,\u201d c) to counteract operations that target U.S. intellectual property, endangering \u2026.. national security and competitiveness,\u201d and d) \u201cto dismantle national and transnational organized criminal enterprises engaging in Internet fraud\u201d (U.S. Department of Justice, n.d., para. 1). These FBI objectives reflect the common illegal acts committed on the internet.\n\nThere are a number of computer or cyber crimes that can impact upon privacy and invades the computer system illegally. Hacking is infiltrating a system without authorization to access confidential information, or entering into a transaction under false representation (Go, 2009). In phishing, spurious emails are sent to a user with links that leads the user to a fake website (presented as an authentic or real website of a company) that would extract username, password or credit card data (Go, 2009). Pharming is an online fraud that redirect users to a fake website that looks authentic in order to steal relevant information (Online Fraud: Pharming, 2010). The user who wants to access a website is redirected to the fake website without the user knowing it, even if the correct web address is entered into the browser (Online Fraud: Pharming, 2010).\n\nCreation and deployment of viruses (programs that replicate themselves) that can cause harm to the computer system without the knowledge of the user (Go, 2009) is a common cyber crime. A virus is a software program attached to a file (e.g. document, excel) to spread to the computer system (Kutner, 2001). The virus runs once the file is opened and then attaches itself to other programs and replicates itself (Kutner, 2001). An email virus is attached to an email that reproduces itself by sending emails automatically to everyone stored in the email address book (Kutner, 2001). There is also the worm that uses the internet to find vulnerable servers wherein it can reproduce (Kutner, 2001). The Trojan horse presents itself as a game or other program that can delete hard drive contents or block the screen with some graphics (Kutner, 2001).\n\nIn identity theft, the criminal takes money, receives benefit or purchases goods using the identity or credit card of another person (Go, 2009). Identity theft is carried out by cyber criminals through phishing and pharming (Brody, Mulig, & Kimball, 2007). Cyberstalking (usually preys on women and children) is a crime wherein the criminal stalks a person by sending emails and threats as well as dissemination of false information (Go, 2009).\n\nAs reported by the U.S. Uniform Crime Reporting Statistics, there are more than 300 million internet users (starting year 2000) worldwide with 1 million of them engaged cyber crimes (Computer Crime \u2013 Definitions, 2010). As of 2004, $30 billion has been used in the maintenance of computer security (Computer Crime \u2013 Definitions, 2010). In the survey conducted by the Computer Security Institute (CSI) and the FBI with 538 private and government institutions surveyed, it was reported that as of the year 2000, 85 percent experienced breaches in security (Computer Crime \u2013 Definitions, 2010).\n\nThe breaches caused financial loss to 65 percent of the respondents while 35 percent (186 firms) quantified its losses to a total of $378 million (Computer Crime \u2013 Definitions, 2010). Three hundred seventy seven (377) respondents said that the breaches occurred through internet connectivity (Computer Crime \u2013 Definitions, 2010). Internal attacks are committed by disgruntled and terminated employees (Computer Crime \u2013 Definitions, 2010). Organized crime groups even recruit telecommunication experts to \u201ccommit fraud, piracy, and money laundering\u201d (Computer Crime \u2013 Definitions, 2010, para. 3).\n\nOne of the first to be prosecuted under the Computer Fraud and Abuse Act is Robert T. Morris (Cornell University student) who deployed a worm to show the vulnerability of computer security but miscalculated the speed the worm replicated itself that by the time he publicly released the instruction on how to kill the worm, it had infected around 6,000 computers causing them to crash (Computer Crime, 2010). The damage suffered was $200 to a maximum of $53,000 for each organization (Computer Crime, 2010).\n\nA computer science student created a virus that momentarily disrupted the operations of military network and contractors, as well as universities in 1988, although no files or data were destroyed (Gerth, 1988). This case is unprecedented without any previous case being prosecuted (Gerth, 1988). The Secret Service admitted difficulty in investigation because numerous computers were affected (Gerth, 1988). Smith, Moteff, Kruger, et al. (2005) stated that the expanse of the problem on computer security cannot be known.\n\nA gang of hackers (called Masters of Deception) was also prosecuted and indicted in 1992 under the Computer Fraud and Abuse Act for unlawfully obtaining computer passwords, illegal possession of long-distance call card numbers and wire fraud (Computer Crime, 2010).\n\nPhishing activities and related fraud reached $1.2 billion annually with around 57 million US citizens targeted in 2004 (Phishing, n.d.). The bill (Anti-Phishing Act of 2005) proposed by US Sen. Patrick Leahy that aims to penalize phishing and pharming with a maximum fine of $250,000 and maximum imprisonment of five years (Phishing, n.d.) was never passed to become a law (S. 472\u2013109th Congress, 2005).\n\nFederal Laws on Cyber Crime\n\nThe federal government generally does not regulate the security of private computer systems but merely requires protection of specific information under the control of private systems against illegal access and dissemination (Moteff, 2004). Even the control of domain name (Domain Name System or DNS) has been transferred from the federal to the private sector (Smith, Moteff, Kruger, et al., 2005).\n\nThe enacted of the Counterfeit Access Device and Computer Fraud and Abuse Act in 1984, the first computer crime law, criminalizes infliction of damage to computer systems, networks, hardware and software, and makes wrongful the act of obtaining financial and credit data protected by statutes (Computer Crime, 2010).\n\nThere are laws enacted to protect privacy and personal information held by the government and private institutions such as the Gramm-Leach-Bliley Act (specific provisions under Title V) (Moteff, 2004; Smith, Moteff, Kruger, et al., 2005), Health Insurance Portability and Accountability Act of 1996 (specific provisions under Title II), and the Sarbannes-Oxley Act of 2002 (mandates accounting firms to certify integrity of their control systems as part of the annual financial reporting requirements) (Smith, Moteff, Kruger, et al., 2005). The privacy concern is confined to financial information (under the Gramm-Leach-Bliley Act, Title V) and medical information (under the Health Insurance Portability and Accountability Act of 1996) (Moteff, 2004). The Secretary of Health is authorized to prescribe the standards to be used in the protection of medical information (Moteff, 2004).\n\nUnder the Health Insurance Portability and Accountability Act of 1996, healthcare institutions must comply with the standards set by the Secretary to ensure the confidentiality of medical information and records which are transferred electronically (Fogie, 2004). Development of standards on financial control under SOA and enforcement of the same is done by the Security Exchange Commission who has authority to prescribe standards and enforce these regulations (Moteff, 2004).\n\nFurther laws that prohibit disclosure of personal information of consumers include Federal Trade Commission Act (Section 5), and the Fair Credit Reporting Act (FCRA) (Smith, Moteff, Kruger, et al., 2005). Congress has also passed laws to protect identity such as the 1998 Identity Theft and Assumption Deterrence Act, the 2003 Fair and Accurate Credit Transactions (FACT) Act, and the 2004 Identity Theft Penalty Enhancement Act with corresponding remedies for victims of identity theft (Smith, Moteff, Kruger, et al., 2005). The Children\u2019s Online Privacy Protection Act (COPPA) was passed by Congress in 1998 (Smith, Moteff, Kruger, et al., 2005) to regulate the collection of personal information of websites created specifically for children (Children\u2019s Online, 1998).\n\nFor acts committed against computer security when no cyber crime laws have been passed yet, government institutions use commerce and federal telecommunications laws to prosecute computer hackers (Fogie, 2004). The US Congress passed in 1984 the Computer Fraud and Abuse Act, the first computer crime statute (Fogie, 2004) that makes it a crime the act of intentionally accessing computer systems of the government without approval and thereby disrupting its normal operation (Gerth, 1988). It was later amended in 1986 and 1994 (Fogie, 2004). It further penalizes use of a password without authority to access a computer system or accomplish fraudulent acts (Fogie, 2004). The penalty for violation of the Act consists of a fine of $5,000 or twice the damage done or benefit gained and one year imprisonment for first time offenders, and a maximum fine of $10,000 plus two times the damage done or gain and imprisonment of ten years for second time offenders (Gerth, 1988).\n\nThe 21st Century Department of Justice Authorization Act mandated the Department of Justice to report to Congress the latter\u2019s use of DCS 1000 software and similar programs at the end of fiscal years 2002 and 2003 (Smith, Moteff, Kruger, et al., 2005). Earlier, the FBI installed DCS 1000 (previously called Carnivore) into the system of ISPs (Internet Service Providers) to intercept email messages and surfing activities (Smith, Moteff, Kruger, et al., 2005). The FBI said that it ceased using the DCS 1000 and substituted identical commercial software instead (Smith, Moteff, Kruger, et al., 2005).\n\nThe Electronic Communications Privacy Act of 1986 (ECPA) updated the Federal Wiretap Act of 1968 (The Federal Wiretap, 2010) to cover intercepting of electronic communications and deliberate illegal access to \u201celectronically stored data\u201d (Fogie, 2004, para. 10). ECPA applies to both private and government institutions to protect access and disclosure of electronic communications (The Federal Wiretap 2010). Although the Act did not specifically mention email messages as covered by the protection, decisions of U.S. courts said that they should be included (The Federal Wiretap 2010). ECPA caused modification in company policies and procedures in that at present, the company has to inform telephone callers that the conversation is recorded for quality control (The Federal Wiretap 2010).\n\nThe U.S. Communications Assistance for Law Enforcement Act of 1994 (CALEA) introduced changes in wiretapping activities by enjoining telecommunication companies to allow wiretapping by law enforcers provided a court order is duly issued (Fogie, 2004). Through CALEA, law enforcement agencies can still perform surveillance while the privacy of individuals is assured (Ask CALEA, 2009).\n\nThe National Information Infrastructure Protection Act of 1996 (NIIPA) defined more computer crimes to enhance protection of computer systems (Fogie, 2004). NIIPA also extended the protection to computer systems used in local and international commercial transactions and communications (Fogie, 2004). The law substantially amends the precursor Computer Fraud and Abuse Act of 1984 (which was amended in 1986 and 1994) (National Information Infrastructure, 2010).\n\nThe Gramm-Leach-Bliley Act (GLBA), otherwise known as Financial Services Modernization Act of 1999 (The Gramm-Leach-Bliley, n.d.) delimited the occurrences that a financial firm can divulge consumer personal information to non-affiliate third parties (Fogie, 2004). Financial agencies are also mandated to reveal their privacy polices and procedures on such information sharing with affiliates and non-affiliate third parties (Fogie, 2004). Private financial records (e.g. balances, account numbers) are regularly sold and purchased by banks, credit cards and financial firms (The Gramm-Leach-Bliley, n.d.). It also provided protection of persons against \u201cpretexting\u201d (i.e. gaining personal information through fraudulent pretension (The Gramm-Leach-Bliley, n.d., para. 1).\n\nThe USA PATRIOT Act (enacted after the September 11, 2001 attack) expanded the government\u2019s intervention on the privacy rights over the internet (Smith, Moteff, Kruger, et al., 2005). Under this law, the ISP is authorized to disclose records and information (excluding the content of message) of a subscriber to specific government agencies if it believes that death or injury might occur (Smith, Moteff, Kruger, et al., 2005). Section 225 of the Homeland Security Act amended in 2002 the provision on disclosure wherein the ISP is now authorized to disclose the content of the communication to local or federal agency on the same grounds (Smith, Moteff, Kruger, et al., 2005).\n\nLaws that Strengthen Computer Security\n\nLaws are also enacted to strengthen computer security besides penalizing the wrongdoer. For instance, the Computer Security Act of 1987 was enacted to strengthen the security of government computers and thus make it difficult for external computers to infect the system with virus (Gerth, 1988). Strengthening the network must be accomplished along with the passage of more laws that would penalize cyber crimes, Democrat Sen. Patrick J. Leahy (Vermont) said (Gerth, 1988).\n\nThe Homeland Security Act of 2002 authorizes the Department of Homeland Security to work with the private sector in protecting the information infrastructure (Moteff, 2004). The passage of Federal Information Security Management Act (in 2002) granted the head of the Management and Budget supervisory authority over the drafting of the standards and security guidelines and conformance thereto (Moteff, 2004). Excluded in that authority are computer systems utilized for national security (governed by the National Security Directive 42) (Moteff, 2004). The Homeland Security Presidential Directive No. 7 and National Strategy for Securing Cyberspace further bolster the department\u2019s role in security reinforcement (Moteff, 2004).\n\nThe Telecommunications Act of 1996 granted authority to the Federal Communications Commission (FCC) if the latter determined that broadband has not been implemented reasonably and timely (Smith, Moteff, Kruger, et al., 2005, p. CRS-4). Pres. Bush even endorsed in March 26, 2004 the deployment of universal broadband access without taxes (Smith, Moteff, Kruger, et al., 2005). The Critical Infrastructure Board (created by E.O. 13231 passed by Pres. George W. Bush, later dissolved by E.O. 13286) issued the National Strategy to Secure Cyberspace that enumerated the responsibilities to the Department of Homeland Security to protect the information infrastructure (Smith, Moteff, Kruger, et al., 2005). The National Cyber Security Division (NCSD, under the Information Analysis and Infrastructure Protection Directorate) managed Homeland\u2019s cybersecurity activities (Smith, Moteff, Kruger, et al., 2005).\n\nThe federal statute Computer Fraud and Abuse was passed in congruence with the Comprehensive Crime Control Act of 1984 (makes as federal crime the unauthorized access and damage to government and private computers that deal with banking and foreign commerce) (Smith, Moteff, Kruger, et al., 2005). The Federal Information Security Act of 2002 (FISMA) lays down the primary statutory needs in securing federal computers and network (Moteff, 2004). FISMA was founded upon the Computer Security Act of 1987, the Paperwork Reduction Act of 1995, and the Information Technology Management Reform Act of 1996 (Moteff, 2004). This Act mandates all agencies to have an inventory of all computer systems, to identify the security protection needed and provide measures to address the need, and to \u201cdevelop, document, and implement an agency-wide information security program\u201d (Moteff, 2004, para. 10).\n\nConclusion\n\nNumerous laws have been passed that cover computer security and protection of information for specific institutions, the government and the private sector. Even FTC Chairwoman Majoras called it (during the Senate Banking Committee hearing in March 10, 2005) a \u201ccomplicated maze\u201d the existence of numerous laws on data protection in the various government and private institutions (Smith, Moteff, Kruger, et al., 2005).\n\nMany possible threats have already been identified and addressed at present with the laws enacted. The voluminous laws should be re-codified so as to streamline them, thus making enforcement, regulation and prosecutions easier. Newer computer acts that cause damage or financial loss occur on the net that cannot be penalized or sanctioned since the law does not define them. And if one jurisdiction has defined the act making it a crime, the law cannot be enforced in another jurisdiction or country when the latter has no law for it or does not recognize the criminal law of the other country whose citizens suffered loss or damage.\n\nIt is therefore necessary to create a few comprehensive cyber crime law that would define all computer crimes at present and the future, even if unknown in the present. This would facilitate prosecution of the offenders and deter others from committing cyber crimes and device new means to infiltrate computer security. As what previously occurred, government prosecutors find difficulty in handling a case due to lack of supporting law. The government has the primary authority in regulating computer security matters and it should assume full responsibility for the task.\n\nOn the wider scale, not all countries have enforced computer security measures strictly. And still, a number of states do not criminalize certain malicious acts on the internet. A study supported by the World Information Technology and Services Alliances (WITSA, an international organization composed of 41 IT industry organizations) revealed that only nine countries out of the 52 subjected to the study criminalized certain acts involving cyber space (Fogie, 2004). Without cooperation by all countries, there will be a break in the international legal system wherein cyber criminals can still commit crimes and find refuge in the holes in the law. Only when there is a global move to prosecute and penalize cyber crimes, together with the strengthening of the computer systems can breach of computer security ceases.\n\nReferences\n\nAsk CALEA. (2009). Web.\n\nBelli, G. (2008). Nonexperimental Quantitative Research, pp. 59-77. Web.\n\nBrody, R.G., Mulig, E., & Kimball, V. (2007). Phishing, pharming and identity theft. Academy of Accounting and Financial Studies Journal. AllBusiness. Web.\n\nChildren\u2019s Online Privacy Protection Act of 1998. Web.\n\nComputer Crime. (2010). TheFreeDictionary. Web.\n\nComputer Crime \u2013 definitions, Types of computer crimes, Anti-cyber-crime legislation, Enforcement agencies, International computer crime. (2010). Free Encyclopedia of Ecommerce. Web.\n\nEngel. N. (2006). Technology users in developing countries \u2013 Do they matter? Web.\n\nFogie, S. (2004). Computer Crime Legislation. InformIT. Web.\n\nGerth, J. (1988). Intruders into Computer Systems Still Hard to Prosecute. The New York Times. Web.\n\nGo, P. (2009). Types of Computer Crimes. EzineArticles.com. Web.\n\nKinkus, J.F. (2002). Computer Security. Science and Technology Resources on the Internet. Web.\n\nKutner, T. (2001). What\u2019s the difference between a Virus and a Worm? Web.\n\nLevine, P. (2003, May-June). Information technology and the social construction of information privacy: Comment. Journal of Accounting and Public Policy , ( 22 )3, pp. 281-285.\n\nMcConnell International. (2000). Cyber Crime\u2026 and Punishment? Archaic Laws Threaten Global Information. Web.\n\nMoteff, J. (2004). Computer security: a summary of selected federal laws, executive orders, and presidential directives. Congressional Research Service (CRS) Reports and Issue Briefs. Web.\n\nNational Information Infrastructure Protection Act (NIIPA) of (1996). (2010). Free Encyclopedia of Ecommerce. Web.\n\nOnline Fraud: Pharming. (2010). Symantec Corporation. Web.\n\nPhillips, D.J. (1998). The social construction of a secure, anonymous electronic payment system: frame alignment and mobilization around Ecash. Journal of Information Technology , ( 13 ), pp. 273\u2013284. Web.\n\nPhishing. (n.d.). Phishing and Pharming Information Site. 2010, Web.\n\nRoss, S.T. (1999). Computer Security: A Practical Definition. Unix System Security Tools. The McGraw-Hill Companies. Web.\n\nS. 472\u2013109th Congress: Anti-phishing Act of 2005. (2005). In GovTrack.us (database of federal legislation). Web.\n\nSmith, M.S., Moteff, J.D., Kruger, L.G., Seifert, J.W., Figliola, P.M. & Tehan, R. (2005). Internet: An overview of key technology policy issues affecting its use and growth. Web.\n\nThe Federal Wiretap Act of 1968 and The Electronic Communications Privacy Act of 1986. (2010). YourDictionary.com. Web.\n\nThe Gramm-Leach-Bliley Act. (n.d.). epic.org. Electronic Privacy Information Center. 2010. Web.\n\nU.S. Department of Justice. (n.d.). Cyber Investigation. Federal Bureau of Investigation. 2010. Web.\n",
        "label": "human"
    },
    {
        "input": "Evolution of Computers in Commercial Industries and Healthcare Report\n\nTable of Contents\n 1. Introduction\n 2. Application in commercial industries\n 3. Computers in Healthcare: Health Information Systems\n 4. Conclusion\n 5. Works Cited\n\nIntroduction\n\nThe computer is perhaps the most iconic invention present in current times that influences multiple aspects of our lives, if not all. It has changed the way we work, our social life, and even our way of thinking. The world has now become one single platform where people can interact and do real business thanks to this great machine- the computer (Hall 156). Originally; the term computer meant a person with the ability to perform calculations of numerical nature with the assistance of a mechanical device of computing. The real computer revolution began in the 1930s with binary computing being central to all aspects of computing of all ages. The mechanical machine of addition of 1642 is the root of computer invention and it is from here that inventions like ABACUS-an early computing tool, John Napier\u2019s logarithm, and William Oughtred\u2019s slide rules evolved from as some of the early computing tools.\n\nThe abacus is the earliest known existence of the current computer\u2019s ancestor, dating back to close to 200 years. It was simply a bracket made of wood that held corresponding wires with beads attached. All forms of arithmetic operations would then be achieved just by moving the beads along the wires by rules of programming. In 1694, Blaise Pascal to help his father-a tax collector, came up with the next phase of computer invention when he invented the digital calculating machine which could only go as far as adding numbers entered through the turning of dials (Soma 32). Charles Babbage, a professor of mathematics designed a steam-powered calculation machine capable of storing up to one thousand fifty digit numbers; and it included built-in operations that are vital to a general modern computer. Cards with holes, commonly referred to as punched cards were used to program the machine and these were also used to store data. However, most of the inventions of this professor were a failure due to a lack of proper techniques of precision machining and poor demand for devices of this nature (Soma 46).\n\nApplication in commercial industries\n\nThere was a witnessed loss of interest in computers especially after the period of the inventions of Babbage until the period between 1850 and 1900 when the interest was reborn due to great advances in mathematics and physics. Some of the progress comprised intricate arithmetic and formulas that hitherto used a lot of time besides being very arduous for individual engagement. The progressive interest was well sustained and in 1890 when computers found major use during the conduction of the U.S. census. This was made possible through a punched card system with the ability to read the information on the cards automatically without the need to depend on humanitarian assistance.\n\nThe computer then proved to be a crucial tool in the process of tabulating the census totals, given the fact that the U.S. population was growing extremely quickly. Commercial industries became aware of these advantages of computers and soon, new versions of punch-card machines specially made for business were developed by IBM and other corporations like Burroughs. The punched card machines were heavily used in most businesses worldwide for purposes of computing. This was especially after the discovery by businesses in other industries that the machines had a powerful capability that could handle most of their work in a short time; hence saving most of the time used in normal activities. They also found a good percentage of application in science for purposes of research especially in analyzing acquired data, a function that is very significant in all works of science. The use of machines of punch-card architecture went on for over fifty years since their first usage and this marked the formal spread of computers to other critical fields like healthcare (Chposky, 1988).\n\nComputers in Healthcare: Health Information Systems\n\nFrom ancient times, healthcare was generally about the collection of information and its processing to identify the specific problem that a patient suffered from to offer appropriate treatment. Hippocrates and Galen are known to be the early physicians to have the healing of their patients documented to improve care through the use of the documented information. However, it is until the 19th century that the technology of computers started to be used in healthcare for purposes of diagnosis and eventual treatment. Hutchinson\u2019s device is one of the initial systems that was used and it served the function of measuring the lungs\u2019 vital capacity. The application of computers in healthcare then underwent a revolutionary. Some of the popular technologies of this period include the thermometer, ophthalmoscope, x-ray, stethoscope, and microscope. The increase of medical technology and the needed specialization also led to the increase of the quantity of data necessary to make a diagnosis and have treatments administered (Brighthub, 2010). Subsequently, medical records became significant as documents of keeping the information for patients hence the need to organize the data and records in fast and efficient ways, and from this point; the era of healthcare information systems was born.\n\nA Healthcare information system is essential, a computerized data system that performs the core function of routine collection, analysis, reporting, and storage of information about all aspects of healthcare; including service delivery, demographic details, cost, and quality. These systems for healthcare bear a significant relationship with most of the information systems used for business operations in companies, industries, institutions, and governments; thus implying that the basic operating principles are largely similar. Therefore, the development of healthcare information systems can also be traced to the early evolution of computers since it is the very same computers that are used to perform the functions of the healthcare information systems through specialized programming. For a long duration, in fact, until the late 1960s, information systems in healthcare, like most other industries were paper-based. Relevant aspects had to be put in place in systems and technology evolution to cater to healthcare organizations. The early application of information systems in healthcare took place in the late 1960s and early 1970s, mostly focusing on financial operations. The clinical area also found considerable usage for the information systems especially for capturing clinical information and making crucial medical decisions. It is during this period that several projects related to healthcare information systems were undertaken. A good example is the Warner project undertaken in Salt lake city, Utah at the Latter Day Saints hospital (Merida, 2002).\n\nIn the recent past, the healthcare industry has experienced a lot of growth as information systems seem to be playing a very critical role in the provision of healthcare services. There is the implementation of large scale applications in electronic medical records, telemedicine-which enables the provision of remote diagnosis, the upgrade of hospital information structures, the use of public networks like the internet for distribution of relevant information to the public and patients in general and setting up of intranets and extranets for sharing crucial information with stakeholders among others (Beaver, 2002). Currently, there is the proposition that healthcare should rely thoroughly on information systems to cut down costs to some reasonable levels. Healthcare spending has been increasing at an alarming rate. This is coupled with the increase in the impact of chronic diseases among the aged whose dependency on the healthcare system has also increased in recent years. In the field of research, there are several projects aimed at accelerating the application of more established healthcare information systems. Some of them include I-living which is essentially a supportive system for living. These systems have been under development by researchers at the University of Illinois, Urbana-Champaign. There is also the smart in-home monitoring system that is in progress at the University of Virginia. This is emphasizing the collection of data by using a low-cost suite of non-intrusive sensors (Durresi & Barroli, 2008).\n\nConclusion\n\nIn conclusion, healthcare information systems have evolved gradually to become significant in the medical practice and all related fields. From diagnosis, treatment to secure and convenient storage of patient information for different valid purposes; these systems have proven their worth in both cost-cutting and ease of patient handling. Governments and all relevant institutions worldwide now need to work closely with the IT industry to clearly outline the role of information systems in healthcare. This will be vital for the security of the information systems. Overall, healthcare information systems are ultimately vital and should be encouraged in all organizations to improve the quality of healthcare which is a very important need for all human beings.\n\nWorks Cited\n\nBeaver, Kevin. Healthcare Information Systems, Second Edition (Best Practices). New York: Auerbach Publications, 2002.\n\nBrighthub (2010). Evolution of Medical Technology. Web.\n\nChposky, James. Blue Magic. New York: Facts on File Publishing. 1988.\n\nDurresi, Arjan. Barolli, Leonard. Secure Ubiquitous Health Monitoring System. New York: Springer, 2008.\n\nHall, Peter. Silicon Landscapes. Boston: Allen & Irwin, 1985 Gulliver, David. Silicon Valley and Beyond. Berkeley, Ca: Berkeley Area Government Press, 1981.\n\nMerida, Johns. Information Management For Health Care Professions (The Health Information Management Series). Kentucky: Delmar Cengage Learning, 2002.\n\nSoma, John T. The History of the Computer. Toronto: Lexington Books, 1976.\n",
        "label": "human"
    },
    {
        "input": "Property and Computer Crimes Essay\n\nTable of Contents\n 1. Introduction\n 2. Kidnapping\n 3. False Imprisonment\n 4. Robbery\n 5. Burglary\n 6. Theft\n 7. Computer / High-Technology Crimes\n 8. Conclusion\n 9. Reference List\n\nIntroduction\n\nOver the years, there has been an increase in the rate of property and computer crimes. Some of these crimes include; kidnapping, robbery, theft, burglary and internet crime. The introduction of computers and internet has accelerated the rate at which these crimes are taking place. For example, most people have taken advantage of the internet to conduct illegal businesses such as drug trafficking and others use it to rob people off their money in the name of trade. According to Siegel (2008), a computer crime can be defined as the intentional access to any computer system or network for the purpose of developing or performing any scheme or pretense, to swindle, extort, and obtain money or property from another person. This paper will give an in-depth analysis of kidnapping, false imprisonment, robbery, burglary, theft and computer crimes.\n\nKidnapping\n\nThis is the taking away of an individual forcibly against the person\u2019s will. If the kidnap involves a woman, it is referred to as abduction while kidnapping of children is called child stealing. Kidnapping is a criminal offence liable for jurisdiction in common law. However, this varies from nation to nation which is determined by the law of jurisdiction applied in that nation (Jacobs & Judge, 2006). Kidnapping has been implied to mean the act of leading someone away by force or fraudulent persuasion. It can also be implied to mean the secreting or holding a person in place where he is not likely to be found.\n\nKidnapping can also be said to be the unlawful grabbing of a person with the intention of carrying him/her away. It is a problem that has been a cause of alarm in many nations\u2019 especially United States. The most common victims of kidnappers are women and children since they are easily manipulated. Kidnapping comes in different forms and level all of which attracts diverse castigations. In United States, jurisdiction of kidnappers is based on the length and purpose of the kidnap, for example if one individual abducts another person for the purpose of ransom, such kind of a kidnap is referred to as first degree kidnapping. The second degree kidnapping is a less serious offence compared to the first degree and it attracts an imprisonment of one to eight years.\n\nFalse Imprisonment\n\nThis is a restraint on the liberty of a person (without lawful cause) either by under duress detaining the party in the streets (against his will) or by impounding him in jail without due cause. False imprisonment can also be defined as the privately carrying off of a person and keeping him/her in a confined place. It is an offence which is punishable by law. Key elements in false imprisonment are: intention, total restraint of the claimant and awareness of the claimant that he is been unlawfully imprisoned.\n\nMany people take false imprisonment to mean, the detention of a person without cause either by the government or private entity. Examples of false imprisonment include: restraining of bank employees and customers by robbers, detention of customers by shop attendants or owners, detention of individuals by police without lawful cause, among others (Walston-Dunham, 2008). False imprisonment can be considered as an act of crime against a person or the public in general. It can be said to have two elements: a criminal act and an evil intent. The government (which refers to the people) prosecutes the offenders on the basis of the intent to commit the criminal act. If a person is found detaining another person without an intention to do so, such a situation cannot be termed as false imprisonment.\n\nRobbery\n\nRobbery is a criminal act that involves forcibly taking or trying to take another person\u2019s property by enacting fear upon the person. In law, robbery takes the definition of permanently taking property from an individual. In most cases, people are robbed off their properties that are of value such as stocks from a shop, money, home appliances among others. In some instances, robbers use weapons such as guns in order to threaten the persons being robbed.\n\nThere are three common types of robberies namely: armed robbery, aggravated robbery, and Highway robbery. Robbery is a criminal act performed with the intention of forcibly taking property from other people for another person\u2019s benefits. It is a serious offence and punishable by law in accordance to the extent of the crime (Boldrewood, 2009). If the offenders are found committing the crime, they can be shot if they fail to surrender or if they run away from the police.\n\nBurglary\n\nBurglary is a criminal act that involves breaking a building and entering in to commit an offence. The offence that is usually committed is theft. There are various elements that are involved in the offence; they include but not limited to trespass, breaking, and entry, dwelling, nighttime, and intent. Trespass occurs when a person enters a building without the consent of the owner. In most cases, it occurs through misrepresentation of a person\u2019s identity (Siegel, 2008). After trespassing, the law offender creates an opening into the building which he uses to enter into the dwelling of the victim (dwelling refers to the house where the victim sleeps even if he may not be in during the time of the burglary).\n\nBurglary normally occurs at night when everybody is assumed to be asleep, however, some jurisdiction have ceased to recognize nighttime as one of the element in burglary arguing that it can also occur during the day time especially when the victim is deemed to be absent. In common law, a person\u2019s intent to commit the crime is investigated before he is persecuted. Almost all the above elements have to be present before jurisdictions takes place. Burglary differs from robbery in that, in robbery there is neither building involved nor entry which are major constitute in burglary.\n\nTheft\n\nTheft is taking other person\u2019s property without permission, either by force or peacefully. It is a criminal act that is punishable by law. A person is said to have stolen another person\u2019s property if he takes it without due consent of the owner. The term theft is also used as a short form of robbery or burglary. A person who carries out the criminal act of theft is referred to as a thief and the act itself is referred to as stealing. The criminal jurisdiction of theft can take the form of the two examples of crime described above (robbery and burglary) depending on the circumstances surrounding theft.\n\nComputer / High-Technology Crimes\n\nThere are various types of computer crimes which uses high technology such as cyber crime which includes, on-line pornography, drug trafficking, among others. Internet drug trafficking has been facilitated by the lack of face to face communication. Most businesses have adopted e-business form of marketing. It has made marketing easier because one does not have to move to the physical location of the end users. In the tradition setting, information regarding drugs was kept secret and was only revealed to specific people. In the modern society everyone can access information concerning drugs from the internet.\n\nPornography is defined as an overt representation of literature or films which are expressive of sexual activities. Online-pornography therefore refers to overt representation of sexual activities through the use of internet. Pornography in all of its forms is meant to stimulate erotic feelings as opposed to emotions (Smith, et al. 2004). Computer based crimes are difficult to investigate because the offenders use high technology devices that are able to delete the messages after communication is over. However, the government, through technology, has devised ways of detecting the offenders if such crimes take place. Their jurisdiction is dependent on the type of crime committed and its consequences to the victims.\n\nConclusion\n\nIn conclusion, I would say that property crimes have been rising due to lack of an efficient security system. People have lost most of their valuable assets such as home appliances and business stocks through theft. Kidnapping refers to taking away of an individual forcibly against his will or the unlawful grabbing of a person with the intention of carrying him/her away. On the other hand false imprisonment can be defined as the privately carrying off of a person and keeping him/her in a confined place. Robbery refers to the criminal act that involves forcibly taking or trying to take another person\u2019s property by enacting fear upon the person. Burglary differs from robbery in that; the former involves breaking a building and entering in to commit an offence. Theft is taking other person\u2019s property without permission, either by force or peacefully. It can occur in the form of robbery or burglary. Last but not least, computer crimes refer to the use of high technology such as use of the internet to commit offenses. They include drug trafficking, on-line pornography among others.\n\nReference List\n\nBoldrewood, R. (2009). Robbery under Arms . Middlesex: Echo Library.\n\nJacobs, T. & Judge (2006). What are My Rights? (EasyRead Super Large 18pt Edition . New York: ReadHowYouWant.com.\n\nSiegel, L. J. (2008). Criminology. New York: Cengage Learning.\n\nSmith, R. G., et al. (2004). Cyber criminals on trial . Cambridge: Cambridge University Press.\n\nWalston-Dunham, B. (2008). Introduction to Law . New York: Cengage Learning\n",
        "label": "human"
    },
    {
        "input": "The Use of Computers in the Aviation Industry Essay\n\nAutopilot\n\nOver time, computers have been introduced into organizations and have been used at different levels to perform different tasks. One of these areas is the Aviation industry. In this industry, computers are used to solve several problems and improve the productivity of the workforce including pilots. Pilots are among the workforce in that industry who have benefited from the use of Autopilot in many ways.\n\nModern aircraft are fitted with Autopilots to guide aircraft with very little support from the pilot. Computer software is installed on the Autopilot to control an aircraft\u2019s control system. This is a real-time system that is divided into three levels. One level monitors the way an aircraft lands, another level monitors the way it takes off, and another level monitors the landscape. In addition to that, an aircraft is fitted with digital computers and backup systems. Should one computer fail, other computers may continue to run the control system. Information is relayed to the computers through digital signals received from sensors that are connected to a data store. A flight management system is integrated into Autopilot\u2019s flight mechanism and the data store. This mechanism ensures that the stability of an aircraft during flight is controlled through the inertia guidance system using data from the data store. The inertia guidance system has software that statistically analyzes errors accumulated during the time of a flight.\n\nDigital Information\n\nThe complicated nature of the software enables the Autopilot to capture all information related to an aircraft\u2019s current position and uses the information to guide the aircraft\u2019s control system. Errors may occur during flight. One of these errors, known as drift may arise due to a malfunctioning mechanical pump or positional data that has been damaged. This corruption may adversely affect the position of an aircraft in terms of its altitude or may lead an aircraft to underperform when flying on routes that require a specific performance factor.\n\nWhen a pilot wants to land an aircraft, the computerized system relays digital information about the landing gear. The computer tests all aspects of the landing gear and if the system is confirmed to work effectively, then the pilot is given a nod to land the aircraft.\n\nThe Autopilot\u2019s hardware and software are designed with reliability and a certain amount of redundancy. Therefore the implementation strategies of the Autopilots vary significantly from one aircraft manufacturer to another. However, the software and hardware in these machines are well controlled. To ensure reliability and redundancy, several quality assurance tests are done on hardware and software that are used to make the Autopilots.\n\nSometimes it is critical to ensure the safety of an aircraft during flight. Different processor architectural designs are used, different languages are used to design computer software, and different implementation strategies are used. Computer-aided landings are categorized into three according to the International Civil Aviation Organization. These definitions depend on the computer system\u2019s automatic response mechanism to landing.\n\nTraining\n\nComputers are also used for training purposes. This ranges from piloting to aircraft maintenance. Computerized training simulates real flights and challenges met by the flight crew. Flight simulation software brings to reality all aspects of flight and enables students to acquire relevant skills needed in the aircraft industry.\n",
        "label": "human"
    },
    {
        "input": "Honeypots and Honeynets in Network Security Research Paper\n\nIntroduction\n\nArguably one of the most epic accomplishments of the 21st century was the invention of the computer and the subsequent creation of the internet. These two entities have virtually transformed the world as far as information processing and communication is concerned. Organizations have extensively employed the use of computer systems as efficient global communications became the defining attribute of successful organizations. However, these advancements have also increased the frequency and sophistication of computer crimes. It is therefore imperative that countermeasures be developed to detect and prevent these attacks. The key to fulfilling these countermeasures is the gathering of information on vulnerabilities and gaining an insight into the strategies employed by attackers. Presenting prospective attackers with honeypots which are the easy target that is in fact traps is one of the tools that is been utilized to enable covert monitoring of intruders. This paper argues that Honeypots and Honeynets are an effective method to identify attackers, system vulnerabilities, and attack strategies, therefore, providing a basis for improved security as well as catching attackers. The paper shall provide a detailed description as to the benefits of this method and its subsequent implementation. The legal issues that surround the use of honeypots and honeynets shall also be addressed so as to determine how one can make use of these tools within the legal framework of our country.\n\nHoneypots and Honeynets: a Brief Introduction\n\nA honeypot is defined by Lance Spitzner as \u201ca security resource whose value lies in being probed, attacked or compromised\u201d. (Pouget, Dacier & Debar, 2003; Spitzner, 2002). As such, a honeypot is a device that is exposed on a network with the aim of attracting unauthorized traffic. A honeynet on the honeyn is simply a network of honeypots with a firewall attached to it. Once the system is compromised by an intruder attack, data is collected on this unauthorized access so as to enable the studying of the same so as to learn about the latest trends and tools used by intruders as well as help in tracing back the traffic to the intruders computer. Since the value of a honeypot lies in its being \u201ccompromised\u201d by an attacker, It makes sense to make it look not only enticing but also authentic to a hacker. A honeynet will therefore consist of standard production systems that may be found within a real organization and generally several computers as with a real intranet. Operating system emulators such as VMware can be utilized to simulate several computer systems in one physical system (Krasser, Grizzard & Owen, 2005).\n\nTypes of honeypots\n\nHoneypots can be categorized into two broad groups: production honeypots and research honeypots. The difference between the two categorizations springs from the role that the honeypot plays in a system. Production honey-phoneypotsed to avert risk to organizational resources by presenting a kind of \u201cred-herring\u201d for the intruders to compromise. Research honeypots on the other hand are meant to gather as much information from attackers as possible. Production honeypots assist in mitigating the risk that organizations face and provide evidence of malicious attempts which may be used in a court of law. Research honeypots are an excellent tool to use as a basis for validating an organization\u2019s security setup since potential threats and risks are assessed to enable administrators to make the best security decisions.\n\nHow Honeypots Work\n\nAn important point to note is that honeypots are not designed to prevent a particular intrusion but rather, their objective is to collect information on atta, therefore, ore enabling administrators to detect attack patterns and make necessary changes in their system so as to protect from attacks on their network infrastructure. A honeypot device is placed openly with the aim of attracting unauthorized activity. The defining characteristic of honeypots is the level of involvement that they afford the attacker. A low-involvement honeypot (also referred to as a low-interaction honeypot) only emulates systems and services running (Carter, 2004). This kind of honeypot does not provide a real OS for the attacker to operate on thus greatly reducing the amount and significance of the data captured from the intruder. Low interaction honeypots can offer information such as the date and time of the attack and the IP address of the attackers. However, their effectiveness is limited only to already discovered attack patterns. The other kind of honeypot is the high-involvement honeypot. This honeypot makes the entire OS along with installed services accessible to the intruder(Carter, 2004). This unlimited access allows for more data to be captured and subsequently analyzed.\n\nTechnical implementation\n\nThe type of honeypot that is implemented is highly dependent on the objective of an organization as well as the amoresourcesesource they have at their resources. Law enforcement agencies require a lot of data so as to reconstruct the aattacker\u2019smotives and identity and as such, a high-interaction honeypot may be utilized. The agencies also have the resources necessary to finance and maintain this system. Corporations may not need to capture as much data and therefore, a low-level honeypot that isbothh easy to set up and provides limited danger may be preferred.\n\nIn ma ost implementation, a single physical machine running multiple virtual operating systems is utilized. Carter (2004) suggests that the honeypot servers should be unsecured to allow the intruder free reign over the system. To track the activity of the intruder, detection tools such as Snort can be utilized to analyze the types of traffic received. One of the setbacks of honeypots is that outgoing traffic cannot be limited. As such, an attacker can use the system to carry out DOS attacks with legal consequences for the honeypot user. as such, placing a firewall in front of the honeypot is vital to ensure that the outbound traffic is controlled thus lowering the risk posed by a hostile take over of the honeypot. VMware is the software that is favored in setting up multiple virtual systems so as to mimic a real network setting.\n\nBenefits of honeypots\n\nHoneynets present a myriad of benefits for an organization or institute which employs them. By the use of honeynets, an administrator is able to detect other compromised systems on the network (Krasser, Grizzard & Owen, 2005). This is possible since attackers use the honeypot as a starting point to hijack other systems. By having the honeynet log files analyzed, one can trace out the path that the attacker used and end up at the other system that was possibly compromised by the intruder.\n\nHoneypots enable an organization to carry out research into the threats that it may face. As such, questions such as who is the attacker and what kind of tools they use in their attacks can be answered. This will enable the IT security branch of the organization to better understand their potential threats thus increase their preparedness and their defense mechanisms.\n\nA production honeypot acts as an easy target therefore distracting the intruder from attacking the real organizations system. This gives an organization some form of protection since the potential attacker compromises the more enticing honeypot therefore leaving the organizations system unscathed. In addition to this, the organization can use the production honeypot to positively identify the attacker. If this information has been lawfully obtained, it can be used to criminally prosecute the attacker in a court of law.\n\nChallenges\n\nDespite the numerous merits that may be reaped from the use of honeypots by an organization or individual, running of this tools comes with its inherent problems. Loss of control over the honeypot by the controller can render the honeypot unbeneficial since its main purpose is to capture unauthorized activity. If an attacker can succeed in infiltrating the system without being notice, then there is a flaw in the device and it is unbeneficial to the owners.\n\nSince honeypots are correlated with the host operating system, there is always the danger of an attacker breaking out from the virtual environment and into the host operating system (Baumann & Plattner, 2002). This will result in the attacker having access to data and resources that are vital to the organization and he/she can therefore compromise the entire system leading to losses.\n\nBaumann and Plattner (2002) affirm that the effectiveness of honeypots can be greatly impeded when encrypted connections are employed by the attacker. while logging and listening in on unauthorized traffic is still possible even when the connection is encrypted, deciphering of what is captured in the attackers packets is at times impossible. In some cases, an attacker can take over the entire system thus rendering the administrator helpless. The attacker can then proceed to utilize the system resources available to him to launch attacks on other systems. This attacks e.g. Denial of Service attacks on other networks can result in the damages to a third party\u2019s network (Krasser, Grizzard & Owen). The consequences for such errors can be costly as the honeypot owner can be held legally liable for the attack and therefore forced to compensate the third party.\n\nLegal Issues\n\nThe use of honeypots presents a number of legal issues to both person or organization that implements them. One of the core legal issues that arises from honeypot usage is the issue of Entrapment. Spitzner (2006) defines entrapment as the act by a government agent to induce a person to commit a crime by fraudulent means or unwarranted induction so as to criminally prosecute the person. As such, an attacker who is taken to court as a result of compromising a honeypot can argue that he were induced to commit the crime thus nullifying the evidence contained in the honeypot logs.\n\nThe issue of privacy which is prevalent in the Information Technology spheres is also applicable with the use of honeypots. Honeypots can be configured to capture the content data of a transmission. These data has privacy issues attached to it and therefore, collection and use of the same may be a violation of the transmitter\u2019s privacy. Spitzner(2006) suggests that placement of banners that obligate individuals to consent to monitoring thus wavering their rights to privacy is a one of the ways in which monitoring in a system can be legitimized.\n\nHoneypots can also lead to attacks on third parties by use of the honeypot as the platform of attack. This presents a legal situation since the owner of the honeypot will be held responsible for the attack even though it was an attacker who utilized the honeypot to attack another persons system. Baumann and Plattner (2002) assert that it is the honeypot owner\u2019s responsibility to ensure that no harm is caused on third parties as a result of their honeypots.\n\nConclusion\n\nThe IT arena is ever evolving and as its effectiveness increases, so do the risks. Preventive and detective measures should therefore be employed to improve security. This paper set forth to illustrate that honeypots can be used to identify and catch security threats as well as identify vulnerabilities in an organizations network system. It has been demonstrated that honeypots can be used to identify attackers and take legal action against them. However, while honeypots do present a versatile tool for revealing the identity of attackers and prosecuting them in a court of law, law enforcers should be careful to ensure that the information they obtain does not infringe on the rights of the individual thus making it inadmissible in court.\n\nWhile honeypots are an important weapon in the IT security personnel\u2019s arsenal against attackers, it is clear from this paper that they do not protect the computer infrastructure of an organization from attacks. It is therefore prudent for organizations to invest in security measures such as firewalls and antivirus softwares and adhere to best security practices so as to safeguard the system. Having done this, organizations and individuals alike can make thrive from the numerous benefits that computer networks present to us.\n\nReferences\n\nBaumann, R. & Plattner, C. (2002). White Paper: Honeypots. Web.\n\nCarter, W. L. (2004). Setting up a Honeypot Using a Bait and Switch Router . Web.\n\nKrasser, S., Grizzard, B. J. & Owen, H.L. (2005). The Use of Honeynets to Increase Computer Network Security and User Awareness. Haworth Press. Web.\n\nPouget, F., Dacier, M. & Debar, H. (2003). White Paper: Honeypot, Honeynet, Honeytoken: Terminological Issues. Web.\n\nSpitzner, L. (2006). Honeypots: are they Illegal ? Web.\n",
        "label": "human"
    },
    {
        "input": "Viruses and Worms in Computers Essay\n\nViruses and worms are harmful computer programs that infect computers, often causing them to malfunction. Although worms and viruses both infect computers and cause some sort of damage or data loss, there is a difference between the two. A virus is a program that attaches itself to any computer file, usually an exe file, or program and spreads across with the help of data transfer from computer to computer. (How Do Computer Viruses Spread?) A worm, on the other hand, does not need files to attach to and spread like viruses. They make use of the already present file transfer systems to spread from one place to another (Vangie, n.d).\n\nWith the advent of internet technology and a globally integrated network, viruses are spreading online more rapidly than any other means. One of the most common ways online viruses spread is through spam email. These are unwanted and often infected emails that clutter your inbox, offering fake product offers and attractions. Viruses not only spread through spam mail but also through regular emails from known sources. These viruses come with email attachments. Another very common channel for viruses is through websites. Many websites leave cookies for later identification but many plant viruses as well. One of the most common forms of online virus spread is through downloading. Downloading files off the internet from unreliable and unverified sources can harbor virus infections. Many peer-to-peer networks such as LimeWire, Kazaa, and Bearshare have virus-infected files in search results that are disguised as actual search result files. Another common way of virus spread is through removable media such as USB flash drives and on computer networks through file sharing and network drives as well. Many popular viruses and worms spreading online, these days, are through automatically generated disguised links on Windows Live Messenger and Facebook. These links appear to be sent by the users themselves and attract friends by offering them to explore provocative and interesting content. An example of such a link would be, \u201cHey! I found your picture online! (Tamiflu spams spread online).\n\nTo prevent the spread of viruses and worms, there are certain precautionary measures that can be taken. First, use a good virus scanner or anti-virus software to scan computers regularly. Also, make sure that the software and scanners are up to date so that the latest virus definitions are supported. Another measure that can be taken is to scan all files before transfer and scan all removable media before use and transfer. Firewalls can also be implemented over computer networks to restrict harmful activities. Another feature on computer networks can be to block harmful website content and not allow access to those websites. Now, certain websites like Google offer page ranking facilities that warn users about harmful websites. Users can also apply security settings to their internet browsers to restrict the inflow of content from untrustworthy websites. Email services offer features such as junk mail filter and attachment virus scan to prevent spam mail and other viruses. With the correct measures and prevention, the spread of online viruses and worms can be controlled to a safe level (McCullough).\n\nWorks Cited\n\nBeal, Vangie. \u201c The Difference Between a Computer Virus, Worm and Trojan Horse \u201d Webopedia. 2009. Web.\n\nMcCullough, Declan. \u201cAnonymous remailers are a virus spreading online!\u201d Government Security. 2009.\n\nn.a. \u201cHow Do Computer Viruses Spread?\u201d AllBusiness. 2009.\n\nn.a. \u201cTamiflu spams spread online\u201d Spam Daily News. 2009.\n",
        "label": "human"
    },
    {
        "input": "Building a PC, Computer Structure Report\n\nTable of Contents\n 1. Introduction\n 2. Casing\n 3. Motherboard\n 4. Memory\n 5. Processor\n 6. Hard Drive\n 7. Graphics Card\n 8. Optical Drive\n 9. References\n\nIntroduction\n\nComputers are the need of the current world. No matter how old one is and how well acquainted with technology one should be, computer literacy and operations should be known to everyone to compete in this fast-moving world. Computers have changed appearances over time and now they are near a palm-size and can do almost all operations performed by workstations 20 years ago. In our case, a desktop is a must for day-to-day operations.\n\nCasing\n\nThe first item to be purchased is the case for the machine. A case should be large enough to give a good exhaust system and entertain a 12\u201d by 12\u201d motherboard for expansion purposes. From the given list, we have Cooler Master Elite 330 (420W) and Thermal takes M9. Dave should go for the Cooler master Elite 330 (420W) casing with supply as it has enough room to contain a full-sized motherboard and spacious enough to keep the system cool. It has expanded capabilities in terms of increased optical drives as it can hold a minimum of three optical drives at a time. The chassis gap is also big enough to hold a 9 series GFX card.\n\nMotherboard\n\nThe next component is the motherboard. This is the heart of the system and the entire PC performance depends on it. The entire system hardware setup depends on the mainboard. It should be able to allow fast data transfer and must have enough PCI slots to entertain further expansion. The motherboard should have 3 RAM slots so that Dave can have a large amount of physical memory available by installing more RAM chips to the system. It should have a PCX 2.0 interface for enhancing video performance. As Dave plans to plug in the plasma TV using the HDMI option, he should go for a motherboard that should support a new generation graphics card and MSI P45 Neo3-FR and Asus M2N68-VM fulfill the needs, however, I would suggest selecting MSI P45, Neo as it offers high connectivity along with all options Dave requires for extension the board has enough PCI slots so he can plug in a 1394 connector easily for the digital camera.\n\nMemory\n\nNext up would be the RAM for the system. To run a video editing tool, we need to have a large physical memory bank. The options are DDR-2 800 and DDR-3 modules. I would suggest going for the DDR-2 800 module with 2048 MB capacity as it is much cheaper and would suffice the requirements. Dave should install 2 chips which would give a total of 4 GBs that is adequate to operate the video editing tools more physical memory is required to run such software plus the fact that Windows Vista alone requires around 2 Gbs of memory to run smoothly.\n\nProcessor\n\nThe processor is regarded as the brain of the computer and it should be fast enough to entertain multitasking at every level. Amongst the choices present Intel core2quad Q8200 and AMD Phenom II X4 9950 seem to be the most suitable. Intel Core2Quad is by far the most appropriate processor for decoding activities as far as the price range is concerned. For encoding activities, we need a processor that can handle many calculations at a single point in time and the Q8200 has this ability as it has 4 cores to perform the massive operations, and furthermore it requires less power as compared to the previous versions.\n\nHard Drive\n\nThe hard drive is the primary storage device for the system and today the size varies from 80GBs to TBs. in our case, Dave does not require a huge hard drive as most of the data is to be burnt on DVDs. The choices available are Western Digital 320GB and Seagate 320GB and my advice would be to go for Western Digital as it is a good performer all along. Why 320 GB? Because today, an HD movie is around 4-7 GB thus 320 GBs would be adequate to contain a few of these videos and Windows Vista.\n\nGraphics Card\n\nThe final item is the GFX card and as Dave wants to plug in his Plasma to turn the PC into a home Theatre then the options are ATI 4670 and 9600GT. Here I will suggest selecting ATI 4670 as ATI\u2019s performance at this level is excellent and the Nvidia series can only compete with the late 9800 GTXX version of the product to compare with ATI 4670, it has Direct X 10.1 support, HDMI and HD ready as well at a comparatively cheaper price than 9600GT.\n\nOptical Drive\n\nAs far as Optical drives are concerned, I would go for the SATA DVD recorder from IDE and SATA given in the list. The motherboard has more SATA connectors than IDE connectors further there is no need to go for a Blu-Ray writer as the HD DVDs can be written to a dual-layer media easily if they exceed the usual size of a DVD (4.2 GBs).\n\nThe table below compares the cost of building the complete system from TIMR and other bargain prices from various sources\n\nComponent                      Cost at TIMR  Best Bargain Price  URL of the best bargain price\nMSI P45 Neo3-FR                $147          $139                Web.                         \nIntel Core 2 Quad Q8200        $298          $254                Web.                         \nCoolermaster Elite 330 (420W)  $131          $95                 Web.                         \nSATA 320GB                     $72           $72                 Web.                         \n2048MB DDR2-800                $78           $59.90              Web.                         \n512M ATI 4670                  $129          $114                Web.                         \n(PCI) Firewire/1394 Card       $33           $8.80               Web.                         \nSATA DVD-Recorder              $32           $27                 Web.                         \nTotal                          $920          $770                                             \n\n\nReferences\n\n\u201cBuild Your Own PC\u201d. 2009. Web.\n\nCCPU Computers. 2009. Web.\n\nComputer Target Online. 2009.\n\nGasior, G. \u201cHow to build a PC, A step-by-step guide to enthusiast system assembly\u201d 2009. Web.\n\nGocomp. 2009. Web.\n\nIntel Processor ratings. 2009. Web.\n\nITSky Online Store. 2009.\n\nTom\u2019s hardware guide comparison charts. 2009.\n\nMSY Technology Pricelist . 2009. Web.\n\nMegaWare Computers. 2009.\n",
        "label": "human"
    },
    {
        "input": "Computer Problems: Review Research Paper\n\nOver the years, there has been an increase in the number of user forum-based websites. This increase has reached levels where this genre is now referred to as a cottage industry. This paper presents the relevance and nature of these websites from the perspective of a personal experience.\n\nI was continuously experiencing difficulty with hard disk detection by several software setups. These also included the Windows XP setup. Upon placing the installation into the CD tray, the setup would self-execute and when the setup would perform the automated system evaluation to determine whether or not the system specifications were up to mark with the software requirements, it would conclude the processing by displaying a message stating that no hard disk could be detected.\n\nI attributed the problem to corruption in the setup software in the beginning, but I soon noticed that the problem was recurring more frequently. The problem became evident when upon trying to install software from a tried and tested CD, I was greeted by the same message once more. It was at this point that I realized that the problem was not with the setup programs, but possibly lay in my system.\n\nTo obtain help, I decided to use some of the many internet-based websites that offer free advice along with tips on tricks on how to achieve optimal performance. These websites are more than often utilize user forums to generate their database of free advice which remains open to all the members of the website who wish to take advantage of the information or add to the information by joining the forum.\n\nNeedless to say, these websites were not my first option. My first inclination was to make use of the windows website to try and figure out if there was a troubleshooter that Microsoft had to offer that could help me figure out the source of the problem I had at hand.\n\nTrouble shooters are special programs that operate by asking the user a number of questions in a sequence (Synergenics, LLC, 2008). These questions are designed to root out the problem that the user might be facing and the program usually functions by instructing the user on how to fix the problem once the problem has been identified by asking the user the sequential questions. In certain cases, some trouble shooters are designed to carry out a an analysis of the system in order to ascertain the nature of the problem and to instruct the user on how exactly the problem has to be solved without taking the user through a lengthy series of questions.\n\nI decided to use no more than three websites in my search to determine the solution to my problem. I was referred to the these three websites by my peers who told me they had experienced problems with their computers in the past and were of the opinion that these websites could provide me with the solution my problem. The three websites that were brought into use and have been covered in this paper as well include:\n\n 1. PC Pitstop\n 2. Computing.Net\n 3. TomsHardware.\n\nAll of these three websites seem to be designed on the same user group forum philosophy that has been mentioned above. Hence, it can be assumed that the advice that these websites gave was the information that the reader would eventually derive after going through the discussions on the query that the user was faced with. In this particular case, I was greeted by a barrage of information on all three websites. All three websites held user forums in which users had posted their advice and their comments regarding various problems and it took a bit of searching to find the user forum for the problem that I was faced with.\n\nIn the case of PC Pitstop , the advice I got was that I should consider completely formatting my hard disk of the previous operating system before initiating the installation and if the problem still persisted, the user forum led me to the conclusion that I should attempt to make my BIOS recognize the Universal Serial Bus as a bootable device in order to get on with the installation (Invision Power Board, 2009). So the advice given by PC Pitstop was based on a slight tinkering with the BIOS, and a complete reformatting of the current operating system from the hard drives.\n\nWhen going through Computing.Net , I was amazed to see that the problem I was facing had not only been experienced by other users the very same way such as I had seen in the PC Pitstop user forum, but was also shared by many other users who had experienced it at different points during the Windows XP setup as well. Computing.Net also offered hard drive recovery tools that were placed within the forum of my interest (Computing.Net LLC, 2005). Computing.Net was of the opinion that I required a few drivers to be installed before I began the actual windows installation. This of course required that I make additional purchases of driver CDs.\n\nMy third stop was Tom\u2019s Hardware , where I discovered that the issue of hard disk detection was even more widely encountered than I had presumed after witnessing the user forum at Computing.Net (Bestofmedia Network, 2005). The advice I found on Tom\u2019s Hardware however was one that I chose to go through after I had successfully installed Windows XP since I found next to no advice on the problem of no detection of hard drives before the installation.\n\nI consider it necessary to mention at this point that I found no significant help on the Windows XP website at all. The Windows XP was continuously subjecting me to a marketing campaign as I found myself drawn to the immense number of options that Windows XP had to offer and the equivalent number of tutorials that the website was overflowing with.\n\nIn times like these, I discovered that the significance of this cottage industry is one that allows users to gain the knowledge that they need quickly and in a cost-efficient manner. The websites also allowed users to take advantage of various shareware and freeware software by either directly downloading it or By doing so, these cottage industry websites allowed users to obtain cost effective technical support on Windows XP and other software while clearing the path for all the parties involved.\n\nWorks Cited\n\nBestofmedia Network. (2005). hard disk not detectable . Web.\n\nComputing.Net LLC. (2005). Hard Disk Not detected in xp instal . Web.\n\nInvision Power Board. (2009). Cannot detect hard drive . Web.\n\nSynergenics, LLC. (2008). EchoLink TroubleShooter . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Sciences Technology: Admonition in IT Security Research Paper\n\nTable of Contents\n 1. How admonition fits into an overall security strategy\n 2. Successful implementing admonition as a security measure\n 3. The effectiveness of admonition as part of a security system\n 4. References\n\nSecure Interaction Designing, a vital component in today\u2019s world of interface designing is the practice of designing a system, which aims at safeguarding the interests of the legal end users of the computer system. Various concerns have cropped up during the past times, which include the likes of viruses, malwares, spywares and phishing apart from traditional issues like online tracking systems and involuntary revelation of personal information.\n\nIn order to save the user from harm, the software\u2019s behavior must be in coherence with the anticipation of the user. Thus attending to security issues need an understanding of the user\u2019s perception of working of the system. If not, the user would not have the means to foresee and comprehend the outcome of his actions. Consequently, secure systems must implement security judgment founded on the grounds of user-oriented events but at the same time should permit those events to be articulated in well-known forms. (Gupta, 2007)\n\nHow admonition fits into an overall security strategy\n\nSecurity can be implemented by means of employing two approaches, namely: security by designation and security by admonition. In the Security by designation approach, subsequent to a user\u2019s triggering an event, the system takes the suitable actions linked with security issues. For instance, double-clicking on a folder icon would lead to rendering the contents of the folder. While in Security by admonition approach, it would present notifications, which the user needs to attend to and take suitable action, based on his judgment, from the available options offered by the system, and the system would display a caution message when the user attempts to proceed with an action, which might cause harm. This paper looks into the intricacies of the Security by Admonition approach and its effectiveness as a security strategy. (Pramod, 2007)\n\nProviding security as a component of the product or service package has increasingly become a trend amongst vendors who consider this as a vital element of their business strategy. However, in many security features, the concluding security-related decision is left to the end-user. According to experts, this policy originates in a clash between offering security and usability, where security typically encumbers and usability generally aids the user in accomplishing a job. (Gupta, 2007)\n\nSuccessful implementing admonition as a security measure\n\nFigure. Security by Admonition.\n\nThe above figure exemplifies a method employed to surmount the previously mentioned conflict. The security strategy symbolized by the rectangle incorporates a wide range of activities, which the user might find either adequate or else improper. Each time the user takes a course of action, which may be undesirable for him, but at the same time is allowable as per the security policy, the system calls for user affirmation of the action. This approach, entitled as security by admonition, leaves the ultimate decision to the end-user. (Pramod, 2007)\n\nSome illustrations of these notifications are the firewall popping up a dialog box requiring the user to authorize a process on a certain server requesting access to run on a particular port, the Internet Explorer phishing filter offering a caution message to the user that the webpage being navigated might be unsafe and the Firefox browser requiring heightened privileges with regards to some signed JavaScript code. Security by admonition policy relies on the understanding of the end-user regarding security and his computing knowledge to take proper decision. (Gupta, 2007)\n\nThe admonition approach allows a user to intervene instead of the system taking a security-related action, which might hinder the task of the user. However, an issue in the policy of admonition is at what time to warn and under what circumstance to proceed. The following figure explains this conflict.\n\nFigure. The system has to presume at which phase the caution should be generated.\n\nThe effectiveness of admonition as part of a security system\n\nProviding too little caution exposes the user to potential hazards whereas on the other hand presenting the user with too many caution messages vexes the user. The greater the incongruity between acceptable and permissible actions, the more difficult it is for the system to tradeoff between security and usability. (Gupta, 2007)\n\nAnother issue in this policy is the aggressiveness of designs in the form of compelling users to respond to a prompt. It generates a lot of annoyance amongst users at times. Effective designs should be conspicuous by their proximity and importance instead of their aggressiveness, which gets in the way of the accomplishment of user tasks. Examples of good designs can be the appearance of a transient bar in browser windows that doesn\u2019t obstruct the view of the page as a display prompt would, displaying information on the cursor trail, or reminders concerning passwords emerging beside a password field when it is active. (Pramod, 2007)\n\nFigure. Example of a good admonition design.\n\nSecurity by admonition as a successful IT security strategy can be analyzed in two distinctive methods. Firstly, as a social process, which makes entities act in a deliberate way based on social, conduct or secondly, with computer systems that plainly request an answer to a question. Social engineering can be considered as one of the most influential instruments in a firm\u2019s IT security policy.\n\nAs children, everyone has been educated to differentiate between correct and incorrect. The admonition is a policy simply based on that teaching. Its effectiveness depends on how well systems are designed and to what extent the balance between security and usability is achieved. To confront the challenges thrown up the admonition approach is not an effortless job for multinational corporations. However, overall it presents a relatively low outlay alternative as compared to existent complex security structures, which may give it an edge above the others. (Pramod, 2007)\n\nReferences\n\nGupta, Ashok K, Uma Chandrashekhar, Suhasini V. Sabnis, Frank A. Bastry; 2007; Building secure products and solutions ; Bell Labs Technical Journal; 12, 3, 21-38; Alcatel-Lucent; Bell Labs, Murray Hill, New Jersey.\n\nPramod, Hari, V. Koppol, Ashok K. Gupta, Dor Skuler; 2007; A service-delivery platform for extending security coverage and IT reach ; Bell Labs Technical Journal; 12, 3, 101-119; Alcatel-Lucent; Bell Labs, Murray Hill, New Jersey; Bell Labs, Cornelius, North Carolina.\n",
        "label": "human"
    },
    {
        "input": "Life, Achievement, and Legacy to Computer Systems of Alan Turing Essay\n\nAlan Turing the computer scientist, logician, cryptanalyst, and English mathematician was born on June 23, 1912. Turing was quite influential in computer science development, in addition to setting the framework for the formalization of the algorithm concept, in addition to the use of the Turing machine that he developed, for computation. Between 1945 and 1947, Turing was involved in an Automatic Computing Engine project. In February 1946, Turing presented a paper that has been considered the \u201cfirst detailed design of a stored-program computer\u201d (Copeland & Proudfoot 2004 par. 3-5). The University of Manchester appointed Turing to head its computing laboratory in 1949, as the deputy director. It was also during this time that Turing progressed with on another project that involved the development of software dubbed, the Manchester Mark I, regarded as amongst the earliest known stored-program computers (Agar 2002 p. 36).\n\nTuring endeavored to attend to the artificial intelligence challenge, and this is what prompted him to propose the Turing test, which is more of an attempt to come up with a standard that would allow for \u201cintelligent\u201d machines. The idea behind the Turing test holds that it is possible to assume that a computer is \u201cthinking\u201d if and when it is capable of hoodwinking an interrogator to harbor the assumption that the conversation that occurred was not with a computer, but with a fellow human being. In his proposal, Turing opined that as opposed to the development of a program that would simulate the mind of a human being, it would be far much easier to design a modest one that would instead simulate the mind of a child, followed by exposure of the same to an education program. Turing was also instrumental in the development of a chess program (Levin 2006 p. 43).\n\nTuring published a significant paper titled, \u201cOn Computable Numbers, with an Application to the Entscheidungsproblem\u201d (Turing 1936 p. 241), whereby he reformulated the results that had been realized by Kurt Godel before him, regarding computation and proof limits. Accordingly, Turing was able to replace the formal language that Godel had realized, and which relied on arithmetic. That is how the simple and formal Turing machines came along. Turing was able to prove that these kinds of machines had the capability to execute imaginable computation in mathematics, as long as an algorithm was used to represent it. The theory of computation heavily relies on Turing machines. The work by Turing is significantly more intuitive and accessible. T\n\nThe Association for Computing Machinery has been giving away the Turing Award on an annual basis since 1966 to individuals that exhibit profound technical contributions towards the world of computers. This is an honor in computing that is regarded in the same rank as the Nobel Prize (Geringer 2007 par. 5). Turing was named by Time Magazine as \u201cone of the 100 Most Important People of the 20th Century\u201d (The Time 100 1999 p.1) due to the role that he played in the development of the modern computer. As Time Magazine has noted \u201cThe fact remains that everyone who taps at a keyboard, opening a spreadsheet or a word-processing program, is working on an incarnation of a Turing machine.\u201d (The Time 100 1999 p.1).\n\nReference List\n\nAgar, J., 2002, The Government Machine . Cambridge, Massachusetts: The MIT Press.\n\nCopeland, J, and Proudfoot, D 2004, \u201cAlan Turing, codebreaker and computer pioneer\u201d . Web.\n\nCopeland, B., 2004, The Essential Turing , Oxford: Oxford University Press\n\nGeringer, S 2007, \u201cACMS Turing Award Prize raised to $ 250,000\u201d. ACM press release . Web.\n\nLevin, J., 2006, A Madman Dreams of Turing Machines . New York: Knopf (34)\n\nThe Time 100 1999, Allan Turing. The Time 100 . Web.\n\nTuring, A.M. (1936), \u201cOn Computable numbers with an application to Entscheidungsproblem\u201d, Proceedings of the London Mathematical Society , Vol. 2, No. 42, pp. 230-65.\n",
        "label": "human"
    },
    {
        "input": "Protecting Computers From Security Threats Research Paper\n\nNowadays, everybody use their personal computers for storing data which when lost creates a lot of trouble for us. In order to prevent spyware and viruses from entering our computer system we need to properly examine the sites before entering them. We should never give our personal information to any online site, like credit card numbers, even if we are asks for it, as we might become an identity theft victim. We should check the URL of the site before entering it.\n\nIf it starts with \u201chttps\u201d then it is very secure, otherwise if it starts with \u201chttp\u201d then the security of our computer may be compromised. The only information we should provide is our name or e-mail address. We should also be careful about certain e-mails that ask for our personal information. It could be a part of a scam called phishing where when we provide our details a spyware gets attached to our computer. (Adams, 2007)\n\nWorms and viruses spread very rapidly in our computer without our knowledge. Thus, we should install antivirus softwares and firewalls to stop unauthorized entries in our computers, before we install any other programs on it. Before purchasing an antivirus software or firewall we should know about its abilities and limitations through customer reviews. The virus and spyware programs are extremely smart ones and can even get past along some antivirus softwares and firewalls. Some cyber criminals are selling certain software to us with the promise that they are going to protect our computers but in reality these softwares are actually harming our system.\n\nThus, we should always use reliable softwares from reputed companies and not just from any online site or download them from anywhere. To ensure the safety and protection of our computer we should always install those antivirus software and firewalls, which automatically updates itself for at least a year after we install it. This guarantees that our software is of the latest version and thus, cyber criminals will have difficulty in penetrating their system to enter our computers.\n\nA number of antivirus softwares and firewalls are available in the market, some for free and some with a price tag. One such antivirus software, which can be used for protecting our computers, is the Avast Antivirus. It protects our computers from Trojan horses, viruses and worms. The official website of Avast is www.avast.com. The Avast 4.8 Home Edition is free to use, updates itself automatically, gives warning about license renewal, has user friendly interface, uses very little system resource, easy to install, has a Virus Recovery Database and Real Time Internet Monitor and detects almost every form of viruses and Trojan horses. (Avast, 2009)\n\nThe Cisco Pix Firewall has been rated as the best firewall in recent years. The official site of Cisco is www.cisco.com. It has many versions depending on the size of company, Cisco Pix 501, 506E, 515E, 525 and 535 Firewall. Its price also varies from $500 to $50000. It gives high performance, strong security, it is affordable, efficient, easy to install, run and almost impenetrable. It works on a real time, embedded, non-Unix security algorithm and does Stateful packet inspection. Their higher model also supports the Gigabit Ethernet interfaces. The adaptive security algorithm or ASA that is used by Cisco PIX Firewall also makes it one of the quickest firewalls available. (Cisco, 2006)\n\nReferences\n\nAdams, G. (2007). Protecting Your Computer From Viruses, Spyware, And Other Security Threats . Web.\n\nAvast; 2009; FREE antivirus software with spyware protection: avast! Home Edition ; avast. Web.\n\nCisco; 2006; Cisco PIX 500 Series Security Appliances ; Cisco. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Sciences Technology and HTTPS Hacking Protection Essay\n\nSynopsis : The author argues that HTTPS protocols used in websites have some security holes that are not covered by using SSL. The SSL technology is used to provide security but hackers can manipulate the SSL by decoding the data, using SSL-MITM or SSL Man in the Middle technique. Thus hackers can exploit the SSL to hack into the transactions made by users when they enter data in eCommerce sites and gain confidential information of the victim. The author proposes three different methodologies that can be used to stop the hacker from using the SSL-MITM technique.\n\nRelevance : The article has some amount of relevance to the class since many of the students do indulge in online shopping. Many are assured when they see the SSL symbol of a lock at the bottom of the screen and feel their information is confidential and the website is protected. However, this is not the case and it is possible for hackers to still hack and steal passwords. For one thing, the article cautions us to be careful while exchanging information on the net. For another, developers and programmers can make use of the article and make websites more secure.\n\nPersonal Reaction : The article was a sort of eye-opener and a bit scary as I always felt that it was safe to transact when the site is protected by SSL. However, this is not the case and even seemingly secure sites are not safe. I also begin to think that this information and the article should receive much more mid publicity and website owners and developers should be made aware of the problems and solutions. Another personal reaction is I begin to wonder if all the eCommerce sites, that are hosted by highly technical people have fooled millions of people into believing that the transactions on their site are safe.\n\nReason for selection : There are two reasons for selecting the article and the first is the interesting subject f the article and the second is the presentation and structure of the document. Website security has become very important as many of us undertake eCommerce transactions. The article has shown how even SSL that is regarded as a high-level security feature can be compromised. The article also gives names of hacking tools such as ARP Spoof, DNS Spoof, Sniffing and SSL Dump and process used by hackers along with screen shots of the hacking tools to show how hacking is done. This is very useful information for the lay reader as well as technical people. The article is also presented and structured properly with an abstract, introduction, body with different headings and a conclusion. The manner in which the information is presented and complex ideas are put forth is a good example of how academic articles should be written.\n\nDiscussion Questions : The article has brought about the very interesting topic of hacking and this concerns all of us. Some of the discussion questions are: Are the eCommerce websites that we use aware of these threats and have they initiated measures to prevent hacking? How do we verify if the websites we use are safe and that some hacker has not already hacked the system? Does changing passwords every time we use eCommerce sites help in increasing protection? Should we trust brick and mortar shopping malls more than eCommerce websites?\n\nReferences\n\nChomsiri Thawatchai, 2007. HTTPS Hacking Protection. IEEE, 21st International Conference on Advanced Information Networking and Applications Workshops. 0-7695-2847-3/07\n",
        "label": "human"
    },
    {
        "input": "The Life, Achievement, and Legacy to Computer Systems of Bill Gates Essay\n\nWilliam Henry Gates, commonly known as \u201cBill Gates,\u201d made major contributions to the world of computer technology. As the president and the CEO of Microsoft Corporation, the market leader in supplying software for personal computers, he became the youngest billionaire in the computer industry. His life and achievements have left a legacy in the field of computer systems (McCrossan, 2000, p.99; Wallace & Erickson, 1992).\n\nOn 28 October 1955, Bill Gates was born in Seattle to a financially stable family. His father, William H. Gates II, is a famous attorney in Seattle while his late mother, Mary Gates was the director of First Interstate Bank. He learned at the private Lakeside School that placed him in an academically challenging environment. At Lakeside, he met his friend and associate, Paul Allen, and together they ventured into the field of computer programming. At the tender age of 15, Gates managed to crash the DEC and the CDC operating systems that were dreaded as the most complicated computer systems then. This achievement brought him some problems and made the way for his first business deal. Gates and Allen founded the Traf-O-Data company where they came up with a machine that was able to monitor progress of traffic in Seattle. This business adventure did not last long due to low returns.\n\nGates and Allen then signed a contract with TRW, a firm that made software products. They gained valuable experience in software development while working for this corporation. Bill Gates joined Harvard University in 1973 as a freshman, where he stayed a few metres from the present Microsoft president, Steve Ballmer. While studying at Harvard, Gates and Allen came up with the BASIC programming language for the initial microcomputer \u2013 the Mango Information Technology Systems (MITS) Altair 8800. In 1975, Gates and his boyhood friend founded Microsoft Corporation. Two years later, he quit Harvard to devote his energies to their company. Propelled by a conviction that the personal computer would become a necessity in every home and office, they started making software for personal computers.\n\nIn 1980, Microsoft signed a contract with International Business Machines Corporation (IBM) to have the rights to O-DOS operating system. They developed O-DOS to use IBM\u2019s new PCs. MS-DOS was developed a year later (Lesinski, 2007, p.42). In 1983, Gates ventured into the field of application software by launching WORD 1. Microsoft also introduced Graphical User Interface (GUI) referred to as Windows and the computer mouse, which transformed how a user interfaces with the personal computer. In May 1990, Microsoft launched Windows 3.0 and became the market leader in providing PCs software.\n\nPresident George Bush recognized Gates role in making Microsoft the largest computer-industry corporation and crowned him with the reputable National Medal of Technology. Gates capitalized on the advancing use of the internet by launching Windows in 1998 that had Microsoft browser incorporated into the software. Under Gate\u2019s management, Microsoft maintained its goal of advancing and improving computer software industry. He strove to ease, reduce costs and make it more pleasant for individuals to use computers. Although Gates resigned in 2000 as the Microsoft CEO, his role in technological innovation, ardent strategy in business, and aggressive competition skills in building the success of Microsoft, is applauded.\n\nReferences\n\nCerf, V. et al., 1974. Specification of internet transmission control program. Web.\n\nNetwork Working Group. Web.\n\nHowe, W., 2009. A Brief History of the Internet . Web.\n\nLesinski, J., 2007. Bill Gates . Minneapolis: Lerner Publications Company.\n\nMcCrossan, J., 2000. Books and reading in the Lives of notable Americans: a biographical sourcebook. Westport: Greenwood Press.\n\nRuthfield, S., 1995. The Internet\u2019s History and Development . Web.\n\nWallace, J. & Erickson, J., 1992. Hard drive: Bill Gates and the making of the Microsoft empire New York: Wiley.\n\nWiggins, W., 1994. The Internet for Everyone: A Guide for Users and Providers. New York: McGraw-Hill.\n",
        "label": "human"
    },
    {
        "input": "Overview of Computer Languages \u2013 Python Report\n\nTable of Contents\n 1. A Brief History of Python Computer Language\n 2. Purpose of the Language\n 3. Advantages and Disadvantages of Python Language\n 4. The Application of Python Programming\n 5. Conclusions\n 6. References\n\nThe history of programming languages dates back to over seventy years since the development of computers. Computers are not only playing a growing role in traditional scientific computing but are also widely used in other fields. The world of computing technology is fast-changing as numerous world\u2019s leading tech firms compete to introduce the most innovative ideas. As a result, the comprehension of programming languages is becoming more necessary. Even students at the elementary levels are currently beginning to learn programming languages, making computer languages all the more relevant. A computer language helps people to speak to the computer in a language that the computer understands. As a consequence, programming is at the heart of technological innovation that is in use today. The paper discusses python computer language.\n\nA Brief History of Python Computer Language\n\nPython language first appeared in 1991 as a successor for ABC language. Python project was created by Guido van Rossum who was by then a lead developer. Python version 2.0 was launched in 2000 and included features such as garbage collection and list comprehensions by means of reference counting (Shukla & Parmar, 2016). Come 2008 and python version 3.0 was launched, with the most critical adjustments being that the language was not backward-compatible and any code made in python version 2.0 was required modifications first before it can run on python version 3.0. The update and release of python 2.0 versions were discontinued early this year \u2013 2020, no more security patches will be expected for python 2.\n\nPurpose of the Language\n\nPython is a high-level, interpreted, and general-purpose language. The language emphases are on the readability of the code as programmers and developers can leverage whitespaces. In addition, the language utilizes an object-oriented and language constructs approach with the goal of aiding programmers to compose logical codes that are clear for both small-scale and large-scale projects. Again python is both garbage-collected and dynamically typed. As a result, the language supports various paradigms, including object-oriented, functional programming, and structured programming. Python was designed to be highly extensive instead of having all its functionalities built into its core (Alyuruk, 2019). As a result, this compact modularity has made python more preferred as a method of adding programmable interfaces to existing applications.\n\nAdvantages and Disadvantages of Python Language\n\nPython language is among the few top-most popular programming languages of 2019. Python continues to be popular, thanks mainly to its role in data science and teaching. The advantages of python are also outstanding\u2014some of the benefits associated with python programming, including a comprehensive standard library for reference. Python\u2019s syntax is very clear, and it is not even a free-form language (Dierbach, 2014). The second feature is that python\u2019s extensibility is reflected in its modules, which have the richest and most powerful class libraries in the scripting language. However, python also has some of the weaknesses of interpreted languages. The first disadvantages are that Python programs run slower than programs developed using programming languages like Java, C, or C++. Again, the open-source nature of python means that the python language cannot be encrypted.\n\nThe Application of Python Programming\n\nThe most crucial application of python is that it is universally embedded in a scripting language, which is a firm foundation on which numerous web frameworks and automation tasks, including 3D software applications. In addition, python can be used to complete the activities of the desktop tools of program and data calculations. Python elasticity makes it possible to develop apps that are compatible with various operating systems, including Android OS.\n\nNumerous Python interpreters are available for various operating systems making python computing language a reliable, robust, and efficient language for use in different platforms. In addition, a world community of programmers develops and maintains a free, and open-source reference implementation called CPython. Also, Python Software Foundation, which is a not-for-profit consortium, directs the resources for the development of both Python and CPython (Bogdanchikov & Zhaparov, 2013). Python programming has been used to create numerous software programs that are doing well in the technology niche, such as YouTube, Google, Reddit, Instagram, Spotify, Dropbox, and Quora.\n\nConclusions\n\nTo sum up, computers are no longer everyone\u2019s previous impression of the desktops or servers but has evolved into the objects around us everywhere. For example, phones, tablets, laptops, and devices that many people don\u2019t realize are all computing devices. Other devices such as TV sets, microwave oven, car, even a small robot that children play with have computing capabilities. The development of computer programming language is developed with the development of computer hardware, and programming language is an indispensable tool to shape the computer from the development history of computer language\u2014the more advanced the language, the closer to people\u2019s thinking convenient to use. Therefore, the development of computer language in the future is bound to be more accessible to human beings and closer to human life.\n\nReferences\n\nAlyuruk, H. (2019). Chapter 1\u2014Introduction to R and Python . In H. Alyuruk (Ed.), R and Python for Oceanographers (pp. 1\u201321). Elsevier. Web.\n\nBogdanchikov, A., & Zhaparov, M. (2013). Python to learn programming. Journal of Physics: Conference Series , 423 , 1\u20135. Web.\n\nDierbach, C. (2014). Python as a first programming language. Journal of Computing Sciences in Colleges , 29 (6), 153\u2013154.\n\nShukla, X. U., & Parmar, D. J. (2016). Python \u2013 A comprehensive yet free programming language for statisticians. Journal of Statistics and Management Systems , 19 (2), 277\u2013284. Web.\n",
        "label": "human"
    },
    {
        "input": "Computerized Profiling Systems Capps II Essay (Critical Writing)\n\nIntroduction\n\nComputerized profiling systems such as CAPPS II can be effective when combined with overarching risk assessment and strategic screen to competently scope airplane security; therefore should be seen as a component rather than the center of security systems (Garrick, 2004). Staff across the airport, not just at security checkpoints can be trained in human profiling and behavioral analysis that would allow identifying suspicious actions or individuals. It can provide a broad security reach in the facility and maintain a security mindset and network to prevent potential threats (Leather, 2019).\n\nMain body\n\nProfiling, particularly digital systems (CAPPS), are meant to be race-neutral techniques, focusing on behavioral aspects while databases are built using security and criminal records. Governments and airports can negotiate ethical use of the data to ensure safety Garrick, 2004).\n\nAirport security costs may be lowered with a comprehensive profiling system that requires one-time installation rather than individual and error-prone human identifications.\n\nComputerized profiling systems with sensors can minimize inconvenience to passengers, reducing the need for constant bag checks, rather than only directing people of interest for additional security checks.\n\nFormal computerized profiling systems as well as human profiling training can be controlled in terms of criteria for better monitoring and to avoid subjective fallacies in identifying behaviors (Erg\u00fcn et al., 2017).\n\nTraditional methods such as x-ray machines, metal detectors, and a small number of government agents on the ground are ineffective when working individually and treating everyone the same (Davies, 2019).\n\nProfiling has subjective elements and both computer and human profiling systems can evolve, eliminating the element of predictability of traditional security systems, making it much more difficult for malicious actors to bypass (Davies, 2019).\n\nProfiling is based on collected data and statistics which contributes to the objectivity of a subjective process. However, using data on patterns of behaviors and characteristics of individuals in the long term is likely to have a positive effect (Leather, 2019).\n\nConclusion\n\nThe subjective argument \u2013 profiling commonly affects a small number of passengers, and for most, it becomes a simple inconvenience of additional security checks. The risk-consequence of profiling outweighs the consequences of a massively deadly terrorist attack which have been prevented through profiling security systems.\n\nReferences\n\nDavies, R. (2019). Point of controversy: does passenger profiling at airports work? Airport Technology. Web.\n\nErg\u00fcn, N., A\u00e7\u0131kel, B. Y., & Turhan, U. (2017). The appropriateness of today\u2019s airport security measures in safeguarding airline passengers. Security Journal, 30(1), 89\u2013105. Web.\n\nGarrick, J. B. (2004). Comments on \u2018CAPPS II: The Foundation of Aviation Security?\u2019 Risk Analysis 24 (4), 925-927.\n\nLeather, A. (2019). Passenger Profiling: cases for and against. Aviation Security International. Web.\n\nMaccario, C. (2010). Debating Behavior Profiling For Airport Security. NPR . Web.\n\nDisadvantages\n\n  * Margin of error and unclear evidence to suggest how computerized profiling systems can identify an actual terrorist. There is no trustworthy method to test the computer systems prior to implementation (Barnett, 2004).\n  * Broad system profiling is subject to base rate fallacy on such a large scale, and such broad profiling, including behavioural will be ineffective from any perspective. The number of actual terrorists is already low, and profiling from a mathematical perspective is highly unlikely to identify a terrorist. In fact, many experts have suggested the use of secondary randomized screening as more effective in many cases (Schneier, 2012).\n  * Profiling systems along with human profilers are likely to engage in racial and ethnic profiling despite this being incorrect or unethical. The profiling process naturally leads to stereotyping that would adversely target specific groups (Edmonds,\n  * Terrorists are more likely to deliberately select subjects that are able to avoid established profiles, particularly if based on demographic, ethnic, or religious elements. It is significantly easier for a terrorist to mask themselves as a member of a particular group that is deemed low-risk by security agencies (Schneier, 2012).\n  * Since profiling inherently is based on classification, passengers are designated based on risk. If reliant on profiling, there is a significant potential of a malicious actor designated as low risk either by accident or through manipulating the system, thus easily bypassing security checks (Barnett, 2004).\n  * Extensive civil liberties concerns both through creation of passenger databases and classification records as well as targeting of innocent individuals despite their excellent records as citizens. Creates instances of unlawful detainment and other violations of human and citizen rights ().\n  * Waste of precious security resources on a marginally effective system plays int the hands of terrorists, subjecting passengers to searches and detainment when they pose no threat to the flight. Creates opportunities to manipulate the system to bypass it (Aggar, 2005).\n  * Profiling creates disruptions to passenger travel and commercial services that airports and airlines have invested significantly to improve passenger-centric experiences. Any incidents or negative press will likely divert passengers from air travel or a specific location which may have negative financial impacts (Shaver & Kennedy, n.d.).\n  * Computerized profiling systems have high initial costs, but in order to be effective they must be installed in the majority of airports in a country. The scale is extremely large and costly, considering the profiling systems are updated every few years and also require technical maintenance (Shaver & Kennedy, n.d.)\n\nReferences\n\nAggar, M. (2005). Passenger profiling: Dispelling the myths surrounding the controversy. Aviation Security International, June , 20-22.\n\nBarnett, A. (2004). CAPPS II: The Foundation of Aviation Security? Risk Analysis 24 (4), 909-916.\n\nEdmonds, D. (2017). Does profiling make sense \u2013 or is it unfair? BBC . Web.\n\nShaver, R., & Kennedy, M. (n.d.). The benefits of positive passenger profiling on baggage screening requirements. Web.\n\nSchneier, B. (2012). The trouble with airport profiling. Forbes . Web.\n",
        "label": "human"
    },
    {
        "input": "Usability of Computerizes Provider Order Entry Systems Essay\n\nThe healthcare information technologies have been adopted to improve the patient experience and achieve more efficient medical results. The implementation of such systems has become a major topic for debate in healthcare research and practice. The issues that prominently emerge include the usability of the systems and their interoperability. This paper summarizes a recent research article on the usability of computerizes provider order entry (CPOE). The article selected is a primary research article by Rabiei, Moghaddasi, and Heydari (2018). Using various other resources, the concept of interoperability of the CPOE will also be highlighted.\n\nThe CPOE is a critical part of the clinical information systems that allows the providers to order issues electronically. The study by Rabiei et al. (2018) seeks to explore the usability of the CPOE in electronic prescription in Tehran, Iran. The researchers used questionnaires in five hospitals that used the CPOE systems; a total of 50 questions were included in the questionnaires that were distributed to 254 nurses who used the system. IBM-SPSS was used for analyzing the data. The elements of usability examined included user friendliness, decision support, and prescription support. The concept of usability, as defined by Rizvi et al. (2017), entails a set of evaluation methods seeking to provide an understanding of the user experiences in healthcare technology. The ISO definition of usability is the degree to which a product is deployed with efficiency, effectiveness, and satisfaction to meet specified goals.\n\nThe evaluation of healthcare technologies also seeks to determine the interoperability of various healthcare information technologies. Interoperability entails the ability of two or more components of systems to share or exchange information and utilize it (Oyeyemi & Scott, 2018). The primary research by Rabiei et al. (2018) does not examine interoperability of the CPOE, but the findings indicate that interoperability is a serious issue. The review of literature presented by the authors indicate that the usability of the CPOE can be examined from its ability to display all orders, the capability of displaying all demographic and prescription data. In addition, the ability to generate alerts, control drug orders, and using multiple screens simultaneously can be also taken into account. Such elements of the CPOE are supported by the researchers, and they indirectly point to interoperability as the CPOE systems do not operate as stand-alone systems (Zahabi, Kaber, & Swangnetr, 2015).\n\nThe examination of the usability of CPOE reveals a number of issues. One of the most interesting findings was that the public hospitals were limited by the bureaucratic systems within which the acquisition and implementation of the CPOE were done. In private hospitals, however, the constraints were absent, and that allowed the private healthcare facilities to have a wider variety from which to select the CPOE systems (Rabiei, et al., 2018). Usability was, therefore, far better in private healthcare as compared to public hospitals. Another key finding that supports both usability and interoperability is the decision support. Rabiei et al (2018) found that decision support had the highest mean score which they interpreted as having the capability to interact with other sub-systems in the hospital information system. Such finding supports subsequent observations by such researchers as Elshayib and Pawola (2020) who find that the CPOE shows a high level of effectiveness when integrated or combined with the decision support systems of the hospitals. The clinical decision support systems can be seen as a major source of the data used in the CPOE.\n\nThe CPOE systems are implemented to improve patient experience, and the usability can be evaluated on the extent to which that objective is achieved. A summary of other key findings in the article by Rabiei et al. (2018) reveals how CPOE systems enhance the safety of the patients through error minimization among other processes. Patient safety was a key usability element with some CPOE systems examined achieving a mean score of 3.88. The patient safety attributes included the display of client medications when prescribing, mandating patients to explain the cause of subscription, requesting for signatures before subscribing, and enabling a subscription invalidation among others. Prescription support was another usability element that involved displaying of current and past prescriptions and the ability to add new drugs to the prescription. User-friendliness was assessed in terms of allowing users to return to previous screens and ease of use among other aspects. Lastly, the researchers examined usability in private hospitals where the score was higher than in public hospitals.\n\nIn conclusion, the research paper on the usability of the CPOE systems addressed the usability and interoperability concerns. The usability aspects examined in the research include patient safety, prescription support, and user-friendliness. Even though addressed as a usability aspect, decision support is seen as an interoperability feature that allows the CPOE systems to share data with other health IT systems and use it. The findings from the study conclude that the CPOE systems score high in terms of usability and interoperability. It has been established that the CPOE systems work best when used alongside the decision support systems.\n\nReferences\n\nElshayib, M., & Pawola, L. (2020). Computerized provider order entry\u2013related medication errors among hospitalized patients: An integrative review. Health Informatics Journal, 1-26.\n\nOyeyemi, A., & Scott, P. (2018). Interoperability in health and social care: Organizational issues are the biggest challenge. Journal of Innovation in Health Informatics, 25 (3), 196-198.\n\nRabiei, R., Moghaddasi, H., & Heydari, M. (2018). Evaluation of computerized provider order entry systems: assessing the usability of systems for electronic prescription. Electronic Physician, 10 (8), 7196-7204.\n\nRizvi, R., Marquard, J., Hultman, G., Adam, T., Harder, K., & Melton, G. (2017). Usability evaluation of electronic health record system around clinical notes usage \u2013 an ethnographyic study. Applied Clinical Informatics, 8 (4), 1095-1105.\n\nZahabi, M., Kaber, D., & Swangnetr, M. (2015). Usability and safety in electronic medical records interface design: A review of recent literature and guideline formulation. Human Factors, 57 (5), 805-834.\n",
        "label": "human"
    },
    {
        "input": "Computers and Transformation from 1980 to 2020 Essay\n\nIntroduction\n\nMain idea\n\nThe humanity dreams about innovative technologies and quantum machines, allowing to make the most complicated mathematical calculations in billionths of a second but forgets how quickly the progress of computers has occurred for the last forty years. Data volumes, sizes, and productivity of modern devices have been updated so much that ancestral forms can be called \u201cdinosaurs\u201d of informatics.\n\nThesis Statement\n\nThe modern computers are largely updated in comparison with the ancestral forms from 1980.\n\nPioneer among Computers\n\nThe development of electronics contributed to the appearance of a non-primitive computing device. By the second half of the twentieth-century, specialists already had at their disposal advanced resistors, relays, and chip elements by those standards of the time (Grudin, 2017). Thus, the first personal computers created for commercial sale to users were presented to the public in the mid-1970s: it was a small staffed device with a processor clock frequency of 1 MHz and RAM up to 48 kilobytes from Apple (Safian, 2018). Despite so few hardware features, this computer was a pioneer, expanding the field for creativity to all other companies.\n\nBody Paragraph 1 \u2013 Discussion Point 1: Processor\n\nTopic Sentence\n\nOver forty years, processors have undergone enormous changes, acquiring considerable computing power.\n\nSubject A \u2013 Supporting Detail 1\n\nBy the end of the 1970s, the market was filled with a wide variety of variations of eight-bit processors. The sales leaders were companies that deserved respect among modern consumers as well \u2013 Intel and Motorola (Zimmermann, 2017).\n\nSubject A \u2013 Supporting Detail 2\n\nSince 1980, there has been a revolution in the world of computer technologies, as 16-bit and even 32-bit models replaced the old processors (Grudin, 2017). Processors of that time were produced by the 3-micrometer technological processor, although in 1982, Intel brought this figure to 1.5 (Martindale, 2020). At the same time, the clock frequency of the processor did not exceed 10 MHz, and the bit rate of registers was 16 bits. The amount of data nested in the device was 1 Mbyte.\n\nSubject B \u2013 Supporting Detail 1\n\nIn 2020, these parameters do not seem to be as grandiose. For comparison, the flagship line of 10th generation Intel Core i7 processors, which most adequately describes the achievements of modern circuits, has 14-nanometer lithography, a base clock frequency up to 2.60 GHz and 128 gigabytes of memory (Martindale, 2020). Such results allow us to achieve billions of calculated actions per second.\n\nSubject B \u2013 Supporting Detail 2\n\nHowever, it should be remembered that processors for computers, as a rule, are based on a microscopic silicon crystal, and silicon technology is rapidly approaching the limit of its physical capabilities. For this reason, the pace of processor development is gradually decreasing.\n\nBody Paragraph 2 \u2013 Discussion Point 2: Memory\n\nTopic Sentence\n\nIn general, RAM and ROM storages have tended to reduce their physical size while increasing the amount of accumulated data.\n\nSubject A \u2013 Supporting Detail 1\n\nIn 1980, the first five-inch hard drive was produced in 1980 with a capacity of 1GB, but at that time, it weighed over half a ton (Zimmermann, 2017). At the time, RAM was measured in megabytes, which was enough for the job.\n\nSubject A \u2013 Supporting Detail 2\n\nModern ROMs and RAMs should be as large as possible to provide the user with a smooth and uninterrupted workflow. Therefore, the current size of RAM is counted in tens of GB, and ROM has long passed the mark of terabytes.\n\nBody Paragraph 3 \u2013 Discussion Point 3: Screen\n\nTopic Sentence\n\nMonitors, as one of the main components of computers, have undergone many modifications in the history of their existence.\n\nSubject A \u2013 Supporting Detail 1\n\nEven in the period up to 1980, electron-beam monitors were replaced by liquid crystal analogs.\n\nSubject A \u2013 Supporting Detail 2\n\nHowever, unlike modern versions of displays with a diagonal to 55 inches and 60-150 Hz refresh rate, versions from 1980 were much more primitive. For example, such monitors were monochrome and required a separate backlight.\n\nSubject B \u2013 Supporting Detail 1\n\nJust over the next five years, IBM and Apple have made significant improvements to the displays, adding color models, and improving transmission quality.\n\nBody Paragraph 3 \u2013 Discussion Point 3: Mouse and Keyboard\n\nTopic Sentence\n\nComputer peripherals have undergone the most changes over the past forty years, not only in terms of the principle of operation but also in terms of appearance.\n\nSubject A \u2013 Supporting Detail 1\n\nComputer mice from the 1980s gradually replaced optical sensors with a rubber ball, but the concept of buttons remained unchanged.\n\nSubject A \u2013 Supporting Detail 2\n\nIn the world of modern devices, there are two ways: either computer mice are replaced by the touchpad, or the mouse improves. New mouse models are usually based on laser systems. Wired mice are a thing of the past and are replaced by Bluetooth or USB devices.\n\nSubject B \u2013 Supporting Detail 1\n\nThe first input devices developed in the 1980s were not separate from the central computer: the motherboard was located in one case with a keyboard. The number of keys was limited, and most of the function keys used today were not available.\n\nSubject B \u2013 Supporting Detail 2\n\nThe most impressive features of 2020 include touchscreen keyboards, wireless keyboards with fingerprint sensors, and high functionality fully.\n\nConclusion\n\nRestate Thesis\n\nIn conclusion, computers have undergone a significant number of transformations over the past forty years.\n\nOpinion\n\nSome have only changed the appearance and form factor, while others have entirely redefined the machine concept. From a comparative analysis, we can see that since 1980 the devices have had several trends for further evolution. Firstly, the chipboard portion has been updated as much as possible: the computing power of computers has increased millions of times. This makes it possible to perform more complex mathematical calculations in the tiniest fraction of a second. Secondly, due to the growing demand for computational power, computers have got more memory. Today, even the size of the clipboard is larger than the RAM of devices in 1980. Finally, the output and input devices have changed. Displays have become more qualitative, acquired different colors, and increased the density of pixels. The mouse and keyboard did not change their concept categorically but began to have a more convenient and organic design in addition to increased functionality.\n\nReferences\n\nGrudin, J. (2017). From tool to partner: The evolution of human-computer interaction. Synthesis Lectures on Human-Centered Interaction , 10(1), 1-183. Web.\n\nMartindale, J. (2020). The best Intel processors for 2020 . Digital trends. Web.\n\nSafian, R. (2018). Why Apple Is the world\u2019s most innovative company . Fast Company. Web.\n\nZimmermann, K. A. (2017). History of Computers: A Brief Timeline . Live Science. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Tech Company\u2019s Medical Leave Problem Case Study\n\nIntroduction\n\nComputer Tech\u2019s human resources director certainly faces a very difficult decision whether or not to prolong Maura Currier\u2019s FMLA leave. On the one hand, it is clear that given the fact that Currier has worked for the company for four years, she has accumulated a lot of valuable experience in dealing with the specific duties of a head supervisor. Such a position demands a lot of knowledge but above all, it demands having good contacts with the subordinated employees and it takes time to build up such relationships within a group. It is clear that losing such a worker would present the company with a serious handicap and could damage its productiveness. Furthermore, the difficult situation she faces in her private life is something that everyone has to understand because being the only one capable of helping a sick parent places ultimate responsibility on the individual. What is more, everyone in the company empathizes with her and is willing to support her. However, certain facts go against prolonging the leave.\n\nThe newly passed FMLA act\n\nFirst off, the newly passed FMLA act places a technical obstacle on prolonging the leave which is very difficult to overcome. Namely, in the act, it is clearly stated that the employee can use up as much as 12 weeks of unpaid leave in a year to take care of a sick parent. The company has already been maximally supportive in terms of distributing the time of the leave in a way that was most suitable to Currier. She has had 60 Fridays off work thereby using up all the available time for the unpaid leave. It is very difficult to go against established rules even in cases where it seems completely justified. In addition, there is also a problem concerning the increased workload of other workers who even though they are willing to give full support to Currier, feel that they cannot cope with the increased amount of work.\n\nIf I were the human resources director, there is no doubt that I would have much difficulty in deciding how to act in this case. However, despite all the supportiveness and understanding I have for her issue, I would have to decide against prolonging her leave.\n\nConclusion\n\nThe main reason for that is the fact that one has to have certain formal regulations that govern his work and it is clear that all possibilities in this respect have been used up and any further extension would be in breach of the company\u2019s policy. In addition, it is unfair towards the workers to suffer an extremely heavy workload despite their willingness to help. Nevertheless, given the fact that Currier is in a very difficult situation and that she is of great value to the company, I would certainly seek other means of helping her to keep her within the company\u2019s workforce. The possibilities for such help are perhaps to be sought on another front and I would consider some kind of financial help. For example, it would perhaps be possible to increase her wage or provide some kind of bonus that could cover the expenses of getting medical and housekeeping services for her mother. That way Currier would not be forced to spend so much time helping her and could devote her energy to working full time. This would also probably have a positive influence on her work motivation since she would have a sense of gratitude towards the company for being supportive in a difficult situation. I think that it is important to respect the regulations imposed by the company\u2019s policies, but they should not be a barrier to helping valuable employees when they fully deserve it and it is always possible to find a solution.\n",
        "label": "human"
    },
    {
        "input": "Maintenance and Establishment of Computer Security Case Study\n\nTable of Contents\n 1. Thesis statement\n 2. Introduction\n 3. Asymmetric and Symmetric Cryptography\n 4. Conclusion\n 5. References\n\nThesis statement\n\nComputer security is a crucial concept in ensuring organizational performance and efficiency and effectiveness in service delivery. This is so, especially since the application of technology in delivering services is now a basic need to any organization that desires to compete favorably in the market economy, given the rise and move towards globalization (Conklin, 2010).\n\nData and network security form part of the foremost computer security concern in most organizations. This includes both asymmetric and asymmetric method of ensuring the maintenance and establishment of security. However, deciding between these two types of computer security is a substantial problem that most organizations are facing.\n\nIntroduction\n\nIn deciding which among the two is suitable for use in ABC, an analysis of the two is necessary. Researchers say public key cryptography is more secure than the private key cryptography, but others dispute this view as being so much biased. In examining the extent of their use, none of the two is more popular than the other, but both apply to their suiting situations and to which their unique benefits pertain (Conklin, 2010).\n\nAsymmetric and Symmetric Cryptography\n\nAsymmetric key algorithms are well known for their use in public key cryptography given their mathematical relationships and links. This also entails the use of keys, where someone ensures that the information is encrypted so that they are in the cipher text. This key is well known for its confidentiality capability and its support for digital signatures used in ensuring authentication and reducing issues of contradictions.\n\nConvenience associated with asymmetric cryptography forms its greatest advantage and makes it a better choice. This is as a result of the fact that public keys are readily available for use by any interested party. Authenticity, validity, and accountability issues are also addressed and effected by use of this method adding up to its strengths.\n\nOn another perspective, this method has some limitations. Key among them all is the need to authenticate them and their slow nature. Asymmetric keys are also hard to replace once lost, are too much compromising on most security issues and require so many computer resources to function effectively (Stallings, 2011).\n\nOn the other hand, symmetric key cryptography is a situation where only one key is used both in encrypting and decrypting. This comes with a challenge to key management if the keys are to be used effectively and securely. This cryptography is also known as private key cryptography or one- key cryptography. This encryption is easier and faster to implement and use as it is less complex especially when the files are managed by one person. It also uses minimal resources, and in the event of a security compromise, only one set of communication is affected while the rest remains secure (Stallings, 2011).\n\nHowever, there are some limitations associated with the use of this method for security of data and networks. Ensuring the key remains a secret and unknown to people who should not access it is a leading limitation, followed by the need to have many keys for all the communications that are to be made. It is also a challenge to verify how much authentic a message is since both parties use the same key.\n\nConclusion\n\nHaving discussed the two possible methods of ensuring security of information in ABC is maintained, use of asymmetric key algorithms is seen as the best solution to their situation. This is true after considering how confidential their data is and the fact that the keys will be accessed by multiple people legible to access to and use of information. It is a somehow public institute that has many users, ranging from the internal users to other third party stakeholders (Conklin, 2010).\n\nReferences\n\nConklin, W. A. (2010). Principles of computer security: CompTIA security+ and beyond (2nd ed.). New York: McGraw-Hill.\n\nStallings, W. (2011). Cryptography and network security: principles and practice (5th ed.). Boston: Prentice Hall.\n",
        "label": "human"
    },
    {
        "input": "Research Tools Used by Computer Forensic Teams Essay\n\nComputer Forensics is a branch of digital forensics which is used in \u201cidentifying, preserving, recovering, analyzing and presenting facts and opinions about the information\u201d (Caloyannides, 2001, p. 22). This branch is mainly associated with the wide range of computer crimes. With the discovery of internet, computer crimes have been reached a significant level. There are many activities like viruses through which computer systems either in digital or electronic form are attacked all over the world. Computer Forensics teams are present to monitor and investigate such intrusions and protect computers and networks from such situations.\n\nSeveral research tools are introduced by the software developing companies which facilitate examinations of cyber-attacks for computer forensics teams. Some of the most wide-spread tools are given below:\n\n  * Appliance for Digital Investigation and Analysis (ADIA)\n  * File Recovery Software\n  * File Viewers\n  * Password Recovery Software\n  * X-Ways Forensics\n  * Writing Tools\n  * Network Tools\n 1. Appliance for Digital Investigation and Analysis (ADIA): It is a VMware based appliance which is used for digital investigation and acquisition. Some of the best features of this tool are that it is built from public domain software and is free of charge. Besides, it is released on March 2012 and publically available.\n 2. File Recovery Software: Another research tool used by computer forensics is file recovery software, which helps to find out the deleted files. Moreover, in many cases it also assists in recovering the files which are very difficult to analyze.\n 3. File Viewers: There are various types of file viewers software which enable computer forensics to view the file without actual opening it.\n 4. Password Recovery Software: This is another tool that helps computer forensics team regain the password. It can be considered one of the best tools since it is used to access password protected files. In addition, even if the password list is destroyed password recovery software can still recover it. By using the tool, computer forensics can investigate cybercrime more efficiently.\n 5. X-Ways Forensics: Integrated Computer Forensics Software: It is an advanced research environmental tool for computer forensics. Being very efficient, it runs fast and finds deleted files. It is comprised of imaging, disk cloning, data interrupter, etc. ( ISFCE: Certified Computer Examiner , 2005).\n 6. Writing Tools: Writing tools replicate the data of hardware and software.\n 7. Network Tools: Network tools are applied to analyze network traffic. For this purpose, packet sniffers are used which help computer forensics detect and analyze a live attack on the system. By using such tools, computer forensics team can perform its duties in a more effective manner. The team will be able to define the place where the attack has been carried out from. All the tools mentioned above are used by computer forensics to detect and control computer intrusions. All of them are also to decrease the rate of computer crimes.\n\nThus, there are several Forensics Toolkits available in the markets which are free of charge, for example, CERT Forensics Toolkit. CERT Forensics Toolkit contains tools that are freely available to federal, state, local law enforcement agencies, including the Department of Defense, within the USA (Caloyannides, 2001). Apart from this, there are also some tools which are open source and are free for everyone. Some of them are Data Acquisition, Volume Systems, File System, Memory and etc.\n\nReferences\n\nCaloyannides, M. (2001). Computer Forensics and Privacy. Boston, MA: Artech House.\n\nISFCE: Certified Computer Examiner . (2005). Web.\n",
        "label": "human"
    },
    {
        "input": "Sales Plan for Computer Equipment Proposal Essay\n\nEvery firm that sells goods or services needs its sales plan. It is a document that allows forecasting a company\u2019s sales and the amount of work for each employee for a specified period. It is quite important to avoid mistakes in the formation of the sales department, set realistic goals, and follow several other principles without which any firm or company can not work successfully. It is possible to see an approximate scheme for building a business strategy and give the example of the plan for selling computer equipment and opening a store with these goods. Such projects are profitable in today\u2019s market in case they are thought out carefully.\n\nGoals and Targets of the Sales Plan\n\nThe purpose of this plan is to solve several tasks. The first aim is to create a cost-effective and profitable computer equipment store. Another goal is to get high profits from sales. One more task is to satisfy consumer demands of this sector of the information technology market. Besides, it is planned to realize the work of the store both online and offline. The enterprise should consist of a large warehouse, a trading hall, where the running range is presented, and a site whose information is duplicated in the terminal inside the store. One of the primary goals of the sales plan is to achieve a full payback in two years after the opening of the trade point.\n\nSales Strategies and Tactics\n\nIt is necessary to perform some procedures for the sales plan to be implemented. First of all, it is important to rent a room with an area of at least 150 m\u00b2, as well as carry out repairs in this place. Then it is planned to buy a domain, hosting and develop a site where all the products sold in the store will be presented. Further, it is necessary to purchase special equipment and furniture (racks, shelves, computers, computer parts, etc.). Also, it is worth remembering about the selection and hiring of personnel (administrators, consultants, cashiers, accountants, etc.), including technical. Another important stage of preparation for selling the goods is to launch an effective advertising campaign.\n\nBy the analysis of the current market, it can be assumed that it is better to orient the sales both to non-professional users who buy ready-made equipment and experienced people who are ready to purchase separate components. The tactic that the store will follow is aimed at hiring qualified staff. Misrepresentation of the goods or the properties of any product to a buyer, which usually happens due to the incompetence of employees, is an anti-advertising that can not be corrected by positive information on the store\u2019s website. Therefore, the demonstration of the characteristics of particular components and the clarification of all the questions of interest will be carried out only by qualified employees who have a sufficient level of training and have passed a fixed period of probation.\n\nPlanned Costs and a Payback Calendar\n\nThe expected costs of the sale of computers and technological equipment depend not only on the volume of purchased products but also on other factors. These expenses include the cost of renting a room, and the cost of creating a site, and advertising, as well as other charges. Thus, if we take the average indices and calculate all the expenses, specific parameters should be mentioned. It is the area of \u200b\u200bthe room of 150 m\u00b2 with a total number of employees of 19-20 people (sellers, cashiers, delivery service, accountant, etc.).\n\nThe expenses are the following: rent and public service \u2013 $800,000; repairs \u2013 $200,000; salaries \u2013 $3,500,000; site and advertising \u2013 $200,000; other expenses \u2013 $150,000; unexpected expenses \u2013 $100,000. An approximate cost plan can be presented in the form of a diagram, which reflects the main sources of expenses.\n\nThus, the highest costs are in the form of salaries, and the lowest are unexpected expenses. The total amount of money that is planned to spend on the implementation of the sales plan is $4,800,000. According to this scheme, all the investments will be paid off by the end of the first year if the expected number of buyers can be attracted at once. These terms are even faster than If the advertising campaign is not very successful, it will take more time to return the money invested in the project and start making profits.\n\nTo summarize all the data, it is possible to say that such a sales plan for computer equipment and a store selling these goods may be profitable if thought out properly. Investing money in this sphere looks quite reasonable, and the audience can be experienced buyers who know all the peculiarities of these goods well, as well as non-professional computer users. Particular attention to the selection of qualified personnel will attract more customers and provide a higher level of sales, so one of the fortunate conditions here is a high-quality personnel policy. The high level of wages will motivate employees to work more efficiently. An expected profit can start to come soon enough if a marketing scheme is carried out correctly, and all the participants of the project fulfill their duties conscientiously.\n",
        "label": "human"
    },
    {
        "input": "Smartwatches: Computer on the Wrist Essay (Article)\n\nIntroduction\n\nHave you ever wondered what modern life would be without all those gadgets? Laptops, tablets, and smartphones seem to be everywhere, and no one can live without them today. Smartwatches have recently joined the ranks of obligatory electronic devices. This state of affairs is not a surprise because smartwatches are tiny gadgets that provide their users with all the necessary functions as smartphones do and even more.\n\nGeneral Information and Main Functions\n\nSmartwatches are fully-featured computers with a few advantages worn on a wrist. Since these devices use touchscreens and offer a lot of particular applications, they seem to be substituted for smartphones. However, the fact that the tiny gadgets can record users\u2019 heart rate and other vital signals makes it clear that smartwatches are something different than just small mobile phones. It is also possible to find mini-computers designed explicitly for withstanding extreme conditions. There is no doubt that these devices can be useful for everyone.\n\nNumerous functions of smartwatches are another explanation of why these devices are so popular nowadays. Firstly, they provide their users with the possibility of media management. Paired with smartphones, for example, a smartwatch can be used to change volume and tracks while listening to music. Secondly, users can send voice messages via their smartwatches if they are paired with smartphones. Finally, these devices are suitable for athletes because of their fitness tracking functions. Thus, this gadget can be an irreplaceable item for modern people.\n\nConclusion\n\nSmartwatches are deservedly said to be popular devices nowadays because they provide users with a lot of useful functions and opportunities. Even though smartwatches should be paired with smartphones, the two are not the same. There are crucial differences between them, and these differences indicate that smartwatches can be more significant than smartphones in some cases. Thus, everyone today should have this tiny computer on their wrist.\n",
        "label": "human"
    },
    {
        "input": "How to Sell Computers: PC Type and End User Correlation Essay\n\nPersonal Computers\n\nPersonal computers are multi-purpose, meaning they can be used for various things. Their size, both physical and software, is usually average compared to institutional or work-specific computers. One can argue that personal computers always target the end-user and not the computer guru. Therefore, they are more user-friendly and easier to manoeuvre. When selling personal computers, one should understand the needs of the end-user to pair him or her with a suitable device. If the end-user is a gamer, for example, he or she will be attracted to detailed specifications on the suggested computer\u2019s ability to handle such heavy graphics, and so forth. The following items are some of the key selling points of any type of personal computer.\n\nRAM and ROM\n\nBoth the Read-Only Memory (ROM) and the Random Access Memory (RAM) are key elements of a personal computer. The specification of each will depend on the major activities the user will conduct on the computer. As a salesperson, it is important to fully understand the difference between the two to understand the client\u2019s needs. Taylor explains that the ROM is a non-volatile storage chip (p. 35). This means that contents and data stored in the ROM are not affected by the presence of power or lack thereof. On the other hand, RAM is volatile (Taylor, p. 35). A client looking for a personal computer mainly for gaming purposes will need one with a larger ROM as compared to the RAM. This is since the ROM cannot be manipulated easily, and secondly, the ROM can handle several applications in one device (like the game console). Additionally, data is stored permanently in ROM while it is only temporarily stored in the RAM chip.\n\nArchitecture and Platforms\n\nFeatherstone explains that a computer platform is an environment in which software is run (p. 45). Using this definition, one can argue that there are many different computer platforms. It is prudent to note that these digital platforms can take the form of both hardware and operating system. Again, as a salesperson, the needs of the client should guide which type of platform you advertise. If the client is interested in the hardware aspect, then some examples that will excite him or her include the video game console for gamers or the ARM architecture that is most suited for people that link their computers to their mobile phones (Tolga, p. 158). Concerning the operating systems, the client will be interested in knowing whether the device uses Linux, Windows NT, FreeBSD, or Solaris, among other similar software. Software frameworks such as JAVA and Mozilla Prism are also important to mention.\n\nAdvantages and Disadvantages of Two Platforms\n\nBoth the platforms mentioned (either hardware or operating system) have advantages and disadvantages. It is common to find a client comparing two different platforms of the same architecture. For instance, one can compare Linux to Windows NT. One advantage of Linux is that it is significantly safe from malware attacks. Additionally, the operating system is very affordable and is one of the most stable operating systems in the market (Whitt 201). Despite this, the software has several disadvantages. For example, a significant number of other software cannot run on Linux. On the other hand, software in the market can run in Windows NT. Another advantage of Windows NT is that it is very user-friendly, especially compared to Linux (Panek, p. 4). Also, because there are many Windows users, there are hardware developed that run on Windows easily compared to any other operating system. Just like Linux, Windows NT has its disadvantages. One major disadvantage is that it is easily attacked by malware. Secondly, the software is also expensive compared to Linux.\n\nFive factors to consider when purchasing a personal computer\n\nApart from ROM and RAM, there are other things that a client will need to consider when purchasing a personal computer. The first is the price of the computer, which will be informed by the amount of money the client is willing to spend. Secondly, the client should also consider the processor, which is essential in both input and output. Third, the client has to think about the graphics. The right graphics will be determined by the major uses of the computer as they affect things such as resolution and so forth. The inbuilt software is also important to note. Indeed, clients can buy and change software at any point of their computer use. However, to save on costs, the client should get a computer that already has stable and reliable inbuilt software. This also goes hand-in-hand with the availability and reliability of anti-virus software that can be installed. There are operating systems that do not require additional anti-virus software. However, the software will require such additional security and this would mean additional costs for the client. To best serve the client, the salesperson needs to understand the specifications of personal computers and also the needs of the buyer.\n\nWorks Cited\n\n 1. Featherstone, Mark. Computer Games Designer . Raintree, 2014.\n 2. Panek, William. MCSA Windows Server 2012 R2 Complete Study Guide: Exams 70-410, 70-411, 70-412, and 70-417, Issue 410 . John Wiley & Sons, 2015.\n 3. Taylor, Graham. Work Out Computer Studies GCSE . Macmillan, 2016.\n 4. Tolga, Soyata. Enabling Real-Time Mobile Cloud Computing through Emerging Technologies . IGI Global, 2015.\n 5. Whitt, Phillip. Pro Freeware and Open Source Solutions for Business . Apress, 2015.",
        "label": "human"
    },
    {
        "input": "Purpose of the Computer Information Science Course Essay\n\nTable of Contents\n 1. Computer Information Sciences Competencies\n 2. Net Neutrality\n 3. Computers and Public Harm\n 4. Works Cited\n\nComputer Information Sciences Competencies\n\nThe Computer Information Sciences course is intended to explore the latest technological development and their influence on communication and knowledge. It explains the functioning of various devices that are involved in most people\u2019s everyday lives, teaches students essential competencies in various aspects of their use, and talks about ethics. During this course, I have improved my ability to create documents and spreadsheets, learned the fundamentals of database management, and created complex multimedia presentations. I have also learned about Web design tools and increased my awareness of the uses of various hardware and software.\n\nIn my opinion, the Computer Information Sciences course should not only be an elective one can choose, but a general requirement for all students. The world of technology is becoming increasingly complex, and progress enables various opportunities. People in most professions could benefit from an awareness of digital tools that assist with their works. Furthermore, malicious behavior is also becoming easier, particularly about people\u2019s data. The course includes explanations of the risks of the online environment and may help students avoid becoming victims of harmful activities. It may also help to increase awareness of current issues in the field.\n\nNet Neutrality\n\nIn 2017, the Federal Communications Commission reviewed a set of laws known as \u201cnet neutrality.\u201d According to Kang, the regulations were repealed despite the campaign to retain them, a move that favored Internet providers over consumers. Kang notes that the FCC chairman, Ajit Pai, expected the measure to promote competition among various providers and thus to benefit consumers. However, the opposition to the decision stated that with the new freedoms given to providers, they could arbitrarily restrict access to websites or applications for monetary or political gains.\n\nNet neutrality regulation fundamentally forces providers to treat the Internet as a monolithic entity instead of a collection of nodes that represents it more accurately. According to Kang, it \u201cprohibited broadband providers from blocking websites or charging for higher-quality service or certain content\u201d and regulated high-speed Internet delivery like a utility (par. 2). I believe that the repeal was not the right choice for consumers, as it provided them with no benefits while potentially damaging their freedoms. I would tell my United States Senator to reinstate the legislation, possibly along with some other programs that have been shut down by the FCC.\n\nComputers and Public Harm\n\nThere are numerous dangers associated with the use of computers, with some stemming from malicious intent and others being more challenging to identify. Baig describes a recent data breach that exposed the personal information of 80 million U.S. families (par.1). Such incidents have been becoming more frequent recently, and while the one in Baig\u2019s article did not leak any critical data, Internet databases often contain essential data. A criminal agent could use credit card numbers or social security numbers to commit fraud. Since I started the course, I have been more careful to keep my personal information safe.\n\nOther dangers are more subtle and associated with the increasing domination of the Internet by a handful of gigantic companies. Dorman describes the recent banning of Michael Morrison, who was maintaining an Alexandria Ocasio-Cortez parody account, from Twitter for unclear reasons. The creator\u2019s page, which had a significant follower count, was also permanently suspended. According to Dorman, many people claim that the banning was political and a continuation of a trend where right-wing people keep getting banned from large social networks. As a social media account is among the best and easiest ways to obtain public reach, one could argue that the platforms are engaging in political censorship, endangering freedom of speech.\n\nWorks Cited\n\n 1. Baig, Edward C. \u201cMassive Data Breach Exposes Ages, Addresses, Income on 80 Million U.S. Families.\u201d USA Today, 2019.\n 2. Dorman, Sam. \u201c Twitter Permanently Suspends AOC Parody Account for Being Misleading. \u201d Fox News, 2019. Web.\n 3. Kang, Cecilia. \u201c F.C.C. Repeals Net Neutrality Rules. \u201d The New York Times, 2017.",
        "label": "human"
    },
    {
        "input": "Technological Facilities: Computers in Education Essay\n\nTable of Contents\n 1. Introduction\n 2. Background\n 3. Theoretical Framework\n 4. Reference\n\nIntroduction\n\nDue to the rapid development of technological achievements computers became an inalienable part of the educational process. Still, using a computer for different educational purposes does not always demonstrate its value in practice. Many experts argue that there is no difference between the educational level of students of high school who used computers and different software is quite the same as the level of students who did not handle any technological facilities during education. However, other experts argue that the benefit of computer usage in high school is just the question of the nearest future. There are various aspects students may master with the help of computers which were unachievable several decades ago. All teachers of Saudi Arabia should be informed about the necessity, inevitability, and benefit of computers and technological achievements introduction to the contemporary system of education.\n\nThe researcher chooses the students who study in the high school stage to investigate the issue of computer usage in high schools Physics classrooms in Saudi Arabia because different computer software is highly beneficial for high school students in the aspect of Physics learning. High school students achieved basic knowledge and skills in previous stages of education. That is why computer application will be an inalienable part of education. The researcher wants to investigate the process of computer introduction into the high school Physics classroom in Saudi Arabia. The researcher highlights the necessity of computer application during Physics classes which would be beneficial for students taking into account different aspects of the field of science. Physics is developing science, and what was a breakthrough many years ago, is an outdated thing. Computer help students on high school level comprehend and develop their skills in the sphere of outdated and new achievements in the sphere of Physics. Moreover, computers help model and display various physical phenomena, which is impossible from the point of view of the common environment.\n\nBackground\n\nSpeaking about computers introduced into the educational process, technological achievements became a part of education on different stages in many countries all over the world. Computer generation in high schools of European and western countries began about thirty years ago. While in Saudi Arabia application of computers and teaching of computer skills commenced in the 90s of the last century.\n\nThe main difficulty of computers introducing into the schools of Saudi Arabia is the question of changing the whole system of education. Nowadays there is the problem of training teachers to use computers during lessons. On the one hand, teachers have to know the basic principles of computer usage. On the other hand, teachers have to master different software which may benefit the education of their concrete subject. However, not all, if not to say that the majority of Saudi Arabia teachers refuse to apply computers during education. They consider that the application of different technological achievements contradicts the pedagogical approach. Still, some teachers admit that there are no effective courses for preparation and re-training teaching staff from the aspect of computers usage.\n\nAnother problem the education system from the point of view of application computer products during lessons Saudi Arabia faces today is the necessity of crucial changes which are to be made in the aspect of rigging and informational support on computer technologies usage. It means that lots of manuals and textbooks should be changed and new ones are to be introduced to make the application of computers not only the subject of technological achievements and computer era but also make it work for the benefit of the educational process.\n\nTheoretical Framework\n\nThe introduction of computers and other technological achievements into the system of education to the schools of Saudi Arabia was held on different stages. On the one hand, various disciplines are closely connected with computer usage. On the other hand, the fact that lots of spheres of contemporary life are connected with computers meant the necessity to introduce computer programs to the educational system. \u201cWhen considering the domain of computers in education, we recognize that it is in a constant flux. There are many pressures for acceptance and incorporation of computers at all levels of education\u201d (Plomp and Pelgrum, p. 250).\n\nApart from the necessity of computer introduction to the system of education, the application of different technological achievements in the high school\u2019s Physics classroom in Saudi Arabia is important from the point of view of visual presenting and modeling of different physical aspects. It is not a secret that school laboratories cannot provide all necessary conditions to realize and display this or that physical phenomenon. However, the fact that the educational program of Physics becomes more complicated on the high school stage, means that laboratories would be more complicated as well. So, it is rather difficult, if not impossible, to display this or that Physical experiment. That is why computer models of different Physical phenomena are highly beneficial for the educational process in the aspect of Physics. Moreover, computer software allows high school students to create different physical phenomena themselves and evaluate their knowledge in the field of Physical.\n\nReference\n\nPlomp T., and Pelgrum W. J. (1991). Introduction of Computers in Education: State of the Art in Eight Countries. Computers Educ . 17(3), 249-258. Web.\n",
        "label": "human"
    },
    {
        "input": "Computers\u2019 Critical Role in Modern Life Research Paper\n\nOverview\n\nThe role played by computers in modern life cannot be overemphasized. The advances which have already been made in computing technology are enormous. Owing to this, modern life has transformed to an extent that most applications and principles require the use of knowledge derived from computers. Computing roles have been integrated into society in facilitating different activities which would take man significant amounts of time to accomplish effectively. This paper explores some of the critical positive and negative roles played by computers in the contemporary world.\n\nPositive Roles\n\nComputing in Medical applications\n\nThe contemporary application of computers has found its way in carrying out research in medicine for improved healthcare (Ward, 2004). Data that has been obtained from patients is recorded, stored, analyzed, retrieved, and transmitted in a very efficient way through the use of computers. In the case of an Intensive Care Unit where urgency is of great importance, the use of computers comes in handy. Additionally, an emergency department has benefitted tremendously from the use of computing knowledge bearing in mind that there is often a lot of data generation from patients and laboratory results that need to be analyzed and interpreted. The use of handwritten patient details has a myriad of shortcomings. For instance, records can easily go missing when information has been recorded on a sheet of paper. Moreover, handwritten data may be poorly documented leading to a misleading interpretation of the same. This may also be coupled with a lack of legibility on the written data leading to further confusion. It is also cumbersome to gather, store and retrieve all clinical information pertaining to patients when such records have been hand-written (Ward, 2004).\n\nComputers are being used to carry out the huge and complicated data storage and analysis which would have otherwise been a nightmare. Indeed, computers are playing a very important role in the advancement of medical knowledge. Interestingly, the contemporary clinical information systems make use of computers and computing knowledge in delivering healthcare to patients. One important attribute of computers is their ability to provide more accurate and regular information which has been stored and analyzed. This is the very principle used in clinical information systems and has proved to be very beneficial. Transcription errors which may result from automated data entry are largely eliminated with the use of computers in medical analysis. There are other category of errors which were prevalent for long when handwritten data was being used in the past. For example, errors that arise from data which is not complete have been alleviated in clinical studies. This is a very important role which computers play in this field. Consequently, time spent on analyzing and evaluating patient data has been reduced significantly hence saving lives which would have otherwise been lost.\n\nAnother notable role of computers in modern clinical information systems is the integration of both primary and secondary data. In most cases, hand generated data which is considered to be a primary source may not be easily harmonized with a secondary data. In other words, making additional information on a primary source of patient data is tedious compared to computer generated data. Hence, computers have facilitated the art of complex signal analysis. Computing system in a clinical set up has enabled the recording of primary data when patients are being diagnosed besides the analysis of such data together with the secondary information.\n\nAs mentioned earlier, computing knowledge has been extended to medical research with great level of efficiency. For a considerable length of time, clinical information systems had been hampered by inadequate technology. However, the development of information technology through the use of computers has made it possible for data bases to be manipulated with ease and efficiency (Ward, 2004). Hence, computers have become formidable tools in carrying out research. One important aspect of a computer which has made it possible for enormous data to be manipulated is the memory component of the device.\n\nIn spite of the vital role played by computers in medicine and of course clinical studies and practices, the use of this device and the accompanying technology has received its own share of challenges. Clinical information system is very sensitive owing to the fact that the data entered into the system, analyzed and eventually interpreted is usually put into use and may have disastrous impact on the patient. The accuracy of data entered is crucial. There are instances when wrong or confusing data can be fed into the system leading to gross mess of the entire clinical information system of a healthcare organization.\n\nComputers in the Military\n\nThe manner in which the military operates in the modern world has completely been influenced and revolutionized by the use of computers. Contemporary military tactics is largely making use of computers to improve efficiency and effectiveness. The military can now perform certain tasks with high speed and precision because the ability to compute fast has been brought on board (Shelly, Vermaat & Quasney, 2010). This has gone a long way in reducing the operational costs to the government of the day.\n\nBefore the advent of modern computers, fighting for the sake of defending territories would result into heavy causalities and massive destruction of property. In order to achieve particular war goals, weapons of mass destruction were the best for use. Such weapons would lead to non selective loss of life of civilians who were not even involved in the war. It became very costly for the military to launch an attack because it was not easy to control the magnitude of devastation.\n\nSince it is almost impossible to evade war either in the near or distant future, the development of mini computers which are equally powerful has been thought as the right step forward towards equipping and strengthening the military. It is now possible to manufacture weapons which are sensitive enough to detect the location and nature of the target. Small custom made computers can now be used by the military to launch a projectile to a target located far away (Shelly, Vermaat & Quasney, 2010). In this case, it is only the target which is hit by the missile contrary to the weapons of mass destruction which often leaves widespread damage that is over and above the specific target. The military has found the use of computers to be less expensive and also effective compared o the old fighting tactics. Moreover, fewer resources are utilized with the use of computers in military warfare. In the case whereby human beings are the target, the use of computers has made it possible to selectively kill those who are participating in the war and spare those who are innocent. The decisions which are made by these modern weapons are automated in the sense that there are lasers and optical systems which are installed in the weapons and together with special computer programs, the target can be reached effectively.\n\nThe military wing of the United States of America largely makes use of Global Positioning System. This computer technology makes it possible for the military to precisely identify where a particular target is located. It is a very vital computer application which the military cannot do without. In achieving this, there are usually a collection of satellites which move round the earth and gather the relevant information regarding the location of the target. This information is concurrently communicated to another receiver on the ground which finally enables the military to locate the target precisely. Launching such satellites require computers without which it is not possible. Furthermore, signals which are transmitted to the receiver have to be generated by the computers. The use of Global Positioning System enables missiles to accurately locate the position or location of the target before launching a projectile.\n\nReconnaissance also utilizes computing technology and satellites. Spectacular images are permitted by computers to be received by the satellites. These images are then transferred to the ground military station where all the information regarding the enemy or target is obtained. This is an intelligence system which tends to gather reliable data from an enemy installation and instantaneously transmit the same for analysis on the other side. Reconnaissance satellites have been used for quite a while and they are still prominent investigative devices that have been made even more powerful with the advancement in computer technology (Shelly, Vermaat & Quasney, 2010). The successful operations of modern aircrafts which are used for fighting purposes rely heavily on the use of computers. A pilot is often not in a position to control a myriad of variables at the same time in a fighter plane. Besides, there are those fighter planes which are relatively not stable and hence require regular checks to monitor their performance. When such controls are in place, an aircraft can function well as far as speed and cruising rage are concerned. For instance, an aircraft which is well maneuvered will more likely accomplish its target.\n\nContemporary military campaigns are utilizing well secured and effective communication which may not be \u201chacked down\u201d by the enemy. Computers which are powerful enough are in use to achieve this requirement. For example, these computers can permit well enhanced encryption methods of receiving, storing and transmitting sensitive information. Modern computers are effectively playing the role of encrypting valuable information contrary to the old methods which could easily be identified by the enemy.\n\nThere is a lot of military research taking place in the background. This research is meant to boost the technological well being of the military especially in regard to inventing better and improved techniques of launching warfare. Computers are playing an invaluable part in this undertaking. The device is used in the gathering, storage, analysis, interpretation and transmission of all the data which is relevant in the research study. The simulating effect which has been caused by a blast from a nuclear explosion is monitored by computers. There are rovers which are currently being developed by NASA so that the planet Mars can be explored.\n\nComputers in modern education\n\nThe use of computer technology and its advances has been correlated to the improvement of student achievement in education. Both the public and private education providers are finding it more necessary to embrace computers in the learning process. Towards the end of the 1990s, there was a paradigm shift in the mode of teaching. New curriculum has been developed by education specialists to embrace the application of computers in day-to-day process of teaching and acquisition of knowledge (Johnson & Maddux, 2003).\n\nAlthough computers have initially been used to deliver the required curriculum, the shift has been tremendous. For instance, using software contained in disks to deliver curriculum has now revolutionized the exploration of the curriculum on the information super highway or the international network.\n\nWhen it comes to curriculum matters, a very vital hardware was first provided by Apple computers. This computer manufacturing company was of the opinion that communication, collaboration and thinking media were all vital components of technology which could only work well if computing skills and competences were put in place. The information which is delivered by the tutor to the students can be counterchecked by the latter on the internet. Furthermore, students have become more knowledgeable in the sense that they do not necessarily rely on the teacher to obtain some information. The computer through the internet has a wealth of information which can be accessed easily by learners.\n\nThe modern curriculum has enhanced the prevalence of paperless learning environment. Much of the content required for learning can be found on the World Wide Web or on removable storage media like CD-ROMs, memory sticks, memory cards and flash discs (Johnson & Maddux, 2003). The learning method has been revolutionized by the concept of e-learning which is currently a very popular mode of distance learning. It is through the use of computers that electronic learning platform has been possible. The classroom is no longer dominated by the chalkboard, the teacher or a piece of pen. Internet use has reinvented the traditional learning methods which relied on the aforementioned parameters for learning to take place. Information can be easily accessed by a learner through a computer instead of the teacher. Moreover, the internet provides the most updated information as possible especially internet portals such as Yahoo or MSN are used as search engines. Hence, the computer has enabled learners to obtain not just mere information but the most current version of the same. This is a very critical role played by computers in modern life whereby information is made available upon the click of a mouse. Nevertheless, the information content available on the internet may not all be useful. The educators have the role of providing clear goals and objectives of the curriculum in addition to filtering the right content for the curriculum.\n\nThe globe has been shrunk by the internet (Johnson & Maddux, 2003). Learning and attaining knowledge can be done from any location of the world so long there is a computer device and internet access. Learners and teachers are now capable of collaborating from remote locations for instance; online file sharing is highly common. In fact, one key advantage of the internet is the capability of the system to manage a worldwide link of a variety of resourceful materials. Hence, al learners can now an individualized program for learning academic resources in nay given curriculum.\n\nE-commerce\n\nElectronic commerce has grown to significant levels over the past few decades. The modern life commerce is not entirely reliant on the buying and selling of goods and services from specific physical locations.\n\nNegative role played by computers\n\nMusic Piracy\n\nComputers have equally been used to perpetrate acts of online theft through music piracy. The 2001 survey revealed that there are over three million worldwide internet enthusiasts who have the access and ability to download music and video files from the huge collection of albums available online (Spinello & Tavani, 2004). Undoubtedly, this number has indeed grown. Besides, the use of broadband-enhanced technologies enables quick downloads of music and video files compared to the old system which would take several minutes to download a small volume of music file. However, there are those who still question the authenticity of the argument that file sharing lowers the overall volume of sales and consequently deprives artists of their expected and rightful returns. Moreover, there has been an on-going debate that downloading music and video file from the internet cannot be pointed out as the main cause behind the drop in sales over the recent past. Meanwhile, it is imperative to have an empirical analysis on whether online file sharing can be blamed on this decline.\n\nTo begin with, the ability to share files at almost no cost over the World Wide Web goes against the boundaries set by copyright laws. Furthermore, it implies that online users have the alternative of obtaining videos and music they want without paying for any returns to this form of intellectual property (Spinello & Tavani, 2004). The end product of this culture is definitely the deprivation of artists through non-violent robbery of their intellectual property. Users who download files from the internet are less likely purchase the same products over the counters. As a matter of fact, the quality of the downloaded material is not compromised at all.\n\nSlightly over two billion U.S dollars were gained from music and video sales during the beginning of new millennium. In these sales, it was found that the Compact Disc was the most famous form of music purchased (Spinello & Tavani 2004).As a result, its price has remained a bit high therefore scaring other users. In seeking for an option, some tend to switch to the internet where they can obtain music and video of their choice. Since all music that is downloaded can be used by more than one person, there is a tendency of such users not reverting to the actual buying of legal products. This will, on the other hand, lead to reduced sales of both audio music and video. In reality, if those who download music would resort to buying legitimate copies, then the total quantity sold would be higher than it is today. Some figures reveal that online file sharing has the potential of reducing the desire to buy by about thirty per cent (Spinello & Tavani, 2004). This is a whooping figure bearing in mind that the number keeps growing on a daily basis.\n\nThe intellectual property rights aim at protecting trademarks, software data and copyrights. These rights are rarely upheld bearing in mind that a variety of ways can be used to acquire data illegally. For instance, some users might modify, transfer or store information in a retrieval system without prior permission from the owner. Duplication of compact discs containing music or video files amounts to intellectual rights violation. Most important issues on moral aspect of communication technology do not receive headlines. As much as we use and appreciate technology, protecting its integrity and creators in this field is imperative.\n\nConclusion\n\nIn summing up this paper, it is imperative to note that computers play both negative and positive roles in modern life. Nonetheless, computer technology has led to a myriad of technological advances and significant growth of the world economy through globalization. Besides, medical field, education, E-commerce and the military are some of the beneficiaries of the advances made in computing. However, music piracy continues to be a setback to the intellectual property rights.\n\nReferences\n\nJohnson, L.D. and Maddux, D.C. (2003). Technology in Education: A Twenty-Year Retrospective , New York: Haworth Press Inc.\n\nShelly, B.G., Vermaat, E.M. and Quasney, J.J. (2010). Discovering Computers 2010: Living in a Digital World, Introductory, MA: Cengage Learning.\n\nSpinello A. R and Tavani T. H (Eds.) (2004) Readings in cyber ethics 2 nd Ed. Mississauga: Jones & Bartlett Publishers.\n\nWard, S.N. (2004). Using Computers for Intensive Care Unit Research, Respiratory Care , 49(5): 518-524.\n",
        "label": "human"
    },
    {
        "input": "History of the Personal Computer: From 1804 to Nowadays Research Paper\n\nThe search for newer, faster and smaller computers has been on for many years.\n\nIn 1804, Joseph Jacquards, a French man, invented an attachment to the mechanical loom for weaving clothes. He realized that the design found in a woven cloth followed a fixed repetitive pattern which was a program. By punching holes at specific patterns and intervals in cards attached to the loom, he was able to control the threads, reducing desired patterns and hence storing information by punching the cards (Chronology of personal computers, 2010).\n\nIn 1833, Charles Babbage designed a steam powered device which was called an Analytical Engine which was a special purpose machine that could perform specific calculations. The Analytical engine was a far more sophisticated general purpose computing device which included five of the key components that performed the basic of modern computers. These are:\n\n  * Input devices that were used to punch cards that contained instructions or data.\n  * A processor/Calculator/Mill-this is where all the calculations were performed.\n  * Memory unit also called a store where data and intermediate calculators could be stored.\n  * The control unit which controlled the sequence in which operations operated.\n  * The output devices-this is where Babbage got his results.\n\nCharles contributed by developing the problem solving instructions that the engine would follow while doing calculations (The history of computers, n. d).\n\nHollerth\u2019s Census Machine\n\nHollerth developed a machine that automated the tabulating process. The machine combined electricity with Jacquard\u2019s method of storing information on punched cards. Whole representing census papers information was punched on stiff paper cards.\n\nBurroughs Adding and Listing Machine\n\nBurroughs invented the 1 st adding and listing machine. It had a full numeric keyboard and was operated by hard crank.\n\nENIAC (Electronic Numerical Integrator and Calculator)\n\nIt was large in size and complex. It was a room sized machine that used 1800 vacuum tubes as internal components.\n\nIt had independent cutis for storing program instructions and numbers. Several mathematical functions could be performed at once by modern standards. It had limited storage capacity, limited memory and did not store instruction as modern computers.\n\nEach new programme required rewriting its program circuit and could multiply numbers in 0.003.The only disadvantage is that it used a lot of electricity and power (Allan, 2001).\n\nVon Neumann\u2019s Logistical/Computers\n\nHe was a mathematician who dealt with ideas and not reality or limitations of technology and as a result he was able to develop a logical framework around which computers have been built. He developed the concepts of storing programmes in the computer memory that was called stored program concept. Before; computers were storing only the numbers with which they worked. He converted each programmed instruction into numeric codes of which were binary digits (0 and 1) that could be stored directly in the computer memory as if they were data.\n\nVon organized the hardware of the computer, broke it into components whereby each component performed a specific task and could be called upon repeatedly to perform its functions.\n\nThe components in his theoretical computer bare a remarkable resemblance found in the Charles Babbage\u2019s\u2019 analytical engine. These components were: An arithmetical unit for performing basic computations, Logical unit where decision and comparison could be performed, a input device for accepting coded instruction and numerical data, memory unit for storing instructions and data, the control unit for accepting the coded instruction and controlling the flow of data, and the output unit for communication of results (PC-History, n. d).\n\nThe Electronic Delay Storage automatic Computer (Edsac) was the first computer to incorporate the stored program idea and use the letter as input and convert them to binary digits in 1949. EDSAC was a stored program machine that used a unique code of binaries Computer Genealogy.\n\nComputer components have decreased in size since the 1950\u2019s.\n\n1st.Generation (1951-1958)\n\nThey had the following characteristics:\n\n  * They used the vacuum tube technology where the input and output of data and instructions used to be done using punched cards.\n  * The machines were programmable. The stored programme machine used numeric codes that were called the machine language. First generation machine were eight hundred meters in size and had huge price tags.\n\nHowever the following drawbacks resulted from these machines:\n\n  * The vacuum tube generated tremendous heat resulting in blowing off of tubes.\n  * They used massive amounts of electricity to power the 1000\u2019s vacuum tubes.\n\n2nd Generation (1959-1964)\n\nThese generations\u2019 machines were developed by John Barden, Water Braltan and William Shock. In nineteen forty eight, the above invented a transistor while they were working for the bell tacs. They were produced at a cheaper cost and in larger quantities in 1959.A transistor is a tiny electronic switch that relay electronic message and yet built as a solid unit with no moving parts to generate computer vacuum tubes were replaced by transistors now that the new machine were once smaller, faster and more reliable than the first generation computers.\n\nThe computers used the solid technology which required no worm up time.\n\n3rd Generation (1965-1970)\n\nThese machines involved more and more circuit to be packed into chips. Technology at that time moved from a large scale integration to a very large scale integration. Ted Hoff, the Intel Co-operate Engineer, introduced the microprocessor in 1979. Computers became smaller by condensing. In addition to developing a highly compact Central Processing Unit, the peripheral devices were designed to make the PC easy to use. Peripheral devices are any devices attached to the CPU. Such devices include the Compact storage devices, colour service (the monitor) and a wide variety of pointing devices such as the mouse, in addition to development of small desktop computers. In 1969, Intel was commissioned to produce an IC, a computer chip, for a Japanese calculator company\u2019s line of calculators (Early History of the personal computer, n. d). Ted created the Microprocessor, which did away with \u2018handwriting\u201d the logic of the calculator into the chip.\n\nLater, the 8008 was created by Intel, and the company retained its marketing rights, although they could sell to CTC. There was need to create support for the programmable 8008 chip and Adam Osborne, an employee of Intel, was assigned the task of writing the manuals for the programming language for the 8008. He later gained fame in the development of the PC for creating the first portable computer (History of the PC, n. d).\n\nThe first operating system for microprocessors called CP/M was developed Gary Kildall. Operating Systems are vital because without them, using a PC can be impossible.\n\nIn Albuquerque in New Mexico, in the early 1970s, a man named Ed Roberts created a kit for assembling a home computer and based it upon a new chip (8080) that had been developed by Intel. He then struck a deal which allowed him to purchase the 8080 chips in large volumes at a discounted price.\n\nOn the other hand, use of ray large scale integration, super computers has a vast storage and processing capacity. Such machines are used in performing complex mathematical tasks. The compact chip Technology has also brought the development of parallel computers which uses multiprocessors working simultaneously to solve problems. Hardware advances were followed closely by software explosion and prewritten software is now available for all sizes of machines (CompInfo- the computer information Center, 2005).\n\nBeyond the 4th Generation (5th Generation)\n\nJudging from the current research in the United States of America and Japan, the next generation of personal computers is likely to have common features as given below:\n\n  * They will be more compact-the hardware will be more compact based on the super chip composed of thousands of already compact smaller chips linked together.\n  * The Personal computers will be faster as they will operate and calculate 100\u2019s and 1000\u2019s times faster than the current machine.\n  * They will be closer to natural language as software will make greater use of natural or spoken language.\n  * They will be smarter-will be considerably more intelligent than modern computers.\n  * They will be friendlier-software will make this new computers even more user friendly meaning that people will find them easier to use and operate because the software will require less technological experts.\n\nReferences\n\n 1. Allan, R. (2001). A history of the personal computer: the people and the technology. United States: Allan Publishing, 2001\n 2. Chronology of personal computers (2010).\n 3. CompInfo- the computer information Center (2005). Software and computers.\n 4. Early History of the personal computer (n. d).\n 5. History of the PC (n. d.).\n 6. PC-History (n. d). Web.\n 7. The history of computers (n. d.). Web.",
        "label": "human"
    },
    {
        "input": "The Five Developments of the Computers Essay\n\nTable of Contents\n 1. Introduction\n 2. Motherboard\n 3. Purchasing Motherboard\n 4. Memory\n 5. Computer BIOS\n 6. Computer Processor\n 7. Computer Peripherals\n 8. Mass Storage\n 9. Works Cited\n\nIntroduction\n\nWho created the computer? This is a question that will not have a simple answer. The real answer is, many people have contributed to the history of this device, but the computer can be defined as a machine which is made up of many parts and each part has its uses and functions, various parts in the computer have been invented and created by different people. \u201cThroughout history computers have undergone a big revolution with it own impacts and characteristics which had an impact in the world. The desktop computer has undergone five different developments over the years\u201d (Stallings, p. 26).\n\nThe first revolution (generation) happened between 1940 to 1956, this type of computer was known as the vacuum computer, its main circuitry consist of vacuum tubes and a magnetic drum which acted as the memory. This type of computer relied on machine language, which is the lowest language a computer can understand to perform its various functions and operations, \u201cit type of computer was able to perform its operations one at a time and the input was paper tapes while output was a printout\u201d (Stallings, p. 26).\n\nThe second revolution (generation) happened between 1956-1963, the main components as it was in the first generation computer, where transistors replaced the vacuum tubes as its circuitry. This device was far better than the vacuum tube which was found in the first-generation computer. This model of the computer was smaller in size when you compare it with first-generation computers, it used to save energy. The heat was a problem in this model of a computer but it was an improvement to that of the first generation computer. This model of the second generation computer was using assembly language with the instructions being given to the computer by a person using it.\n\nThe third revolution of this model happened between the years 1964-1971, the major difference with this model of the computer, the user of this machine was able to interact with it by the use of a keyboard and monitor and this model of the computer used to have an Operation System which enables the user to run more than one applications. \u201cThis type of computer was produced to the members of the public because it was cheap as you compare it to other generations of computers and it was smaller in size\u201d (Stallings, p. 27).\n\nThe Fourth revolution of this model of the computer happened between the years 1971 to the present and with the invention of the microprocessor, this model of the computer started using a microprocessor which consisted of many integrated circuits (IC) built on a smaller silicon chip. The first generation computer filled the whole room but with the development of the Intel 4004 chip, the fourth generation computer was able to fit into the palm of the hand. \u201cIn the year 1981, the computer manufacturing company IBM introduced the first home user machine and in the year 1984 Apple computer company introduced Macintosh microprocessor which led to the development of the computer moving into other areas of life\u201d (Hura, p. 46).\n\nThe last revolution of computers is happening between the present and the future, this model of computer will depend on artificial intelligence which is the norm of the world today, The application of artificial intelligence has already been developed in the world but its application of is still minimal because of the cost and the technology which is being used, an example of such technology (artificial intelligence) is voice recognition, this model of computer will change the face of the world in the years to come.\n\nMotherboard\n\nThe motherboard is the main circuitry in the function of a computer machine, each component in a computer is connected with the motherboard. Many devices are communicating with the motherboard through the ports which are available throughout the motherboard, \u201cthe main function of the motherboard is to provide a platform for all other components inside the computer to communicate with each other\u201d (Stallings, p. 35).\n\nMotherboard various from one machine to another, each machine has its own design of the motherboard. But you can classify motherboards by the sockets they have, for example, socket A, socket 476. The type of motherboard a person buys with a computer is very important to person specifications because when buying a motherboard it will tell you the type of socket the motherboard is coming with. \u201cMotherboard has various components such as expansion slots (ISA, & PC) and external connectors such as USB and PS/2 ports, to be used for keyboard and the mouse\u201d (Bullinger, 18). The chipset is available in the motherboard, its specific function is to control the data which is being given to the motherboard. \u201cIts main function is to control the availability of the data into the correct areas within the motherboard, into the correct components\u201d (Bullinger, p. 19).\n\nPurchasing Motherboard\n\nIf a person wants to buy a computer there are some various things he must consider with the motherboard and the components which come with it, for example, if you have many components which have a PCI interface. There is no point in buying a computer that has 3PCI slots because it will not work with equipment that has a PCI interface and this will lead to wastage or resources. Memory is another component which a person must consider when buying a computer, 10 years back many computers used to run with 128 Mb memories and this cannot take any memory that is above 2 GB memory, the person should make sure the computer has an expansion slot to upgrade the computer. A person needs to check the type of motherboard he is buying because some older components will not be supported with a new motherboard. A different model of computers come with different chipsets, this difference will have different performance and also the features will be different\n\nMemory\n\nMemory in the computer in the compute plays a very major role, in the memory is where the computer holds its current programs and even the data that are in use. Memory in a computer can be divided into two main parts, the first part is RAM and the second part is ROM\n\nThe ROM is small but it will determine how the computer will work and it is usually found in the motherboard and it does not change. Its main works in the motherboard, it gives instructions to the hardware that is found in the motherboard but the data which is found in it will remain intact even when the computer has been shut down. If the ROM is damaged then the computer will not work.\n\nRAM main function in a computer, only holds data over a shorter period of time i.e. when the computer is on but when it has been shut down the data will disappear that why it being called \u201cvolatile\u201d. When the computer is working it functions changes from time to time, the computer will need information\u2019s all these time, this is where the RAM comes in it gives the computer quick information that is stored in the hard disks \u201cAt the beginning of the millennium most computers were running on 128Mb RAM but at the moment almost all the newly manufactured computers run with 512 Mb RAM and there is an expansion for that\u201d (Hura, p. 56).\n\nComputer BIOS\n\nBIOS is an acronym for Basic Input/output system it plays an important role in the operations of a computer without it nothing will work in a computer. Its main function in the computer is to configure the hardware that is available in the main motherboard, with these configurations the computer will be able to recognize the operating system that is installed in the computer i.e. windows or Linux. The Windows operating system is widely used in the world because it can be easily be used by many people and it very interactive as being compared to the computers which run on Linux Operating Systems.\n\nComputer Processor\n\nProcessor (Central Processing Unit) this is the brain of the computer its work is to process data and information that are entered into the computer and it executes instructions that are stored in the memory of the computer. Some factors will contribute to the normal working of the processor and If one of these factors is poorly designed then the performance of the computer will be affected and if any of these factors is well implemented it will be difficult to compensate for the other limitations that are found in other factors.\n\nCache on the processor has the same functions as those found in other parts, like on the hardware. The design of the earliest cache was a single cache and it used to be a dominant cache but the need for speed increased in computers the cache with several layers was manufactured and is the one widely used in computers nowadays.\n\nComputer Peripherals\n\nComputer peripherals are any device that is not part of the computer but can be closely found near the computer but some peripherals are installed in the computer the same way the main part of the computer is mounted, for example, I/O card, CD drive and network card. They are some peripherals which are found outside the computer, for example, printers and scanner through a connecting cable or a wireless connection.\n\nMass Storage\n\nThe advantage of using a mass drive is because it is portable and large information which cannot be sent through emails or the internet can be accessed manually in a different place\u2019 for example of mass storage is an external hard disk and flash disk. This mass storage does not require any file system to be used. Instead, a simple device is used to read and write data in it, \u201cin the computer the mass storage is treated the same way as the hard drive and can be format the same way the hard drive can be format\u201d (Stallings, p. 45).\n\nWorks Cited\n\n 1. Bullinger, Hans-J\u00f6rg. Human-computer Interaction: Communication, cooperation and application design . New York: Routledge, 1999.Print.\n 2. Hura, Gurdeep. Data and computer communications: networking and internetworking . London: CRC Press, 2001.Print.\n 3. Stallings, William. Data and computer communications .London: Prentice Hall, 2007. Print.",
        "label": "human"
    },
    {
        "input": "Malware: Code Red Computer Worm Research Paper\n\nComputers are gifts of technology to mankind. With the advancement of technology came advanced software and the internet; the whole world got connected in a wink of an eye and accessibility of distant information and knowledge became a child\u2019s play. But as technology brings hazards with itself when handled with bad intention, software became \u2018Malware\u2019 in the hands of some people with no good intention and with the help of the internet they started spreading to all the computers connected in the network (Zhang and Ma 573-586).\n\nThey have attacked several times till today and will do that in the future also as epidemics had been spread and will be spread by germs to bring disease in the human body. \u201cMalicious software, or malware, is software that enters a computer system without the owner\u2019s knowledge or consent.\u2026\u2026\u2026.conceal the malware\u2019s malicious actions, or bring profit from the action that it performs\u201d (Ciampa 41). Malware is of various types, as for example, virus, spam, Trojan horse, worm, etc.\n\nOne such malware is a worm. A worm is \u201ca program that is designed to take advantage of a vulnerability in an application or an operating system in order to enter a system\u201d (Ciampa 63). One notorious worm that had infected a huge number of computers running Microsoft IIS Web Server through the internet was the Code Red worm. It was first discovered on July 13, 2001, by eEye Digital Security persons Mark Maiffret and Ryan Permet. By July 19, the worm had infected 359,000 hosts (Zhang and Ma 573-586).\n\nThe worm works by sending its code in the place of an HTTP address. It takes advantage of a vulnerability of the computer running Microsoft IIS called the Buffer Overflow through which it gets entry into the host computer. \u201cA buffer overflow occurs when a process attempts to store data in random access memory \u2026.This extra data overflows into the adjacent memory locations and under certain conditions may cause the computer to stop functioning\u201d (Ciampa 85). The worm used the vulnerability by running a long string of the repeated character \u2018N\u2019 to get entry to the computer. The code gets inserted in a file instead of being saved as one and operates from the memory.\n\nThe malicious code checks for the availability of the file C: Notworm and if found the thread does not run. It goes to an indefinite sleep. But if the file C:Notworm is not there and the date is before the 20th of the month the thread creates new threads and attempts to infect new IP addresses in a random manner. While infecting the thread avoids looping back to the source computer (Zhang and Ma 573-586).\n\nThere is one more feature of the worm. It causes websites to appear defaced if the default language of the computer is American English. On these computers, the thread creates further threads and runs a function responding to HTTP requests after staying inert for two hours. The correct website is not returned and instead, the own HTML code of the worm is returned. The defaced website looks as below:\n\n\u201cHacked By Chinese!\u201d (Lemos 1)\n\nThis infection stayed on the computer for 10 hours and then was removed. But there was always the fear of repeat infection by other threads. When the date of the month is between 20th and 28th the threads tried to impart a Denial of Service attack on a particular IP address. The website of Whitehouse was affected and had to be changed. Finally, after the 28th of the month, the threads stopped working and went to an indefinite sleeping state (Zhang and Ma 573-586).\n\nThere is another version of the worm which was first found on August 4, 2001, called the Code Red II. It was actually a new worm that did not cause the defacing of the websites. Though the worm had created havoc it is not that it can be taken precaution against. Symantec Security Response has devised guidelines for both home and corporate users to prevent the worm from entering computers. A few of them are:\n\n  * Complex password systems decrease the vulnerability of a computer to a great extent.\n  * File sharing should only be turned on when absolutely necessary. If necessary anonymous access should be prevented by using ACLs and complex passwords.\n  * Unnecessary auxiliary services installed by many operating systems are vulnerable to threads. They must be removed to lessen the chances of attack.\n  * vbs,.exe,.bat,.pif,.scr extensions might be indicators of threat. These must be blocked by email servers when present as an attachment.\n  * For corporate offices, it is necessary for the employees to avoid opening unexpected attachments and running downloaded software before scanning them.\n  * Patch levels need to be updated.\n  * If a computer appears infected it should be separated and re-installed only after further analysis (Borders 102-113).\n\nThus, threats keep on changing their appearance and ways of operation. But by adopting security measures it is possible to lessen the chances of infection and threats to the computers.\n\nReferences\n\nBorders, Kevin. Malnets: large-scale malicious networks and compromised wireless access points.\u201d Security and Communication Networks 3.2-3 (2010): 102-113.\n\nCiampa, Mark. Security+ Guide to Network Security Fundamentals . Ed 3. NY: Cengage Learning, 2008.\n\nLemos,Robert. \u201cCode Red worm claims 12,000 servers.\u201d CNET News. CNET News, 2001. Web.\n\nZhang, Yunkai and Jianfeng Ma. \u201cModeling and analysis of a self-learning worm based on good point set scanning.\u201d Wireless Communications and Mobile Computing 9.4 (2009): 573-586.\n",
        "label": "human"
    },
    {
        "input": "Computer Assisted Language Learning in English Essay\n\nTable of Contents\n 1. Objectives of the Proposed Study\n 2. Proposed Study Background\n 3. Significance\n 4. Methods\n 5. References\n\nObjectives of the Proposed Study\n\nThe modern world is a highly technological environment in which all human activities sooner or later get technological solutions, and education is not an exception in this respect. The use of computer-assisted language learning (CALL) in education is becoming a more and more important part of the curriculum of the bulk of colleges and universities (Murday et al., 2008, p. 125; Leahy, 2008, p. 254). Thus, it is important to study the modern state of things with the use of CALL in education. Accordingly, the objectives of the proposed study include:\n\n  * To study the use of CALL in English teaching for college students using English as a second language;\n  * To research scholarly opinions regarding the use of CALL for college students with limited English language proficiency;\n  * To study the opinions of college students and teachers regarding the use of CALL for their English courses;\n  * To study the numeric data representing the effectiveness of the use of CALL for the specific purposes of teaching college students.\n\nThe above-listed objectives will allow the proposed study to focus on the theoretical considerations that surround the use of CALL and see how scholars have already researched the practical side of the CALL implementation in various educational establishments. As well, the above objectives provide for the use of both qualitative and quantitative methods of research, which will allow the proposed study to specifically consider the attitudes of college students and teachers regarding CALL as well as the actual numeric data reflecting the efficiency of the CALL use.\n\nProposed Study Background\n\nThe background of the topic chosen for the proposed study can also help explain why namely the use of CALL for college students is selected for the future research. Thus, it is a fact that CALL is the learning tool for students with limited language proficiency, but little is known about the relation of teachers towards the use of CALL in their practice. Kessler and Plakans (2008) discuss this topic in their article, thus contributing greatly to the research of this side of CALL. According to Kessler and Plakans (2008), teachers\u2019 confidence in operating the CALL software solutions depends on numerous factors, including training, previous computer experience, specialization in a certain CALL field, etc (p. 270). Accordingly, the teachers\u2019 side of the CALL use for college students\u2019 programs is essential for understanding the overall efficiency of CALL in education.\n\nFurther on, CALL is obviously not a single tool for language learning. The notion of CALL, according to Futagi et al. (2008), includes the whole range of computerized, both online and offline, software solutions that help students, although to different extents, to master English as their second language through the study of collocations, i. e. basics of spoken and written language (p. 353). These solutions include the databases of word-for-word collocations, software for error recognition in written or typed collocations, web-based collocation-detection systems, and even the grammar-checkers that show the wrong variants instead of correcting them to the needed ones (Futagi et al., 2008, pp. 354 \u2013 355).\n\nFinally, the issue of CALL use presupposes the considerable preparation and training activities for both teachers and students to enable them to operate and use CALL tools at the highest rate of efficiency (Compton, 2009, p. 73; Finkbeiner, 2001, pp. 340 \u2013 341). Thus, the topic of CALL for college students is rather significant.\n\nSignificance\n\nIn more detail, the proposed research of the use of CALL for college students will be a significant scholarly because the topic of computerized solutions for education is a rather widely discussed one today. Kessler and Plakans (2008, p. 270) and Finkbeiner (2001, p. 341), for example, argue that the educational needs and opportunities develop at a rather high speed, and if in the 16 th century a printed book was only a fantasy for educators, nowadays students and teacher can choose among numerous software online solutions that enhance their performance in class. At the same time, the controversy that surrounds the use of CALL tools only adds to the significance of the proposed research, as it is necessary to determine, at least on the sample of several dozens of people, what is the actual effect of CALL on students\u2019 performance and teachers\u2019 confidence in class.\n\nMethods\n\nFinally, it is necessary to define the methodological approaches that will be used during the proposed study. As far as the research of the CALL use for college students will involve the direct work with people and the analysis of purely numeric data, the combination of qualitative and quantitative methods will be implemented. As Golafshani (2003, p. 597) claims reliability and validity to have critical meaning for any research, the proposed study will also be developed based on these concepts.\n\nIn more detail, the qualitative part of the methodology for the proposed study will include the review of the previous research works on the topic of CALL in education, the analysis of those works, the conduct of interviews and surveys of students and teachers. The sole aim of all the implemented qualitative methods will be to study the attitudes of students and teachers towards the use of CALL in their practices and assess, at least approximately, the efficiency of students\u2019 and teachers\u2019 performance while CALL methods are practiced. Golafshani (2003) defines qualitative research as \u201cany kind of research that produces findings not arrived at utilizing statistical procedures or other means of quantification\u201d (p. 600.). Accordingly, the qualitative methods listed above will deal more with the implications of the findings, rather than with their numerical meanings.\n\nThe use of the quantitative method in the proposed research will be limited to two main processes. First, the results of surveys of CALL effectiveness will be presented in the numeric form in relation to the measurements scale that will allow seeing whether a figure displayed by a student reflects the high, moderate, or low. These data will then be related to the effectiveness of students\u2019 and teachers\u2019 performance without CALL. Finally, both data sets will be compared and respective conclusions on the efficiency and overall necessity of CALL methods in language learning for college students will be made.\n\nThe reliability of both qualitative and quantitative methods will be supported by the objective data about the academic performance of studies students before and after CALL implementation. The validity of the research methods will be ensured through the proper sampling that will include all groups of students and teachers according to their approximate psychological, educational, personal, and professional characteristics.\n\nReferences\n\nCompton, Lily K. L. (2009). Preparing language teachers to teach language online: a look at skills, roles, and responsibilities. Computer Assisted Language Learning, 22 (1), 73 \u2014 99.\n\nFinkbeiner, C. (2001). One and All in CALL? Learner-Moderator-Researcher. Computer Assisted Language Learning, 14 (3), 339 \u2014 361.\n\nFutagi, Y., Deane, P., Chodorow, M., and Tetreault, J. (2008). A computational approach to detecting collocation errors in the writing of non-native speakers of English. Computer Assisted Language Learning, 21 (4), 353 \u2014 367.\n\nGolafshani, N. (2003). Understanding reliability and validity in qualitative research. The Qualitative Report , 8(4), 597-606. Web.\n\nKessler, G. and Plakans, L. (2008). Does teachers\u2019 confidence with CALL equal innovative and integrated use? Computer Assisted Language Learning, 21 (3), 269 \u2014 282.\n\nLeahy, C. (2008). Learner activities in a collaborative CALL task. Computer Assisted Language Learning, 21 (3), 253 \u2014 268.\n\nMurday, K., Ushida, E., and Ann Chenoweth, N. (2008). Learners\u2019 and teachers\u2019 perspectives on language online. Computer Assisted Language Learning, 21 (2), 125 \u2014 142.\n",
        "label": "human"
    },
    {
        "input": "Computer Laboratory Staff and Their Work Term Paper\n\nIntroduction\n\nThe computer laboratory has been a very important facility for institution and organizations alike. As I have had a chance to work in this facility I will explore the various responsibilities and the various sections of the computer laboratory which is typical of any computer laboratory. The sections are categorized according to the work that is done by the technicians in these various sections.\n\nUser account management\n\n 1. The user account management section is concerned with managing the various users who have been created in the system. One of the responsibilities of staff working in this area includes creating and managing accounts of the current users of the system. Since most of the computer laboratories are found in learning institutions, they manage the students who are currently in the institution or manage accounts of the alumni.\n 2. They are also tasked with unlocking accounts which has already been blocked due a breach in the security policy in the institution. This will depend on the discretion of the staff to look into it that the rules that have been set in the system are followed dully.\n 3. They manage passwords for the uses. With computer fraud on the increase, there are many cases where computer fraud has been done due to the fact that users have carelessly left the passwords. The laboratory staffs in this section also have the mandate of making sure that the passwords are in safe custody. One of the key elements that networks still lack is the framework/infrastructure that we will use to manage the network. For this to be achieved there is need to have in place at least one management workstation, one tftp servers, and at least one syslog servers. This is the work of the staff in this section to manage these servers. It is evident that the organization and the institution also need to have password management servers that will be used to manage the passwords that will be used for authentication purposes. These servers will be used to manage the security of the organization. The staff will be required to manage the authenticity of this (Tichy, & McGill, 2003).\n\nPrinting management\n\n 1. With large networks being installed, there is a high increase in network printers so that there is need to have staff to manage the printers because they are under heavy use by the network. They help the students, users, and faculty to manage and preview their printing statements. In this case, they also help the users of the lab to understand the changes that may be experienced in their printing statements. There is also the coming up of online applications. With this in the offing, there is an increase in organizations who store their employee data in the network. The lab staffs are therefore responsible for resetting the PIN numbers of the users online.\n 2. They also help the users into depositing funds into their printing account. There are some instances where the printing is done online.\n 3. They also add and edit print jobs on the network. This will include tasks like changing and status of print jobs during printer downtime like when testing plotter paper. They also diagnose problems that are based on copier/printers.\n 4. They also assist students to print on special types of papers like embossed papers, and using different types of color combinations like inkjet papers, transparencies, labels and stick-backs. There are many requirements that the printer will need to be able to print the required material on the network. It is upon the staff working in the laboratory to make sure that the clients get the best out of the system.\n\nEquipment maintenance\n\nThe laboratory staff is also concerned with maintenance of the equipment which is in the institution in which they are attached. The maintenance of the computers and their accessories can be something which is ongoing or something which is done seasonally. They can be subdivided into equipment maintenance, and supervisor assistance. In the issue of equipment maintenance, the staff are tasked with the installation, configuration and troubleshooting of all the information systems which are working in the institution. With the hacking going hi-tech of late, it is imperative that the people concerned with maintaining the computer laboratory staff are well versed with the systems and all the holes that might be a source of attacks to the institution system.\n\nThey are also tasked with making sure that the supervisors have images which will enable them to do computer configuration. They make ease for the maintenance of security within the system because of the fact a central management of the system is achieved. With systems being controlled from one point, it is easy to track the various points that act as points of intrusion to the system. They also do imaging on working stations. Apart from these tasks, they also do make sure that the computers are installed at the various locations in which they are defined to be and also make sure that these computers which are installed are also secured. They do so in the process to fulfill what the users of the system want to be done at a particular time.\n\nThey also make sure that the network is working and that all the points are functioning in the network. If there is a point which is not getting network connectivity, they will always report it to the network manager who will make sure that that point is troubleshoot (Tichy, & McGill, 2003). They have all the port numbers and their status in the network. Any port which fails to function as is expected will be attended to according to the flow of the network in the system.\n\nProjector management\n\nThey also make sure that the equipment that is used for making presentation like projectors are in good condition always (Belli & Radermacher, 2002). An example is situations where the projectors have light which are burnt, it will be the work of the laboratory staff to make sure that the lamps are replaced and that the focus is clear enough for the viewers.\n\nAudio/Visual\n\nComputer laboratory users are to make sure that the projectors that used in the laboratory are functional and that the speakers are in good condition. This should be done immediately the laboratory is opened. The will, in line with this, make sure that the computers and the computers communicate well with each other so that the users and the presenters can have a smooth session. Students and instructors/presenters that do not know how to use laboratory projectors and scanners are supposed to be assisted by the laboratory staff. The A/V staff and users who have scanty knowledge on the use of A/V equipment should be assisted in the laboratory. This work squarely lies with the computer laboratory staff that should make sure that they are all comfortable with this equipment (Futrell, Shafer, & Shafer, 2002).\n\nProblem reports\n\nThey are also tasked wit making sure that all the laboratory assessment reports are clearly documented. All the issues that are associated with the laboratory should be well documented. This will help in making sure that error logs and solving of problems is made easier. With the problem logs available, the people solving the problem will find it easy to diagnose the problem.\n\nThere should be a system where they report all the issues that deal with computer laboratory. They also make sure that the issues which are reported on the open list on the network are well monitored so that the problems which are persistent are well taken care of. All the problems are to be documented on a daily basis on the network portal.\n\nTechnical support\n\nThe laboratory staff is also supposed to assist the users of the organization to do their work but they should not help them do their work. They should only make sure that they have a good working environment. There has been a vague distinction of roles when it comes to this issue of assisting workers. There has been overstepping of mandate by the users who feel that the laboratory staff is supposed to do the donkey work like printing. This is not possible because they are only supposed to make sure their systems and machines are in good working conditions.\n\nKnowledge sharing\n\nComputer laboratory staff is also tasked with making sure that the users are trained to use the various information systems in the organization. This is done so that the users have the basic knowledge of how the systems work (Futrell, Shafer, & Shafer, 2002). Another reason for this is that all the laboratory technicians will be at the same even level.\n\nConclusion\n\nIt is good to conclude by giving the areas where the laboratory technicians are not supposed to work. They include not supposed to help students and laboratory users do their work. They are not supposed to help students make drawing models to fit into scale. They should also not install software on personal laptops and computers. This not only breaches the working policy of the organization but also breaks the license agreement with the software company. They should make sure that all the works they do are for the organization they work for.\n\nReferences\n\nBelli, F., & Radermacher, F. (2002). Managing computer houses. Springer.\n\nFutrell, R., Shafer, D., & Shafer, L. (2002). Quality computer management. Prentice Hall.\n\nTichy, M., & McGill, A. (2003). The ethical challenge: how to lead in unyielding ethical practices in computer management. John Wiley and sons.\n",
        "label": "human"
    },
    {
        "input": "Computers Brief History: From Pre-Computer Hardware to Modern Computers Essay\n\nTable of Contents\n 1. Introduction\n 2. Pre-computer hardware\n 3. Historical overview\n 4. Conclusion\n 5. Works Cited\n\nIntroduction\n\nThe computer is one of the outstanding scientific inventions by man tracing back to the mid-twentieth century. Many advances have been made with the advent of computers. For instance, the organization of data has become more efficient and effective at the same time. Exchanging communication has also been revolutionized by computers. Moreover, the human mindset has equally made some significant strides in terms of critical thinking and analytical skills. Lots of calculations that used to be done manually have now been computerized. Important economic sectors like business and recreation have benefited more from this technological growth.\n\nModern computer devices have a high efficiency level than the first generation machines. The size is also smaller enhancing its portability. This paper reviews the life history of computers from ancient ages to modern times.\n\nPre-computer hardware\n\nComputers first came to be mentioned way back in 1613 (Allan 27). The term referred to an individual who had the task of performing calculations. The use of this word became a norm. In any case, a computer was not a device as we know it today but a live person. This continued until the end of the first half of the twentieth century. The beginning of the 20th Century witnessed a gradual change in meaning whereby it referred to more of a device used to carry out calculations rather than a human being.\n\nHistorical overview\n\nOne of the most ancient machines used to count was in resemblance to tallying sticks (Ceruzzi 23). Later developments saw the use of cones. They had a general name \u201ccalculi\u201d. In this context, objects or items were bound together in certain quantities to facilitate easy counting. Tiny, well-shaped rods were used.\n\nLater on, a counting instrument called abacus was initiated. Its main purpose was to assist mathematicians in arithmetic. An abacus worked on the principle of reckoning and application of arithmetic functions without necessarily going through the long tedious counting process. Simply made but huge analog computers then followed during the Middle Ages to assist space scientists in their calculations (Ceruzzi 44). In 1206, an astronomical clock was invented by Al-Jazari (Allan 59). This clock created the foundation for the ability of analog computers to perform programming. This was followed by the first-ever manual calculator in 1623 courtesy of Shickard Wilhelm who invented it. This machine was digital and it marked the real onset of the computer age.\n\nThereafter, the skill of using cards punched with holes made its entry into computing. Joseph \u2013Marie Jacquard made a form of a loom whose operation was initiated by holed cards. It made it easier for early computer scientists to appreciate the skill of programming.\n\nThe beginning of the 20th Century was characterized by the use of desktop calculators. Earlier inventions were re-structured and more features were added to allow them to use electric energy. Before and during the First World War, manual and electrical analog computers were considered to be the best in computing technology. They were seen as the future of computing competence. James Thomson took the technology of analog computing a notch higher in 1876 when he innovated a device called differential analyzer (Swedin & Ferro 63). The beginning of the Second World War in 1939 was accompanied by computing competence from analog to digital. In essence, modern computer technology started during this period. Every component switched from the use of extensive wiring to electronic boards. The period marked by the Second World War was characterized by three development stages that took place simultaneously.\n\nAmerican scientists were also pursuing their developments around this time. For instance, Claude Shanon expounded on the relationship between Boolean concepts and electrical circuits (Ceruzzi 27). One of the multi-purpose computers built in the United States was called the ENIAC. It stood for Electronic Numerical Integrator and Computer. The outstanding feature of this machine was its high speed and ability to handle complicated programs. Although ENIAC was perceived to be a state-of-the-art piece of technology, it had some limitations (Swedin & Ferro 83). This then prompted the need to improve both its capacity and efficiency. This led to the introduction of first-generation computers. EDVAC came into existence as a result. It could store its programs, unlike ENIAC.\n\nBy the start of the mid-twentieth century, commercial computers which had a faster multiplier were already in use. It was a modification from previous machines. The second-generation computers used tubes instead of transistors. These machines had fewer power requirements. IBM 1401 was very popular in the global market. The third generation computers did not follow immediately. Between the two generations, hybrid systems like UNIVAC 494 took the centre stage (Allan 176).\n\nThe 21st Century computer technology base has been broadened. For instance, the multi-core Central Processing units are used on large scale. There are several versions of microcomputers and micro-processors in use today.\n\nConclusion\n\nThe synopsis of computer history is traced back to the early seventeenth century. This era was characterized by manual and crude forms of performing arithmetic. Interestingly, the word computer had a human definition as opposed to a device as it is known today. Major technological advances in computing have eventually resulted in the use of microprocessors and microcomputers which consume less power, occupy less space, are highly programmable, and generally efficient.\n\nWorks Cited\n\nAllan A. Roy. A history of the personal computer: the people and the technology 1 st ed. Ontario: Allan Publishing, 2001. Print\n\nCeruzzi E. Paul. A history of modern computing 2 nd ed. M.A: Techset Composition Ltd, 2003. Print\n\nSwedin Eric Gottfrid and Ferro L. David. Computers: The life story of a Technology: Westport: GreenWood Press, 2005. Print\n",
        "label": "human"
    },
    {
        "input": "Mathematics as a Basis in Computer Science Essay\n\nIntroduction\n\nThere is a lot of resemblance between mathematics and computer since. Through this essay, I demonstrate how a mastery of mathematics laid a foundation for my interest in computer science and more specifically, website design and computer programming. I have also explored how my love for mathematics helped improve my performance in other subjects as well. This is an indication that mathematics as a discipline cuts across virtually every other subject. Over the years, I have been involved in various extracurricular activities, but none has been as compelling as my desire to help poor children. Again, I demonstrate why I am motivated to assist these needy members of our society.\n\nCorrelation between computer science and mathematics\n\nThere is a strong correlation between mathematics and computer science. In this case, the field of computer science borrows heavily from the mathematics discipline. From a very young age, I started manifesting an interest in mathematics. This interest was influence in part by the fact that both of my parents taught mathematics at high school level. Therefore, naturally, the expectation would have been that I would also take an interest in the discipline of my parents, and I did not disappoint them. As a teenager, I discovered that I also had an interest in computer science. Perhaps this was as a result of my good command of such branches of mathematics as discrete mathematics and Boolean algebra. Many will now agree that the involuntary application of computing skills in mathematics is now an indispensable activity. This is a further testament to the connection between the two disciplines. For example, computational thinking or algorithm as used in computer science resembles abstractions development and empirical verifications that finds application in mathematics.\n\nDue to my passion for both computer science and mathematics, I have managed to receive recognition by wining various contests, both in high school and at college level. Winning these contest only served to enhance my resolve to do better in these two disciplines. The two branches, along with several others, find application in the wider field of computer science. Furthermore, computer science requires logical and mathematical applications. These are the necessary ingredients for the design as well as control of computers.\n\nOne other reason why mathematics as a discipline fascinates me is because in enables one to solve problems facing them in a very logical manner. In addition, I have also learnt that thanks to my love for mathematics, my thoughts and words process is also precise. Another discovery that I made very early in high school is that mathematics finds application in other disciplines as well. Consequently, my grades in the other subjects were also improving dramatically. For example, my scores in physics and chemistry were also comparable to those I obtained in mathematics, a further testament to the importance of mathematics in other disciplines.\n\nI am really indebted to my high school teacher who discovered my love for mathematics and encouraged me to further my interest in mathematics and computer science. By helping me nurture my skills, I have since managed to retain interest in the two disciplines. My hands \u2013on and practical skills approach has helped shape my desire to pursue computer programming. Over the last 6 years, I have been involved in the designing of websites for clients at a fee. The most interesting thing is that I am a self-taught website designer, an indication of the level to which an individual can go when they are self-motivated. Again, good command for mathematics has come in handy because programming entails a lot for computation in algorithms. Upon completing high school I was involved in the creation of toy projects and small games. This was a necessary move, because I end up borrowing a lot from the experience that I gained then, when I am either undertaking computer programming activities, or in handling large amounts of data in my work.\n\nExtracurricular activities\n\nIn today\u2019s business environment, the issue of corporate social responsibility is very common. It is usually perceived as an indication of the fact that an organization would wish to give back to society after their have recorded success in their core activities. However, even individuals are themselves socially responsible. Some prefer to call this action extracurricular activity, while others perceive it as the desire to help those in need. From a personal point of view, I have taken part in numerous extracurricular activities, a majority of which involves the helping of poor children. Some people prefer to spend their spare time with families and friends, sharing in their experiences or just relaxing in a jovial and homely atmosphere. Others prefer to spend their free time in solitude. In my case, this is the time that I dedicate to charity work. I believe the driving force to my participating in charity has been influenced in more ways than one by my mother. She is a social worker by profession. Although her income modest, nonetheless, she manages to dedicate part of her salary to acts of charity, such as footing the hospital bills of friends in need.\n\nThrough the years, I have come to appreciate the fact that when you give, one gets more blessings. For example, although my mother is a generous woman, both financially and in terms of advice, we have never lacked the basics in life. She is also at peace with her family friends and more importantly, her creator. I have also personally experienced this during my charity work. I recall giving a homeless person a hundred dollar bill on the eve of one Christmas. Although I had saved this money for a long time, I shall forever remember the bright face of the man to whom I gave that money. This experience taught me the need to share with others the little that I have. A majority of the poor children that I interact with are homeless, and some have no parent. Interacting with these children has also helped me to see the world from their perspective. Given the fact that a majority of the rich have no intention of sharing part of their wealth with the homeless and poor children do they ever wonder what would become of them if these children ganged up against them? Many of these poor children have big dreams that they would wish to fulfill. When I interact with them and they share their fears and dreams with me, I am compelled to do the little I can to help them realize these dreams. This, I believe, is what keeps me motivated to help them.\n",
        "label": "human"
    },
    {
        "input": "Sidetrack Computer Tech Business Description Essay\n\nBackground information\n\nTo increase the level of income, the entrepreneur settled at a decision to venture into business by establishing a small business entity. The firm will be established within the Information Communication Technology (ICT) industry. The business name of the firm will be Sidetrack Computer Tech and will be located in Mexico City, US. In deciding to venture into the industry, comprehensive market research was conducted to determine the viability of the venture. Findings of the market research revealed a high probability of the firm succeeding due to the lucrative nature of the industry. This results from the fact that organizations in different economic sectors in Mexico are incorporating computer technology to attain efficiency in their operation (Piatkowski, 2004, p. 1).\n\nSidetrack Computer Tech will deal with a variety of computer products such as merchandising in computers and computer accessories. To develop a competitive edge in the industry, the firm will specialize in the development of electronic commerce software specific to firms in different economic sectors. The software will be developed to enhance the process of the client\u2019s firms incorporating electronic commerce concepts in their operation. The firm will also specialize in the provision of computer technology consultancy services. This means that the firm will operate as a good and service-oriented firm. Currently, Mexico has a single-digit growth rate within the software industry. However, the past few years have been characterized by an increment in demand for ERP amongst SMEs (Pedraaza, 2004, para. 9).\n\nVision statement\n\nSidetrack Computer Tech intends to become the leading supplier of computer technology for the distribution of computers, computer accessories, and the development of electronic commerce technologies. The firm also intends have expanded its electronic software development to all the Small and Medium Enterprises (SMEs) in the US within five years from its inception.\n\nMission statement\n\nIn its operation, Sidetrack Computer Tech will be committed to ensuring that it delivers quality products and services to its customers. This will contribute towards the firm developing a strong customer experience. This will be achieved by translating the technologies that the firm develops (software) into value. As a result, the firm will be able to increase the level of customer satisfaction received. To attain this, value addition will be incorporated into the firm\u2019s course of operation.\n\nBusiness structure\n\nEvery business should integrate a structure that is legally recognized. This is because different business structures have got diverse legal implications such as the amount of tax charged on the specific business structure. Various business structures are legally recognized. These include partnership, corporation, and sole proprietorship (Australian Taxation Office, n.d, p.2).\n\nPartnership form of business structure will be adopted the operation of Sidetrack Computer Technology. The decision to adopt a partnership form of business structure resulted from the realization of the numerous benefits associated with such a business structure. A partnership form of business is defined as a business entity that is operated by two or more parties. Sidetrack Computer Technologies will have two partners. The decision to adopt partnership form of business structure arose from the inexpensive characteristic associated with starting up a partnership type of business structure. This is because each partner contributes a significant proportion of finance to support the establishment of the firm.\n\nThrough this, part structure, Sidetrack Computer Technologies will be able to improve on its performance. For example, the firm will be able to conduct value addition more effectively. This is because there will be ease in the process of the firm securing external sources of finance thus enabling the firm to conduct product innovation more effectively.\n\nIn addition, there will be sharing of management and operational tasks amongst the partners. As a result, there will be a reduction in the amount of workload assigned to individual partners. This form of business structure will also contribute towards the firm attaining optimal performance since the partners will share business skills amongst themselves.\n\nManagement of the firm\n\nCustomer centric\n\nTo attain the vision formulated, a high level of efficiency and effectiveness will be integrated in the firms\u2019 course of operation. One of the strategies that will be incorporated in the firm\u2019s course of operation includes consumer centric. Customer centric refers to the process through which the entire firm\u2019s operations are focused at meeting the customers\u2019 requirements (IBM, n.d, p.4). By being customer centric, the firm will be able to address the customers\u2019 computer product, accessories and electronic business software demands. This will be attained by motivating the employees to consider the customer as the most important asset in the success of the firm. By being customer centric, it will be possible for the firm to undertake product personalization and customization. Product customization will be attained by focusing at developing a product that meets the consumers\u2019 specific needs. This will result into an increment in the credibility of the firm\u2019s products amongst the individual and institutional customers. The ultimate effect is that there will be an increment in customer loyalty culminating into an increase in the firm\u2019s level of profitability.\n\nHigh credibility of the firm\u2019s products will also be attained through incorporation of total quality management (TQM). Murray (2010, para.2) defines TQM as the process through which quality and performance are improved to meet the customers expectations. TQM will result into an increment in the customers trusting the firm\u2019s products due to their high degree of operability and reliability. A comprehensive TQM program will be developed. In addition, strict adherence to the program will be ensured in the entire supply chain.\n\nTeam work\n\nTo attain effectiveness and efficiency in developing the electronic commerce software, the concept of teamwork will be integrated. This will be attained by incorporating a high level of collaboration amongst the firm\u2019s employees. Teamwork is necessary in software development due to the high degree of complexity involved and hence the need to share the skills. According to previous studies, it has been established that team performance exceeds independent performance. Demirors and Sarmasik (2004, para. 1) asserts that attaining efficiency in a team is a challenging undertaking and demands a significant degree of commitment. To attain efficiency in the process of developing the software, a good relationship amongst the employees will be developed. This will contribute towards the concept of teamwork being successfully incorporated in the firm\u2019s course of operation.\n\nReference\n\nDemirors, E. & Sarmasik, G. (2004). The role of teamwork in software development: Microsoft case study. Izmir, Turkey: Budapest. Web.\n\nIBM. (n.d). The customer centric store. New York: IBM Business Consulting Services.2010. Web.\n\nMurray, M. (2010). Total quality management. About.com. Web.\n\nPedraaza, K. (2009). ICT to Mexico: trends and opportunities. Web.\n\nPiatkowski, M. (2004). Potential of ICT for development and economic restructuring in the new EU member states. London: Tiger Incorporation. Web.\n",
        "label": "human"
    },
    {
        "input": "Strayer University\u2019s Computer Labs Policy Essay\n\nOverview\n\nStrayer University Computer labs have been set up to facilitate the pursuit of academic excellence as provided by the University\u2019s mission of academic merit. The opportunity to access, and use computer lab equipment and the networked system is important to all members of Strayer University. To protect that opportunity for current and prospective students and faculty members, all users must comply with the University\u2019s standards for acceptable use of computer lab resources. The University\u2019s administration fully understands that personal use of computer labs systems may improve personal skills, but these resources must be limited primarily for administrative and academic purposes. By accessing and using the University\u2019s computer lab and all equipment installed, users agree to follow all University\u2019s policies and applicable federal and state laws.\n\nPurpose\n\nThis document provides guidelines for acceptable use of Strayer University\u2019s computer lab and further defines who is authorized to use computer lab facilities. The purpose of this policy is to protect the integrity of information and guard the privacy and confidentiality of users. The policy ensures that computation complies with Strayer University policy and other applicable federal and state laws. The policy will also ensure free flow and exchange of information that promotes sharing of ideas that leads to academic freedom and excellence. If users are not sure whether any action or use of computer lab resources amounts to a violation of this policy, they should contact the lab administrator. If an issue is not addressed explicitly by Strayer University Computer Lab use policy, the system administrator\u2019s resolve will prevail.\n\nScope\n\nThis policy applies to all users of Strayer University Computer lab resources. The definition of \u201cuser\u201d in this policy refers to any person who gains access and uses computer lab computing and networked resources. In this regard, users include students, members of faculty, administrator, and guests. Computer lab resources include computers, network systems, printers, scanners, plotters, instructional resources and any other related equipment installed or placed in the computer labs. All personal computing equipments connected to computer lab resources and networked systems are also subject to this policy.\n\nGeneral Use and Ownership\n\nStrayer computing resources are limited and as such, all computers are allocated on a first come first served basis. Priority is given to those users seeking to fulfill their academic objectives. In some cases, user may be asked to and prove that they are engaged in academic work. Those engaged in activities not related to academic may be asked to give up machines to users who need to carry out academic work. Some machines may be set aside for specific purposes and may be attached to unique hardware or contain dedicated software. These systems will be marked for easier identification and users who need such machines will be given priority to use.\n\nSecurity and Proprietary Information\n\nStrayer University has put in place measures to ensure that all computing systems in the computer lab and all users\u2019 accounts are secure. However, the university does not offer any guarantee to users of such security. User should exercise discretion and engage in practices that promote safe computing. Users are encouraged to set appropriate accounts access restrictions. Passwords should be changed regularly and should be closely guarded. All users are responsible for encryption and protection of personal documents and should maintain personal back-up. The university respects the privacy of all computer labs user. However, users should realize that use of computing resources in the computer lab is not exclusively private. User should also be aware that the university regulations require that data and communication records including user logs and general usage monitoring be backed-up, for administrative purposes.\n\nUnacceptable use\n\nTo prevent computing activities that adversely affect other users, the university will ensure that any form of misuse does not occur. Display of obscene material is prohibited in all Strayer University computer labs. Any user who engages in viewing explicit material will be asked to stop and leave the lab immediately. Computer users are expected to show respect for other users in the computer lab. Lab users are expected to work quietly. Disruptive behavior and loud discussions may disturb other users working on their academic assignments. Any person who exhibit threatening, disruptive or hostile behavior such as but not limited to yelling, disregarding lab attendant\u2019s requests etc will be asked to leave the computer lab immediately.\n\nSystem and Network Activities\n\nAll computers and related equipments are configured to meet the need of student and faculty. Users should not make any changes to computing equipment settings. Any equipment in the computer lab connected to the University network system can only use IP addresses assigned by the computer lab administrator. Any device connected between lab computing equipment and the university network must be approved by the University network administrator. Any device connected to the university network system is subject to this provision.\n\nE-mail and Communication Activities\n\nThe University has established e-mail system to facilitate efficient communications between members of university community. The university reserves the right to change or withdraw email services especially when there is cause to believe misuse of the university e-mail system. Use of University mailing list, should be limited to academic and official business only. The content of any e-mail sent through the university computing system, and/or using the university mailing system should be appropriate and should not be in violations of acceptable use regulations. User should not send unsolicited emails, or try to conceal or alter their identity. No user should attempt to hinder or intercept email communication. Authorized university staff may view the content of user email if they have a reason to believe the user violated this policy.\n\nBlogging\n\nThe University has established discussion forum in form of blogs to facilitate online discussion between students and faculty. User using blog should be aware that blog spaces are online class room. Any material that is not appropriate for normal classroom discussion is also not suitable for blogs. All user should exercise discretions when blogging and should conduct themselves in a manner that shows respect for other bloggers. Blog should be limited to student expression of ideas. They should not be used for non academic purposes.\n\nEnforcement\n\nAny reports of violation of computer lab policy may be made to Strayer University IT department staff, supervisor, attendant or network and system administrator. Admonition and face to face discussions will be used to address alleged minor violations. Depending with the severity of violation that calls for sanction, the following may be applicable: loss of computer lab access and use privileges, formal reprimand, probation, termination of employment in case of staff and expulsion from the university in case of students.\n",
        "label": "human"
    },
    {
        "input": "Advantages of Using Computers at Work Essay\n\nTable of Contents\n 1. Introduction\n 2. Advantages in Discovering Computers Book\n 3. Advantages in Source Article\n 4. Conclusion\n 5. Works Cited\n\nIntroduction\n\nIn today\u2019s society, using a computer is nothing unique. From communication to shopping, from learning to playing games, almost everything is digitized so that most processes will involve the usage of a computer. It is therefore easy to take for granted the advantages of using a computer especially considering that today\u2019s generation already found computers in use and might therefore have a hard time appreciating just how hard things were before. In doing this assignment, I am hoping not only to become aware of these advantages, but also to appreciate them and in the process make others aware of them as well.\n\nAdvantages in Discovering Computers Book\n\nSome of these advantages, as discussed in the \u2018Discovering Computers Book\u2019 are as follows: Computers accelerate the speed by which certain tasks can be done, for example entering and retrieving information which would otherwise take much longer were it to be done manually through the filing system. Computers are more reliable than human beings since they will rarely make mistakes as long as the input was correct. This is because they don\u2019t get tired and can perform the same task over and over, resulting in reliable consistency. The storage capacity of computers is huge. Things like maps can be stored in much smaller spaces than they would if they were to be stored manually. Computers have taken communication to a whole new level in that people from all over the world can communicate via the internet. Information on just about anything can be obtained at the click of a button and this makes it easier to learn about different places all over the world.\n\nAdvantages in Source Article\n\nSome other advantages include making work easier in offices, creating fun for children and accessing the internet. Computers make work easier in offices by providing safe storage to information and the easy retrieval of the same. In computers so much information can be stored in so little space. If this same information were to be stored manually it would fill up so many rooms. Children find fun in computers by playing games and also having a lively place from which they can learn since computers provide an interactive mode of learning. The internet is one of the greatest benefits of computers since it is an easy and fast source of much information, and it connects the whole world. Today people do not have to travel in order to experience different cities since they can \u2018visit\u2019 those places through virtual reality. This way they are more informed before committing to touring certain places.\n\nConclusion\n\nIt may not be possible to list down every advantage of using computers as these are numerous and they apply to people differently. Whilst this may be true, I have learnt a lot from this research. I feel that it is so easy to take for granted the benefits of having computers. Before computers, processes like filing and retrieving information would take so much more time and space. Computers have solved these problems by making work much easier, faster and more fun. I have learned what I hoped to learn in that I have become more aware of the advantages of using computers and why I should not take them for granted. I have never heard about this as a topic because it is so easy to overlook the importance of learning these advantages. Both sources, i.e. Discovering Computers and the source article give more or less similar advantages of using computers.\n\nWorks Cited\n\nDiscovering Computers Book. 2009. Web.\n",
        "label": "human"
    },
    {
        "input": "Computers: The History of Invention and Development Essay\n\nThe invention of the computer in 1948 is often regarded as the beginning of the digital revolution. It is hard to disagree that computers have indeed penetrated into the lives of people have changed them once and for all. Computer technologies have affected every single sphere of human activities starting from entertainment and ending with work and education. They facilitate the work of any enterprise, they are of great assistance for scientists in laboratories, they make it possible to diagnose diseases much faster, they control the work of ATMs, and help the banks to function properly. The first computers occupied almost the whole room and were very slow in processing data and performance in general. The modern world witnesses the development of computer technologies daily with computers turning into tiny machines and working unbelievably smoothly. A computer is now trusted as a best friend and advisor. It is treated as a reliable machine able to process and store a large amount of data and help out in any situation. \u201cThe storage, retrieval, and use of information are more important than ever\u201d since \u201c(w)e are in the midst of a profound change, going from hardcopy storage to online storage of the collected knowledge of the human race\u201d (Dave, 2007), which is why the computers are of great assistance to us. However, to become a successful person, it is not enough to simply have a computer at home. It is often the case that people use computers merely to play games without knowing about the wide range of activities they may engage a person in. One has to know more about computers and use all their capabilities for one\u2019s own benefit. Knowing the capabilities of one\u2019s computer can help in the work and educational process, as well as it can save time and money. In this essay, you will find out reasons as to why it is important to know your computer; and how much time and money you will save by using all the capabilities of your computer.\n\nWhat should be mentioned above all is that knowing one\u2019s computer perfectly gives an opportunity of using it for the most various purposes. It depends on what exactly a person needs a computer for, in other words, whether it is needed for studying, for work, or for entertainment. Using a computer for work or education purposes involves much more than is required for playing computer games. These days most of the students are permitted to submit only typed essays, research papers, and other works, which makes mastering the computer vital. \u201cInformation technologies have played a vital role in higher education for decades\u201d (McArthur & Lewis, n.d.); they contributed and still continue to contribute to students\u2019 gaining knowledge from outside sources by means of using the World Wide Web where information is easily accessible and available for everyone. To have access to this information one has to know how to use a computer and to develop certain skills for this. These skills should include, first of all, using a Web browser. \u201cIn 1995, Microsoft invented a competing Web browser called Microsoft Internet Explorer\u201d (Walter, n.d.), but there exist other browsers the choice of which depends on the user. Moreover, knowing different search engines (for instance, Google, Yahoo, etc,) is required; the user should also be able to process, analyze, and group similar sources by means of extracting the most relevant information. At this, the user is supposed to know that not all Internet sources should be trusted, especially when the information is gathered for a research paper. Trusting the information presented in ad banners is unwise for their main purpose is attracting the users\u2019 attention. They may contain false or obsolete data misleading the user. Utilizing the information obtained from the Internet for scholarly works, one should remember about plagiarism or responsibility for copying somebody else\u2019s works. Students who use such information should cite it properly and refer to the works of other scholars rather than simply stealing their ideas. Plagiarism is punishable and may result in dropping out of school or college. This testifies to the fact that using a computer for studies demands the acquisition of certain computer programs and practice in working with them, which would give a perfect idea on how to search and process the information needed for completion of different assignments.\n\nWhat\u2019s more, knowing a computer for work is no less important. Mastering certain computer programs depend on the type of work. Any prestigious work demands a definite level of computer skills from the basic to the advanced one. The work of a company involves sometimes more than using standard computer programs; the software is usually designed specifically for the company depending on the business\u2019s application. This means that acquisition of a special program may be needed and a new worker will have to complete computer courses and gain knowledge on a particular program. Nevertheless, the knowledge of basic computer programs is crucial for getting a job one desires. Since the work of most companies is computerized, one will need to deal with a computer anyways and the skills obtained while playing computer games will not suffice. A person seeking a job should be a confident user of basic computer programs, such as Microsoft Office Word, Microsoft Office Excel, Internet Explorer (or other browsers), etc. A confident user is also supposed to know what to do with the computer when some malfunctions arise. Of course, each company has system administrators who deal with computer defects but minor problems are usually born by the users themselves. Apart from knowing the computer, a person should be aware of the policy of using it in the office. For instance, some companies prohibit using office computers for personal purposes, especially when it comes to downloading software and installing it on the computer without notifying the system administrator. This may be connected either with the fact that incorrectly installed software may harm the system of the computer in general or, if the software has been downloaded from the Internet, it may contain spyware which makes the information from your computer accessible for other users. This can hardly be beneficial for the company dealing with economic, political, governmental, or any other kind of issues. Therefore, knowing a computer is necessary for getting a prestigious job and ensuring proper and safe performance of the company one is working for.\n\nAnd finally, using all the capabilities of a computer can save time and money. Firstly, a personal computer has a number of tools which facilitate people\u2019s life. Special software, for instance, Microsoft Money, makes it possible to plan the budget, to discover faults in the plan, and correct it easily without having to rewrite it from the beginning; the program itself can manage financial information provided by the user and balance checkbooks in addition. Such computer tools as word processors enable the users to make corrections at any stage of the work; moreover by means of them, one may change the size of letters and overall design of the work to give it a better look. Mapping programs can also be useful; by means of a computer one may install such a program (GPS) into the car; the program then will take care about planning the route avoiding traffic jams and choosing the shortest ways. Secondly, electronic mail allows keeping in touch with people not only in your country but abroad. It is cheaper and much faster than writing letters or communicating over the telephone when the connection is often of low quality and the conversation is constantly interrupted. Most telephone companies are aimed at getting profits from people\u2019s communication with their friends and relatives whereas electronic mail is almost free; all that one needs to do is to pay a monthly fee to the Internet Service Provider. Eventually, computer users have an opportunity to do shopping without leaving the apartment; the choice of the products one may want to buy is practically unlimited and the user can always find recommendations from those people who already purchased the product. A personal computer can also help to save money due to its being multifunctional. Knowing much about the capabilities of the computer, one may start using it as a TV set watching favorite programs online, and as a Playstation playing the same games on the personal computer. Not only can a user watch favorite TV shows by means of his/her computer, but can download them at various torrent sites for free. Using a PC to send faxes through online fax services saves money for one does not have to buy a fax machine and to use an additional telephone line; it also saves paper and ink which one would have to buy otherwise.\n\nTaking into consideration everything mentioned above, it can be stated that knowing a computer is important for it can make people\u2019s life much easier. Firstly, computers are helpful in getting an education since by means of them the students can find any possible information necessary for writing research papers and other kinds of written assignments. To do this, a student needs to know how to search the Internet and to process the information he/she can find there. Secondly, knowing a computer raises one\u2019s chances of getting a good job because most of the companies look for employees with a sufficient level of computer skills. When working for a company one should also remember about its policy regarding the use of computer for personal purposes and be able to cope with minor problems arising in the course of work with the computer. Finally, a computer allows saving time and money. It saves the users\u2019 time due to utilizing such tools as word processors, budget planning, and mapping programs which facilitate the users\u2019 life. The computer can also save money serving as a TV, fax, and Playstation giving access to TV shows, online fax services, and allowing playing video games without buying special devices for this.\n\nReferences\n\nMcArthur, D., Lewis, W.M., ND. Web.\n\nMoursund, D. (2007). A College Student\u2019s Guide to Computers in Education . Web.\n\nWalter, R. ND. The Secret Guide to Computers . Web.\n",
        "label": "human"
    },
    {
        "input": "\u201cFailure to Connect \u2013 How Computers Affect Our Children\u2019s Minds and What We Can Do About It\u201d by Jane M. Healy Essay\n\nThe modern world witnesses wide use of technology which assists in different kinds of processes people are involved into. The book \u201cFailure to Connect \u2013 How Computers Affect Our Children\u2019s Minds and What We Can Do About It\u201d by Jane M. Healy deals with the assistance of technology in children\u2019s learning process and the effects the technology produces on their minds. The book describes stages of children\u2019s cognitive development as well as measures which should be taken by parents in order to \u201coptimize their children\u2019s mental growth\u201d (Healy, 2000). Detailed analysis of several chapters of the book will help to understand the impact of computer technologies on children\u2019s health and mental development.\n\nTo begin with, chapter 4 of the book deals with the impact of video games on children\u2019s mental health. The author of the book advises parents to \u201c(b)e alert for abnormally \u201cspaced-out\u201d behavior that can signal rare video-game-related seizures\u201d (Healy, 2000). Indeed, these days most of children are obsessed with videogames. It has been proved that video games cause addiction which leads to children being abnormally dependent on them. It seems that the only way out for parents in case of their child\u2019s dependence on video games is to choose those games which \u201cencourage reading and original problem-solving instead of memorizing procedural routines\u201d (Healy, 2000).\n\nIn addition, chapter 5 deals with learning environment and learning conditions of children. Healy emphasizes that \u201cchildren who learn in one medium (screen vs. page) will always be inclined to prefer the one in which they learned\u201d (Healy, 2000). From this it can be concluded that children who got used to studying by means of books are unlikely to have the same level of academic achievements when studying by means of computer. This happens due to a particular frame of the perception of information by a child for whom it is easier to comprehend the text written on the paper, rather than the one which appears at the screen.\n\nChapter six explores the impact of computer technologies on children\u2019s health. It has been discovered that \u201csome uses of computers can trigger hormonal changes that raise blood pressure or affect the immune system\u201d (Healy, 2000). This is especially harmful for the health of children, since even the adults\u2019 brain \u201cis very responsive to small alterations in the hormonal system\u201d (Healy, 2000). Negative effect of computer technologies on children\u2019s health has been known for decades. Hormonal changes, raised blood pressure, and weakening of immune system may result in child\u2019s serious problems with health in future. This being the reason, the time which the child spends using the computer should be limited.\n\nIn chapter 7 Healy states that \u201c(p)reschoolers and even six- to eight-year-olds in no way need computers\u201d (Healy, 2000). It is hard to disagree with her, since children at that age mostly use computers for fun, rather then for studying. Computers are more important for older children, though again, their usage for entertainment should be restricted.\n\nChapter 8 continues developing the idea presented in chapter 7. It describes the influence of computer technologies on preschoolers and children younger than eight years old. It turns out that using computers affects not only children\u2019s physical health but \u201ccan \u201cscaffold\u201d [their mental] development\u201d (Healy, 2000). Therefore, children should be allowed to use computers only for study purposes.\n\nFinally, chapter 9 deals with a computer as \u201ca \u201cthinking\u201d or \u201creasoning\u201d substitute for human intelligence\u201d (Healy, 2000). Indeed, computers facilitate a number of human activities and make people\u2019s life easier. However, they will never be able to substitute for human intelligence for they are devoid of feelings and emotions being able to perform only a limited set of activities they have been programmed for.\n\nReferences\n\nHealy, J. (2000). Failure to Connect: How Computers Affect Our Children\u2019s Minds\u2013For Better and Worse . Simon & Schuster.\n",
        "label": "human"
    },
    {
        "input": "Human Mind Simply: A Biological Computer Essay\n\nIs the Human Mind Simply a Biological Computer \u2013 Except Slower?\n\nWhen contemplating the man-like intelligence of machines, the computer immediately comes to mind but how does the \u2018mind\u2019 of such a machine compare to the mind of man? A human brain assimilates and processes much the same as a computer. However, because the mind of man possesses consciousness, it perceives beauty, generates moral judgments and formulates rationalizations which the machine cannot execute. When the computer was in its early development stages, it was thought of as an electronic, thinking device, the mechanical equivalent of the human brain. This misconception is a gross oversimplification of the seemingly limitless boundaries of the human mind. The potential of machines were thought to be able to eventually encompass \u201can inductive and creative mind, capable of taking initiative, to which human beings could confide all their problems and obtain instant solutions in return.\u201d (Ifrah, p. 1679 1997)\n\nSimply stated, computers are machines which effectively carry out algorithmic functions. The machine discerns formalized input through a sequence of fixed stages through a predetermined, straightforward set of rules of a standardized and exacting description. This allows computers to perform procedures in a precise number of steps. Mechanical computers, unlike the functions of the computer-like brain does not have the capability to determine right from wrong nor can it make judgments, has no feelings and cannot think on its own. It cannot be denied that some types of intelligence can be attributed to computers but this capacity is very limited when balanced against the boundless intricacies in a human\u2019s brain. However, the computer is superior when considering its capability to process information at a higher speed. This has provided humans a useful tool for a myriad of endeavors. Nevertheless, computers cannot reason, imagine, invent, create, express thoughts, manage ideas, make judgments or possess the ability to adapt to differing situations and therefore cannot solve problems that are new to it. Unlike the human brain computers aren\u2019t conscious of its own being therefore has no concept of the world around it and cannot execute voluntary activities. (Kak, 2005)\n\nBecause machines only able to follow directives, they do not possess the capability to be self-aware. Conversely, if it is accepted that computers do not and will never become aware of its own being, then it is reasonable to ask what enables the human\u2019s biological machine to attain consciousness while the silicon-based computerized \u2018brain\u2019 cannot. Possibly, the answer to this question is the fact that the structure of the human brain is self-organizing. It responds to the individual characteristics and the independent nature of interactions between itself and the particular environment. Computers do not have the ability to accomplish this.\n\nConsciousness allows for the acknowledgement of beauty which is known only to those that possess biological intelligence. Aesthetics value has very little in common with the processing of information. Beauty is a known but knowing this information is not a process of mathematical computations. Both the brain and the computer can add numbers but the computer is not impressed with this knowledge nor does it feel pride in accomplishing new tasks such as the biological mind might. The reasoning for why the brain knows to perform a function then knowingly yearns for more knowledge or finds the procedure a fulfilling experience remains unclear. The computer, by contrast only knows to perform the function when prompted. It has no contemplations regarding the knowing of the experience. The human mind can contemplate its own functions and existence. It may also think that the various functions of it or a computer is wondrous, beautiful event. This, along with the fact that the machine produces predictable results is the factor that separates the two processing entities. However, this viewpoint is a superficial observation of the human mind because there remains much more regarding the mysteries of the brain as opposed to the mechanical function of the computer. The brain has the ability to reject new knowledge where the computer does not. This allows for an aura of individuality that machines do not enjoy. (Clear 2003)\n\nThe human mind has the ability to know what is morally right or wrong almost instantly without the need for assimilating much information. It can make decisions based on the unknown knowledge and can rationalize, justify and reason which is traits only known to that which is conscious. Knowledge has no life. It is based only on cold facts whereas knowing is uniquely biological in nature. There is much puzzlement regarding knowledge and knowing.\n\nIn generally terms, it is assumed that the activities that differentiate human thought from that of a machines conceptualization are best characterized by the understanding of language although it cannot be denied that those who are deaf or mute do in fact think though they do not speak at the same level as others. In addition, studies have shown that most types of animal life have the capability to learn and solve problems. The use of language is as a compartment of a larger inventory of behaviors. Computers do not possess the ability of humans or even of animals to formulate or initiate any type of language on its own. Computer \u2018language\u2019 is pre-programmed. The use of uninitiated language, no matter how primitive, is within the realm of biological beings alone. (Clear 2003)\n\nThe difference between the human brain and a machine of any type is that humans create machines to be used as a tool. Human intellect is extremely intricate and consciousness too mysterious to be duplicated. On the day that a computer can lie or cheat, when it prays to an unknown entity and feels shame or sorrow then, possibly, it can be compared to the human mind. Until then, the only similarity is that both process information but to vastly different extents and by different methods.\n\nWorks Cited\n\nClear, Bruce \u2018Knowing What We Don\u2019t Know That We Know\u2019 (2003). Web.\n\nIfrah, G. Historia universal de las cifras. Madrid: Espasa Calpe. (1997). Web.\n\nKak, Subhash \u2018Artificial and Biological Intelligence\u2019 Ubiquity Volume 6, Issue 42 (2005). Web.\n",
        "label": "human"
    },
    {
        "input": "Shaping and Profiting From the Computer Revolution: Bill Gates Essay\n\nThe Man\n\nWilliam H. Gates III, erstwhile Chairman/\u201cChief Software Architect\u201d of Microsoft Corporation, and now co-founder of the Bill and Melinda Gates Foundation, famously ranked as the wealthiest man in America for fifteen years running. Though he had an attorney for a father, it is likely that his entrepreneurial spirit came from a \u201cmulti-tasking\u201d mother, Mary Gates, who juggled teaching, being regent at the University of Washington in Seattle, and chairing United Way International.\n\nThe story of how Bill Gates helped lay the foundation for the desktop personal computer revolution \u2013 first by cooperating with Steve Ballmer to \u201cport\u201d the mainframe language BASIC for microcomputers right in their Harvard dorm rooms and, after dropping out of Harvard, by writing the Microsoft Disk Operating System (MS-DOS) with lifelong friend Paul Allen \u2013 parallels the entrepreneurial legend of Steve Jobs at Apple Computer in the mid-1970s.\n\nThe evident personal philosophy of Bill Gates comprised being hard-driving, looking out for long-term results, being a ruthless competitor, defending market gains, and maximizing profits. This last typifies even the couple\u2019s philanthropic foundation, which has been accused of investing for returns more than donating to alleviate poverty in developing countries. In Mintzberg\u2019s typology, therefore, Gates has shown himself more interested in being, first, the entrepreneur, and subsequently figurehead/ spokesperson, and world-renowned leader.\n\nThe Company\n\nMicrosoft was registered in 1975 as a privately-held corporation and went public with an initial public offering in 1986. From then on, or for the next 23 years, the company has held the top position in the global software industry by a combination of ruthless and unethical business practices.\n\nRight from the start, Microsoft concentrated on writing the software that would interface between machine language (that only programmers had the patience to learn) and true end-user software such as word processors, spreadsheets, accounting programs, or Internet browsers. These interface software layers are called compilers and one of the first the company developed was the TASC, The \u201cAppleSoft Compiler\u201d, to run BASIC on the unique machine language that was Apple\u2019s, The aforementioned BASIC was written for the Altair 8800, one of many start-up brands competing for a fair share of the embryonic desktop computer market at that time.\n\nThe real breakthrough came when Microsoft repackaged a clone of the CP/M operating system and licensed it to IBM, then about to launch the category-creating modular IBM PC, as \u201cIBM DOS\u201d. When other hardware makers managed to clone the IBM BIOS (the \u201cBasic Input/Output System\u201d), Microsoft sold the same thing to all manufacturers as MS (Microsoft) DOS. Since the product by then cost nothing except the nominal expense of disk-to-disk copying, the foundation was laid for the enormous cash flow that allowed now Seattle-based Microsoft to develop dozens of end-user software packages far into the future.\n\nSince Bill Gates was never shy about proclaiming his dream of seeing \u201ca computer in every home,\u201d the Microsoft target markets comprise both consumer and industrial users. To attain this goal, the company struck deals with PC makers/ assemblers to have MS-DOS pre-installed on every PC sold. Next, the company developed the Windows operating system in a blatant attempt to imitate the Apple graphic user interface, which was extremely intuitive and easy to use. Not till version 3.1, however, did Microsoft have something the market could grudgingly accept. Then Microsoft struck at all the software companies that tailored their end-user software to work on MS-DOS and Windows. Again by striking pre-installed deals and bundling everything possible into Microsoft Office, the company killed off, in succession, Netscape, Lotus 1-2-3, WordStar, dBase III, Eudora, Freelance Graphics, and even Norton Utilities. By the time regulatory authorities in the U.S. and the EU could file suit for monopolistic practices, the dust had settled and bloated Microsoft software owned the market.\n",
        "label": "human"
    },
    {
        "input": "The Effectiveness of the Computer Essay\n\nThe modern computer is the product of close to a century of sequential inventions. It is also the result of a corroboration that has spanned national and continental borders. And all through, this evolution has been driven by one need: to automate tasks. Mankind realized that rearranging inanimate matter could result in certain mechanical advantages. This is the essence of machines. Knowledge accumulated, and we came up with machines that could totally replace humans in some tasks. This discovery steadied up the evolution, since human limitations, at least on the physical level, were reduced. And now, we are using these machines to catalogue all the knowledge we have accumulated over the years. We are in the information age, and the computer has become a central determinant of our performance.\n\nThe effectiveness of the computer pivots on one major characteristic: speed. Every task that a computer does is first broken down into simple, logical steps. These steps are then converted into number values that so that the computer can understand. The manipulation of these numbers can be seen as a kind of arithmetic. A modern computer does thousands, even millions, of calculations, every second, in order to maintain a satisfactory level of output for the user. This is because it has to break down natural language into machine language. Text and numbers are easy for a computer to handle. Media like music and images require more sophisticated manipulation. To illustrate this fact, consider that in order for a computer to play an audio file lasting four minutes, it has to go through the same rigors needed to read a bible, from cover to cover, twice!\n\nSo fast are modern computers that they can now multitask. Multitasking is simply performing several tasks at the same time. An average computer can be accessing the internet, displaying a video, and processing some calculations at the same time. This dimension in computers has expanded the horizons, as far as their applicability is concerned. Computers, if well used, can drastically cut down on the amount of time required to accomplish menial tasks. Mainframes and mini-frame computers have taken this a step further, with one central computer being available for use by several people at the same time.\n\nWhile operating at incredible speeds, computers have the added advantage of being very accurate. Actually, if fed correct data, a computer can always be counted on to produce the correct responses, unless there is a problem with its programming. This advantage can be directed correlated to the fact that now people can concentrate more on the big picture, instead of wasting time rechecking their information and statistics for accuracy. Progress in all sorts of human endeavors is thus much faster.\n\nComputers have forever changed the field of communication. A mere couple of decades ago, trans-continental communication used to take days, sometimes even weeks, depending on the mode of communication. Nowadays, communication to any part of the world can be instantaneous, and requires only the click of a button. This is the significance of the internet. Through the internet, computers communicate with each other at dizzying speeds, spewing forth amazing loads of data, and saving billions of dollars in communication charges. This has even changed the way business is done today. With a minimum of resources, any business can nowadays operate on the international scale. Some businesses are already doing this, by outsourcing some of their interests across national borders. Billion-dollar decisions are now made in split-seconds, and effected with the click of a button. Little wonder, then, that some businesses are shooting up from obscurity into empires overnight.\n\nThe primary reason for inventing computers in the first place has been taken to a whole new level. Nowadays, computers have automated so many tasks previously done by humans that some jobs are nowadays the reserves of computers. Any repetitive task can be relegated to a computer. This is because modern computers can be instructed to perform new tasks, and will execute their instructions precisely, as long as they are equipped with the necessary accessories. With the evolution in computers, robots have been made to handle tasks that are either repetitive or dangerous for humans. This way, computers have increased the overall safety level of humans.\n\nThe first computers were hard to use. They needed specialized knowledge, since information had to be fed to the computer in special formats. However, with time, more convenient methods of communicating with computers have evolved. These methods approach natural language with each day. For example, computers can now recognize speech, and execute commands that have been fed to them verbally. Scientists are working towards optical recognition in computers, so that they can recognize faces, expressions and gestures. With such breakthroughs in technology, computers will soon be just as user friendly as the next person. A future when computers will respond to human emotions is foreseeable. At that stage, computers will be at their highest effectiveness, tackling problems with their amplified efficiency, while drawing little attention to themselves.\n\nOf course, computers are not without some drawbacks. One of the most significant drawbacks with them is that they have now divided societies into two major groups: the computer literate, and the computer illiterate. One group has a distinct advantage in this information age. The other has to make do with traditional methods of tackling problems, or learn computers. Anybody who can not utilize the advantages of the computer is soon left behind by the rest of the society.\n\nAnother disadvantage is that computers have replaced many people in the work place. It can be argued that these replacements are part of human advancements, but the rates at which computers are replacing people have left many people reeling from the unexpected. It now requires a higher level of innovativeness to stay relevant in the changing landscape of technology. Very soon, most job descriptions will be the preserve of computers, and humans will just have to invent other job positions.\n\nThe ease at which information can be transmitted through computers has some drawbacks too. For example, some unscrupulous individuals have used internet channels to defraud others. The internet offers anonymity, and thus immunity from legal recourses. Billions of dollars are lost every year from these online fraudsters, and some of them simply disappear. Efforts are being done to tighten online security but somehow, fraudsters seem capable of keeping up with technological advances. Information flow through the internet can also compromise the security of institutions. For example, through satellite mapping, it is now possible to gather sensitive information on such premises as army barracks. Some terrorists have been known to use this while planning their attacks.\n\nAnd finally, while computers are all good at storing information and processing it, the rapid increase and transfer of information is producing an information-overload scenario. This is where people are being confronted with so much information that they end up uncertain of what to make of it. Information overload is a real threat in these modern days, and can result in all sorts of eventualities, including mental breakdowns in people.\n\nAll in all, computers have both advantages and disadvantages. The advantages far outweigh the disadvantages. And with each advance in technology, the disadvantages associated with computers reduce. Their usefulness becomes increase. A future when mankind\u2019s activities will be inextricably interwoven with computers is not only foreseeable, it is inevitable.\n",
        "label": "human"
    },
    {
        "input": "TUI University: Computer Capacity Evaluation Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Planning\n 3. Congestion\n 4. Conclusion\n 5. Reference\n\nIntroduction\n\nComputer capacity evaluation involves the process by which an institution estimates the space, computer software, hardware, and resources for infrastructure connection that the institution will need in the future. The major concern is the capacity of a system to work effectively as the number of requests increases and the number of users increases between users increases with time. Many institutions have networked computers and these networks need to have the capacity to grow and meet future needs. TUI University is an institution that offers its bachelor, master\u2019s, and Ph.D. degree programs over the internet. Most of its courses are directed towards working professionals and therefore do not need them to reside in the university. This paper will seek to analyze how the application of computer capacity evaluation can help an educational institution like TUI University.\n\nPlanning\n\nTUI University offers its programs over the internet. This means that the number of individuals using the internet at any given time varies. The demand for education is increasing every day meaning that the institution will need systems with higher speeds to process the requests by the many users. A time will come when the information that requires processing needs some large memory and higher bandwidth. When the amount of information to be transmitted is quite high, there is a chance that the internet speed will become slow and a user will wait for a long time before his or her request is processed (Thacker, 2009, prg.9). Evaluation of the computer capacity and the system, in general, will help the institution plan for the system so that at any given time, the needs of a user will be met. Proper capacity planning will ensure a healthy network that can always grow and meet the needs of any number of users on the internet. This will enable the development of a system where user requests will be processed depending on their urgency. Evaluation will also ensure that the connection infrastructure resources, hardware, and software meet the needs of the user. This will help attract more students because their time is not wasted when they visit the school website or when they have some requests.\n\nCongestion\n\nInstitutions that provide their services over the internet are likely to experience congestion as many users try to access the same information or as different users have different requests that need to be processed. Evaluating the capacity of the computers in the institution will help increase the speed at which requests are processed. The institution will also be able to locate and measure the availability and capacity of the bandwidth whenever there is a bottleneck. This will help in controlling congestion and streamlining applications (Hui, Yongji & Qing, 2005, prg.2). Evaluation will also help in improving the quality of service to clients. Algorithms will be developed that can be used to measure the bottleneck properties and control how large amounts of data are processed. The most important thing to such an institution is for its network system to be effective and reliable. Reliability means that the system will always work and not fail even if a problem occurs. Educational institutes such as TUI University need to need to perform computer capacity evaluation to increase the speed at which customers\u2019 requests are processed both over the internet and in the computer memory.\n\nConclusion\n\nComputer capacity evaluation can help an institution plan on how to meet its future needs. An institution like TUI University that offers its programs over the internet can perform computer capacity evaluation to be able to meet the future needs of many students. Evaluation also helps an institution avoid congestion when there is a lot of work to be processed.\n\nReference\n\nHui, Z., Yongji, W., & Qing, W. (2005). Measuring internet Bottlenecks: Location, Capacity. Web.\n\nThacker, S. M. (2009). Capacity Management. Web.\n",
        "label": "human"
    },
    {
        "input": "Analogical Reasoning in Computer Ethics Essay\n\nAnalogical reasoning is a means of processing information that relates the familiarity between new and understood concepts and then uses those similarities to comprehend the new concept. After the process, either certain actions are accepted or the differences between the two scenarios are identified. It is a method of inductive reasoning since it seeks to shade more light on what is true, as opposed to deductively reasoning the fact of something. It is vital in advancing the frontiers of knowledge (Boelcke, 2009, para. 1).\n\nThe uses of analogical reasoning in computer ethics have many benefits. The reasoning method is in line with moral standards. If you believe that it is morally not acceptable to break the bank and steal money, you should equally consider it wrong to steal someone\u2019s personal computer. This aspect of reasoning enables the application of what is deemed to be morally wrong in society and relates it to computer technology.\n\nAnalogical reasoning enables one to come to terms with the gains or outcomes of each action done in computer technology. Suppose you arrive early one day to work to complete an urgent research work required for presentation by your boss by noon, unfortunately, you find the saved file corrupted and inaccessible. What would you do? The chances of you recovering the file before the deadline are slim. You may consider duplicating data from previously done works, but this may lead to unauthorized access to someone\u2019s intellectual property and possible infringement of the company\u2019s policy. Not accomplishing the work in time may also put you in trouble. In such an ethical dilemma, you will have to reason analogically in weighing the advantages and disadvantages of your next course of action.\n\nOn the other hand, involving in analogical reasoning has dangers involved. One of these is being too oriented toward the similarities between different situations without recognizing their significant differences. A superb example is a hacker, who in his or her thinking, seeks to demonstrate justice by finding errors in a computer\u2019s software or operating system. The social norm is that anyone seeking access to information without prior permission has been deemed a lawbreaker. In this regard, the innocent hacker is branded names as a thief or a con.\n\nAnalogical reasoning fails to draw the thin line between pleasure and business. For example, a company employee may wish to notify his or her boss on holiday about the progress of the company and does not mind demonstrating professionalism in sending the notification. The employee may fail to adhere to grammatical standards or correctly format the finished text.\n\nThe advent of the internet has made the world to become a global village. Over two hundred countries utilize this innovation. The computer age has brought the problem of global ethics. If a country wishes to protect its freedom of speech on the internet, whose rules are they going to use? The earth is witnessing discussions of ethics and values that are not limited to one geographical location. Countries may soon be forced to come up with universal laws to curb cybercrimes since they are not limited to any geographical location.\n\nIn summary, analogical reasoning lacks advantages and disadvantages when used in computer ethics. This is because it enables careful assessment of both sides of the coin before coming up with a concise route to take.\n\nReference\n\nBoelcke, A (2009). What is Analogical Reasoning. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Security System: Identity Theft Research Paper\n\nIntroduction\n\nIdentity theft can be defined as an offense or deception where somebody makes up to be what he or she is not in order to get definite profit or steal funds. It is also sometimes known as iJacking. The term is a misnomer as naturally it is impossible to steal someone\u2019s identity. (WebMediaBrands, 1) Thus, an identity thief uses a person\u2019s identity by obtaining his important personal information, like driving license number or bank account number, so as to impersonate him. The impersonator can use this information to obtain merchandise, credit or other stuff using the victim\u2019s name. He can also give fake identification to police authorities who may issue arrest warrants or issue criminal records in the name of the victim. The identity thieves for the most part look for the victim\u2019s name, date of birth, social security number, account numbers, address, online user IDs, personal recognition information and internet passwords. Thus, it is important to understand the nature of Identity thefts and prepare accordingly. (NYT, 1)\n\nIdentity Theft\n\nThe Identity Theft Resource Center along with other sources has divided identity theft into five classes. They are:\n\nFinancial identity theft\n\nIt takes place when a victim\u2019s identity is used to acquire credit, goods or services. It further has two versions:\n\n  * The impersonator can open new accounts in the victim\u2019s name or by using a fake identity. He can then use the victim\u2019s goodwill and credit history to get funds, like loans, or an account that he can overdraft.\n  * The imitator can also get finances, by pretending as an existing account possessor, from a genuine bank account belonging to the sufferer. For this, the perpetrator has to obtain a PIN code or card number of the victim and use that for accessing funds through an ATM or branch teller.\n\nIt is very difficult for the victims to discover financial identity theft. They can simply recognize when they are deprived of credits or there are grievances in their credit narration or if they are contacted by the creditors. Their credit reports may be greatly affected unless they are able to prove that they are not responsible for the frauds. (Smith, 33)\n\nWe can also categorize financial identity theft into two ways. They are:\n\n  * True name identity theft \u2013 It implies that the impersonator uses the victim\u2019s personal document and information for opening new bank accounts. The impersonator may establish a new cellular phone service or create a checking account to acquire blank checks or open a credit card bank account. (Harris, 268)\n  * Account takeover identity theft \u2013 It entails that the imitator uses the sufferer\u2019s private credentials and details for accessing the victim\u2019s accessible accounts. Even before the victim realizes the problem, the impersonator changes the victim\u2019s address and increases billing amount. (Milhorn, 162)\n\nIdentity concealment and cloning\n\nA circumstance of distinctiveness disguise and cloning arises when a mimic gets hold of private identifiers of the sufferer and masquerades as the sufferer in order to conceal from establishment. The imitator mainly does this to keep away from getting under arrest for an offense, be able to illegitimately work in an overseas nation or to be able to conceal from creditors and other populace. Unlike financial identity theft, identity concealment may continue to take place for an indefinite period without being discovered. To create an extremely concealed and convincing impersonation the impersonator may even attempt to obtain fake IDs and documents that are coherent with the actual cloned identity.\n\nCriminal identity theft\n\nIt takes place when an impersonator presents himself as a different individual, i.e. as the victim, in front of authorities. To do this the impersonator has to obtain national IDs or personal documents belonging to the victim or simple fake them. When the police take into custody the imitator, they present their counterfeit IDs and illicit charges are placed under the victim\u2019s name. The impersonator is then released, but when he fails to attend his court hearing, the warrant that is issued is in the victim\u2019s name. The victims discover the fraud when they are arrested or a background check is done on them. The process of clearing the victim\u2019s record is a difficult one and sometimes the authorities may forever list the victim\u2019s name as the criminal\u2019s alias. (Miller, 5)\n\nSynthetic identity theft\n\nIt has recently become very common and takes place when the identity acquired by the impersonator is partially or completely made up. The most common process of doing this is by combining an actual social security number of one victim with the name and date of birth of another victim. It is probably the most difficult to track of all the identity thefts as the credit score of neither of the victims will show the fraud directly but will actually appear as a completely new entry or as a sub-file of the credit report of the victims in the credit bureau. This mainly harms the creditors who unknowingly grant credit to the impersonators. The name of the victims may be confused with the false identities and harmful information in their credit report may affect their credit.\n\nMedical identity theft\n\nIt takes place when the impressionist uses the victim\u2019s private details, health cover credentials, to get hold of drugs, diverse medicinal treatments or even acquire compensations from insurers through counterfeit claims. Thus, it is possible that our health care documents or medical history may contain other people\u2019s information. This can even be life threatening for us. For example we may be given wrong blood type during a surgery or the records may even show a surgery we may have never had or leave out those which we have had. In the future, our doctors may perform accurate diagnoses relying on the false records that may delay our proper treatment and we may even loose our life. (Muir and Criddle, 286)\n\nDiscussion\n\nIdentity theft is not at all a new crime. It has merely mutated itself by including new technologies like ATMs and online banking. Nowadays it is even easier for the identity thieves to use stolen information due to the advent of the Internet since transactions can be made online eliminating personal interactions. Due to the computerization of the banking and other financial dealings and credit cards it has become much easier for the pretenders to pilfer other\u2019s personal details and thus can camouflage as the victim. Credit cards are often used for verifying people\u2019s identities and thus, an impersonator can pretend to be someone else by using their credit card. This also enables them to steal money. Thus, all the impersonator really has to do is obtain a succession of appropriate numbers for completing the crime. The victim of identity theft can suffer serious consequences if held responsible in place of the imposter. The various activities that are undertaken by the impersonators may even cause the victims to loose their jobs. Medical identity thefts can even cause us to loose our lives or the lives of our near and dear ones. Many nations have precise and explicit laws in opposition to using other\u2019s personal uniqueness and details for one\u2019s private gains.\n\nIn order to avoid identity theft we regularly verify our credit scores with the credit bureaus, destroy any unwanted credit applications, confirm with our creditors if our bills are not on time and protect ourselves by not broadcasting our personal information in unknown e-mails. Identity theft can also be used for smooth progress of offenses like counter nation surveillance, unlawful migration, and blackmail and terror campaigns.\n\nRecommendations\n\nActions that can be taken by individual users as well as organizations to restrict spam emails are as such installing software like Norton Spam Blocker, Best Spam Filter, Anti spam Mail, Spam Free, Lotus Anti Spam, Virus Spam, Free Spyware, SMTP Filter, Mail Software, Spam Blacklist, etc. (Shareware Connection, 1) These softwares do away with spam and fraud automatically. These soft wares are easy to use with no configuration. Cloudmark Desktop does not transform the way the user uses his or her email, no arrangement essential for its installation, just after installing immediately the user is protected from email threats. (Shareware Connection, 1)\n\nIn many anti-spam systems provide a safe senders list, which indicates that mail from those on the list is good and spam free. But any safe senders list is personal to each user, it is not possible to have a global safe senders list in view of the fact that one\u2019s friend may be another\u2019s enemy as far as the type of email one sends and another receives. In a spam control system which has a safe senders list and where subscribers can add email addresses to, but more significantly delete emails from for senders who are no longer active. Safe senders list should be examined at least monthly and redundant emails removed lest one have several known spammers on your safe senders list. Various online groups have been created where users can join and counteract against spam emails. Spam attacks may be prevented up to some extent by the use of these measures. In this present world of prevailing dynamism computer system operate as the supreme authority. With the extensive use of computers in every aspect of life computer security has become very essential. Thus for securing privacy of the user\u2019s computer security should be given more priority.\n\nConclusion\n\nHowever, assuming someone\u2019s identity with his or her approval and knowledge, like giving an exam in someone\u2019s place or cheating cannot be considered as identity theft. It has a very broad concept and mainly deals with credit card hoaxes. There are many techniques used by the imposters to get hold of personal identification, information and documents of the sufferers and can range from stealing their mail, going through their junk for paper documents, stealing it from their computers and files to infiltrating organizations which stock up huge numbers of private information. Over the years a large number of innocent people have been arrested due to people who have committed crimes exploiting their names. It is among the fastest developing crimes in the USA and the victims have to spend huge amounts of money and time in order to resolve their problems due to identity theft.\n\nWorks Cited\n\nHarris, Shon. CISSP All-in-One Exam Guide. Ed 4. NY: McGraw-Hill Professional, 2007.\n\nMilhorn, Thomas. Cybercrime: How to Avoid Becoming a Victim. NY: Universal-Publishers, 2007.\n\nMiller, Michael. Is It Safe?: Protecting Your Computer, Your Business, and Yourself Online. London: Que Publishing, 2008.\n\nMuir, Nancy and Linda Criddle. Using the Internet Safely For Seniors For Dummies. London: John Wiley and Sons, 2009.\n\nNYT. \u201cIdentity Theft\u201d. The New York Times Company. Monday, 2009. Web.\n\nShareware Connection. \u2018Software Downloads for \u201cSpam Servers\u201d\u2018. Shareware Connection. 2009. Web.\n\nSmith, Robert Ellis. Compilation of State and Federal Privacy Laws: 2002. Ed 9. LA: Privacy Journal.\n\nWebMediaBrands. \u201ciJack\u201d. Webopedia. 2009. WebMediaBrands Inc. Web.\n",
        "label": "human"
    },
    {
        "input": "Computers Have Changed Education for the Better Essay\n\nIntroduction\n\nAlthough computers were invented less than a century ago, they have revolutionized how we carry out our day-to-day activities. Hardly any aspect of our modern lives has escaped the influence of these systems. The education field is one such arena that has experienced the positive influence of computers. Considering the significant effects that computers have had in the educational field, this paper shall set out to illustrate how computer systems have changed education for the better. This shall be accomplished by highlighting different avenues in the educational field where computers have made a positive impact.\n\nBenefits accrued from computerized learning\n\nComputers have greatly increased the research abilities of the student. With the help of computer systems, students are no longer confined to physical libraries as their only source of educational information. They can now tap into the vast resources that computers present through the internet. The internet\u2019s connectivity has also led to more collaboration among students in their quest for knowledge thus further enhancing their educational experience. For example, students can join chat rooms and hold productive discussions about school work. Websites billboards can also act as forums through which students can exchange ideas and help one another therefore greatly increasing their performance in class.\n\nIt has always been acknowledged that students vary in levels of intellect and reception. In most cases, teachers cannot set a constant pace while teaching because the levels of understanding per student differ. For example, a mentally disabled student may be slower in grasping some concepts than their normal counterparts in the same class. Computers have helped solve this predicament as they act as private tutors. Students can set their own pace and the level of challenge that they are best suited in. this helps in building their self-confidence and esteem making the learning process fun rather than frustrating.\n\nIn the early years, one had to physically attend school facilities for educational purposes. This locked out multitudes of people who had the will to obtain an education but lacked the time to make it happen. Computers have changed this by enabling learning to be more dynamic by the use of online classes. Students can take classes from wherever they may be and at any time by using virtual classrooms. Computers have therefore increased the number of people currently enrolled in higher learning institutes thus making education more universal than it was before the inception of computers.\n\nComputers also command a profound attraction from the youth. Recent statistics show that many students go through their primary education without acquiring any essential knowledge and skills and they subsequently never finish the programs. In as much as there are many reasons as to why students drop out of school, lack of interest is the most prominent, and computerized education has proven to tackle this issue. Computers keep the students engaged and focused more than any other form of education and this is probably why students using a computer-based learning system learn better and faster than those in regular classes.\n\nConclusion\n\nWith the above discussions in mind, it can be irrefutably argued that computers have changed the education field for the better. Computers present a novel way of enhancing education both for the good of the student and the educational institutes. Not only do they ensure efficiency in learning, but they also facilitate equality and fairness in the educational realm. Computer-based programs should therefore be implemented in all schools as their benefits towards the intellectual development of the students is seen to be immersed.\n",
        "label": "human"
    },
    {
        "input": "Quasar Computers Company\u2019s Economic Strategies Coursework\n\nTable of Contents\n 1. Introduction\n 2. Main Body\n 3. Conclusion\n 4. Reference\n\nIntroduction\n\nIn order to earn economic profit on a sustainable basis the firm must consider the market structure and then choose from the pool of strategies available. The following provides the strategies to be opted by Quasar computers at different stages.\n\nMain Body\n\nPhase 1: Neutron being a technological innovation enjoys market monopoly. Therefore, the market structure in which Quasar Computers operates is monopolistic in structure. In order to maximize profit in monopolistic market, the economic solution is to equate marginal revenue to marginal cost, which will provide the profit maximizing price and quantity derived from the demand curve. In case of Quasar Computers, for Neutrons, the profit-maximizing price is $2550 and quantity demand at that price is 5 units.\n\nPhase 2: The profit-maximizing price is good at a situation where there is effect of advertisements and consumer choice on product buying decision and that are no close substitutes. However, optical computers have close substitutes in traditional microchip based computers. Therefore, for a new product advertising budget and price according to it must be considered. Therefore, we have two aims:\n\n 1. advertising budget,\n 2. pricing strategy to maximize profit.\n\nIf we keep, last year\u2019s advertising budget i.e. 400, the price of the computer must come down to $ 2350 wherein the company earns a profit maximizing profit of $1.34 billion. Reducing the cost of advertising is not an option as in both the cases as suggested by Jane, we earn either negative profit (in case of advertising cost $100 billion) or lower profit than what we may earn by retaining last year\u2019s advertising cost. Therefore, I decided to take Robert\u2019s suggestion. The reason for this decision is \u2013 (a) we have to increase awareness and acceptance of the new technology to increase market, and (b) earn higher profit.\n\nI decide to increase the advertising expenditure from $400 billion to $ 600 billion. At $500 billion advertising cost, the company sells the product at a price of $2350 and earns a profit of $1.91 billion. However if the cost of advertising is the price of the product is $2450 and earns profit of $2.74 billion. Moreover we sell maximum number of units at this pricing (i.e. 16.6). Therefore I decide that the advertising cost must be $600 billion and price of the product $2450.\n\nPhase 3: In this phase, the target is to streamline manufacturing facilities to minimize costs. Aim is to select best production proposal for Quasar and accordingly set the pricing for Neutron. When we do no change in production process, the price of Neutron is $2300 and 8.8 million units are sold. The profit earned is $0.68 billion.\n\nFirst, we will check the proposal of Dave who suggests upgrading production process. This will lower the average cost and marginal cost of production. Setting the price at profit maximizing level we see that the price of Neutron becomes $2200 and units sold 9.4 million. The cost of production is $18.53 million and profit is set at $2.2 million.\n\nIf we consider the suggestion of Jane wherein she suggests to specific improvements, we see that the cost of production becomes $18.77 billion, which is higher than that required to upgrade production systems. Further, the price of Neutron remains at $2300 with fewer units sold (8.8 million) and profit falls to $1.52 billion. Therefore, the best decision to minimize cost and maximize profit is to opt for Dave\u2019s suggestion to upgrade production process.\n\nPhase 4: New entrants in market increasing competition. Orion Technologies is the new entrant. Market structure becomes oligopolistic in nature. Aim: Orion\u2019s price for next month and Neutron\u2019s counter price.\n\nThe pricing strategy if Neutron will be at $1750 wherein the company\u2019s profit will be $56 billion and a market share of 50%. My strategy is to set the price at $1750, which is the predicted price for Orion in order to stabilize the price in the market in order to prevent a price war or slump in the market. This has helped me retain 50% market share and a profit of $56 billion.\n\nPhase 5: Market structure is Monopolistic Competition. The aim is to decide if Neutron brand must be built or launch a new product Ceres. I decide to launch a new brand Ceres. This is because it will maximize our combined profit to $1305 million with no unused capacity. This will also help Quasar differentiate its products and sell computers in an already competitive market.\n\nPhase 6: The aim is to allocate budget for continuous improvement. The aim will be to reduce cost of production. First, I decide of investing $40 million on process improvement. The semiannual report shows that per unit profit earned is $0.20. Next, I decide to decrease the profit improvement cost to $30 million, which helped in increasing profit per unit and reducing cost and savings.\n\nThe pricing strategies that should be considered are as follows:\n\n 1. Profit maximization is the strategy to opt in case of monopoly where in price is set at marginal revenue equals to marginal cost.\n 2. Combined profit optimization, in case of oligopoly in order to maintain stable price in market.\n 3. Pricing strategy should aim at removing excess capacity in monopolistic competition.\n 4. Cost reduction as prices are set by market in perfect competition through the forces of demand and supply interaction in the market.\n\nConclusion\n\nSome non-pricing strategies that I recommend are \u2013 increasing cost of brand promotion and advertising for new launch of a model, continuous improvement in order to reduce cost, and increased R&D expenditure to help product differentiation.\n\nInnovations that will help the company differentiate its Neutron from its competitors will help the company sustain its competitive position.\n\nReference\n\nMcConnell, C. R. (2009). Economics: Principles, Problems, Policies, 18e. McGraw-Hill. Web.\n\nSimulation. (2009). Market Structures . Web.\n\nUOP. (2009). UOP-custom course for Basic Business Statistics. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Security: Intrusion Detection System Policy Essay\n\nTable of Contents\n 1. Introduction\n 2. Discussion\n 3. Conclusion\n 4. Reference\n\nIntroduction\n\nAn Intrusion detection system referred to as IDS may be software and/or hardware installed to identify unauthorized efforts of gaining access, controlling, and/or immobilizing computer systems, primarily by means of networked services, like the Internet. These efforts may manifest in the shape of network attacks, malware, and/or discontented workers. The IDS is mainly used to identify various forms of malicious activities that can jeopardize the security of a networked computer system. This involves the detection of attacks against susceptible services; data-based application anomalies, host-driven attacks such as privileges upgrades, illegitimate access to confidential data, and malware such as Viruses, Trojans, and Worms. (Dollard, 2006)\n\nWith extensive diffusion into the industry and integration of IDS systems, it is plainly evident that IDS constitute an integral component of the organizational infrastructure. The requirement for such systems is accentuated by a simple premise in the field of network security: defense-in-depth. It is a layered mechanism of defending the organizational information system and communications framework against malicious assaults and illegitimate access to confidential data and information. This technique entails compound, overlapping structures that facilitate organizations to prevent, identify and counter suspected interferences with network-based services.\n\nIn this document, the IDS policy relevant to the organizational requirements of Gem Infosys is formulated following a logical evaluation of the scenario .\n\nDiscussion\n\nAppreciating the necessity for IDS security, and consequently formulating an IDS policy that is aligned with the organizational requirements are significant steps in the pursuit of developing an effective overall information security framework. Nevertheless, these measures constitute just the preliminary phases of a comprehensive IDS implementation procedure. After procuring an adequate IDS structure, an organization should suitably and resourcefully deploy it across all organizational levels. (Fletcher, 2009) For the effective deployment of suitable IDS in Gem Infosys the following components are paid particular attention in the IDS policy:\n\n  * Incident response guidelines\n  * Staffing\n  * Configuration\n  * Training\n  * Updating signatures\n\nIncident response guidelines: The IDS facilitate the detection of security incidents and, identification of intruders. The organization may choose to file lawsuits, seek consultation, counter the intrusion attempt, disregard the intrusion or embark on different measures, based on the severity of the security breach. The incident response guidelines would assist the management to formulate an effective company response in such cases.\n\nStaffing: The IDS is expected to produce relevant information about the network used within the organization. This requires consequent assessment of the produced data. The services of a qualified network analyst would be hired to and shall be assigned to IDS management, log examination, and analysis.\n\nConfiguration: The IDS should be suitably configured to generate pertinent data only. Striking an ideal sense of balance between excessive data generation and insufficient data generation is important for effective deployment. Refined and efficient IDS configuration is required thus a comprehensive configuration process involving designing, tuning, and trial would be carried out.\n\nTraining: So as to productively exploit the IDS, the workforce should have access to necessary training. Personnel with job requisites associated with configuration, incident response, and data analysis would be provided with the latest IDS learning tools.\n\nUpdate signatures: To facilitate security maximization, the attack records that the IDS is configured to detect must be updated frequently. Intruders continuously modify attack techniques. Thus, to optimize safety the IDS signature files will be modified and updated regularly. (Fletcher, 2009)\n\nConclusion\n\nIn general, much importance is given to security and deterrence using measures like routers, firewalls, antivirus, and public key infrastructures. However, decisive identification and response activities like those facilitated by IDSs are frequently disregarded. Such systems play the role of monitoring devices within networks and facilitate attack avoidance, intrusion identification, damage evaluation, and prosecution evidence. They constitute a fundamental layer of a defense-in-depth construct and play a central role in the development of a complete information security framework.\n\nReference\n\nDollard, J. (2006). Secured Aggression . New Haven and London: Yale University Press.\n\nFletcher, R. (2009). Software Security: Beliefs and Knowledge. Auckland: Howard & Price.\n",
        "label": "human"
    },
    {
        "input": "Dell Computer Corporation: Management Control System Report (Assessment)\n\nDell\u2019s strategy involved making use of the simple rules of functional business. The first of these was to make the customer\u2019s needs not only get met, but satisfied beyond other businesses capabilities. According to Birger (1995), the customer will always choose the goods that satisfy his needs more. They achieved this by making the products especially personalized. They also were able to cut the cost of purchase by eliminating middle men in their chain of supply. By so doing they established direct contact with the customers, from manufacture straight to the customer. This may also explain why they were able to sense the needs of the various customers and thereby respond to market changes faster than any of its competitors. Thus by understanding the customer, the company was in a better position to react to their needs (Foxall, 2005). They were also able to create a personalized experience for each customer by tailoring the product to specifications of the customers. This was easier for customers who were ordering in large quantities. This was all done in direct communication with the company, no middle-men.\n\nThe other strategy involved the product handling. By making high quality products tailored to the current needs of the customer, Dell was able to build market dominance over its rivals. They also outsourced the making of the products and their plants were reduced to assembly points. Outsourcing helps reduce costs, thus making products cheaper and profit margins higher. As a result, the company was able to eliminate the need for manufacture assets reducing its function to assembly and distribution. They also reduced the number of times staff directly handled the products, reducing the chances of quality compromise.\n\nDell also worked on its staff and management policies. The involvement of all stakeholders by displaying and honestly interpreting statistics for the company energized the staff. They also engaged a consultancy company to come up with useful parameters and statistics of judging performance. The result was that the company became more target oriented. This data was given in real time allowing for rapid adjustment of procedure so that they could efficiently operate. With information concerning their financial performance and especially customer needs coming, they were much better placed. Schiffman (1993) advocated for the knowledge of the customer so as to provide target specific goods, a policy that Dell followed well. This made the company united and goal oriented in step with the real situation on the ground.\n\nSo how did their strategies affect the company\u2019s statistics? The result of competitive customer relations resulted in increased sales and rate of stock turnover. These relations include the customization, relevance of product and pricing. The fact that there were no middle men also contributes to reduced cost of sales. This happens because the middle men would add their mark up and pass on this cost to the next level of distribution, causing the end product to be expensive. The end result is the customer would find the products competitively affordable and therefore buy, increasing stock turnover and finally a higher return on equity.\n\nDell\u2019s innovation and direct customer handling produced an increased stock turnover. This was also aided by the fact that they responded quickly and efficiently to technology changes thus their products did not have a long shelf life. This is a great saving on the cost of storage and handling, a cost that other company\u2019s incurred. Therefore, by keeping stock constantly flying off directly from its assembly, the company cut off the need for developing elaborate storage assets and warehouses leaving cash open for other purposes.\n\nThe other strategy employed was the outsourcing and reduced handling by employees. This would cause a decrease in the cost of production and increase the return on investment. This is because comparatively little assets and finances were committed to turning over a large amount of stock. There was also reduced need for assets such as plants and equipment such that capital could be committed elsewhere. As a result, the company engaged only in assembly. This would be advantageous to Dell helping it to be able to use relatively little capital to give a larger output thus boosting its return on capital statistics.\n\nThe motivation and cohesion of workers and stakeholders towards a common goal of increasing efficiency helped improve the competitive nature of the company. This is important as it helps reduce the management and handling costs of employees. Also, the employees contribute a lot more than if they are not motivated. The statistical impact is that there is good return per employee and a lower administrative cost outlay. This would enable the company to be competitive as the employees feel the responsibility to make the company better.\n\nThe cost of hiring a consultant was a worthy investment. It was a strategy of obtaining expertise and a critical outside opinion for the benefit of the company. This investment paid off as it was through this that the company was able to gain the use of statistics in their daily operation. The statistics, given in real time, helped make relevant decisions that would allow the company to react to changing market situations as they occur. The result is that they would be able to remain competitive, taking advantage of unique situations way before their competition and thus gain more sales.\n\nReferences\n\nBirger, W., (1995), Using Market Data to Infer Utilities, Consumer Research Britain. Dell Computer Corporation article, n.d.\n\nFoxall, G. (2005.) Understanding Consumer Choice. Baingstoke. Palgrave Macmillian.\n\nSchiffman, L.G. (1993), Consumer Behavior, Prentice Hall International, London.\n",
        "label": "human"
    },
    {
        "input": "Computer Mediated Communication Enhance or Inhibit Essay\n\nTable of Contents\n 1. Introduction\n 2. Definition of Computer Mediated Communication\n 3. Definition of Interaction\n 4. Factors that might enhance or inhibit Interaction\n 5. Conclusion\n 6. Bibliography\n\nIntroduction\n\nCommunication can be defined as the act of transmitting information (Merriam-Webster Online Dictionary). It is a process whereby people appoint and transmit meaning in order to establish a common understanding. In order to effectively communicate a message, intrapersonal and interpersonal skills are needed. Skills like listening, speaking, questioning are needed in order to effectively communicate. Furthermore, skills such as processing, observing and analyzing are also needed in order to create a more effective communication.\n\nThe rapid change that is happening in the world today, affects how people communicate with each other, both in business and interpersonal level of relationships. The advancement of technology made communication and interaction with other people more accessible.\n\nThe use of computer-mediated communication both enhanced and inhibited interaction between people. Computer mediated communication has been applied in education. Researches suggest that this method of learning can enhance the learning of students. Researches also conclude that through computer mediated communication, satisfaction and motivation increases. Computer mediated communication can also reduce the feelings of isolation for students. On the other hand, some aspect of face to face communication can be diminished through computer mediate communication. Being comfortable and trust to the medium being used is also an issue in computer mediated communication.\n\nThis paper discusses up to what extend computer mediated communication enhances or inhibits interaction. The first section discusses the definition of computer mediated communication. The second section discusses the definition of interaction. The third section presents the factors that might enhance or inhibit interaction.\n\nDefinition of Computer Mediated Communication\n\nComputer Mediate Communication or CMC refers to any communication transaction that happens by means of two or more computers that are linked with each other via a network (McQuail, 2005). Originally, the term only referred to communicative transactions between computers or formats that are attributed to computers. Such computer-mediated formats are electronic mails, instant messages and chat rooms.\n\nElectronic mails or simply e-mails or email is referred to as any way of composing, sending-receiving, and saving generally text-based communication through the use of digital communication systems. At the beginning, different systems of electronic mails that are designed by different companies are most of the time not compatible or can not be operated with each other. When the Internet was established in the early 1980s, it paved the way for the effort to standardize the structure of the Internet in order to implement a singe standard in electronic mails which is called the Simple Mail Transfer Protocol or SMTP. The protocol is originally included in RFC 821 when it was first introduced in 1982 as Internet Standard 10.\n\nThe modern electronic mails system is now on a store-and-forward model. In this model, the computer server of the electronic mail system is the one that receives, sends and stores the messages instead of the users themselves. The user of the electronic mail only needs to connect to the electronic mail infrastructure through their personal computer or other devices that enables network connection for the period of the transmission of the message or retrieval of the message from its specific server. It is very infrequent that an electronic mail is directly transmitted from one person\u2019s device to another person\u2019s device.\n\nWhen the electronic mail was first introduced, it can only support text messages under the ASCII character set, but now any type of media format is supported by the electronic mail system. These various types of media format can be sent and can also include attachments of images, audio and video files.\n\nInstant messaging is considered as a form of real-time communication. It considered as such because the sending and receiving of information by the other party of the communication happens at almost the same time. Instant messaging involves the communication of two or more individual on a typed text format. The text-based message is transmitted through the use of devices that are linked on a network like the Internet or an intranet.\n\nThe difference between instant messaging and electronic mail is the synchronicity aspect. Instant messaging happens in real-time. In the contrary, some instant messaging systems permits the user to send messages to other users who are not logged on, in this context, it erases the primary dissimilarity among instant messaging and electronic mail.\n\nInstant messaging systems can also allow the user to save a previous conversation for reference in the future. When a message log is installed, the instant messages will be logged in a message history file in the computer. In this way, it removes the advantage of persistency of electronic mails over instant messaging.\n\nChatrooms are generally described as a form of synchronous conferencing. In reality, chatroom nowadays can be both synchronous and asynchronous conferencing. Thus, chatroom can refer to both real-time online chat and instant messaging to online forums that acts like a digital and graphical social environment.\n\nThrough online chat, people can communicate via text-based messages to other users in the same chatroom in synchronously or in real-time. Historically, the first and oldest chat rooms can only support the transmission of text-based messages. Talkomatic, which was introduced by PLATO System in the year 1974 is being claimed as the first ever text-only chatroom. Another significant text-based chatroom was the Freelancing\u2019 Round Table which gained popularity during the 1980s. Another popular text-based chat system is the Internet Relay Chat or IRC. Now, other chatrooms like the one provided by Yahoo! allows the use of text and voice chat at the same time. The gaining of recognition of the chatrooms paved the way for the development of instant messaging. Other noteworthy chat rooms are the one provided by AOL and other web chat sites.\n\nOther impressive feature of chat rooms is that it uses graphical user interface or GUI. In this way, the user is permitted to choose from different icons that identify him or her and he or she can also change the appearance of the chat environment.\n\nAt present, interactions that are based on text are also considered as computer-mediated communication. An example of which, is the use of text messaging (Thurlow, et al., 2004).\n\nStudies regarding the use of computer mediated communication usually centers on the social effects of those communication channels that are supported by computer technology. A lot of new researches focus mainly on the so-called social networking in the internet which is supported by social softwares.\n\nSocial networking or social network service has gained popularity overtime. It centers on creating online communities for people who have common interest and activities. Almost all of the social network services are based on the web. Social networking services provide the users different methods of interacting and communication with each other such as electronic mail and instant messaging.\n\nFresh and new ways of communicating and sharing information has been introduced by social networking systems. Social networking websites are currently being used by millions of people around the globe. It is now almost evident that social networking will be occupying a great part of everybody\u2019s everyday life.\n\nSocial networks became the most talked about channel for media for many years now. The major strength of these social networks is its capability to get together millions of different users. The major drawback of the social networks is its lack of ability to convert the quantity of the signed-up clients into monetary value.\n\nIt is true that electronic mails and websites also have the main features of social network services. The thing that separates social network services from the two is the idea of ownership of the service. Social network services allow the user to create their own \u201chome page\u201d.\n\nThe most popular social network service providers are MySpace, Friendster, Facebook, Twitter, LinkedIn, Tagged, Hi5, Nexopia, Bebo, dol2day, Skyrock, Orkut, Xiaonei and Cyworld.\n\nAttempts to standardize the social network services in order to eliminate the redundancy or duplication of entries have been unsuccessful because it called for some issues regarding the privacy of the users.\n\nComputer mediated communication covers a variation of fields. Scholars from different fields make studies about phenomena that happen in computer mediated communication. Internet studies.\n\nAn example of study regarding phenomena associated with computer mediated communication is the one done by J.B. Walther. In his study, he looked at how humans utilize computers or any other digital media in handling interpersonal interaction. Also covered in his study is the way human make us of computers or the digital media in dealing with forming and maintaining impressions and relationships (Walther, J. B. 1992 & 1996). These researches have most of the time covered then dissimilarities among offline and online interactions. At present, studies regarding computer mediated communication is on its way in creating a perception that computer mediated transactions must be considered as a very important part of a person\u2019s everyday life (Haythorntwaite and Wellman, 2002). Other studies regarding computer mediate transactions focuses on the utilization of paralinguistic aspect. The paralinguistic aspect of computer mediated communication refers to the use of emoticons.\n\nEmoticons started off from text representations. Emoticons are basically textual portrayal of the mood or facial expression of the writer of the message. They are mostly utilized in order to give an idea to the responder the temper or the tenor of the message. It has the ability to change or enhance the interpretation of the text message. The word emoticon is a combination of two English word, emotion or emote and icon. In using web forums and instant messages, the emoticons in text format can be automatically changed to a specific image that corresponds to the emotion being portrayed by the text emoticon. These images are also referred to as emoticons. The graphical emoticons can be simple or complex. The most popular of all emoticons are the smiley face \u263a and the frowny face \u2639.\n\nWay back in the 1800s, the use of emoticons can already be traced especially for casual and humorous literature. The emoticons in the internet can be attributed to the proposal of a guy named Scott Fahlman in a message he sent on the nineteenth of September, 1982.\n\nThe sequential analysis and the different sociolects or terminologies used in different computer mediated environment are also receiving much attention from researchers (Herring, 1996 and Markman, 2006).\n\nSociolects are defined as the different languages that are used or associated only to a specific group of people or society. Different people categorized on different characteristics like socio-economic status, age, gender and occupation might have their distinct type of a language.\n\nPragmatic rules in the computer mediated communication environment such as turn-talking (Garcia and Jacobs, 1999), and the different registers and styles are also being studied by scholars. Computer mediated discourse analysis refers to the study of the languages used in the computer mediated communication environment in text-based forms (Herring, 2004)\n\nStyles are the various language associated with the different topics or aspect of society. There is the so called language of politics, religion, business, advertising, and many more. These languages belong to a particular situation or period of time.\n\nRegisters are the various languages that are categorized according to the specific purpose in which that language is used more particularly in its social setting.\n\nOther researched on computer mediated communication is concerned with the way people interact in social, professional as well as educational set up. In these kinds of settings, the way of communicating between the people involved differs not only because of the environment itself but at the same time the method of communication the participants use. The use of information communication technology affects the communication and interaction among people. Computer supported collaboration is the term used to refer to researches regarding communication in order to gain collaboration or the so called common work products.\n\nThe most famous forms of computer mediated communication are the electronic mail, chat through video, audio and text conferencing and instant messaging. Online bulletin boards, MMOs and list-servs are also popular forms of computer mediated communication. These platforms for communication are quickly changing as new inventions and technologies are developed. Weblogs or simply blogs already gained popularity and the barter of RSS data made it even easier for blog users to publish their own blogs.\n\nComputer mediated communication can be categorized into synchronous and asynchronous modes. In the synchronous mode of communication, the users or parties in the conversation are online at same moment. On the other hand, asynchronous mode of communication has no restriction in time. The parties in the conversation can respond to the messages at a time different from the time the message was sent.\n\nDefinition of Interaction\n\nInteraction refers to the type of action that happens between two or more objects when they affect each other. The concept of having a two-way effect is very important in interaction as compared to the one-way causal effect. The term interconnectivity is strongly associated with interaction. Interconnectivity refers to the interactions between the interactions within systems.\n\nThe concept of interconnectivity is widely used in various fields like non-linear dynamics, biology, network theory, cybernetics and ecology. Interconnectivity simply says that all the participants of a system closely interact with each other. They rely with each other just because they are within the same system. It should be recognized that it would be very difficult or even impossible to examine or study a system through its specific parts considered as independent to each other..The combination or the merger of several simple interactions might lead to emergent phenomena.\n\nDifferent kinds of sciences define interaction in different ways.\n\nThe media art uses the term interactivity as term to refer the feature of the media that is characterized as the level of accessibility of the media to the masses.\n\nIn physics, the primary interaction or fundamental force is a process where the elementary particles interact with other elementary particles. Basically, nature has four primary interactions. These interactions are electromagnetic, gravitational, strong and weak interactions.\n\nIn the field of sociology, social interaction is the continuous change in the social actions among individuals and groups. These individual and groups changes their actions and reactions because of the action and reactions of the other parties. The foundation of social relations is social interactions.\n\nIn the field of medicine, interactions between the effects of the combinations of different medicines are monitored. This is usually done by pharmacists. In molecular biology, the study of the gene\u2019s interaction between each other and their metabolites is called molecular pathways.\n\nIn the field of communications, interactive communications happens as the sources or parties of the communication process alternately transmit messages between one another. It should be noted that interactive communication is different from transactive communication. In transactive communication, the sources or parties in the communication process sends messages simultaneously. Transactive communication includes newer modes of communication. These modes mentioned may include teletext, teleshopping, video, video on demand, internet, computers, tele-conferencing and many more. Tele-communication can also be considered as transactive communication. On the other hand, the use of cell phones, mobile phones, pagers, electronic mails, and instant messaging are considered as interactive communication. These can also be classified further into three: Interpersonal, group and mass. Interpersonal communication includes the use of telephone and other services provided by telephone companies. Group communication will have to include the use of tele-conferences and video-conferences. Lastly, mass communication involves the use of internet and the World Wide Web.\n\nIn the parlance of distance education, interaction refers to the exchange of information, ideas, opinions between and among the learners and teachers, frequently, through the use of technology that is aimed to help improve and facilitate the learning process (Global Distance Education Net\u2019 glossary)\n\nFactors that might enhance or inhibit Interaction\n\nInteraction between individual can be affected by a lot of factors. Some factors would tend to enhance the interaction while some would inhibit effective interaction. In order to identify what factors affect interaction and the capability of the people to communicate, the process of communication and interaction must first be understood. The sender who already has an idea in mind, which is usually a creation of research or his thought. The sender must put his or her idea into words or actions, as the case may be, and then he or she must transmit or send the message to the recipient of the message. When the recipient receives the message sent by the sender, the recipient should understand the message and draw conclusion from the message. After, understanding and drawing meaning from the message, that is the time the recipient should give feedback to the original sender. This simple process of sending, receiving and providing feedback can create problems when factors that inhibit interaction are present. At the same time, improving these factors would lead to an enhancement of the interaction process.\n\nA factor that affects interaction is the competence of the parties. Competence refers to the ability of the person to communicate. The person must communicate to others in a way that it is approved by others and it effectively achieves its goals. If the person doesn\u2019t have much competence, the person would experience difficulty in putting his or her ideas into a form that other people would understand its meaning. People who can easily transform his or her ideas into a form that can be easily understood and accepted can enhance the interaction between the parties.\n\nAnother factor that affects interaction is the language barriers between the parties. An effective communication requires understanding between the parties involved. If the parties doesn\u2019t share a common language, it would be very difficult to effectively communicate, thus interaction between them is affected. Problems regarding language barriers doesn\u2019t only rise from difference in the language itself, sometimes eventhough the language is the same, problems still occur. This is true most especially when the words used have different meaning. A word or a statement may have a denotative definition, a dictionary meaning and a connotative meaning. Problem will occur when the recipient mistakenly perceived the message in a way different from what the sender think.\n\nThe wrong perception of a person to another may also inhibit interaction. Perception can be defined as the way you view other people and also the other way around, the way other people view you. This perception, which is basically mental in nature, significantly affects the way messages are interpreted by one person.\n\nDifferences in culture might also affect interaction. There are things that are appropriate to one culture but are deemed inappropriate to another. This encompasses both verbal and nonverbal features of a message. Furthermore the context of the message, the time, place and the relationship between the two parties interacting must also be considered.\n\nEnvironmental factors such as visual, auditory and individual factors can affect interaction. Visual factors such as the lighting, distractions, distance, talker\u2019s face, viewing angle and vision can inhibit or enhance interaction. Poor lighting and physical distractions can affect the person\u2019s ability to interpret a message. The same with, the distance, vision, and viewing angle of the people that interact, this will ultimately have an impact in their way of receiving and understanding the message being conveyed.\n\nAuditor factors such as noise, distance and echo can also be a cause for problem or enhancement of interaction. Individual factors such as fatigue, illness, stress, ventilation, attitude, preparation and the situation of the parties involve in the interaction are also important factors to consider.\n\nConclusion\n\nA lot of the different features of an interaction have been greatly affected by computer mediated communication. Several of these issues are being studied. Such issues involved impression formation. The formation of relationship on computer mediated communication received much attention, as well as behaviors of deceit and lies and group dynamics.\n\nComputer mediated communication is analyzed and compared to other methods of communication through characteristics that are believed to be present in all methods of communication. The characteristics mentioned earlier involve anonymity, synchronicity and persistence. The different method of communication shows different association in the characteristics mentioned. Like for example, the use of instant messaging basically have high synchronicity from its very nature. On the other hand, it lacks persistence or recordability because the users cannot save all the previous conversations they had when the dialog boxes are closed not unless the user installed a message log or the user copy-pasted the entire conversation manually.\n\nElectronic mails and message boards on the other hand lacks synchronicity because the time from sending the message to the receipt of reply varies greatly. If instant messaging is low in persistence, electronic mails and message boards excel in this characteristic because through electronic mails and message boards, the messages that are sent and received can be automatically saved. In general, computer mediated communication differs from other methods of communication because computer mediated communication is transient and naturally multimodal. At the same time, computer mediated communications do not have the codes of conduct that should govern such form of communication (McQuail, 2005). Computer mediated communication has the ability to break the barriers of physical and social limitations that are present in other methods of communication. Thus, computer mediated communication enhances the interaction among the people who can not physically communicate with each other because they are not in the same physical location.\n\nComputer mediated communication gives massive chances for language learners to enhance their skill in the language they want to learn. A study done by Warschauer discussed the effects of using electronic mails and discussion boards in language classes. The study concluded that the use of information and communications technology serves as the linking bridge between speech and writing (Warshauer, 2006)\n\nAnonymity, privacy and security of users of computer mediated communication are based on the specific program used or website visited. Some programs and sites support communication that provides the user full anonymity, privacy and security. Some other sites and programs at the other hand, fails in this aspect. Most studies done by researchers are concerned in the significance of taking into account the social and psychological implications of the said factors.\n\nInteraction can be enhanced by computer mediated communication because it reduces the issue of the parties\u2019 confidence and trust. Some people are more comfortable to talking through chat or electronic mail instead of talking to other people personally. This enhances the interaction between the people communicating. This is because of the anonymity and privacy features of computer mediated communication. Users are freer to express themselves. At the same time, inhibitions to effective interaction may also arise. Deception and lies can easily go undetected. Because of the lack of physical contact, it will be very hard to identify the truths from the lies. Unlike, in physical interaction, the tone of the voice, the body language and other factors can help people detect if the person is lying or not.\n\nComputer mediated communication inhibits interaction because it is susceptible to misinterpretation especially on text-based messages. The tone and emotion of the message can not be easily recognized in text-based messages. On the other hand, the use of emoticon might help in reducing this problem. Furthermore, computer mediated communication today also includes other forms of communication like voice and video. Through these, it is almost like talking to the other party in person.\n\nThe most important thing that computer mediated communication does, is it eliminates the physical barrier for interaction. Before, people located in different physical location are impossible to communicate with each other in a physical level. The use of the traditional mail system, helped to eliminate this but it lacks the aspect of synchronicity. It takes quite a long time before the message is received by the recipient up to the time the response is received by the original sender. The use of telephones also helped in eliminating the physical barrier for interaction. Though, this is mainly auditory in nature. A lot of misinterpretation can still occur. Furthermore, though it has synchronicity, it lacks the aspect of persistence unless a person recorded the entire conversation. Through computer mediated communication, people that doesn\u2019t share the same physical location can now interact with synchronicity and persistence. Computer mediated communication also provides features that will let you see the person in a camera and talk to the person by using microphones.\n\nComputer mediated communication permits a more effective and more efficient communication in the sense of the timely receipt and acknowledgement or response of the message. Other computer mediate communication allows the users to talk not only on text-based communication but also via the use of microphones and web-cams. In that way, you can talk to the other party and see them at the same time.\n\nIn an educational setting, computer mediate communication has the ability to enhance the learning of the students. It also has the power to reduce the feelings of isolation that the students experience in traditional educational setting. Furthermore, increase satisfaction and motivation in the course being studied can also be expected.\n\nBibliography\n\n\u201cCommunication \u2013 Definition from the Mirriam-Webster online dictionary\u201d . Mirriam-Webster. Web.\n\n\u201cThe markets get anti-social with social networks\u201d. Deloitte TMT Predictions . Web.\n\nMarkman, K. M. (2006). Computer-mediated conversation: The organization of talk in chat-based virtual team meetings. Dissertation Abstracts International, 67 (12A), 4388. (UMI No. 3244348).\n\nAhern, T.C., Peck, K., & Laycock, M. (1992). The effects of teacher discourse in computer-mediated discussion. Journal of Educational Computing Research, 8(3), 291-309.\n\nBannan-Ritland, B. (2002). Computer-mediated communication, elearning, and interactivity: A review of the research. Quarterly Review of Distance Education, 3(2), 161-180.\n\nBoyd, Danah and Ellison, Nicole. \u201cSocial Network Sites: Definition, History, and Scholarship.\u201d Journal of Computer-Mediated Communication, volume 13, issue 1, 2007.\n\nBruns, Axel, and Joanne Jacobs, eds. Uses of Blogs , Peter Lang, New York, 2006.\n\nCockrell, Cathy, \u201cPlumbing the mysterious practices of \u2018digital youth\u2019: In first public report from a \u2018seminal\u2019 study, UC Berkeley scholars shed light on kids\u2019 use of Web 2.0 tools\u201d, UC Berkeley News , University of California, Berkeley, News Center, 2008.\n\nCooper, M.M., & Selfe, C.L. (1990). Computer conferences and learning: Authority, resistance, and internally persuasive discourse. College English, 52(8), 847-869.\n\nGarcia, A. C., & Jacobs, J. B. (1999). The eyes of the beholder: Understanding the turn-taking system in quasi-synchronous computer-mediated communication. Research on Language & Social Interaction, 32, 337-367.\n\nGlobal Distance Education Net. Glossary. Web.\n\nGrudin, Jonathan (1994). \u201cComputer-Supported Cooperative Work: History and Focus\u201d. Computer (IEEE).\n\nGunawardena, C.H., Nolla, A.C., Wilson, P.L., Lopez-Isias, Jr. et al. (2001). A cross-cultural study of group process and development in online conferences. Distance Education, 22(1), 85-122.\n\nHaythornthwaite, C. and Wellman, B. (2002). The Internet in everyday life: An introduction. In B. Wellman and C. Haythornthwaite (Eds.), The Internet in Everyday Life (pp. 3-41). Oxford: Blackwell.\n\nHerring, S. (1999). Interactional coherence in CMC. Journal of Computer-Mediated Communication, 4(4). Web.\n\nHerring, S. C. (2004). Computer-mediated discourse analysis: An approach to researching online behavior. In: S. A. Barab, R. Kling, and J. H. Gray (Eds.), Designing for Virtual Communities in the Service of Learning (pp. 338-376). New York: Cambridge University Press.\n\nHiltz, S. R. (1994). The virtual classroom: Learning without limits via computer networks. New Jersey: Ablex Publishing Corporation.\n\nHughes, S. C., Wickersham, L., Ryan-Jones, D. L., Smith, S. A. (2002). Overcoming social and pyschological barriers to effective on-line collaboration. Educational Technology and Society. Florida, USA.\n\nJenkin, Martha N. (2007) Barriers to effective communication at work. Allinace Training and Consulting, Inc. Web.\n\nLapadat, J.C. (2003). Teachers in an online seminar talking about talk: Classroom discourse and school change. Language and Education, 17(1), 21\n\nLauz, Kimberly A. Barriers to effective communication. 2009. Web.\n\nLeinonen, P., Jarvela, S., & Lipponen, L. (2003). Individual students\u2019 interpretations of their contribution to the computer-mediated discussions. Journal of Interactive Learning Research, 14(1), 99-122.\n\nMcQuail, Denis. (2005). Mcquail\u2019s Mass Communication Theory . 5th ed. London: SAGE Publications.\n\nMcQuail, Denis. (2005). Mcquail\u2019s Mass Communication Theory. 5th ed. London: SAGE Publications.\n\nPoole, D.M. (2000). Student participation in a discussion-oriented online course: A case study. Journal of Research on Computing in Education, 33(2), 162-176.\n\nSeverin, Werner J., Tankard, James W., Jr., (1979). Communication Theories: Origins, Methods, Uses . New York: Hastings House.\n\nThurlow, C., Lengel, L. & Tomic, A. (2004). Computer mediated communication: Social interaction and the internet. London: Sage.\n\nVonderwell, S. (2002). An examination of asynchronous communication experiences and perspectives of students in an online course: A case study. The Internet and Higher Education, 6, 77-90.\n\nWalther, J. B. (1996). Computer-mediated communication: Impersonal, interpersonal, and hyperpersonal interaction. Communication Research, 23, 3-43.\n\nWalther, J. B., & Burgoon, J. K. (1992). Relational communication in computer-mediated interaction. Human Communication Research, 19, 50-88.\n\nWarschauer, M. (1998). Electronic literacies: Language, culture and power in online education. Mahwah, NJ: Lawrence Erlbaum Associates.\n\nWarschauer, M. (2006). Laptops and literacy: learning in the wireless classroom: Teachers College, Columbia University.\n",
        "label": "human"
    },
    {
        "input": "How Computers Have Changed the Way People Communicate Research Paper\n\nTable of Contents\n 1. Introduction\n 2. History\n 3. The Times They Are A-Changing\n 4. Blogs, Video Blogs, Webcasts, Chatrooms, Listservs\n 5. Readers on the Internet vs. Newspapers\n 6. How Banking Has Been Revolutionized By the Internet\n 7. Trade/Investment\n 8. Conclusion\n 9. Works Cited\n\nIntroduction\n\nWith the introduction of the internet and communications occurring literally at the speed of light, new practices in modern living are evolving, such as e-commerce, e-trade and e-finance creating a much more interconnected world even as we remain physically quite far apart. People are finding it possible to do everything they need to do from their own desktop, including receiving the news, interacting with others, working, shopping, banking, trading and any number of other activities. Based upon its level of use in current society as it grows and expands in response to both consumer and corporate directives, it is safe to say that the internet will become even more integrated into the everyday life of individuals throughout the world in the years to come.\n\nHistory\n\nWhen the telegraph was invented, it helped speed up human communication over long distances because it didn\u2019t depend on human travel and was the infancy of e-commerce. This invention was followed shortly afterward by the telephone which evolved into the mobile and cell phones that have become an almost required part of everyday life. The development of the internet has further taken communications and trade beyond the scope of time and distance. With the advent of the internet and massive computing systems that are increasingly becoming smaller in physical scale, the world is continually proportionately shrinking in sociological terms and becoming economically interdependent. This worldwide establishment of the internet throughout most cultures and countries has revolutionized the way people live their lives on many fronts (Rowland, 1997).\n\nAbout the time that television was becoming household equipment, the first computers were becoming available for scientific use. As early as 1951, they were being produced and sold on the general market. However, computers as a communication tool didn\u2019t really become an option until the advent of ARPANET, the first form of a networked internet that was developed in 1969. These connections were made faster with the introduction of cable wire services in 1972. At this point, communications methods began speeding up, constantly changing and improving efficiency. In Japan in 1979, the first mobile phones began to be used, not becoming popular worldwide until sometime around 1985. By then, personal computers and laptops had entered the marketplace, introducing the idea that communications could easily travel with you to home, work, school or wherever. These wireless services were available by 1981, with the growth in the internet market since serving to completely change the ways in which we communicate and trade globally (Rowland, 1997).\n\nThe Times They Are A-Changing\n\nThe ability of people to connect with each other across long distances at practically the speed of light via the web has changed not only the way people communicate, but also the expectations they have for the retrieval of information. Email is free and instantaneous causing its use by businesses and individuals to grow at a phenomenal rate over the past decade. Few people write letters or read the newspapers anymore as these are seemingly archaic methods of communication and have the added disadvantage of costing money, regardless of how little this sum might actually be. \u201cNowadays, we think nothing of emailing our aunts in Germany and getting an answer back within minutes, or seeing the latest streaming video full of up to the minute news\u201d (Boswell, 2006). Neither letter-writing nor newspaper delivery can provide the instant information that is today essential for businesses in the modern world to compete effectively and simply a fact of life that is taken for granted for most in the developed world. People are now turning to alternative forms of receiving information, such as blogs, video blogs, webcasts, chatrooms and listservs. However, this introduces new challenges in determining just how to determine credible sources from noncredible and just how far the internet should be allowed to go in terms of free speech and expression.\n\nBlogs, Video Blogs, Webcasts, Chatrooms, Listservs\n\nThrough such user-directed content as blogs, webcasts and listservs, the common man is now, for perhaps the first time in history, able to have some form of interaction with the major news mediums of the world. Noah Shachtman (2002) points out how it was internet bloggers that created such a stir regarding Trent Lott\u2019s racist comments that brought the politician\u2019s past to the attention of major news organizations who would, in all likelihood, have ignored them otherwise. \u201cThey kept focusing on Lott\u2019s hateful past \u2013 until the national press corps finally had to take notice. \u2026 Lott\u2019s bile is one of several issues that this burgeoning community helped push on to the national agenda\u201d (Shachtman, 2002). The term \u2018blog\u2019 is actually techno-geek shorthand for \u2018web logging\u2019, and, as such, has a well-earned reputation for being merely an online diary for a variety of people.\n\nHowever, as has been noted by Shachtman and others, blogs have been taking on new meanings in recent years as internet use and knowledge grow among the general public. \u201cBlogs can be used to make political statements, promote products, provide research information and give tutorials\u201d (Ng, 2007). As such, blogs can be found written by politicians, musicians, novelists, sports figures, newscasters and other notable figures as attempts to not only share their opinions, but to promote their own personal agendas separate from the restrictive and uncontrolled traditional venues of newspaper and television. As might be imagined, video blogs add the elements of sound and video to the presentation, providing these users with the added credibility of a news-type presentation. \u201cThe potential for everyone to self-publish has the ability to revolutionize the world by sharing video across cultures and countries\u201d (Clint Sharp cited in Dean, 2005). By turning to video, vloggers are able to bring multiple subjects to the screen rather than being concerned about the filters of mass media publishers.\n\nOther methods of internet communication have opened up the concept of communication as well. Through listservs, internet users have been able to post their questions to the world even from its earliest days. This process is almost like standing atop a mountain and shouting a question out to the great beyond. The only difference is, with listservs, users can expect to receive an answer if they\u2019ve selected an appropriate forum. Their question remains online for as long as the server cares to host it, allowing users from all places and multiple times to respond while providing ample room for further discussion. Chatrooms build on this concept by adding instant responses to this text-based mode of communication. As users type in their thoughts, other users can immediately respond, having \u2018virtual conversations\u2019 in real time. Organized in community groups of shared interests, chatrooms have enabled internet users to share information on a wide scale without dependence upon more traditional methods of news media and without the wait typically associated with previous community-specific publications such as magazines and newspapers (Pack, 2003). There remain several inherent risks with this method of communication however, the most pressing being that of internet predators seeking younger internet users for harmful purposes. This is only possible because of the close relationships that can form with frequent conversation and sharing of interests.\n\nCombining the self-expression attributes of the blogs with the interactive aspects of listservs and chatrooms and placing it all in streaming media, webcasts allow any average internet user to become their own interactive news program discussing whichever topics they choose and allowing other internet users to call or write in their responses and questions. Many large and small businesses have already discovered the advantages of webcasting as an alternative means of conducting business rather than flying various constituents in from around the country or the world. \u201cOnline meetings are interactive and collaborative. They\u2019re in real time so there\u2019s two-way communication via instant messaging or other software between a participant and the conference leader or across the team or group, depending on how you customize the meeting.\n\nYou can instantly share content and visuals, watch and listen to presentations or ask questions and make comments \u2013 often simultaneously\u201d (Krotz, 2007). In the business setting, webcasts have been used to train employees, distribute new product information and provide customers with live help or education among other things. However, they have often been used for other applications as well. Distance learning and even traditional learning scenarios have employed the use of webcasts with an eye toward future technology integration in and out of the classroom, politicians have used them to help launch campaigns or to advocate party affiliations and other organizations have used them as effective press conferences or to inform research markets and analysts. As with the other communication technologies, webcasts have enabled the average individual regardless of their home country, business, vocation or interest to work around the national media chains and present their views in their own way, introducing a great deal more information into the general community than has been available previously.\n\nReaders on the Internet vs. Newspapers\n\nThe effects of the internet are being felt in many ways, not just in the greater freedom of speech being experienced or the topics being addressed, but also in the competition for readership/viewership between the internet and more traditional media outlets. According to the 2006 Office of Communications report, people in the UK are generally turning away from television, radio and newspapers in favor of the more recent, more relevant, more accessible services available on the internet. \u201cTelevision is of declining interest to many 16-24 year olds; on average they watch television for one hour less per day than the average television viewer\u201d (Ofcom, 2006). More than simply turning to the internet for downloads and information, the report found that as many as 70 percent of this age group had actively contributed to some form of internet communication, in the form of chatting, blogs or other types of message boards. \u201cThe sector is being transformed by greater competition, falling prices and the erosion of traditional revenues and audiences. A new generation of consumers is emerging for whom online is the lead medium and convergence is instinctive\u201d (Ed Richards, Ofcom Chief Operating Officer, cited in Ofcom, 2006).\n\nHow Banking Has Been Revolutionized By the Internet\n\nThe move to the internet has meant a great deal of change in the way banks do business, not only with their individual customers, but also as they relate to each other. Like the retail world, the internet allows even the smallest community bank to compete online as if it were one of the big boys. \u201cA well-designed interactive website, an internet home banking solution combined with a decent array of products, and customers can\u2019t really see the difference between a community bank and a mega-bank\u201d (Lorence, 1999). In some cases, the ability to move banking services online has even eliminated the need for brick and mortar structures while providing the means for completely internet-based banks to remain competitive.\n\nLike retail shopping, banking online has become relatively simple, consisting of easy-to-navigate menus and step-by-step processes. Given the nature of the business, banks take a few additional precautions to the login process before an individual is given access to an account, \u201cthe industry standard 128-bit encryption is about as safe as you can get, it\u2019s the same as you find when purchasing online through secure sites\u201d (Nickson, 2006). The process of banking online allows customers to check accounts at any time of day or night without any kind of paper transaction taking place, reducing the cost of both manpower and office supplies. In addition, many banks are now offering online bill payment, through agreements with various companies and utilities, generating a new revenue stream while providing an asset to customers, reducing the cost of check printing and processing and saving customers the cost of postage. \u201cBanking online allows you to check your balances and move money between your accounts at the click of a mouse. It helps eliminate the danger of being overdrawn, while still letting you keep money in a higher-interest savings or other accounts until it has to be moved\u201d (Nickson, 2006). Whether banks are a combination of brick and mortar and internet or internet only, many are now offering more services, approaching the full service levels of brick and mortar structures within the virtual environment. Services that are already available include home and auto loans, credit cards and insurance as well as several other services.\n\nTrade/Investment\n\nThe internet has had similar effects in the trading industry as the ease of trading online helped create a resurgence of interest among the general population. Through advances in technologies, the dynamics of the marketplace have changed, eliminating many of the handicaps experienced by those who were unable to maintain physical proximity to the trading floor. \u201cOn an electronic trading system, everyone is in the same cyberspace, so time and place advantages disappear\u201d (Unger, 1999). In addition, more people can participate in various markets without the need to be in a membership organization or within a specific geographic location. Investors can track projects on their own without the need for intermediaries, communicating directly with buyers or sellers. Many of the trades that occur can be linked electronically, automatically bringing buyers and sellers together with a few mouse clicks while still remaining stable and secure.\n\nHowever, direct access trading firms have some of the same disadvantages brought forward in internet-only banking in that there are few available resources for those individuals not quite comfortable with the process or who need a little extra help in making things work. Depending upon the level of education, previous experience or general market knowledge an individual has, direct access trading may open an investor up to unwise investments and potential disaster. In this time-sensitive arena, the uncontrollable issues of network stability take on new importance. \u201cThe greatest disadvantage of online trading is the inability of a network to be fail-safe. Computers in spite of the technological advances are by no means perfect\u201d (Mathews, 2002).\n\nConclusion\n\nThe internet is continually evolving, adapting itself to meet new challenges and new needs, addressing the needs of consumers and businesses while working to maintain a sense of confidence, stability, security and simplicity. The cutting edge technology of today will eventually go the way of vacuum tubes in televisions. Developing countries, specifically those that don\u2019t enjoy freedom of speech rights, will create their own internets which will likely interconnect with other networks, such as the current interest in a spider web type relationship, selectively blocking off entire segments to its population while enabling access to others. As it becomes easier and easier to allow services and features to cross platforms from PC computer to handheld device and from cell phone to MP3 player and beyond, prices on electronics will continue to fall and make it possible for individuals in third world countries to join in the internet revolution.\n\nWorks Cited\n\nBoswell, Wendy. \u201cHow the World Wide Web has Changed Society.\u201d About the Internet. (2006). Web.\n\nKrotz, Joanna. \u201cHow to host a successful B2B Webcast.\u201d Small Business Center. Microsoft Office. (2007). Web.\n\nLorence, Chris. \u201cInternet Banking: Can You Afford Not To?\u201d Northwestern Financial Review. (1999).\n\nMathews, Isaac G. \u201cBenefits, Costs and Limitations of Online Investing to the Individual Investor.\u201d (2002). Web.\n\nNg, Deborah. \u201cWhat are Blogs?\u201d WiseGeek. Conjecture Corporation. (2007). Web.\n\nNickson, Chris. \u201cShouldn\u2019t You be Banking Online?\u201d Digital Trends. (2006). Web.\n\nOffice of Communications (Ofcom). \u201cOfcom Communications Market Report Reveals New Industry Trends and Changes in Consumer Behaviour.\u201d News Release Archive. Office of Communications. (2006). Web.\n\nPack, Thomas.\u201cCreating Community.\u201d Information Technology. All Technology. (2003). Web.\n\nRowland, Wade. \u201cThe Sprit of the Web: The Age of Information From Telegraph to the Internet.\u201d Toronto: Somerville House. (1997).\n\nShachtman, Noah. \u201cBlogs Make the Headlines.\u201d Wired News. (2002). Web.\n\nUnger, Laura. \u201cThe Internet \u2013 Will it be the End of the Stock Market as we Know it?\u201d Speech by SEC Commissioner. US Securities and Exchange Commission. (1999). Web.\n",
        "label": "human"
    },
    {
        "input": "Technical Communication: Principles of Computer Security Report\n\nIntroduction\n\nCompound business applications, e-commerce, and transaction automation demand tough and accurate security procedures. Corporations employing the Internet as a means to carry out business operations can be more productive and successful if their decisions uphold the requirements of security-conscious consumers. At present Internet, consumers insist on strict security protocols to safeguard their welfare, privacy, interactions, and resources.\n\nPublic key cryptography facilitates security aspects like privacy, reliability, validation, and non-repudiation. Nevertheless, to effectively implement such security factors, a carefully drafted management plan is required to monitor the security infrastructure. The public key infrastructure (PKI) provides a keystone, based on which other systems, modules, applications, and security components are developed. A PKI is an indispensable element of the general security policy which is aligned with other security aspects, business operations, and risk management initiatives. (Conklin, 2004)\n\nIn this document, we look at the issues which require attention when deciding on whether the PKI infrastructure should be provided by in-house facilities or commercial services.\n\nDiscussion\n\nThe Public Key Infrastructure (PKI) involves a set of computer hardware, software, personnel, strategies, and required to generate, control, store, allocate, and validate digital certificates. It links cryptographically generated public keys with user identities through a certificate authority (CA). This linkage is ascertained through the registration and issuance procedure. The PKI functionality that warrants this linkage is known as the Registration Authority (RA). In some cases, the expression trusted third party (TTP) is synonymous with a certificate authority (CA). (Rothke, 2005)\n\nWhen a corporation\u2019s network security necessities require them to use digital certificates for transactions, then it has to decide from where to procure the certificates. The certificates may be purchased from a commercial or third-party certificate authority like VeriSign or Thawte, or instead, an in-house facility may be set up to issue one\u2019s certificate. The three primary issues when deciding on whether to set up an internal PKI or use commercial PKI are cost, liability, and repute. (Conklin, 2004)\n\nFor a medium-sized enterprise, like in this case, an external commercial PKI is highly recommended for reasons discussed in this paragraph. Procuring a large number of certificates from a commercial provider can be a costly issue wherein an in-house facility could cut costs. However, in this case, where only a few certificates are to be issued a commercial facility is a much more feasible option. Secondly, in case there is a disaster such as data loss or system failure the liability is owned by the certificate issuer. Thus, implementing risk management frameworks to deal with such a crisis is a painstaking and costly affair. For an in-house facility, such frameworks have to be designed by a dedicated team. Hence, employing the services of a commercial provider is a better choice for a medium-sized company.\n\nLastly, a dedicated commercial provider is better known and their reputation is much higher throughout the markets. Consequently, the level of trust, in the case of a commercial outfit is much higher than its in-house counterpart. Thus more customers would be assured to use the system if it is warranted by a reputed commercial provider. (Rothke, 2005)\n\nTo protect a wireless network and enhance security measures the following measures should be in place\n\n 1. The default values of the System ID called the Service Set Identifier (SSID) or Extended Service Set Identifier (ESSID) should be changed.\n 2. Identifier Broadcasting should be disabled.\n 3. Wi-Fi Protected Access (WPA) encryption standards must be met.\n 4. Hardware, as well as Software Firewalls, needs to be installed.\n 5. Anti-hacking tools need to be installed in the systems using wireless connectivity as a last line of defense. (Conklin, 2004)\n\nConclusion\n\nMedium-sized companies like in this case should prefer a commercial PKI over an in-house approach as it decreases costs, transfers the liability, and takes advantage of the repute and level of trust enjoyed by dedicated commercial service providers. In addition, wireless networks must be secured by following a strict, understandable, and clearly communicated policy and ensuring that basic security measures are in place.\n\nReferences\n\nConklin, A. (2004). Principles of Computer Security: Security and Beyond . NY: McGraw-Hill Technology Education.\n\nRothke, B. (2005). Computer security: 20 things every employee should know . NY: McGraw Hill Professional.\n",
        "label": "human"
    },
    {
        "input": "Biometrics and Computer Security Annotated Bibliography\n\nThe article describes that modern organizations and government bodies should pay a special attention to threats and vulnerability related to sensitive data. In this case, it is possible to distinguish two types of threats: internal and external.\n\nInternal threats include damage of laptops and disclosure of personal information by employees. External threats are hackers and data thieves. Because the biometrics data does hold must be accurate, it is worth thinking from the outset about how managers are going to keep it that way. The article describes history of biometrics, its pros and cons. The author pays a special attention to biometric technology, fingerprinting, hand geometry, Iris and Retina Scanning and face recognition. The article is objective and is based on substantial literature review. The author supports ideas and suppositions with detailed facts and arguments related to the topic.\n\nBielski, L. Striving to Create a Safe Haven Online: ID Theft, Worms, Bugs, and Virtual Eavesdropping Banks Cope with Escalating Threat. ABA Banking Journal , 95 (2003), 54.\n\nThe article discusses the problems of safety and technological risks associated with data protection and hacker attacks. This starts with deciding what information to collect and how to get it. Good design of data capture forms can help; so can choosing reliable and up to date sources if an organization is not acquiring the data directly from the Data Subject. This means that government agencies must hold enough data but, importantly, not too much.\n\nThe biggest risk to security is almost always the company\u2019s own staff. The damage they do can be deliberate\u2014stealing information about people, such as business contacts they want to use for their own purposes, for example, or trashing the database out of frustration on being demoted. The arguments in the article are well-supported by facts and research studies conducted on this topic. A special attention is given to banking sector and possible tools used to protect privacy issues.\n\nCasella, R. The False Allure of Security Technologies. Social Justice , 30 (2003), 82.\n\nThe article states that biometrics and other elated fields of research require huge investments and financial support in order to protect data and electric information. More often it is un-thinking or inadvertent\u2014giving information over the telephone to someone who should not have it, leaving confidential files at home for a neighbor to see when they are working at home, or chatting in the canteen about a user\u2019s borrowing habits where other people can overhear.\n\nThe role of the government is to control data protection and develop innovative technologies against attacks and intrusion of the third parties. \u201cThe use of security technology in public places in the form of biometrics, detectors, surveillance equipment, and advanced forms of access control are relatively recent developments\u201d (92). The article is based on current literature review and state documents related to the problem of biometrics.\n\nLineberry, S. The Human Element: The Weakest Link in Information Security Journal of Accountancy 204 (2007), 44.\n\nThe article pays a special attention to such problem as \u201chuman elements\u201d which can be a risk factor in security. Security must be seen in the context of wider organizational policies. Many aspects of security will be taken care of by, for example, the IT department or its equivalent.\n\nHowever, high level security provision on its own is not enough; the systems have to work in practice. Facial recognition is an important area of concern for many state agencies. Also, the state agonies should maintain a perimeter security system. This system consists of firewalls, intrusion detection systems and anti virus measures installed on each laptop. Specific issues may arise where a Data Controller feels the need to monitor the behavior of staff or members of the public. The organization must be careful only to provide the information to the right person. This article proposes readers a unique approach to data and information security connected and depended upon human motivation and fairness.\n\nOrr, B. Time to Start Planning for Biometric. ABA Banking Journal , 92 (2000), 54.\n\nThis article is devoted to importance of biometrics as a science and opportunities proposed by further development of face recognition technologies. This means that the state institutions should ask for information to verify their identity. State institutions may also ask for information to help GCI locate their records. State institutions might, for example, want to ask what part of organization they originally dealt with, or the approximate date they were last in contact.\n\nA data access request is not valid until employees have received any of this information needs, but can only ask for \u2018reasonable\u2019 information. The first line of defense is therefore to ensure that staff are aware of the possibilities and operate within a culture where information, and especially personal data, is handled carefully and responsibly. This article is objective and is based on a detailed analysis and data collection methods.\n\nPapacharissi, Z., Fernback, J., Online Privacy and Consumer Protection: An Analysis of Portal Privacy Statements. J ournal of Broadcasting & Electronic Media , 49 (2005), 259.\n\nThe article proposes analysis of online privacy issues related to consumer marketing and biometrics. Principle is that precautions must be taken against \u2018unauthorized\u2019 processing. The staff must therefore not use data in any way that they are not permitted to, and they must not disclose it to anyone else who is not permitted to have it. But in order for this to make sense, someone has to do the authorizing. Unless there are clear guidelines on what is permitted, staff cannot be expected to comply.\n\nThe second Protection Principle says that all processing must be compatible\u2019 with the purposes it was obtained for. Therefore in deciding who is authorized to see any particular type of data, it is important to think about what type of access is compatible with the purpose. The article is based on well-thought analysis and up to date information related to the filed of face recognition and biometrics. As a minimum it is usually best to get from the requesting agency in writing the legal basis on which they are asking for the information.\n",
        "label": "human"
    },
    {
        "input": "Principles of Computer Security Report (Assessment)\n\nTable of Contents\n 1. Introduction\n 2. Body: Ping sweep and port scans\n 3. Conclusion\n 4. References\n\nIntroduction\n\nToday, the email system of our company has become one of the most indispensable and widely used business communication tools. But due to its increased popularity, our email has also become a suitable target for crackers and hackers who intend to cause harm to our company. Although email is a very convenient and efficient tool, it has certain vulnerabilities which the hackers exploit. Internet communication systems using UDP or TCP are the most vulnerable to such attacks. The attackers try to discover the services that are present at the network target, i.e. us. Then they use techniques like ping sweeps and TCP and UDP port scans for gathering data from that remote network. (Fletcher, 2009)\n\nBody: Ping sweep and port scans\n\nPing sweep and port scans are the most common types of reconnaissance network probes. The port scan technique can be used by attackers for discovering the services that run on our machine. By using port scans an attacker can find out the live services that are running on our machines. Then he can plan any type of attack on the services that he has found. The attackers can port scan all the possible UDP and TCP ports and can even limit the ports scanned for avoiding getting detected. Port scans are extremely simple to carry out since the intruder simply has to link up with the ports of our machine and determine which out of them are active. UDP scans are a little more difficult than the TCP scans since the former is a connectionless protocol. The attacker simply sends to an intended port a garbage UDP packet to check the machines that are active. Since TCP scans are easy the attacker can use stealth scans, FIN scans and TCP connections for determining whether a machine is active. (Dollard, 2006)\n\nIn ping sweep a series of ICMP ECHO packets can be sent to the network where the machines have a range of IP addresses. By this way the attacker determines which machines are active and responsive so that he can focus on a particular active machine for attacking it. By using this mechanism an intruder can choose a list of our IP addresses and then send those ping packets to us. But unlike a normal ping operation, a ping sweep will send one of the packets to a single IP address and the next one to another IP address. This goes on continuing in a round robin fashion. (Fletcher, 2009)\n\nConclusion\n\nAlthough ping sweeps and port scans can be used by attackers for hacking into our systems, they are not very harmful if proper precautions are taken. Also, sometimes we have seen that network administrators use ping sweeps and port scans on their networks for determining which of the machines are active and which are not so as to perform a diagnosis. Our company needs to be aware of the different types of network probes that can be extremely harmful for our company. But network probes like ping sweep and port scans cannot be stopped and this is the reason that they need to be taken somewhat seriously. (Dollard, 2006) Since we cannot stop them we need to be ready in case either a ping sweep or port scan takes place so that we can immediately protect our vulnerable systems and data.\n\nReferences\n\nDollard, J. (2006). Secured Aggression . New Haven and London: Yale University Press.\n\nFletcher, R. (2009); Software Security: Beliefs and Knowledge. Auckland: Howard & Price.\n",
        "label": "human"
    },
    {
        "input": "Why to Choose Mac Over Windows Personal Computer Research Paper\n\nTable of Contents\n 1. Purpose\n 2. Introduction to Mac and PC\n 3. Advantages and Disadvantages of Mac OS\n 4. Advantages and Disadvantages of Windows OS/ PC\n 5. Why Mac is better than PC\n 6. Conclusion\n 7. References\n\nPurpose\n\nThe purpose of this paper is to highlight why a consumer should select a MAC operating system over the Windows PC. Both Mac and PC are popular operating systems, readily available in the mainstream market catering to the various needs of the consumers. While the basic functionality of the operating system on a computer is the same, both operating systems differ in terms of their characteristics and the specific benefits that they provide to the end user. Through this paper a justified argument, supported by evidence based on books and scholarly journals, is presented on why consumers should look towards adopting a Mac computer instead of one based on Windows PC.\n\nIntroduction to Mac and PC\n\nThe Mac operating system was a computer software developed by the company Apple Inc to provide operating systems for Apple computers. Initially the software was only available for use on Apple computers and apple devices, however now other computers can also make use of Mac OS. The original line of computers that were launched in 1984 and supported the Mac OS where named Macintosh and were sold exclusively by Apple Inc.\n\nThis was a revolutionary computer operating system as previously the only operating system that was available to consumers was MS Dos a command line and prompt in nature and was tedious for majority of the consumers. On the other hand the Macintosh computers with the Mac operating system were unique and revolutionary as they provided a graphical user interface for the consumers and enabled them to use programs, and software developed on the object oriented programming language. This form of a computer interface was more user-friendly and easy to comprehend for the consumers.\n\nThe Mac OS made use of the hierarchical directory tree approach for navigation and file addressing and enabled the users to create their own files and folders with relative ease. The system itself promoted multiple and comparative multitasking as the user could use more than one program at the same time which was previously not possible with other non graphical user interfaces. Aside from this the Mac operating system was also developed and supported by the UNIX language which meant that it was open source.\n\nAs a result the Mac users were able to customize their own systems according to their requirements and share the knowledge of new developed upgrades and programs with others in the UNIX based OS community. This enabled the Mac users to have highly customized and efficient operating systems that were based on upgrades and programs developed by other Mac users purely for their convenience and facilitation. Other application programs that are provided by the Mac operating system included image processing software, audio and video processing software as well as software for word processing, email and development of databases and spreadsheets.\n\nWhile the Mac OS was launched by Apple Inc, the Windows OS was launched by Microsoft Windows in 1985. PC\u2019s are usually associated with Microsoft Windows as they are computers that run on the Windows based operating system. The Windows operating system was also a unique operating system in the market at the time of its launch as it provided the consumers in the market to be able to use a graphical user interface for operating their computers and navigating instead of using the old command line based operating system.\n\nThe Windows operating system was launched by Microsoft Windows as an additional product/ service to the currently available MS-DOS, however the new operating system became more popular than MS-DOS amongst the users as it was more user friendly and allowed just about anyone to be able to use the computer with minimum knowledge of computers and programming languages or command line instructions. The Windows operating system also provided a range of application software for video and audio playback, video and audio recording as well as the famous Office Suite known as MS Office.\n\nThe MS Office suit provided application software for word processing (MS Word), spreadsheets, (MS Excel), data base and file management (MS Access), email management, (MS Outlook), and multimedia based presentations (MS PowerPoint). The other characteristic of the windows OS is that it is not an open source system. This means that the code of the Windows OS is restricted and accessible to only Microsoft Company. As a result it is not possible for the users to make changes to the underlying code of Windows OS in order to make customizations or change any flaws or errors that might be present in order to make the operating system more efficient.\n\nAdvantages and Disadvantages of Mac OS\n\nThe advantages that are associated with a Mac OS pertain to the fact that the Mac operating system is more reliable and has less chance getting a virus. Specifically after the launch of the Mac OS X in the year 2000, the number of viruses, ad ware or malicious spy ware that attack the computers has greatly reduced for computers having a Mac OS. Having a Mac does not guarantee that there are no viruses that can attack the computer, however having a Mac OS greatly reduces the chances of acquiring a virus through normal use or internet browsing.\n\nAside from this the Mac operating system is very friendly for the user. The users are easily able to comprehend navigation and control on the operating system. Moreover the Mac operating system is often available on machines pre-installed or can be installed at the request of the customer. This is specifically true for Apple machines or Macintosh.\n\nAnother great feature and advantage of the Mac operating System is that it allows the user to use both Mac OS as well as MS Windows OS on the same machine, therefore providing the user with a choice of dual OS usage. In fact it is also relatively easy to convert files created on the Mac OS to be transferred and used on the MS Windows OS through migration and conversion. The Mac OS and Mac computers provide the user with exceptional video, audio and photo processing technology. Using the Mac OS it is very easy for the users to run professional applications for video, audio and photo editing.\n\nMac OS based computers are highly capable when it comes to dealing with multimedia as their performance using multimedia is excellent. \u201cToday, many computer users in business and industry are adopting Macintosh computers as a primary multimedia tool because of its superior video, images, and sound\u201d (Jun Na Rajaravivarma, 2003). Aside from this the Mac OS also provides application software unique in nature like iChat for chatting using audio and video, iLife for managing files related to multimedia and the Time machine application which allows the user to schedule data backups.\n\nThe Mac OS is based on the UNIX platform in the open source. This has enabled the users to create and highly customize the Mac OS according to their specific requirements and needs. Alben highlighted in her Muriel prize winning article about the approach taken by apple for Mac OS that provides that apple conceived the Mac OS to be \u201ca computing environment that allows people to choose what works for them. Instead of having to conform to the confines pf the computer, they will work and play and learn in a way that better fits their needs and wants. These customizable appearances take the Mac OS beyond the utilitarian operating systems currently available\u201d (Alben, 1997).\n\nThe UNIX base of the Mac OS has made it more reliable as well as less faulty as it has been extensively tested, used and adjusted to eliminate any possible faulty code or programming by peers. The open source nature of the Linux and UNIX has allowed users to fine tune software and additional application software for the Mac OS (Lerner & Tirole, 2005).\n\nThe open source nature of the Mac OS has enabled users of Mac OS based machines and computers to create new application programs for the Mac OS system. These application software have been accessed by Apple and have been provided licensing by the company while integrating them into the Mac OS package for future users. One such development that has been developed using the Mac OS open source environment are the technical or virtual bulletin boards that were originally used by the Mac OS open source programmers to share code and advise on developing short code for Mac OS (Luca & McLoughlin, 2003). The Mac OS was specifically designed for Apple products by Apple Inc.\n\nTherefore it is more reliable when it is run on apple products as it provides a complete solution of hardware and software that works together in a mutually cohesive manner to benefit the user and make using a computer machine easier for them.\n\nThe disadvantages that are associated with a Mac or a Mac OS pertain to upgrading the OS. As no upgrades of the Mac OS are routinely launched by Apple, it is not possible for the users to upgrade their Mac operating systems. However the users can always download shortstopped from the internet provided by other Mac users and customize and adjust their Mac Os features and application programs. Aside form this a recent trend analysis of the Mac systems on the internet has revealed that Mac systems are more expensive as opposed to PC\u2019s to purchase as they are more multimedia oriented with additional features.\n\nThe old Mac OS is somewhat redundant and some of the applications that traditionally have worked on Windows OS might not work on the Mac OS unless the user installs the Mac OS X software. Similarly it is often difficulty to repair a Mac and resolve issues as the code for the Mac is often different and not standardized. Moreover one drawback for serious gamers is that most of the games that are readily available on the MS Windows based PC are not compatible with the Mac OS platform and therefore cannot be run on Macs. \u201cAlthough many cross-platform file types are currently available, connection type depends mainly on network purpose, and media types are sufficiently diverse that uninformed users can encounter serious problems\u201d (Jun Na Rajaravivarma, 2003).\n\nAdvantages and Disadvantages of Windows OS/ PC\n\nThe main advantage of a MS Windows based PC is that the MS Windows Company provides extensive and unlimited upgrades for the Windows OS. As a result the users only have to have access to the internet in able to automatically download upgrades for the Windows OS and the related application software that run on the Windows OS.\n\nAnother advantage of the PC based on MS Windows OS is that majority of the programs that are available in the market and the games that are available to the users are those which are created keeping the MS Windows in mind. As a result they run much better with MS Windows and some are only able to run if a Windows MS based platform is provided on the PC. The PC is also a very common computer that is extensively available in the market, relatively cheap as well as having a high percentage of population using it.\n\nThe disadvantages that are associated with PCs and MS Windows OS is that they are highly prone to viruses and bugs. This makes them very unstable and unreliable. The PCs as a result have to be secured with complex antivirus programs that can often be very expensive for the consumers to purchase. The system of a PC can often crash and become slow or unresponsive over time with heavy usage. This is another major problem with the PC\u2019s that makes it unsuitable for extensive heavy usage. Moreover as PC\u2019s have a standardized code based operating system provided by MS Windows, the hackers specifically target PC\u2019s with their viruses and malicious code.\n\nAS a result the MS Windows OS and PC users have to extensively take care of their computers, look after them and keep them upgraded in terms of their systems and their anti virus software in order to have smooth operations on the computer. This is especially true for the new Windows Vista which has been released. Vista makes PC\u2019s more unreliable and unstable in terms of performance. Another disadvantage that is present to the PCs and MS Windows OS is that PCs that are bought with MS Windows OS are often provided with the most basic of application software in the package. However the Mac systems with the Mac OS have customized media applications and software that can only be run on Mac. As a result this reduces the appeal of PCs for the younger generation.\n\nWhy Mac is better than PC\n\nIn today\u2019s day and age a Mac is better than a PC. This is mainly because the software and media that is used on the computer by an average user is highly complex in nature with high level of multitasking taking place. In such a situation that Mac fares much better than a PC as it especially designed for multimedia applications and use of heavy duty multitasking programs. In addition to this the Mac is also exceptional for gamming and multimedia processing with the enhanced applications dedicated to this and high level of graphics provided. Moreover the design of the hardware for Mac anther GUI interface makes Mac innovative and suave choice for style conscious users. On the other hand PCs are considered to be boring with lack of stylish appeal.\n\nThe PCs are readily available as the most affordable systems in the market. However the Macs are more affordable for the users in the long term. This is because the Macs have integrated customizable application software for office as well as connectivity, multimedia and online chat that are not available on PCs. The PC users as a result have to purchase MS office suite in additional to the Windows OS and the PC, while additional security protection software and graphics cards also have to be bought in order to bring the Pc up to the level of a readily available Mac therefore increasing the cost of the PC.\n\nThe Macs are more reliable for the users as they tend to break down less often and suffer from much fewer crashes as compared to the PCs. The PCs are prone to malicious code, viruses and faulty which have to be addressed by Microsoft. This makes them unreliable. The Macs however are less prone to viruses and attacks form hackers as it is much more difficult to hack a Mac as compared to a PC. Aside from this the Macs are also well known for their high level of performance and processing Speed. The Macs are designed specifically for heavy duty usage with multitasking of multimedia applications and software.\n\nThe Macs, as a result deliver a much more efficient performance in terms of speed and reliability. On the other hand PCs tend to slow down over time and can often crash when multiple multimedia applications are being used on a PC. Similarly if high quality gamming is to be conducted on the PC, an additional graphic card is required which is not necessary for a Mac.\n\nThe Macs are also easy to use for the end users of the system. The Macs provide an interactive graphic user interface that is specifically made keeping the users\u2019 requirements in mind. As a result the Macs are much easier to navigate and use for users as compared to PCs. Regular updates are available for Macs provided by the company as well as other users which can upgrade the Mac OS X. aside form this the Macs also feature instant connections to external devices, internet based communication devices, other apple products. The provision of the iChat software along with a web camera allows the users to have access to video based chat at the click on a button.\n\nThe Mac also features a unique capability of housing two operating systems at the same time, enabling the user to make use of Mac based OS as well as MS Windows based US on the same machine. This dual operating system is particularly useful for families where multiple people prefer different types of platforms and operating systems. Apple Inc provides the Mac computers on which Mac Os is provided by the company.\n\nThe company designs hardware which corresponds with the Mac OS and adjusts the software of the Mac OS to the changes in the hardware making both the hardware and software mutually cohesive. The integration of hardware and software provided by Apple in the Macs allows Mac to feature unique capabilities where services like chatting and internet connectivity is instantly available to the user.\n\nThe Mac users are spoilt to choice when it comes to what application they want to have on their systems and the level of customizations that they prefer for their systems. The Mac is available in unique designs and covers which cannot be rivaled by PC in terms of design or style. Moreover the Mac provides the user with a range of customized application software which can be loaded on to the machine at the time of Purchase. A limited number of such applications often are provided by MS Windows based PC at an additional charge.\n\nThe main reason as to why the Mac is so diverse and is able to provide the user with a range of benefits and customization is because of the open source nature of its UNIX and Linux platform. As the Mac OS is open sourced, users can make changes to the system code and develop new application software according to their requirements that can be used on Mac Systems. The sharing of this information enable Apple Inc to provide these newly developed application to consumers in the Mac packages, therefore making the entire Mac offering more customized for the end user. The increased performance, reliability and diverse capabilities of the Mac are also based on its open source nature.\n\nConclusion\n\nThe Macs of today are highly evolved with better reliability, speed, performance security and customizations offered to the users as opposed to the PCs. The consistency of the Macs, along with their predictability, the low level of security and virus threats as well as the increased solutions provided by Apple for Mac users makes Macs a better choice for users instead of a PCs.\n\nReferences\n\nLerner, J., Tirole, J., \u201c The Scope of Open Source Licensing. \u201d Journal of Law, Economics, and Organization , 21.1 (2005): 20-26. Web.\n\nShaffer, G., Zettelmeyer, F., \u201cWhen Good News About Your Rival Is Good for You: The Effect of Third-Party Information on the Division of Channel Profits.\u201d Marketing Science, 21.3 (2002): 273-293. Web.\n\nJun Na Rajaravivarma, V., \u201cMultimedia file sharing in multimedia home or office business networks.\u201d System Theory Proceedings of the 35th Southeastern Symposium, (2003): 237-241. Web.\n\nAlben L., \u201cAt the Heart of Design.\u201d Design Management Journal , (1997): 9-27.\n\nLuca, J., McLoughlin, C., \u201cPeers Supporting Peers through structured bulletin boards.\u201d Digital Voyages, (2003).\n\nCasadesus-Masanell, R., Pankaj, G., \u201cDynamic Mixed Duopoly: A Model Motivated by Linux vs. Windows\u201d, Strategy Unit Working Paper No. 04-012, (2003). Web.\n\nSpeight, E., Bennett, J.K., \u201cBrazos: A Third Generation DSM System.\u201d USENIX Windows NT Workshop, (1997): 95-106. Web.\n\nBitzer, J., \u201cCommercial versus open source software: the role of product heterogeneity in competition\u201d, Elsevier B.V., (2005). Web.\n",
        "label": "human"
    },
    {
        "input": "Boot Process of a CISCO Router and Computer Research Paper\n\nCisco router and any other computer don\u2019t have a very different set of processes for booting if they are not very similar. An understanding of the processes that would help in setting up the configuration of a router and its various elements can lead to relating the booting process of the router to that of any other computer. Both router and a simple computer have a sequence of events that lead to the booting process.\n\nBeginning with a computer, the BIOS (Basic Input/ Output System) is used for ensuring three main functions that make way for booting up of the system. The BIOS provides a set of machine code subroutines. The codes are called by the operating system and various hardware components of the computer are being accessed. The BIOS then causes the initiation of the boot sequence and allows the third process of changing the low-level setup options. The BIOS code is later burned onto a Flash EPROM memory chip installed on the motherboard (Mossywell). In a Cisco router (for example Cisco 2501 router), the Flash memory contains a valid IOS image which is similar to the BIOS of a simple computer. The router is yet to be configured and here also the sequence of events goes through during the completion of the boot process (DiNicolo, 2006).\n\nThe boot process in the router begins with POST (Power-on self-test). The router carries out a POST after being powered up. The purpose of this POST is to check the capability of CPU and router interfaces about their ability to function properly. Next in the line is the execution of bootstrap to load IOS (Input/ Output System). The successful outcome of POST, the router executes the Bootstrap program already burned previously in ROM. The bootstrap searches the Flash memory for a valid Cisco IOS image and is then loaded after the positive completion of the search. In case of non-availability of any IOS image, the router gets booted with the RxBoot limited IOS version found in ROM. IOS loading is basically the loading of the configuration file. The IOS image upon loading searches the NVRAM for a valid startup configuration. In case if there is no valid startup configuration file, the router then looks for System Configuration Dialog which is also called setup mode. This mode also enables the user to perform the initial configuration of the router (DiNicolo, 2006).\n\nThe simpler computers begin their processes after the Reset signal is issued. Various components of the motherboard are made to undergo the checking process. The CPU is allowed to execute codes only after the Reset signal is turned off. Testing and initialization of the hardware are as per the Power on Self Test (POST) protocol and upon successful completion of the same, the BIOS initiates the boot sequence from the hard disk or any other device specified in the BIOS setup. The process begins with ROM BIOS which initiates Power on Self Test. The BIOS discovers the boot device. Upon discovery, the BIOS loads the contents of the very first physical sector of the disk into memory and then instructs the CPU to execute the code. The machine code routines are the most basic part of BIOS with the size of each of the routines differing with different BIOS and are accessed through issuing software interrupts to the CPU. The BIOS loads the interrupt vector table and thereby enables the mapping process between the interrupt number and the corresponding routine\u2019s memory location (Mossywell).\n\nReference\n\nDiNicolo, D. (2006), Cisco Router Boot Process Posted in CCNA Study Guide Chapter 07. Web.\n\nMossywell, Computer Boot Sequence. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Addiction: Side Effects and Possible Solutions Essay\n\nSince 1979 when the first microcomputer came to the US and Great Britain there has been very extensive growth in the areas of computers. Since that time humanity started to speak of different signs of \u201ccomputer addiction\u201d \u2013 the term stands to emphasize the seriousness of the problem and implies the possibility of drastic consequences that computer mania might have. Contrary to existing views the problem of computer addiction exists, humanity needs to be extremely cautious of its bad effects and it has to work out appropriate treatment to help computer-dependent people to solve their problems. In the current paper, we will explore the problem with the emphasis made on the bad effects of computer addiction and possible solutions that might be offered to reduce them to a minimum or to get rid of computer addiction at all.\n\nThe reality of modern life is such that personal computers and Internet access have been proliferating across the world. Everyday activities at home, work, and school are increasingly dependent upon computers. Through the Internet, the computer becomes a useful tool for communication. The importance of computers and the Internet, in particular, can hardly be overestimated but their properties that promote addictive behaviors should not be neglected. Cyberspace that computers and the Internet create may result in cyber disorders such as virtual relationships that evolve into online marital infidelity or online sexually compulsive behaviors. An increasing mental health concern, cyber disorders have been identified to diagnose serious issues related to Internet use (Young & Rogers 25). The side effects that computer overuse has to speak for the fact that computer addiction exists.\n\nMoreover, the existence of computer addiction in modern society is proven by Margaret Shotton\u2019s research. The author conducts an investigation that proves the existence of computer dependency through obtaining the facts from a number of sources, namely, existing literature on the problem, the authors of the literature, and other psychologists, computer studies teachers, and professional care agencies. Personal contact was made with people who worked with computers and average computer users. They all showed belief in the occurrence of the syndrome of computer dependency. Though the respondents did not consider the syndrome to be a widespread problem believing that it affected only a few individuals among the vast number of computer users but the research still confirmed the existence of the syndrome of computer dependency (Shotton 20).\n\nThe research has shown that interviewees had both psychological (inability to stop the activity; neglect of family and friends; problems with them, etc) and physical (carpal tunnel syndrome; dry eyes; migraine headaches; backaches) symptoms of computer addiction. Though most of the dependents did not want to admit their addiction some did admit that they experienced negative effects from their activities. Some interviewees spoke of the negative effects that their computer addiction had on their employment. One person admitted that in the past he had spent time playing with the computer while he was at work. But the addiction disappeared when he bought his own home microcomputer upon which he had unrestricted access to the program as he wished. Another interviewee admitted that it was difficult for him to concentrate at work, instead of handling the problems that appeared he found himself puzzling over computer problems that he could explore only while at home. This turned out to be especially problematic when he was cycling home from work as not once he had nearly ridden into parked vehicles. Computer addiction was observed with people of different occupations, therefore, there is no strict rule as to who can fall victim to computer addiction and who cannot.\n\nWhat one can state firmly is that computer addiction has negative effects on the process of learning. Students of different ages admitted that their academic performance suffered because of their devotion to computing. \u201cMy schoolwork suffers\u201d, \u201cIt detracts from my schoolwork\u201d, were the commonest answers of those who realized they were achieving lower standards academically (212) Therefore, it is also teachers\u2019 concern to help the students get rid of computer addiction.\n\nConstant involvement with computer activities influences the social lives of dependents. Computers take much of dependents\u2019 free time and, as a result, less time is spent communicating with others. Different misunderstanding appears on the basis of the absence of common interest to computing. Many respondents from Shotton\u2019s research admitted that they found computing an escape from social interaction: they found it stressful in the past and computers enabled them to get rid of the undesired communication. Domestic disharmony, problems in sexual relationships \u2013 these were the commonest Negative effects of computer addiction on a family scale (213). To reduce them some help from dependent friends, family members are needed. Their task is to help the addict realize that no machine can substitute real communication.\n\nOther side effects are those on the personalities of the dependents. People addicted to computers tend to be more intolerant and impatient with others. The computer becomes a sort of refuge from real life for them and they become more reserved and restrained. Sometimes a computer-dependent person loses interest in the world around him and does not care about one\u2019s happiness. This occurs because the computer is seen by this person as the only \u201ccreature\u201d in the world that can understand him or her.\n\nThe negative physical effects of computer addiction include restlessness, sleeplessness, tension, headache, and backache. If computer dependents suffer from some disease the disease worsens while constantly working on the computer.\n\nInternet addiction as a form of computer addiction deserves special consideration. An Internet-addicted person loses the ability to control one\u2019s use of the Internet. This lack of control causes marked distress and/or functional impairment (Shapira et al. 270). The addiction may go along with depression, obsessive-compulsive disorder, or alcohol and drug addiction. The most obvious symptoms of Internet addiction include social isolation, family discord, divorce, academic failure, job loss, and debt (Young et al. 475). Other symptoms are: neglecting responsibilities, feelings of depression, tension, anger when the Internet is not accessible, etc. There are five Internet-related issues that Internet addiction is composed of cyber sexual addiction, cyber relationship addiction, net compulsions, information overload, and addiction to interactive computer games (476). No matter what type of addiction a dependent suffers from the side effects of it are always serious. Actually, they are the same as the ones of computer addiction in general. Internet addiction influences negatively relationships within families and may even bring significant financial problems. As it was stated above the dependent\u2019s realization of the problem is needed as well as help from friends, teachers, and members of the family. Still, specialized help is the most effective.\n\nTreatment options for computer addiction include:\n\n  * Cognitive-Behavioral Therapy. It is based on Beck\u2019s (1976) theory that thoughts determine feelings. The theory encourages the dependent to recognize thoughts and feelings causing him/her to inappropriately use the computer to meet personal needs (Orzack, 1999 15). This is achieved through the following stages: pre-contemplation, contemplation, preparation, maintenance, and termination. In the first stage, the dependent does not recognize the problem that he or she has and, therefore, does not want to change anything. Thus, the therapist\u2019s task at this stage is to help the patient understand that the problem exists and help him/her realize the necessity of solving it. In the second stage, the individual recognizes the need for change but still does not have a substantial desire to change something. In the preparation stage, the individual is ready to establish a plan to address the problem. The dependent person realizes that he or she needs to establish control over computer use by setting time limits. The person seeks other activities that can be done when the urge to use the computer exists. The maintenance stage begins when the dependent learn to control one\u2019s computer use. The goal of this phase is to prevent relapse. The final stage, termination, occurs when the individual has no more desire to overuse the computer. During the treatment, the therapist\u2019s task is to support the dependent in every stage encouraging him or her to enter another stage (Wieland 153).\n  * Psychopharmacology. It is aimed at the treatment of depression, obsessive-compulsive disorder, and other anxiety disorders that computer addiction goes along with.\n  * Addictions Treatment and Self-Help Groups. The treatment is organized by special addiction services like the Center for Online Addiction, the Computer Addiction Services at McLean Hospital in Belmont, etc.\n\nSolutions to computer addition may include keeping oneself busy by involvement in work irrelevant to the work of the computer. Hobbies become of much importance when there is a desire to get rid of computer dependency. Psychotherapy, marital counseling, addiction counseling, parent counseling, social skills training are among the solutions available. Though they are applicable depending on this or that particular case, the use of them in the complex is also rather beneficial for the dependent if appropriately organized and controlled.\n\nThus, the paper defined computer and Internet addiction showed their side effects, and offered the most effective ways of solving the problem. Computer addiction exists and requires emergency actions from the dependent and people around him/her. Though the solutions proposed are rather helpful, it is a computer user\u2019s responsibility not to become a computer addict.\n\nWorks Cited\n\nCooper, A. 1999, \u201cSexuality and the Internet: Surfing into the new millennium\u201d, CyberPsychology and Behavior , no. 1, pp. 181-187.\n\nLamberg, L. 1999, \u2018Computers enter mainstream psychiatry\u2019, Journal of the American Medical Association , no. 278, pp. 799-801.\n\nOrzack, M. 1999, \u201cHow to recognize and treat computer addictions\u201d, Directions , vol. 9, no. 2, pp. 13-20.\n\nOrzack, M. H. 2003, \u201cComputer addiction services\u201d, Web.\n\nShapira, N. et al. 2000, \u201cPsychiatric features of individuals with problematic Internet use\u201d, Journal of Affective Disorders, no. 57, pp. 267-272.\n\nShotton, M. A. 1989, Computer Addiction? A Study of Computer Dependency , London, Taylor & Francis.\n\nWieland, D. M. 2005, \u201cComputer addiction: Implications for nursing psychotherapy practice\u201d, Perspectives in Psychiatric Care, vol. 41, no. 4, p. 153.\n\nYoung, K. 1998, Caught in the Net , New York, Wiley.\n\nYoung, K. et al. 1999, \u201cCyber disorders: The mental health concern for the new millennium\u201d, CyberPsychology and Behavior , vol. 2, no. 5, pp. 475-479.\n\nYoung, K., & Rogers, R. 1998, \u201cThe relationship between depression and Internet addiction\u201d, CyberPsychology and Behavior , vol.1, no. 1, pp. 25-28.\n",
        "label": "human"
    },
    {
        "input": "Computer Systems: Technology Impact on Society Research Paper\n\nTable of Contents\n 1. Abstract\n 2. Introduction\n 3. Impacts of the new technology in our societies today\n 4. Conclusion\n 5. Reference\n\nAbstract\n\nTechnology has had so many impacts in our societies. It was with the invention and the adoption of the new technology, which saw the rise of so many communities. This is because technology involves the use of advanced tools which are used in the production of goods and services and a good example to explain this is the use of computers. So through the paper, we will try to see some of the uses of the new technology in our societies today.\n\nIntroduction\n\nIn a broader concept, technology can be taken to deal with the use of knowledge of tools and crafts hence its impacts on the ability to try and adapt and control our environment. Technology has involved the use of science and advanced tools in many economic activities we do today. It is out of this advanced technology which has seen so many communities benefiting so much since those communities that have used the new technology have benefited through effective work. It is due to the role of the new technology that we will try and see some of the impacts of the new technology in most of the societies today. So through the paper, we will try to see some of the uses of the new technology in our societies today. (Abbate, 2000).\n\nImpacts of the new technology in our societies today\n\nOne of the impacts of technology today can be seen with the introduction of computers in many schools. The new technology has been in a position to provide a good learning and effective learning to most of our students hence leading to improved performance by most of these students. This is because the educational technology which is used by many schools has enabled these students to even learn online hence making it possible for most of them to do other activities during their free times. The students can be in a position to even learn through the internet hence can add on their knowledge what they have been told in class. You tend to find that most of the students make use of these computers even at there homes hence you will find that in most of the times, these students will stick in these computers hence try and learn more. It is through the introduction of the computers in the classroom and also at home which has made the learning system to be quite effective for them since it has led to improved performance. These students will try and learn better in less time when they receive computer based instructions. They will also try and develop a positive attitude towards learning with the introduction of such computers in their classes since they would wish to learn more from them hence leading to improved performance by these students. This will eventually lead to educative society which has so many people who very much enlightened are hence leading to community development. This is very much important in any community since you will find that effective labor will be used in the allocation of the available resources hence leading to the growth of the country. (Adams, 2000).\n\nThe new technology has led to the growth of our economy today. You find that due to the global economy, many countries have applied the new technology in the production of their goods and services hence leading to the growth of these societies. An example to explain this phenomena can be seen in the case of internet whereby many companies today in our country has made use of the internet in advertising of their many products. This is one of the key competencies which so many people have applied in their business since the internet is much more common to everyone hence its through advertising through the internet you will find that many people will have access of these products hence leading to the realization of their competitive advantage. So in this case, you find that many business men in our societies use the internet to advertise their many products hence can be in a position to compete so well with the rest of their competitors. It is due to this fact they will be in a position to have a competitive advantage hence leading to the growth of our economy so much. (Agency for instructional technology. 2000).\n\nThe new technology is also applicable in the production of goods and services. You find that so many machines have been used in the production of goods and services hence leading to a more and quality output in the society. An example to explain this is the case of the combine harvesters and the use of tractors by many farmers in their farms. Unlike the tradional ways of farming whereby many farmers use the traditional means of farming say the case of animals to plough their farms, the new technology had been in a position to replace all these things hence leading to increased production in the country. You will find that many farmers in the society have adapted in the use of these machines in the farming and harvesting hence it means that quality products can be produced in these farms hence leading to improved living standards of these people. IT is with the introduction of the new technology you will find that many people can be in a position to make use of quality goods and services which are produced by these farmers hence leading to improved living standards. (Caro, 1998).\n\nBiometric technology is also another technology which has been used in many societies and this is used in trying to prevent criminals in our country. Due to the increase of criminals in many societies which has led to insecurity, the new technology has been used in trying to identify these criminals. This technology may be taken to refer to those technologies which are used to measure and analyze persons physiological and behavioral characteristics and some of the behavioral characteristics include irises, voice patterns, fingerprints, hand measurements and facial patterns. These measurements are usually taken for identification and verification purposes. In the ancient times, identification was usually based on showing something which you already have and some of the things which were used for identification include passport and licence.If one did not have any of these things, other measures were used which include something you know like a pin or a password. It was due to the difficulties in these measurements which were used traditionally which led to the development of biometrics technology which is a new technology and could be used to identify individual behaviors and characteristics. This type of technology has benefited the society so much since it can be used to identify criminals in our societies. This is one of the leading contributors to poor performance of the economy since you will find that many resources were used in trying to fight these criminals. But with the use of the new technology, it has become so much easy to try and get these people hence criminal justice can then be followed. (Commoner, 2000).\n\nDespite the many impacts which the new technology has in our societies, it has also some disadvantages which may include the impact on our environment. You will find that the new technology has led to the pollution of the environment and a good example is the case of the greenhouse gasses which are produced by these plants. This is because they use the new technology which has lead to global warming something affecting our climate so much. The new technology has also led to unemployment since you will find that ,many machines which are used in industries have replaced human labor hence so many people have been rendered jobless in most of our societies. (Drucker, 2005).\n\nConclusion\n\nTechnology is one of the key contributors to our economic growth. This is because due to globalization process, many societies have adopted the new technology hence leading to growth of the community so much. You find that the new technology has been applied by so many societies in their production farms hence leading to more output. It is due to this fact that more inventions and innovations are still going on in trying to venture on more technologies since it has more advantages in our societies than the disadvantages.\n\nReference\n\nAbbate, J. (2000): Inventing the internet. Cambridge, Mass.\n\nAdams, M. (2000): Machines as the measure of men: Science, ideologies and technology. Ithaca.\n\nAdriano, C. (2000): Teachers competence, familiarity, affects and perceived skill. Regarding instructional technology. Princeton, N.J.\n\nBasalla, G. (2003): The evolution of technology. New York. Free Press.\n\nAgency for instructional technology. (2000): A survey of the use of technology with Students at risk of school failure. Bloomington, IN.\n\nCaro, R. (1998): The managerial revolution in American business. Cambridge, Mass.\n\nCowan, R. (2001): The impact of the new technology in education. Journal of economics. Vol. 20(1).\n\nCommoner, B. (2000): Technology and the natural environment. The architectural Forum. Vol. 8(1).\n\nDrucker, P. (2005): technology and business. New York. Free Press.\n\nHafner, K. (1996): Power, pleasure and technology. Boston.\n",
        "label": "human"
    },
    {
        "input": "\u201cESL Students\u2019 Computer-Mediated Communication Practices\u201d by Dong Shin Essay (Critical Writing)\n\nThe research article \u201cESL Students\u2019 Computer-Mediated Communication Practices\u201d evaluates and analyses language learning practices based on computer-mediated communication (CMC). The author concentrates on the problem of joint activities and on \u201chow a group of ESL students jointly constructed the context of their CMC activities through interactional patterns and norms\u201d This topic is extremely interesting to theorists and practitioners because it helps to understand students\u2019 communication and participation in e-learning activities and the best possible ways to design effective language practices. Computer-mediated communication use in language learning is catching on for training and education worldwide at all levels. Computer-mediated communication is not just a trendy word. It is a new approach built on what we have learned from developing and instructing with thirty years of computer-based methods and on what we know about how to help people learn. The concept of Computer-mediated communication is changing the way educators instruct and learn. At the same time, Computer-mediated communication is evolving, and it is likely that what we call e-learning today will be different in a few years. Previous studies discuss the context and environment of language learning and teaching, technologies, pedagogy, curriculum, and social discourses on CMC. The author hopes to add facts and data about the interactional patterns of ESL students, interactional norms the ESL students establishing, and answer the question \u201chow do the ESL students utilize CMC activities for their linguistic, social, and academic goals?\u201d (Shin 65). All the questions are specific and clear based on careful literature analysis and problems identified by previous studies. While inputs are important to the success of learning, they are only part of the story. Instructional designers should concentrate on outputs such as measured increases in job and business performance, either in the quality of goods and services produced or in the job satisfaction of employees producing those goods and services. Solid instruction is concentrated on what people need to learn to enhance their performance as students or workers. A better understanding of what constitutes successful learning and how to achieve it helps managers with the selection of courseware from vendors and helps instructional designers in the creation of materials.\n\nThe research study follows an ecological Perspectives of Second Language Learning. Special attention is given to human learning processes and its environment perceived as \u201can integrated entity involving cognitive, social, and environmental elements\u201d (Shin 66). The author uses interpretive parading of the research. This means that the article is based on personal experience, observational and interaction approaches. Inductive reasoning and deductive reasoning are both subsumed under scientific inquiry, yet they characterize a distinction between purely qualitative and purely quantitative methods. Student teachers\u2019 quantitative ratings of their experiences on questionnaires followed interpretive analyses of their narrative responses. The research design is based on the ethnographic method. \u201cThis ethnographic case study was conducted in an intermediate adult ESL class with 16 students at a university in the northeastern United States\u201d (Shin 66). Ethnography helps the author to study the commonsense features of everyday situations the common, ordinary happenings in a particular set of interests. In these studies, social interaction as an ongoing process is scrutinized and recorded in descriptive detail. The participants of the research are international graduate students, visiting scholars, and their spouses. All of the participants were from Northern Asia (1 student from Peru). All of them have at least a bachelor\u2019s degree. They were selected based on the questionnaire method and personal interviews. Personal information was the main criterion for selection. The main principles were: (1) Asian origin, (2) BA degree obtained in a home country, (3) engineering or natural sciences field of study, (4) \u201cgraduate student and visiting scholar participants were men, and all the spousal participants were women\u201d (Shin 66), (5) good relations with a teacher (Johnson 77).\n\nThe main data collection methods were observations and surveys, formal and informal interviews with participants, e-mail exchanges between the teacher and the ESL participants. also, the data was collected from offline FtF class meetings, In all situations, the researcher acted as a participant-observer. The researcher \u201cidentified recursive patterns through triangulation of field notes, transcripts of recorded FtF class meetings, interview data, and electronically saved chat data\u201d (Shin 67). Interviewing informants involved using phrasing and vocabulary more closely in tune with the subjects\u2019 own and less abstractly than in instruments used in quantitative studies. This, therefore, increased the likelihood of the instrument being able to tap the information for which it was developed. Participant observation was conducted in natural settings that were the reality of the life experiences of subjects more so than are contrived settings of quantitative studies. The analysis in ethnography used a process of \u201cresearcher self-monitoring,\u201d a \u201cdisciplined subjectivity\u201d that brings the study under continual questioning. The data were analyzed systematically and appropriately answered the research questions. The analysis allowed the researcher to reconstruct Interactional Patterns among students.\n\nThe findings show that the teacher should pay special attention to computer-mediated learning environments and communication practices among students because they have a major impact on language learning. \u201cThe participants\u2019 learning experiences in online chatting demonstrate that language learning and language socialization are interwoven into the fabric of CMC practices\u201d (Shin 70). An informal computer-mediated communication resource is the online discussion group. Participants can learn a great deal by formulating and posting queries and by just lurking and observing questions and answers posted by other participants. Some discussion groups archive the replies to questions, and many have a section where key questions and answers are posted. In self-paced e-learning, the learners themselves determine the speed\u2014and sometimes the sequence\u2014of their progress through a professionally developed training course (Morrison 31). Because self-paced courses and materials are designed and developed by training professionals, they are a formal rather than informal method of learning. The researcher identifies that offline lives influence language learning practices and the success of language acquisition. \u201cEcologically exploring the ways illustrates their identities/subjectivities regarding co-constructed norms, rules, and goals, as well as specific interests and concerns embedded in their language socialization processes through CMC\u201d (Shin 70). Generally, students appreciate the convenience, choice, and flexibility that CMC activities offer. Instructional designers value the standardized framework and flexibility of CMC activities. Instructors think CMC activities are convenient; they applaud the ease of record-keeping and the reduced travel that are part of the e-learning revolution. The author finds that learners must first acquire the lower-level skills (knowledge, comprehension, and application, for example) before they can perform the higher-level skills (analysis, synthesis, and evaluation). The different levels are also taught differently. The higher-level skills require carefully designed application exercises to ensure that the learning has occurred and has \u201cstuck.\u201d For example, if the focus is on learning to synthesize the concepts of e-learning as presented in this book, an effective learning program would ensure that participants acquired the basic knowledge of the four types of e-learning, were able to explain them either orally or in writing, could explain the relationships between e-learning and conventional learning, and would be able to synthesize or use this knowledge in a specific situation to recommend what types of e-learning to use. The first step to understanding learning, in general, is an understanding of learning styles. People have ways they prefer to learn, which are often called preferred learning styles. The most effective teaching methods, whether in the classroom or through a computer, are those that accommodate the preferred learning styles of the people being taught.\n\nThe author concludes that CMC activities are seen as language socialization and help students to understand complex lives and \u201ca complexity that comes from multiple social roles\u201d (Shin 70). There are no conclusions made on the researcher\u2019s guesses thus he states that it is important to examine \u201chow language learners carry their interests and life stories over to online language learning spaces\u201d (Shin 70). All recommendations follow logically from the findings and data analysis. There are many schools of thought about learning styles, but no universally accepted approach to defining them or adjusting instructional designs and methods to account for them.\n\nThe research study proposes an interesting analysis of computer-mediated learning and language acquisition practices. I would like to ask the researcher about problems he was faced during the research and the attitude of students towards this research.\n\nThe research is well-constructed and based on appropriate methodology. Thus, the researcher ignores external validity. First, the purpose is to describe in detail aspects of a single subject, group, or unit. And, even if multiple sites are used, the researcher is obligated to enter each site as if he or she had no other information and as if this site were unique. Therefore, there are no bases for comparison or generalizability. Third, the problem studied, the nature of the goals and the application of the findings differ substantially from traditional quantitative methods, and so definitions of external validity must vary. Concerning the problem studied, the credibility of quantitative designs should be based on examining effects in controlled situations, looking at variables uniquely, one at a time. Regarding the goals of studies, the goal of ethnographic research should be to develop theory not to test it, which requires that a priori relationships be avoided. While quantitative researchers aim to generalize from the sample to the population, and external validity must be (Garrison and Anderson 23).\n\nThe author did not consider the different learning styles. First, it is critical to identify the predominant learning styles of the people who will be taking the e-learning program. Research indicates, for example, that recent graduates from Asian countries prefer concrete action. The same research indicates that instructors tend to prefer an abstract approach to learning. In other words, young people prefer to learn by doing, while instructors prefer to learn by developing concepts. Potential trouble here! At least there could be trouble if instructors teach youth in the way that they personally prefer to learn. The researcher materials must also take care to ensure that their own preferred learning styles do not govern the instructional design. The researcher must consider the learning styles of the people who will be taking the program\u2014and they could be convergers, divergers, assimilators, or accommodators, or likely a mix of all four (Morrison 102).\n\nMuch has been written about the virtue of interactive learning\u2014using questions, exercises, and other activities to engage learners as active participants in the learning process. The research under analysis does not take into account the nature and types of tasks and different perceptions of students. Interactive learning should keep students energized and help participants absorb information and remember it. It would be important to analyze how interactive learning help students focus. To understand how this works, consider that the human brain functions five or six times faster than instructors speak or e-learning audio files play. If a classroom instructor, an online instructor, or an e-learning module limits the messages to facts, participants whose minds are working five times as fast as the information is being delivered will start to draw their own conclusions\u2014and perhaps daydream about subjects not related to the material being taught. Clearly, interaction is a valuable component of a successful learning experience. But how to create interaction in e-learning? As an instructor, instructional designer, or administrator, you must think clearly about interactivity. also, the researcher does not take into account the fact that carefully designed learning activities challenge learners, even in a self-directed format. Games are one engaging activity from the classroom that can be applied to e-learning, and there are several others. Learning works best, you have been taught, when the designer follows a structured process of needs analysis, development of performance objectives, rigorous development of content, a structured delivery, and evaluation.\n\nWorks Cited\n\nGarrison, D. R., Anderson, T.R. E-Learning in the 21st Century: A Framework for Research and Practice. RoutledgeFalmer, 2003.\n\nJohnson, D. M. Approaches to Research in Second Language Learning. Longman Publishing Group, 1992.\n\nMorrison, D. E-Learning Strategies: How to Get Implementation and Delivery Right First Time. Wiley, 2003.\n\nShin, D., ESL Students\u2019 Computer-Mediated Communication Practices: Context Configuration. Language, Learning & Technology, 10 (2006), 65-73.\n",
        "label": "human"
    },
    {
        "input": "Marketing: Graphic and Voice Capabilities of a Computer Software Technology Essay\n\nThe graphic and voice capabilities of computer software have been improving with passing time. More and more software is being developed that is mimicking human intelligence, capabilities of locomotion, vision, etc (John, 2007). Scientific and technological advancements in the artificial intelligence domain are developing specialized systems to mimic human nature as well. It is not difficult to now imagine the development of software that can mimic human voice along with the variations in the emotions of the dialogue.\n\nIf such software is designed that can mimic human voice along with the variations in emotions, it would prove to be a breakthrough in many domains. This product could easily be incorporated into the virtual reality systems that are being used for training by many industries such as the airline, defense, etc. Another possible application of this product comes in the user interface for many customer care systems, making them more personal and real lifelike. Moreover, small-scale usage by consumers can include usage by babysitters to mimic the voice of the mother in front of toddlers when the mother is not around. This product could also be used in games to give real-life experience while gaming.\n\nInternet would be one of the basic promotional bases for such a product. Setting up a website that provides a trial version for this product would be fruitful in making the target audience understand the wonders of this product. Also, as the internet proves to be a medium where there are no geographical boundaries; selling it online would increase its market coverage to the whole world (Don, 2008).\n\nBe it the media industry or the police, such software would be welcome with open arms because it would help both areas in imitating individuals according to their needs. The Game Development industry would be one of the biggest customers for this software; as games become virtually real, the need for such software to bring more life to the game is evident. It would be of great help to the federal and the police departments for the training of their personnel for combat situations, crisis conditions, and other training procedures. The virtual reality systems used in these departments will become more sophisticated for training, hence making it an ideal target as well.\n\nA typical consumer can be a person from the navy who is being trained with the help of virtual reality systems to a sibling who is using this software to mimic the voice of her/his mother in front of her/his baby sister in the absence of their mother. In military training systems, the software would be catering to individuals coming from different backgrounds and lifestyles, pertaining to somewhat the age of early 20s. When it comes to household usage, the software would be used by the upper class living in lavish conditions. The product could be used by anybody ranging from a father to a teacher in a pre-school to use it to take care of the toddlers.\n\nBranding such a product would require specific strategy decisions from the software developer. A name that best describes the product characteristics would make it widely acceptable by the wide genre of consumers that it can cater to. Using a \u2018Family\u2019 branding strategy would be fruitful in this case (Stephen, 2002). The company that develops this product would have its name in the brand such as \u201cXYZ Emotelligent 1.0\u201d; XYZ being the name of the company.\n\nWorks Cited\n\nLinda Pinson, Jerry Jinnett (1996). Target Marketing: Researching, Reading & Retaining Your Target Market. Dearborn Trade Publishing.\n\nStephen Coomber (2002). Branding. Capstone Publishers.\n\nDon Schnure (2008) The Advantages of E-commerce for your Business. Web.\n\nJohn McCarthy (2007). What is Artificial Intelligence. Web.\n",
        "label": "human"
    },
    {
        "input": "Computers in Education: More a Boon Than a Bane Essay\n\nTable of Contents\n 1. Introduction\n 2. Main body\n 3. Conclusion\n 4. References\n\nIntroduction\n\nThere can be no doubt that computers present modern-day children with unprecedented opportunities to add fun and excitement, precision and accuracy, speed of comprehension, and comprehensiveness to their education. It is also common knowledge that computers, especially when connected to the internet, present distractions that could lead to the self-destruction of even adults, not to speak of young impressionable, sensitive, vulnerable children. Parents and teachers, however, sometimes forget the fact that the use of computers by children could lead to certain physical, emotional, and psycho-social problems. This paper surveys the ways in which computers can aid education, and, while taking note of the common dangers of computer use among children, arrives at the conclusion that computers, wisely used, under proper guidance, are more a boon than a bane to young minds that hunger and thirst for knowledge.\n\nMain body\n\nComputers are a fact of life in every area of modern life, and there is no wishing away their impact on the minds of children. In such a context, it would surely be foolish not to make use of computers in education. The enormous benefits of computer-aided education cannot be over-emphasized Most schools in the developed world use some kind of Information Communication Technology in their classrooms. Information Technology is also a core subject that is taught and tested in many schools. The infinite resources of the World Wide Web can be and are being used to great advantage in almost all schools. Most schools, moreover, have a system by which children can communicate with their teachers online, from home, or elsewhere. They can use the system to raise questions, clarify doubts, even submit assignments from remote locations; and it is an attested fact that most children, (if not all teachers) soon become enamored of this manner of learning and teaching.\n\nChildren who have become used to the sight of computers at home are fascinated by them and most parents find that the computer is used as an educational tool by the children, even without their suggesting it. Once children get \u2018the hang of\u2019 a computer, they quickly become aware of the various uses they can put it to, and even learn for themselves what they need to do when the operating system \u2018hangs\u2019. Thus, one of the greatest advantages of the computer as a tool in education is the fact that it builds the child\u2019s capacity to learn things independently. It is a well-known principle of pedagogy that the things that one learns on one\u2019s own initiative serve to enhance both one\u2019s core knowledge and to strengthen one\u2019s capacity to increase it. As Richard Feynman tells the schoolteacher in his account of one of several adventures of a \u2018curious character\u2019: \u201cYou\u2019ll have to go ask the girls\u2014they understood what\u2026it was right away\u201d (Feynman 1992, p. 44). Some children, however, may perhaps need to be guided to appreciate the vast educational possibilities of the computer. Such children may need to be introduced to the techniques of creating and editing word documents or spreadsheets; they may need to watch someone else at work before creating a drawing using the \u201cPaint\u201d tool on the computer, but no child ever seems to need any instruction on how to play computer games.\n\nComputer games can serve a double purpose when such programs are used to support educational initiatives. The child sees it as a game, as something that he can enjoy, as something that he might approach in a competitive spirit, and finally after finding that he could easily excel in it, might be surprised to know that it had fulfilled an educational purpose too! Teachers and parents have discovered the technique of using computer game activities to sugarcoat lessons and have seen for themselves how easily educational pills are swallowed along with the syrup of computer games.\n\nBlinded by the miracles of the technological revolution, one might perhaps need to be reminded of the possible dangers presented by the ready access to such \u201cnew worlds at the speed of light\u201d (Coveney and Highfield 1991, p. 78). Some of these dangers relate to the use, misuse, abuse, or overuse of computers by children and the possibility of adults/ amoral children maliciously targeting innocent children. Fritjof Capra\u2019s statement in a different context appears particularly relevant here: \u201cit should by now be abundantly clear that unlimited expansion in a finite environment can only lead to disaster\u201d (Capra 1982, p. 223).\n\nOne would only need to picture a child who might start off by using the computer for legitimate reasons of study, hoping to gain an A+ grade. Such a child would initially use the system and the attendant facilities of available accessories and the internet purely for purposes of genuine study and research. The next stage would begin with the rationalization that, since the computer is capable of a large number of simultaneous functions, there could be no serious objection to leaving one\u2019s mailbox or chat window open for ready access, whenever one would need to \u2018take a break\u2019 from learning. Such a child might then begin to think that since the computer offers the facility of listening to music while one is engaged in one\u2019s research, one should make full use of such an excellent property. Within weeks or months, or even days or hours, the situation might degenerate to one in which research/learning provides the break in instructive activities such as chatting, emailing, or downloading music.\n\nSome children might make the mistake of over-using computers\u2014such as by basing all their learning activities (work on assignments and seminars or preparing for tests and examinations) on information that can readily be gained, frequently in a predigested form from Wikipedia or Spark Notes . This is perhaps the main reason why children fail to acquire the real knowledge that should last to serve them a lifetime. Such an advantage would have accrued to them if only they had made intelligent use of the resources available in libraries in the form of books, journals, magazines, pamphlets, and reports, in addition to the resources available online. Overuse of computers can cause health problems too\u2014it can lead to eye fatigue, radiation syndrome, carpal tunnel syndrome, and various aches and pains of the body, especially of the vertebral column. Specialists of all systems of medicine emphasize the fact that such health problems caused by the use of computers can be avoided by the simple expedient of taking short breaks while working on the system.\n\nAs everyone knows, the various resources of the internet can be and are frequently abused. One hears more and more these days of cyberbullying, cybersex, and various other ways in which children can be exploited via the World Wide Web. There have been cases of cyberbullies driving their victims to suicide; students have also entered into suicide pacts or joined suicide cults as a result of the attractively packaged information made available on certain websites. Other children have found their personal details (revealed by them to persons whom they had considered worthy of trust) splashed in the public webspace. Some have been surprised to find \u2018doctored\u2019 pictures depicting them in poses they would never even have imagined in their wildest dreams. Others are lured by strangers who reveal attractive but false bits of \u2018information and ultimately end up in extremely vulnerable situations.\n\nThe solution to most of the problems that may accompany the use of computers in education is proper adult control, supervision, and guidance. Teachers in school and parents at home should realize the need to make responsible decisions for and on behalf of the minor children. Adults should personally monitor (with the help of software or hardware if necessary) the activities that the children under their care engage in, on computers. A few simple safeguards may go a long way in ensuring this\u2014the placing of computers in locations of public view, the regular monitoring of computer activity, and above all, the creation of an atmosphere of trust in which the younger and the older generation find that each can help and be helped by the other.\n\nIan Jack speaks of how \u201cduring our school lunch hours in the late 1950s, a group of us would amuse ourselves by crossing the playing fields and following a small river that ran through a culvert under the railway, to emerge by the side of what had been a square reservoir, now empty of water and full of weeds\u201d (Jack 2005, p. 11). This activity probably helped Jack to make some sense of his life and the universe at that point in time. However, the twenty-first century would definitely find young George\u2019s prescription of computer-aided learning of physics more appealing than the vignette painted by Jack, for \u201cthat\u2019s what you [now] need to understand the Universe around you\u201d (Hawking and Hawking 2007, p. 289).\n\nConclusion\n\nThere can be no better exposition of the invaluable benefits of computer-mediated education than this book co-authored by the great Stephen Hawking, in which the protagonist George, wins an inter-school competition and wins over his science-fighting technology-hating father to a more rational approach to the use of Science for the good of humanity and the good of the planet Earth. One can only wish that every child in the world could be welcomed to the computer-aided learning adventure that education can and should be, with the words used by Cosmos the computer to young George in Hawking\u2019s story: \u201cWelcome\u2026to the Universe\u201d (Hawking and Hawking 2007, p.42).\n\nReferences\n\nCapra, Fritjof. (1982). The Turning Point: Science, Society, and the Rising Culture . London: Flamingo.\n\nCoveney, Peter, and Roger Highfield. (1991). The Arrow of Time . London: Flamingo.\n\nFeynman, Richard P. (1992) \u201cSurely You\u2019re Joking, Mr Feynman!\u201d:Adventures of a Curious Character . London: Vintage.\n\nHawking, Lucy, and Stephen Hawking. (2007). George\u2019s Secret Key to the Universe . London: Doubleday.\n\nJack, Ian. \u201cMotley Notes.\u201d Granta 89: The Factory , 2005, pp.7-11.\n",
        "label": "human"
    },
    {
        "input": "State-Of-The-Art in Computer Numerical Control Essay\n\nIntroduction\n\nComputer Numerical Control \u2013 CNC machines have become very popular in the past few days since they offer acceptable reputability of the machined parameters, allows many operations to be combined, allows machining in more than three axes, carries out the process with very little human intervention and a single operator can operate many machines. The paper provides an in-depth analysis of CNC machines and automated machining.\n\nCharacteristics of CNC Machines\n\nA CNC machine, depending on the type may have two or more axis that can be controlled by a computer program. A CNC machine is defined as \u201cA system in which the actions are controlled by direct insertion of data and the system must automatically interpret and carry out the instructions\u201d. A dedicated computer is built into the control system of the machine and is connected to the servo controllers that provide motion to the machine axes. Based on the program written in G Codes, the axis would move to the new position at the required feed while the machine spindle would rotate at the specified cutting speed. Depending on the type of machine, the turret may have a number of tools that are fixed in special tool holders, and the distance from the tooltip to the spindle seating face is entered into the computer and this is called pre-setting.\n\nThis distance is called the offset and the spindly axis will move back or forward to ensure that only the tooltip, tool sides are in contact with the faces to be machined. A CNC machine would have multiple programmable axes such as X, Y, and Z and in addition, the machine table which can be programmed to turn so that a fresh face is presented when required. By using proper tooling and design and depending on the number of setups required, it is possible to load more than one component on the different faces of the tooling. The machines can have two fixtures, one that is inside the machine with components and undergoing machines and the other outside so that un-machined components can be loaded and kept ready for the next cycle so that idle time is kept to a minimum (Pabla, 2007).\n\nCNC machines can also be integrated with CAD and CAM processes and it is possible to convert a CAD design into a programmable set of instructions so that the required dimensions can be machined.\n\nPrimary processes and their characteristics\n\nCNC machines can be used for performing a number of operations such as milling, drilling, boring, tapping, reaming, spot facing, turning, EDM, grinding, and so on. Materials that can be machined include castings, forgings, bar stock material, roughing operations for dies and molds, stamping, drawing, plastic and wood components for prototyping, and so on. When it comes to CNC machines there is a different class of machines and these are designated as per the machining operations that they can carry out. The different primary processes are grinding, drilling, boring, milling, turning, EDM, drilling/ boring and tapping, metal spinning, deep drawing, and others. The different primary processes and the machine types are given in this section (Smid, 2005).\n\nCNC Turning\n\nCNC Turning machines are special types of lathes that are used to turn stock material, castings of odd shapes, forgings, and other components. These machines have a rotating spindle that can be made to rotate at different surface speeds. A turret is placed at the back and it can have multiple tool holders that are used for turning off the external and internal diameters, internal and external threading, fine boring, internal grooving, circlip machining, taper turning, and others. The machines are limited by the maximum dimension and weight of the component. Maximum size refers to the maximum swing over the bed that can be accommodated and the component has to swing clear of the bed while rotating so that the component does not dash against the bed while rotating. Other factors to be considered are the maximum stock to be removed, the number of tools required in one setup, tooling required, and so on. Cutting tools that are used include carbide inserts, cermets, HSS tools, and diamond insert tools that can only be used for nonferrous machining (Stephen, 2008). The following figure shows the layout of a typical CNC lathe.\n\nFigure 1. CNC Lathe layout (Stephen, 2008).\n\nWhile machining longer components such as rods and bars, a dead center, as shown left side should be used as this gives proper support for the job and reduces problems of overhang. Threading of different forms such as metric, inch, NPTF, and others can be cut and the required pitch can be maintained. These machines come equipped with coolant so that the machined component can be cooled and the burr can be removed. By using rigid work holding devices, complex-shaped parts such as castings, small housings can be machined for the bore, internal and external circlip grooves, and other geometries. Typically components such as shafts, propeller shafts, driveshafts, pulleys and others can be machined. There would essentially be two setups since the side that is clamped in the spindle would have to be reversed and machined in the second setup (Stephen, 2008).\n\nCNC Milling and Machining Centres\n\nCNC milling and machining centers are the workhorses of the manufacturing industry and are used to perform a number of operations. The main types of CNC machining centers are horizontal and vertical. While CNC milling is used to perform only milling operations, CNC machining centers perform a number of operations such as milling, drilling, boring, internal threading, slot milling, and many others. The machines are used to combine many operations and drastically reduce the setup time. They are most useful when the parts require complicated tooling, different faces have to be machined in the same setup, different bores need to be aligned with a common reference point, and when the requirement is for a medium output. Since these machines have a central table on which the component can be clamped, it is possible to machine all the faces by just reloading the component so that different faces are exposed.\n\nIt is essential that clamping must be done on a flat surface of the component and that the required master dowel holes are machined so that subsequent operations take the location of these master dowels. The machines have an automatic tool changer in which a number of tool holders and qualified tools can be placed. The tool holders are designated with numbers such as T1, T2, and so on and the required tool can be called in the CNC program. A tool changer is used to remove the tool from the spindle and load a new tool as per the requirement. By employing appropriate G codes for the program, it is possible to carry out canned cycles, peck drilling, contour milling, and even cam lobe milling.\n\nWhile three axes machines are common, machines with 5 axes are also available. Some manufacturers also offer very high precision machines for jig boring operations that are used in tool rooms for the manufacture of jigs and fixtures, dies and molds, and other precision parts. Internal threads are cut using taps of the required pitch and thread diameters and special tapping attachments need to be used. In tapping, the tap is fed as per the required feed rate while rotating clockwise, stops at the end of the stroke, and then rotates anticlockwise and retracts the tap to produce a fully finished thread. The machines can be used for machining of castings, housings such as crankcase, gearbox, fuel pumps, governor housing, water pump casing, flywheel housing, gear case cover, cylinder block, and cylinder head, and many others (Bannister, 2006).\n\nRotary Tables\n\nRotary tables are a key component of the CNC machining center and jigs and fixtures, as well as components, can be clamped on them so that machining can be done. The devices are classified as per the table size, the weight they can accommodate, type of servomotor used, number of indexing possible, and mounting diameter. The tables are provided with a flat face that is hand-scraped for perfect flatness and very low face run out. Machines with the tiltable axis are also available and these can be used for machining inclined and angular holes and faces. Depending on the type of machine and control, the table can be either programmed to index at a rapid rate to the required rotation angle or it can have a programmable ax so that the table can be given a feed when it is rotating and this facility is used to mill cam lobes (HAAS, 2008).\n\nFigure 2. Indexing Table (HAAS, 2008).\n\nWork Holding Devices\n\nCNC fixtures are usually of modular design and allow additional elements to be added or replaced as per the component requirements. The following table shows the types of fixtures used for CNC machining centers.\n\nTable 1. Types of Fixtures (Rong, 1999).\n\nThe fixtures need to have a master locating dowel so that the component can be seated accurately and the dowel pins also serve as the reference point for the machining program. The fixtures should be rigid enough to withstand the severe vibrations from the machining process and the clamping should not cause the component to be distorted.\n\nFigure 3. The layout of Fixture for CNC Machining Centre (Hayden, 1997).\n\nA modular fixture with multiple tools is shown below.\n\nFigure 4. Modular Fixture with multiple components (Doosan, 2008).\n\nTooling\n\nTool Holders play a very critical role in the accurate machining of components. The tool holders have an ISO or a Morse taper on the outside and these are pushed into the machine spindle by the tool changer. Locking lugs at the sides prevent the tool holder from rotating or getting dislodged in the spindle. A locking stud at the back helps the tool holder to be locked into position during machining. The tool shank can be pushed into the tool holder and a tang ensures that the tool remains firmly seated in the tool holder. Different types of tool holders are available as per the tool diameter, spindle taper size, and others. For a specific machine, since the spindle size is fixed, all the tool holders would need to have the same external taper. The following diagram gives an illustration of different tool holders.\n\nFigure 5. Different Tool Holders (Doosan, 2008).\n\nSpindle Power\n\nSpindle power defines the power of the machine and specifies the maximum amount of material that can be removed, the maximum size of the tool holder, and the machining parameters that can be used. The spindle power is specified by the maximum speed in revolutions per minute and the power expressed in kilowatts and the power is a direct function of the servo motor that is used to drive the spindle. The following graph shows the performance of the machine with reference to the spindle speed, torque, and power output.\n\nGraph 1. Spindle Speed, Torque and the Power (Madison, 1996)).\n\nIn the graph, it can be seen that as the speed increased, the available power would increase but the torque would reduce. For this reason, when using heavy milling cutters or when the depth of cut is more, lower speeds are used.\n\nMachine specifications of a typical CNC machine\n\nWhile there are many parameters that are used for specifying a CNC machine, some important parameters to be used include the table size, the maximum size of the component, number of axes, spindle power and speeds, number of tools available in the tool magazine, the maximum size of the tool and so on. The following table gives a typical specification for a CNC machine.\n\nTable 2. Specifications of a CNC Machine (Doosan, 2008).\n\nMarket Analysis\n\nThe CNC market is projected to be worth more than 5000 million dollars and the global demand varies with the economies of different areas. Countries such as India and China that have seen increased outsourcing of components from the US and Europe show an increase in demand. Please refer to the following graph that shows the trend.\n\nGraph 2. The World CNC Market (ARC, 2008).\n\nThe report by the ARC advisory group suggests that many leading manufacturers such as Dixie, HAAS, Traub, Vomard, Makino, BFW, HMT, and others provide customized services that address the needs of the low end and low cost to high cost and high precision machines.\n\nConclusion\n\nThe paper has analyzed different types of CNC machines and conducted an in-depth study of tooling, work holding devices, spindle power, and other parameters for the CNC machining centers.\n\nReferences\n\nARC. 2008. CNC Worldwide outlook. Web.\n\nBannister Ken. 2006. Programming of CNC Machines: Student Workbook 2 edition. Industrial Press, Inc. ISBN-13: 978-0831131623.\n\nDoosan. 2008. Doosan Infracore Machine Tools. Web.\n\nHAAS. 2008. Rotary Indexing Table. Web.\n\nHayden David. 1997. 7 Easy Steps to CNC Programming. A Beginner\u2019s Guide. Haydenpub.Com; Spi edition. ISBN-13: 978-0970530905.\n\nMadison James. 1996. CNC Machining Handbook: Basic Theory, Production Data, and Machining Procedures. Industrial Press, Inc. ISBN-13: 978-0831130640.\n\nPabla, B.S. 2007. CNC Machines. New Age International Publications. ISBN : 978-81-224-2019-7.\n\nRong Yiming. 1999. Computer-Aided Fixture Design: Manufacturing Engineering and Materials Processing Series/55 (Manufacturing Engineering and Materials Processing). CRC Publications. ISBN-13: 978-0824799618.\n\nSmid Peter. December 20, 2005. CNC Programming Handbook, 2nd Edition. Industrial Press Inc. ISBN-13: 978-0831131586.\n\nStephen. 2008. Factory automation: Siemens equips Kia Motors first European factory. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Adventure Games Analysis Term Paper\n\nIntroduction\n\nAn adventure game is a category of computer entertainment software or video game, featured by examination, examination, solving of different puzzles, communication with game characters, and a concentration on narrative rather than reflex-grounded challenges. It is necessary to note that this expression is in no way related to adventure movies, and adventure literature, and is not analytic of theme or issue matter. (Adventure Game, 2008)\n\nHistory\n\nThe adventure games became quite admired during the late 1980s and early 1990s, and most gamers and developers regarded them to be among the most technically higher kinds. While few developers go on producing adventure games, some are still being liberated, and the adventure game genre has had some components that carry over into other genres. The expression \u201cadventure game\u201d is applied with the same notion in North America, Europe, and Japan, and is viewed as a pure genre in all regions. (Deubel, 2006)\n\nKing\u2019s Quest \u201cbroke\u201d the genre (1984)\n\nKing\u2019s Quest is an adventure game sequence made by the company Sierra Entertainment.\n\nThe popularity of this game is stipulated by the fact that the world of King\u2019s Quest is complicated and immersive. It includes lots of various kingdoms and paranormal empires. Technically, the series founded the use of animatronics and pseudo-3D surroundings in adventure games, so that the main character could, for instance, walk behind objects. (King\u2019s Quest, 2008)\n\nManiac Mansion, SCUMM engine (point & click) (1987)\n\nManiac Mansion is a game initially discharged in 1987 by Lucasfilm Games.. It was the game for which the SCUMM (\u201cScript Creation Utility for Maniac Mansion\u201d) engine was elaborated. (Maniac Mansion, 2008)\n\nMyst popularized the genre to casual gamers (1993)\n\nThe Myst permission consists of a sequence of video games and novels, midpoint on the plot of Atrus and his family, who are successors of the fallen D\u2019ni empire \u2013 a subterranean city whose nation could connect to other worlds by writing an explanatory book about that world. In 1993, after the release of the first game, people were also attracted by the fact that the game was created after the novel, and the plot had been animated on the screen. (Mackereth, 2000)\n\nThe decline of the popularity in 1990 th\n\nFor much of the 1980s, adventure games were one of the most admired genres of computer games created. However, their promotion share radically refused in the mid-1990s; action games took a superior sector of the market, mainly first-person shooters such as Doom and Half-Life which increasingly started attributing strong, story-arranged solo games. This slouch in reputation led many publishers and developers to see the adventure genre as economically impracticable in contrast. (Smith, Lachlan, 2003)\n\nOnline-based games (1997)\n\nThe appearance of online games started the new era of adventures, as gamers could not only challenge the Artificial Intelligence of the bosses, but from now on they had the great opportunity to play with real people. Online games can enable players to cooperate and compete with each other on a grand scale, and sometimes to interact meaningfully with people around the world. They include a variety of gameplay types, representing many video game genres.\n\nTelltale games revitalizes the genre (2004)\n\nThe company was founded by a group of previous LucasArts workers who had been working on Sam & Max Freelance Police , a series to the 1993 game Sam & Max Hit the Road , previous to its termination on March 3, 2004. The company\u2019s chief executive is Dan Connors who leads the company next to cofounder and CTO Kevin Bruner. The Telltale Games team has a large joint practice of working on distinguished adventure games, some of which comprise Grim Fandango , Monkey Island , and Sam & Max . (Adventure Game, 2008)\n\nWell Known Representatives of the Genre\n\nKing\u2019s Quest\n\nThe main personalities in the sequence are King Graham, initially a knight of Daventry who won the throne of the realm through chasing and solving puzzles, and his family. Because of this and the accent on RPG and action components in Mask of Eternity , lots of fans reject to regard this game as a true sequel. Mask of Eternity also did not bear a number in the English-speaking version, as did all the number of other games from the sequel, but most had been translated for more than 10 languages.\n\nThe secret of Monkey Island\n\nThe game initiates Guybrush Threepwood, a young man who wishes to become a pirate. At the start of the game, he finds himself on a Caribbean island.\n\nGuybrush meets the Pirate Leaders who set him three examinations to prove himself as a pirate: beat the island\u2019s swordmaster in death sword fighting, pilfer a statue from the Governor\u2019s house, and discover buried gold. During the plot, he meets some interesting characters, comprising swords master Carla, Meathook (a guy with hooks on his hands), a hostage named Otis, and, most considerably, the gorgeous Governor Elaine Marley.\n\nMyst\n\nThe game permits the player to move along fictional worlds by clicking on preferred images. On the technological side, a 360-degree panorama system was innovated, permitting the gamer to look around easily, with video clips flawlessly incorporated.\n\nPhaser-type: type commands to the game\n\nThis original way of puzzle-solving was stipulated by the technical opportunities, of computer technologies. As the WYSIWYG appeared much later, all the commands, including commands for the games had to be typed.\n\nPoint & Click\n\nAs has been stated above, Maniac Mansion became the first game, using the P&C system. It has become the revolutionary conception, as it took less time for making the necessary action, and gamers could focus on the game and plot more.\n\nKeyboard & Mouse\n\nThe latest and widely used method of playing became the combination of the two previously described methods. The fact is, all the actions may be done only with a mouse, but to accelerate the process, the gamer may use keyboard shortcuts and hotkeys. Sometimes the amount of the commands and functions is too numerous, and all of them can not be appointed for the mouse.\n\nThe Impacts of Video Games\n\nSingle playing and anti-socialism\n\nIt is claimed, that people, who play games alone lose the ability to communicate and live in society. But it has been also claimed, that all the facts of anti-socialism are the consequences of improper nature training and education.\n\nPuzzle-solving\n\nSolving quests and puzzles enhance logic and train an abstract way of thinking. This significant fact in no way should be concealed. It is necessary to mention here, that all the adventure games include some puzzles in the plot, and the difficulty of these puzzles varies and increases from level to level. Solutions to puzzles may necessitate recognizing prototypes and making a particular arrangement. People with a high inductive reasoning ability may be better at resolving these puzzles than others. Puzzles grounded on the procedure of investigation and discovery to accomplish may be resolved quicker by those with good deduction skills.\n\nPlot and storytelling\n\nGood storytelling keeps the gamer thrilled and interested in the faster completing of the game. As sooner as it is completed, the sooner one would get back to normal life. The games may be useful for children also. Children who have difficulties with attention, self-regard, and boundaries are often assisted by the gaming practice, and video games are now being applied to treatment for these types of children. Children that regard themselves as breakdowns also get assistance from playing video games, as they offer the player a common sense of chipping in and exhilaration in basic life-like locations. Video games in addition \u201cempower\u201d some children who have a complicated time in societal circumstances. For instance, a child who is an outsider may gain a social position as he is a video game devotee.\n\nFrustratingly Difficult\n\nThe game with easy tasks and a simple plot will never be completed. It is an axiom. But the difficulty should be regulated, and do not exceed the reasonable extent, as the failure may cause depression, and more serious consequences with devoted gamers, or just one, who went up crazy with games\n\nReferences\n\nAdventure Game (2008). In Wikipedia, the free encyclopedia . Web.\n\nDeubel, P. (2006). Game on! Now Educators Can Translate Their Students\u2019 Love of Video Games into the Use of a Valuable, Multifaceted Learning Tool. T H E Journal (Technological Horizons In Education), 33 (6), 30.\n\nKing\u2019s Quest (2008). In Wikipedia, the free encyclopedia . Web.\n\nMackereth, M., & Anderson, J. (2000). Computers, Video Games, and Literacy: What Do Girls Think?. Australian Journal of Language and Literacy, 23 (3), 184.\n\nManiac Mansion (2008). In Wikipedia, the free encyclopedia . Web.\n\nMyst (Video Game) (2008). In Wikipedia, the free encyclopedia . Web.\n\nSmith, S. L., Lachlan, K., & Tamborini, R. (2003). Popular Video Games: Quantifying the Presentation of Violence and Its Context. Journal of Broadcasting & Electronic Media, 47 (1), 58.\n\nThe Longest Journey (2008). In Wikipedia, the free encyclopedia . Web.\n",
        "label": "human"
    },
    {
        "input": "Recommendations for Computer to Purchase Essay\n\nThe computers at our organization have been giving us problems, the problems we have been experiencing with our computers have made us to look for a solution and we had to look for a better computer model. There are a number of things I had to consider when deciding on which model to purchase. I have taken into account the practicality of usage of the computers, durability, attractiveness, maintenance services and the worldwide reputations as well. I finally decided on the Dell because it has a good temperature control system and virus security which is excellent for the work our organization is involved in.\n\nIntroduction\n\nIn a company which employs many people, there a number of issues which affect the performance of the workers and therefore contribute towards improving the morale of the employees for the job they may be doing. Apart from the salaries they are given, the other factors that affects the employee\u2019s performance is the work condition. This may be in terms of the equipment they use for the job, that is how efficient, effective, how good are they for doing the job. Currently in our department we have been using some computers which have had problems of overheating and shutting down anyhow. This problem of overheating has really taken away our morale due to the many problems this usually cause for us as we usually lose data when this happens. This made me to look into and compare the different models of computers which can be good for the kind of work we do. I did this by making comparisons of features of the models from different manufacturers.\n\nThe main Factors that I considered when looking for the models to buy are:\n\nAfter going through the websites of different computer manufacturers and sending enquiries as to the features of their computers, I decided to pick on Dell (XPS ONE) due to:\n\n 1. Practicality of computer- The Apple, though uses a different operating system from windows and is not so wisely used, and the users would need new training to use it, has proven to be more user friendly. Dell, like HP use the popular windows. Apart from this, Apple\u2019s and Dell\u2019s security against viruses is much better. This is excellent for the company.\n 2. Computers Longevity and aesthetics\u2013 the Dell is a long lasting and strong computer. Its monitor and Central Processing Unit comes in various sleek attractive shapes and colors.\n 3. Service Availability- it is easy to get the replaceable parts for it since it is very popular. Apart from this there is excellent customer service hotline, online services, remote diagnostics and online customer feedback base.\n 4. Reputation- Dell and Apple computers have a solid international reputation. This has been built due to the excellent services they give their customers.\n 5. Costs- in comparison to the costs of the other computers in terms of purchase and transport to our premises, the software and hardware update, Apple will cost the company more than the other computers, followed by Dell. This is due to its excellent features.\n\nConclusions and Recommendations\n\nAfter comparing all the computers, I found out that some customers have been dissatisfied with the HP computer even though it has a very low price. Considering that our work is very important, we would not want to reduce its quality and efficiency so we would not risk anything with the HP. The Dell and the Apple on the other on the other hand basically have similar qualities and capabilities. The deciding factor therefore is the cost. Apple is more expensive than Dell and since we need many computers, I think it will be better to purchase the Dell (XPS ONE) because it has good virus protection and excellent temperature control which are perfect qualities for long hours office use.\n\nThe report is based on the report already presented by the customer.\n",
        "label": "human"
    },
    {
        "input": "The Increasing Human Dependence on Computers Research Paper\n\nThe computer has in the last few years become indispensable to humans in performing a whole lot of functions with convenience and speed that could not be imagined before computers became affordable and a household name for almost everybody in the modern world. Computers allow us to shop online, play interesting games with people, and work from home in regard to several personal and official functions. Large amounts of information can be stored in the computer that is instantly available with the click of the mouse, which enables the ordering and tracking of resources in a very convenient and fast manner. Computers allow people to interact with each other and to conduct meetings from different locations for business purposes. Businesses use computers in conducting their financial and accounting work with speed and accuracy, most of which was previously done manually by office workers with possibilities of human error that could cause a setback to the predictions of company objectives which were based on such data. Computers are a big asset in enabling education by way of the internet, allowing access to thousands of options to extract relevant research material as relevant to the subject of study. It also allows colleagues to interact with one another to enhance learning experiences and to come to required conclusions. People across the world can communicate with each other with convenience and without any high cost. The internet is the biggest asset for the common man in benefiting from the use of computers, while it is an excellent means of entertainment in providing a wide array of choices in choosing the modes of entertainment which are readily available (Joshua Givens 2008). Indeed man has become extensively dependent on the computer for meeting his needs for several of his requirements, and life without is absolutely unimaginable.\n\nThe extent to which we have got addicted to the computer is evident from the fact that a few years back we had difficulty in picking up the phone and calling someone without assistance from an operator. Parents whose children were studying away from home used to send them letters by post or telegrams for urgent work, much in contrast to the instant messages and emails that are used today. Indeed we have travelled a great distance with the technological advancements that are the order of the day in today\u2019s world. It is impossible for the average urban citizen to do a single day without using a computer, mobile phone, printer or fax machine. People in all circles of life have now started to wonder whether it is appropriate and a healthy practice for mankind to continue becoming so severely dependent on advanced technology of this kind which may put us all in extreme jeopardy if one day the entire computer technology failed and vanished. This would entail severe negative consequences in creating a vacuum for all human roles and ways of doing things. Agreed this is a purely illusionary thought, but it is essential to ponder over the issue in the light of our over-dependence on computers.\n\nThere is no doubt about the fact that computers are now an indispensable part of our lives that affect us at home and office. It is now feared by psychologists and sociologists that if such a pattern continues, the day is not far when our minds will lose their original capabilities in being creative in view of their tasks being now performed by computers. Computers are now capable of performing jobs that were being done by workers in factories, and there seems to be no end to the additional jobs that computers will undertake in the future, thus rendering human labour and brain to lesser utility value and reducing human role to be increasingly limited to lesser functions. With the overuse of computers, especially in the 21 st century, there has come to be a difference between users of computers who use them for work that truly facilitates the work and those who have in fact become addicted to them to such an extent that they just cannot do without them. It is observed that often there is compulsive overuse of computers in staying glued to them for the sake of convenience to avoid effort in using other physical and mental means (Shotton M A, 1989).\n\nIndeed, the catching on with the internet and related services by more and more people and the increasing dependence on convenience that is enabled by using practices such as e-trading, e-shopping, e-therapy and several other services that are e-enabled, is making more and more people savour the excessive convenience enabled by these services. Due to the proliferation of such services, concerns have started to arise whether humans need to interact with each other in pursuance of their goals for getting information and assistance since the internet can provide almost everything and consequently, there is no evident need to relate with people. In this context, there would soon be a time when newspapers will become obsolete because better news services are available on the internet. Most entertainment establishments would have to close down in view of the vast choice of entertainment available online, and the huge plethora of information material available on the internet is so exhaustive that people consider its size to be incomprehensible. The threat of the internet has now become so strong that sociologists have come to believe that it may adversely impact the religious, moral and ethical value systems of the coming generations. Some theorists have started predicting what may be unimaginable for some, but they say that if there is no control and reduction in such excessive dependence on technology, it may soon gain control over the human race. Indeed a far fetched observation, but it does indicate the extent to which we have become over-dependent on technology in our daily lives. We need to contemplate over the issue in that our brain will be required to work lesser and consequently; we may soon reduce the potential of our mental faculties to achieve the wonders that mankind has achieved so far by sheer dint of the power of the brain.\n\nIt is believed by many that the increasing dependence on information as taken from the internet and the overdependence on e-books for academic purposes will eventually erode the importance of literary works, and the overall standard of literacy will begin to decline steadily in due course because the regular printed academic books will become scarce in view of their reduced popularity. In such a situation, book stores and public libraries will begin to vanish, and considering the situation that in furtherance to such developments, the printing of textbooks and other literary works may not prove to be commercially viable (Manali Oak 2008). In this context, as a consequence of high dependence on the internet, if there is a crash in the entire internet system, the situation will become deplorable. There would be very scarce means for education available, and people will have a lot of difficulty in getting access to information and knowledge. We surely cannot imagine a world without books and knowledge material. The message is clear that there should not be too much dependence on computers.\n\nThe fact remains that it was the human brain that invented the computer, and subsequent innovations that have led to the present state of affairs is making us over-dependent on such technology. There is no doubt that technology has played a vital role in the betterment of our circumstances and in the improvement in quality of life, and we will continue to do so. It does have a profound and vital effect on mankind in making society and the human race much advanced in meeting the goals of betterment and development (Todd Ayers 2008). But the high rate of such acceleration should not be allowed to erode the basic foundation in terms of adversely impacting human capabilities of thinking, creativity and communication. The development and use of advanced technology have to be balanced so that it is in keeping with the need for humans to interact and correspond with one another and continue to maintain their capabilities in the furtherance of the improved quality of the human race. We have to be in control in all situations and not allow our own creations to dominate over us in making us mere puppets in the hands of such technologies. We must retain and maintain our ideologies and individuality in keeping the use of technology as a separate identity for facilitating our tasks and not for making us a slave of them. We need to understand that technology is our dominion, and it cannot claim to have dominion over us.\n\nReferences\n\n 1. Alfred Lewis, New World of Computers, 1965, Dodd Mead, New York\n 2. David Williamson Shaffer, How Computer Games Help Children learn, 2008, Palgrave Macmillan\n 3. Eric Filiol, Computer Viruses: from theory to applications, 2005, Springer\n 4. Importance of Computers Stressed, 2005, The Hindu\n 5. Joshua Givens, The Increasing Human Dependence on Computers, 2008.\n 6. Katie Kashmanian, The Impact of Computers on Schools, 2000, The Technology Source Archives, University of North Carolina\n 7. Manali Oak, Importance of Computer education, 2008.\n 8. Paul N Edwards, From Impact to Social Process: Computers in Society and Culture, Handbook of Science and Technology Studies, 1994, Sage Publications, Beverley Hills\n 9. Shotton M A, Computer Addiction? A study of computer dependency. 1989 New York, Taylor & Francis.\n10. Todd Ayers, Importance of computers.\n11. Vladimir Vapnik, Estimation of Dependences Based on Empirical Data: Empirical Inference Science, 2006, Springer",
        "label": "human"
    },
    {
        "input": "Resolving Software Problem: Timberjack Company Case Study\n\nTable of Contents\n 1. Executive summary\n 2. Problem statement\n 3. Data analysis\n 4. Key decision criteria\n 5. Alternative analysis\n 6. Recommendations\n 7. Lessons learned\n\nExecutive summary\n\nThe case study deals with resolving software problem which occurred in Timberjack company, one of the most important loggers in global economy. Company top management came across difficult strategic problems concerning software as different patterns of software were used in different regional headquarters which created additional costs and resulted in essential inefficacy. That is why it was decided to create general global software framework preventing the risks of software misuse, low coordination and rationalization. The software design process which started centered upon creating flexible software which could react to minor and major manufacturing changes. There is no denying the importance of the fact that Timberjack came across many difficulties during implementation problems which concerned the choice between different software packages, vendor propositions which not always complied with the interest of company, cleavages between various departments and design teams and customizations difficulties. All these is very interesting and helpful in terms of theoretical experience of software systems creation.\n\nProblem statement\n\nThe problem which this case study opens and describes relates to difficult issues concerning manufacturing software development, best available methodology which can be used in this process and positive approaches to coordination between various company\u2019s branches, sectors and departments as well as vendors, project promoters and counterparts. For instance, the first stages of software project (outlining its counters and desirable results) created opposition of viewpoints of Sweden team and North American group involved in this project. The former thought that it would be more efficient to outline the scope and directions of the software project in just few pages and give it to specialists which would realize it. However, the latter, following Coopers and Lybrand methodology thought that it would be best to spend much time for careful examination of all possible impacts and implications for manufacturing process, which will be run by software and finally this viewpoint became predominant.\n\nData analysis\n\nThe analyzed case gives interesting empirical data on the difficulties concerning organization and realization of software system projects within transnational organization with a joint capital like in this case \u2013 Swedish and American. There is no denying the importance of the fact that much of the analyzed data may be used in our future research and practical activities. One of the most important directions in this respect are coordination difficulties which should be tackled in terms of constructing joint strategic vision and cooperation. This will prevent occurring such problems like in this case when American counterparts say that the Swedish partners received information late because \u2018it was difficult to translate reports from English into Swedish\u2019. Besides this the lack of efficiency and understanding resulted in low effectiveness of software project development. For instance, as report shows Swedish partners new that Oracle databases cost 40% more in exploitation and customization than Progress databases but it was late since Timberjack already included Oracle databases in the project.\n\nKey decision criteria\n\nThe decision which was finally taken in the case of Timberjack was made on the basis on efficiency and reducing opposition between Swedish and North American partners. It was a result of compromise. There is no denying the importance of the fact that some elements of software projects were considered as being not functional in Swedish conditions. This was due to North American team trying to do such customizations which best suited the interests of manufacturing and operations in the North America.\n\nAlternative analysis\n\nAlternative analysis which I proposed is based on greater coordination between software groups, better organization of process and designing functional approach to software implementation. The current case shows that the task of creating comprehensive software system in Timberjack company proved to be inefficient due to regional peculiarities of business activities. Hence, it was difficult to accommodate the changes to software in Europe due to previous software and manufacturing practices and procedures. That is why it seems more appropriate functional approach to software system where several subprojects were created within global system, each designed to meet specific regional and operational interests. Each project should be influenced by North American or Swedish groups depending on the stakes they had in it. In my view, such approach would be effective in avoiding ineffective software use and inflexible global software system in Timberjack.\n\nRecommendations\n\n  * create coordination team on the basis of Sweden and North American group which would design global and regional subprojects on the basis of the bests efficacy in given conditions, operational and manufacturing practices.\n  * Develop appropriate budget and financing policies which will prevent inefficient organization of projects.\n  * Organize effective communication process between involved departments to avoid misguided actions and misunderstanding.\n\nLessons learned\n\nSome of the lessons which may be learned from Timberjack mistakes were already mentioned. Among others is poor organization of budgeting policies which led to low interest on the part of vendors. Besides this, one should mention poor work of company\u2019s computer technology departments which failed to provide adequate information on the difference between Unix and other systems, Oracle and Progress and other elements of software system which essentially effect business processes.\n",
        "label": "human"
    },
    {
        "input": "Computer Security: Bell-Lapadula & Biba Models Essay\n\nTable of Contents\n 1. Computer Security Attributes and Computer Security Models\n 2. The Principle of Defense in Depth\n 3. Conclusion\n 4. References\n\nInformation security has three basic attributes viz., Availability, Confidentiality, and Integrity, and the effectiveness of computer security policies depend on the efficacy of the methods by which these three attributes are sought to be implemented and/or strengthened. Cybersecurity policies require the formulation and implementation of security access control models like the Bell-LaPadula (Bell, D.E. and LaPadula, L.J, 1973) and the Biba, (Biba, K.J.,1977) to successfully ensure availability, integrity, and confidentiality of information flows via network access.\n\nIn addition, modeling for computer security is based upon some fundamental principles like the Principle of Defense in Depth. This brief paper outlines the fundamental principles governing the Bell-LaPadula and Biba Security Access Control Models as also examines the defense in depth principle\n\nComputer Security Attributes and Computer Security Models\n\nComputer security access control modeling takes into consideration basic information security attributes of availability, confidentiality, and integrity. In other words, the purpose of a computer security access model is to help prevent unauthorized alteration (Integrity), disclosure (Confidentiality) and to a lesser extent, loss of access to computer resources and data (Availability). Depending on requirements by the government and public corporations doing business, a variety of models have been developed over the years. Some well-known ones are the Bell-LaPadula (1973) and the Biba (1977).\n\nOthers are Clark-Wilson, Brewer and Nash, Graham-Denning, etc. All these have been developed to address specific issues like ensuring information availability, confidentiality, and/or integrity. Based on how the models define relationships amongst subjects, objects, permissions, and operations access control models may be classified as Discretionary Access Model, DAC, Mandatory Access Model, MAC or Role-Based Access Control Model, RBAC (Ferraiolo, D.F., Kuhn, R., and Chandramouli, R., 2003).\n\nThe Bell-LaPadula Computer Access Control Model is a Static Machine Model developed in 1973 for analyzing MLS operating systems. In this model, the ordering of information is based on various security levels and a security matrix through which permissions are defined and the flow of information from a higher level is governed by the Discretionary Security, the Simple Security, and the Star Properties. Clearances are given to users and objects are classified as per given rules.\n\nThe advantage is that system security can be easily checked by using BLP Model and its state machine model characteristics can be applied for other attributes like integrity, However, the model contains covert channels, is meant for static security levels, and its functionality of system testing is restricted to checking confidentiality.\n\nThe Biba model was developed in 1977 for ensuring the integrity of computer systems. In addition to maintaining data consistency, the model also restricts the unauthorized alteration of data and computer resources (Bishop, 2003). It has a strict restrictive integrity property which is the exact opposite of the Bell-LaPadula Model property. There are three conditions in which this property operates, viz., a simple integrity condition in which \u201cno reading down\u201d is enforced, the star integrity property which enforces \u201cno write up\u201d and the invocation property in which the subject may invoke another with lower integrity level (Balon, N., and Thabet, I., 2004). While the model has numerous dynamic policies, it has its disadvantages; it does not support the granting and revocation of authorizations nor is it able to enforce confidentiality\n\nThe Principle of Defense in Depth\n\nThe principle is one of the key contributions of the US military which developed it for ensuring that the defense is hidden from attackers and has ample time and opportunity to respond to their (adversaries\u2019) attacks. In information security, this implies a layered security approach to modeling computer architecture and invariably envisions multi-layered security levels for defense. For example, we can develop two firewalls (one internal layer and another outside) as a more effective defense mechanism against cyber attacks instead of relying on a single firewall. Essentially, use is made of multiple layers of security as also the application of technologies at each layer that complements one another (Pereira, J.P., 2004).\n\nFour sub-principles are envisioned in implementing the defense in depth principle to computer systems; the network infrastructure is distributed to broad base security application, multiple and parallel layers of security are built up, support infrastructure is strengthened, and data mining and data analysis of security events is continuously done to help evolve more effective and foolproof security systems. Examples of a few multiple layers of defense using various controls may be given as follows:\n\n 1. Applications Layer: This may include validity controls for data entry and processing, host and network controls for guarding against possible flaws in software applications, etc\n 2. Physical layer: The related controls help protect the organizational assets against physical threats and the assets include entire physical systems like computers, UPS, routers, switches, etc\n 3. Distribution Layer: Two defense controls may be the usage of trusted software & distribution and the application of run-time integrity controls\n\nConclusion\n\nThe topic of computer security is too vast to warrant a comprehensive description in a few pages of text. The security control models described above have their merits and demerits and their actual selection for any particular computer system architecture depends on the end-use, complexity, and purpose of the same. However, the technology landscape is changing very fast and it may not be long before more innovative and fool-proof IS systems are developed to effectively tackle the growing incidences and variety of computer security violations\n\nReferences\n\nBalon, N., and Thabet, I., 2004, Biba Security Model Comparison.\n\nBell, D.E. and LaPadula, L.J, 1973 A mathematical model, Technical report esd-tr-278, vol. 2, The Mitre Corporation, Bedford.\n\nBiba, K.J., 1977, Integrity considerations for secure computer systems. Technical report tr-3153, The Mitre Corporation, Bedford.\n\nBishop, M., 2003, Computer Security: Art and Science, Addison Wesley, Boston, MA\n\nDefense in Depth, Design Notes, Wilson, M., 2001, Decision Support Systems Inc. Web.\n\nEydt, B., Security Models and Architecture, CISSP Exam Preparation Guide.\n\nFerraiolo, D.F., Kuhn, R., and Chandramouli, R., 2003.\n\nFormal Security Policy Models, Siemens AG, CT IC 3, Volkmar Lotz, 2003. Web.\n\nIn Depth Defense applied to Information Systems (Memo Version 1.1), 2004. Web.\n\nPereira, J.P. 2004, Defense in Depth. A Strategy To Secure Federal Networks, Jupiter Networks, Inc.\n\nSecurity Models and Architecture, CISSP Certification All-in-one Exam Guide, Chap 5.\n\nStoneburger, G., Hayden, C., and Feringa, 2004, Engineering Principles for IT Security (Rev.A). NIST.\n",
        "label": "human"
    },
    {
        "input": "Legal and Ethical Issues in Computer Security Essay\n\nWith the rapid development of the software and computer technologies the necessity to protect intellectual property and raise computer security has sharply grown. In the USA and in many other developing countries of the world any software and hardware can be completely protected according to intellectual property laws existing in the country.\n\nThe situation under analysis discloses the idea of software algorithm invention by Joe and commercialization of this product by Stan. From the ethical point of view Stan committed a kind of \u201ccrime\u201d as he stole Joe\u2019s idea and wanted to make use of it without his partner\u2019s involvement and participation. Joe, being the inventor rather than business man, is not aware of all details of legal intellectual property protection and could loose any rights for his own inventory idea but for Stan\u2019s guilt pangs.\n\nThe idea of invention was worked out by Joe and could become a commercial product in case entrepreneur Stan and inventor Joe joined their efforts and abilities to create common business. Under the law this kind of inventions can be protected by Stan according to the trade secret law.\n\nTrade secret law is considered to be the best way of intellectual property protection if the invention is software algorithm. It is the most appropriate mechanism for proprietary rights preservation because it can provide the broadest protection form for Joe\u2019s invention. Stan as a soul proprietor could get a percent form the income.\n\nBut is it necessary to stress the fact that there can be some difficulties connected with the intellectual property protection under the trade secret law. The point is that algorithm can be protected under this law in case the following conditions are observed:\n\n  * The invention is not widely known to the publicity;\n  * Software algorithm is preserved the confidentiality and it is considered to be a trade secret;\n  * The invented product provides the owner with a competitive advantage and beneficial position.\n\nThis type of legal security for the invented product of Joe is really important but it provides protection only for the period of time when the algorithm is under the secret and gives advantages in competitiveness.\n\nOn the other hand, Stan could use patent protection as a way of product security. The requirements for this type of intellectual rights protection are not so high:\n\n  * Novelty;\n  * Utility;\n  * Non obviousness.\n\nJoe and Stan should create the trade mark for their software product because it provides very strong level of ownership for the invention.\n\nThus, the situation analyzed above disclosed the idea that every invention should be protected by the existing law providing security for your intellectual rights for the product; this should be done before sharing the idea as in case with Joe and Stan. The partners are to sign a contract about cooperation in case they are eager to develop and promote the invention together and get profit out of it. To reach success in software business all the mentioned steps are to be observed.\n\nReferences\n\nPfleeger, Charles P. & Pfleeger, Shari L. (2006) Security in Computing. 4 th Edition. USA.\n\nVaughan-Nichols, Steve. (2003). Protect Your Business Software. Information Week, NY.\n",
        "label": "human"
    },
    {
        "input": "Mind, Brains, and Computer: Homunculus Theories Essay\n\nA computer has become an important part of our life. Most of the work in today\u2019s world is done by computer and information technology. Information technology plays an important role in different fields. Some different fields are completely influenced by information technology. The word \u201chomunculus\u201d is derived from the Latin word homunculus, which means \u201dlittle man.\u201d Homunculus is usually used for defining or description of a system. In a scientific system, it is generally viewed as an entity or an agent of a complete system. This term was first used by Alchemist Paracelsus. He claimed in the early decade that he had created a false human being named \u201cHomunculus.\u201d His creature was 12 inches tall, and it does all work associated with a golem. After a short time, homunculus turned on his creator and ran away. Alchemists made homunculus with a combination of different elements such as skin fragments, sperm, and hair of any animal. Homunculus can be created by combining hairs, skin fragments, bones, sperm, and skin elements from any animal of which a person wants to make a hybrid. For the formation of an embryo, this structure was to be laid down on the ground for about forty days. The ground was surrounded by horse manure for forty days to form an embryo (Frege, 1892). Mandrake method is also used and described by other alchemists for the formation of a homunculus. Various methods are now used for the homunculus, and some alchemist refers Mandrake use.\n\nThe most popular method for the creation of a homunculus is usually cited by other alchemists emphasizes the use of the mandrake. A famous belief about homunculus was held that a plant usually grew where the semen often produced by hanged men within last conclusive ends prior to death fell to the ground. Roots usually resemble a human shape to varying degrees. A black dog usually picked a root in the early morning, then washing of root was done. After that, it used to be \u201cfed\u201d with milk and honey in the same perception as blood, fully develops into a miniature human that usually guards and protects its owner or creator(Homunculus, 1994).\n\nAnother method was used by Dr. David Christianus at the University of Giessen in the 18th century. He took an egg of a black hen. In his method, he emphasized a lot on egg laid by a black hen. The egg was poked in a tiny hole with the aid of a shell, and a bean-sized portion was replaced with white human sperm. And the opening was sealed with virgin parchment, and the egg was buried on the first day of the March lunar cycle. There was an expectation that a miniature humanoid would emerge from the egg within thirty to thirty-five days, which would surely help and protect the creator from a steady diet of different seeds, including lavender seeds and earthworms.\n\nToday, the term \u201chomunculus\u201d is used in different domains to describe a system that thought little man was working inside. There are several homunculus theories about brain modules and sensory events. Both theories are effective in the way of responding mind. Both theories fall in homunculus because everyone agrees that homunculi are responsible for our consciousness, but there is a centralized entity inside our head named the brain. Generally, homunculi are regarded as eventually as absurd as a smear against other theories. According to Daniel Daniel, there is a black box inside our mind which enables all mental abilities. Both theories count as homunculus theory because both are common in the theory of vision.\n\nA simple theory proposed a solution in which a light beam forms an image in the eyes and produces an image in the brain which looks the same as if they are on screen. Some homunculus arguments proved that this is not a complete view because all that has been done on a place as an entire person, or homunculus, which always present behind the eyes and gazes at the retinas (Gregory & Richard L. (1990). Homunculus theories are considered bad because it is a philosophy of diminutive humans and it is fully formed individual believed by adherents of early biological theory about the per formation to be present in the sperm cell. Fodor and Dennett\u2019s ideas have been challenged by a variety of philosophers. Fodor explains the natural language learning process as a formation process and confirmation of hypothesis in a LOT, the basic point of LOT is based on the majority\u2019s behavior towards computers but also in regards to human\u2019s unconscious behavior, which does not consider that representation is necessary for defining proportional attitudes.\n\nSome things humans do while playing games on the computer like playing a chess game, sometimes he says, \u201cI think queen should be moved on left.\u201d Here, no one would agree that the computer is actually thinking or believing, but it represents propositional attitudes (Fodor, 1993). Dennett also suggests many of our daily attitudes, such as a desire to breathe in a stuffy atmosphere. Fodor and Dennett suggest that homunculus theories are good enough as several homunculi are nested; this is a virtuous theory (Fodor, 1998). The consequences of people by a conscious concept would replace this theory because it\u2019s true to some extent that there is a little man inside our brain who responded to our actions and brain messages. Some of Fodor and Dennett\u2019s thoughts are challenged by many philosophers from different domains as Fodor considers such concepts as effect, islands, vixen, and week to be all innate and primitive. The use of a similar strategy to replace bad theory is possible because sometimes homunculus doesn\u2019t fulfill the requirements associated with a little man. Homunculus arguments are popular in the theory of vision. Arguments and criticism on homunculus theories are common; several homunculus theories have been challenged by some other philosophers. In the field of physiology and philosophy of mind, homunculus theories are extremely useful for detecting certain concepts where the mind failed or could not be able to solve the issue. Very few people would propose that a little man is sitting inside the brain while actively looking at the brain. This proposal is very common and has been used in several theories as a Straw man of mind.\n\nSome homunculi theories are common but usually consider failing or incomplete, and bad. Many people do not believe that there is something like a little man sitting inside the brain or there are homunculi. Many theories are available in this domain, but they have to achieve great success in the domain. Different human behaviors reflect due to human thinking and behavior towards computers which shows human interaction with computers. Mind, brain, and computer all lie in the same line, but the computer doesn\u2019t have a mind to think, and also it doesn\u2019t respond like a human on brain messages.\n",
        "label": "human"
    },
    {
        "input": "OpenOffice.org in Microcomputer Applications Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Problem Statement\n 3. Why OpenOffice.org\n 4. Conclusion\n 5. Work Cited\n\nIntroduction\n\nOpenOffice.org is software that is used for various applications such as spreadsheet, word, presentation, and graphics among others which is an alternative to Microsoft office. It is called open because, any one can download at no cost and use it for any application. Also, it can be used in any operating system platform, for example Mac, Linux, Unix etc. OpenOffice.org can also be used to read or write on other types of applications. In other words, OpenOffice.org is an open source where any one can make changes to fit without contacting the developer or even distributing it to other people. There are many advantages of using OpenOffice.org which include simplicity of usage, free to acquire, possible to modify to meet users\u2019 purpose, can be used for business purpose or personal use and can be used in any platform. According to Weber (2004), \u201cA completely open development process means that anyone can report bugs, request new features, or enhance the software. The result: does everything you want your office software to do, the way you want it to.\u201d\n\nProblem Statement\n\nIn the past, there had been no way of accessing someone work or code without contacting the developer and be given authority to do so. This had various limitations because some could not get it due to high cost and one could not improve it or use it at any application. Also the advancement of the software was not possible. But due to open source, it is possible for the user to modify in order to meet his need or market. This paper describes about OpenOffice.org, how to obtain it, features and some of the applications which differentiates it with Microsoft office and how compatible they are.\n\nWhy OpenOffice.org\n\nOpenOffice.org can be obtained by downloading it from the internet at no cost. Therefore, the user is free to run it for any application, learn how it works, distribute it to others, make some improvements and redistribute it at a charge or for free but to some extents, access to code is precondition. To run it means, one can install it for personal or organization use to any type of computer system, do any type of work and purpose without having to make contact with the developer. In this case it depends with the purpose of the user but not the developer. When redistributing, one is free to redistribute it with its code, all versions including modified one and usable form of the software if the language used allows it. Although it is said to be free, commercializing it does not translate to a crime. Haugland (2004) observes that, \u201cIt\u2019s free, but is it good? It\u2019s a full fledged office suite, as big as MS Office. It\u2019s got a great set of features\u2014everything you need to do word processing, spreadsheets, and presentation slides. Plus extra programs for drawing that you don\u2019t get with Microsoft Office: OpenOffice.org Draw is like Canvas, with a bit of Visio thrown in.\u201d In this case, open office has given chance to users to modify it for their own benefit unlike in Microsoft office where the user must pay and no modification to fit his purposes.\n\nOpenOffice.org is an open source. This is because it can be used in operating system platform such as windows, Mac, Linux and some of Unix version. Also it matches with all application of Microsoft office applications such as word, spread sheet, presentation and drawings as well as databases.\n\nOpen sources have licenses which is copyright. This license allows access of code, which makes it possible for modification without contacting the actual developer. But sometimes, there are restrictions to maintain the name of the developer and a statement to indicate it\u2019s a copyright. Hence open source licenses gives what the user is supposed to fulfill in order to use the software. Some licenses do not offer the privilege to access the code. According to Gurley (2006) \u201cThe proliferation of open source licenses is one of the few negatives aspects of the open movement because it is often difficult to understand the legal implications of the differences between licenses.\u201d\n\nMany peoples have adapted to using OpenOffice.org due to its simplicity and free to acquire. As noted by Gurley (2006), \u201cWhen you\u2019re starting something new, it\u2019s great to know tens of millions of people have been there before you. OpenOffice.org 3 is developed, translated, supported, and promoted by an international community of tens of thousands of enthusiasts. If you have ever waited for ages for paid-for support from a commercial software provider, community support from enthusiastic and knowledgeable volunteers may come as a surprise\u201d. The prediction shows that people will change to OpenOffice.org and less will be left to use other varieties. OpenOffice.org was started by Sun Microsystems but other companies like IBM have joined.\n\nOpenOffice.org provides many features to the user. The user is provided with many shapes which are similar to those of Microsoft\u2019s auto shapes. After placing it on the sheet, the user can modify its size. In case of shapes that are not provided, there are tools for drawing. It also provides various animations that can be used on slides for presentation. This feature resembles that of Microsoft Power point but with slight improvements. It allows PDF export. One is able to open a document in PDF format or save a document as PDF unlike Microsoft office. OpenOffice.org provides tools for database, although not accessible in some of the versions. Those tools allow development of database and its management. It allows sending a mail to many people at once. In this case, guidelines are provided within the interface in Help form. In word processing, recent versions provide a tool for word count, but the older versions do not. It provides a chance to create a table within another table which is not possible in Microsoft. It also allows creating interfaces basing on W3C Xforms. It allows creation of groups, filter data, give differences and percentages instead of values. It is easier to move toolbars by either detaching or attaching it according to the users need. It also have features like digital signature support, Xforms support, word perfect filters, enhance pivot table support, native installers, Native desktop integration, open standards XML file format and viewing of many forms at once. Other features are dictionary spell checker in all languages unlike in Microsoft Office and provide interface to access other forms and reports in which they can be modified. But those are not the only features, as development goes on day after day more features are added according to the need of users. (Gurley, 2006).\n\nOpenOffice.org has an important property of fitting in all platforms. That means, it can be used in many operating systems such as Windows, some of Unix versions, Linux, Mac among others. This property allows it to be used by a variety of people along the world. Also it is compatible to Microsoft office which many people have been using. That means, if a document is developed in Microsoft office and sent to you, then it can be opened using OpenOffice.org and vice versa.\n\nAlthough OpenOffice.org has many applications, some of them include text document, spread sheet, presentation, drawing, database and reviews. All this applications are similar to those of Microsoft office and they are compatible, which means that any document developed in OpenOffice.org and save as.doc can be opened in Microsoft office. (Finkelstein and Leete, 2005).\n\nWhy use OpenOffice.org? As discussed above, OpenOffice.org is free to acquire, easier to use, it is compatible with Microsoft office, one can modify to meet his/her needs, has more applications among other things.\n\nConclusion\n\nTo conclude, OpenOffice.org can be said to be very important software to all people due to its features. Also it is easier to use and free to acquire. The software has many applications as discussed. Therefore, people should move from Microsoft office to OpenOffice.org as it can be modified according to the users need. Also there is a room for improvement due to its open source property. Although it is easier to use, one should avoid making mistakes like when saving a word document, one should save it as.doc to avoid embarrassment when opening it on Microsoft Office because other features like.odt will not be accessible.\n\nWork Cited\n\nG.Leete, E. Finkelstein and M. Leete. Openofficeorg for Dummies. New York: Wiley & Sons, 2005.\n\nJean Hollis Weber. OpenOffice.org Writer: The Free Alternative to Microsoft Word. London: O\u2019Reilly, 2004.\n\nR. Gabriel Gurley. A Conceptual Guide to Openoffice.org 2.0: standard edition. Washington DC, Concise Concepts, 2006.\n\nSolveig Haugland, Floyd Jones. OpenOffice.org. Helion: WyDaw, 2004.\n",
        "label": "human"
    },
    {
        "input": "The Research of Computer Assisted Instructions Report\n\nIntroduction\n\nTwenty-first century education has a wealth of technology available to draw on to enhance student engagement in the learning of new knowledge. Understanding of the best ways in which to deliver learning materials to students is essential in narrowing the gap between those of different learning styles who find traditional lecture methods inadequate in engaging them with learning of materials (Bayek & Layne, 1988; Greenhalagh, 2001). Given that modern workplaces utilize computers to a greater degree than traditionally it is likely that use of computer assisted programs to complement lectures will broaden undergraduate generic skills needed in the workplace, as well as enhance the learning experience of those who are ill-suited to traditional lecture methods.\n\nTo evaluate the efficacy of a computer assisted instructional method used as a complement to traditional lectures at Bryston College an empirical study has been designed. The research question for the study is: Does the use of computer assisted instructions enhance student learning of developmental basic algebra, as compared to traditional lecture methods? It is anticipated that the mean test scores on the Descriptive Test of Mathematics Skills (DTMS) (Bryston College Board, 1995) of students receiving computer assisted instruction in conjunction with the traditional lecture will be significantly higher (alpha =.05) as compared to the mean scores of students receiving only traditional lectures. As such the null hypothesis was that there would be no difference in mean scores across the groups.\n\nMethod\n\nParticipants\n\n140 undergraduate students taking the developmental basic algebra course at the Bryston College will participate in the study. Seventy students have been registered for the course on Monday/Wednesday mornings with Instructor 1, and seventy students have registered for the course on Monday/Wednesday evenings with Instructor 2. To control for pre-existing differences across students, the students will be randomly allocated to their class using a \u201cnumber from the hat method\u201d. Each student will draw a number, either \u201c1\u201d or \u201c2\u201d from the hat which will indicate their allocation to either the traditional lecture group (1) or the traditional lecture and computer assistance (2). The class days will be randomly selected using the number form the hat method, to be either the control group or the experimental group.\n\nMaterials\n\nThe Descriptive Test of Mathematics Skills (DTMS) was published by the Bryston College Board of Education (1995).The test measures skills of math ability and is untimed. The passing score is 506.This test has been found to have high comparative validity across a range of studies. Bridgeman(1982) compared the DTMS with scores from the College Board Scholastic Aptitude Test-Mathematics (SAT-M) in predicting grades in freshman level mathematics courses across two institutions; his research has been supported in more recent literature (Gray & Sheehan, 1992; Suddick & Collins, 1992; Sireci & Talento-Miller, 2006). The DTMS has also been found to have high predictive validity (Meyer, Woodard & Suddick, 1994).\n\nThe reliability coefficient for the Basic Arithmetic section of the Descriptive Tests of Mathematical Skills is.84 (Cronbach\u2019s alpha) with a 2.1 standard error (Bryston College Board of Education, 1995).\n\nDesign\n\nThis study will use a 2 x 2 mixed method experimental design; this being Lecture type (traditional or traditional + computer assisted) x Instructor (Instructor 1 and Instructor 2). This design will control for differences across Instructors of their method of delivery. The IVs in this study are the Lecture methods and the assigned Instructors. The DV will be the student\u2019s total score on the DTMS.\n\nDescriptive statistics such as frequency and percentages will be used to make general comparisons across groups and to describe the data. Inferential statistics will be the use of mixed-method ANOVA (SPANOVA) to compare variance across the groups and to identify the main effects (if any) and interactions (if any) of variables.\n\nProcedure\n\nStudents will arrive at their assigned class and asked by their assigned Instructor to complete the DTMS individually, without consulting with each other. The test is untimed, so students will be given the entire class period to complete the questions. The Instructor will then collect the test and thank the students for their participation.\n\nDiscussion\n\nThreats to Internal Validity\n\nAn experiment is valid if it has internal and external validity. Internal validity is the extent to which any change in the dependent variable is actually due to the independent variable. As there are two Instructors there will obviously be slight differences in the delivery of materials, regardless of attempts to standardize delivery. Other differences including the gender of the Instructor and their age may also influence student engagement with the material. For example, Patten (2002) states that in western cultures men tend to be seen as figures of authority as compared to women and older persons are more likely to be perceived as knowlegable about a topic as compared to younger persons. Another threat to internal validity is demand characteristics on the part of the students. They may try to guess the hypothesis and so seek to \u201cplease\u201d the Instructor by answering in a way that y think is wanted. A further threat to internal validity is that of researcher bias in that the Instructors may not present the learning material in a standard format as they want a particular lecture method to be successful. The aptitude-treatment-interaction occurs when the sample has certain characteristics that interact with the independent variables that limit generalization (Shutt, 1999). So that those students who have a better grasp of developmental basic algebra prior to the learning material exposure will likely do better on the final test.\n\nAs no other instruments are being used to pre-test students as to their current knowledge of mathematics it may be that the students are unable to grasp the topics covered in the Lectures and so at the experimental test they are unable to adequately answer the DTMS and so the test would not measure what it is meant to, this would be a threat to internal validity. Also there is no pre-test of students and or Instructors computer abilities, as those who are computer illiterate would skew the data in that they would not know what they are doing as compared to students/Instructors who do.\n\nThe history of the students is unknown throughout the course, as is that of the Instructors (Patten, 2002). Personal circumstances could negatively impact on a student\u2019s ability to concentrate in class or an Instructor\u2019s ability to remain consistent in the delivery of materials. This does not make the DTMS test itself inaccurate, rather it shows a flaw in the design. Maturation effects of students and Instructors across time could also negatively influence final test scores, as the course runs over the duration of a semester of college (Shutt, 1999). For example, aging and development could cause an older female student to be going through menopause, resulting in flushes (a common source of embarrassment) and/or swings in her emotions due to hormonal changes. These states could affect her ability to concentrate in class, or to interact with other students or the Instructor to discuss her questions about the material learnt. Also, taking the DTMS test itself may negatively impact on a student\u2019s ability to concentrate and so they do not perform to the best of their ability.\n\nThreats to External Validity\n\nTurning now to external threats of validity that could impact on how the results affect the wider population of undergraduates taking developmental basic algebra courses, these include; selection bias; reactive effects of experimental arrangements; and timing of the experiment (Patten, 2002; Schutt, 1999). External validity is the extent that the relationship observed between the independent and dependent variable during the experiment is generalizable to the \u201creal world.\u201d\n\nFirstly, selection bias could occur in that the sample in this study is not in fact representative of the wider population of students taking developmental basic algebra courses in North America. This could be due to the sampling technique used (Meyer, Woodard, & Suddick, 1994). Although students were randomly allocated to groups, the sample itself was a convenience sample in that the students were already enrolled in the course. Also, the demographics of students enrolled at Bryston College may be diametrically different to those of students in other parts of the USA. For example, the college may only take students with an above average GPA score, predisposing the students to an above average ability to grasp class materials irrespective of the Lecture delivery method. Alternatively the students may come from predominantly higher socio-economic brackets predisposing them to access to resources and materials that aid their learning experience. A student who does not have to worry about rent, full-time work or lack of child care facilities will have more time and energy to devote to study materials in their own time (Meyer, Woodard, & Suddick, 1994).\n\nSecondly, the experimental setting itself could influence student scores. If the test does not take place in the classroom in which material was learnt there is evidence that students may not do as well. Physical and ambient cues within the classroom, even something as simple as sitting in the same seat where one learnt most class material, can have a dramatic effect on how a student answers questions during a test (Patten, 2002; Solso, Johnson, & Beal, 1998). In this way the final scores on tests may not be a true reflection of the target population.\n\nFinally, the timing of the experiment could threaten external validity (Solso, Johnson, & Beal, 1998). For example, if the experiment takes place after a major incident, such as a terrorist attack as experienced by the USA in New York in 2001, the students may not be in the frame of mind to concentrate on the test. If they were to go ahead with the test, those within the target population that take the test at another time when such major incidents have not occurred they will not have the socio-environmental influences to impact on their test taking abilities.\n\nReferences\n\nBaek, Y. & Layne, B. (1988). Color, graphics, and animation in a computer-assisted learning tutorial lesson. Journal of Computer-Based Instruction, 15(4), 131-35.\n\nBridgeman, B. (1982). Comparative validity of the College Board Scholastic Aptitude Test-Mathematics and the Descriptive Tests of Mathematics Skills for predicting performance in college mathematics courses\u2019. Educational and Psychological Measurement , 42 (1), 361-366.\n\nGray, M. W. & Sheehan, K. R. (1992). Sex bias in the SAT and the DTMS. The Journal of General Psychology, 12(1).\n\nGreenhalagh, T. (2001). Computer assisted learning in undergraduate medical education. BM, 322 (6), 40\u201344.\n\nMeyer, J., Woodard, P., & Suddick, D. E. (1994). The Descriptive Tests of Mathematics Skills: Predictive validity for an elementary mathematics concepts and structures course. Educational and Psychological Measurement, 54 (1), 115-117.\n\nPatten, M. L. (2002). Understanding research methods: An Overview of the essentials (3rd ed.). Los Angeles: Pyrczak Publishing.\n\nSchutt, R. K. (1999). Investigating the social world: the Process and practice of research (2nd ed.). Thousand Oaks: Pine Forge Press.\n\nSireci, S. & Talento-Miller, e. (2006). Evaluating the predictive validity of graduate management admission test scores. Educational and Psychological Measurement, 66, 305 \u2013 317.\n\nSolso, R. L., Johnson, H. H., & Beal, M. K. (1998). Experimental psychology: A case approach (6th ed.). New York: Longman.\n\nSuddick, D. & Collins, B. A. (1982). The Descriptive Tests of Mathematics Skills: A follow-up of performance of older upper division students. Educational and Psychological Measurement, 42 , 559 \u2013 561.\n",
        "label": "human"
    },
    {
        "input": "Computer and Information Tech Program in Education Essay\n\nTable of Contents\n 1. Introduction\n 2. Moore\u2019s law\n 3. Metcalfe\u2019s law\n 4. Communication\n 5. Historical\n 6. Present Situation\n 7. Future\n 8. References\n\nIntroduction\n\nThe world now becomes a global village, a number of things are now changed as they were in past decades. Computer and information technology has bought a number of changes in the computer field. Due to rapid growth in information technology, there is a need to educate students about this field. ICT is a subject in education countries like the Philippines has also ICT as an educational subject in the United States, the ICT program is linked and sponsored by Becta, and a recent scheme of online education has been introduced in order to educate students in schools. A number of schools also uses ICT to enhance capabilities and skills of learning and teaching.\n\nMoore\u2019s law\n\nMoore\u2019s law describes the historical trend in computer hardware. This law was first published in 1965 by Gordon E.Moore. Moore\u2019s law deals with capabilities and features of almost every component of computer hardware, i.e. processing speed, memory capacities and pixel arrangements in digital cameras. The trend of using digital devices for the betterment of the economy has become very common, Moore\u2019s law deals with the driving force of computer technology in the late 20& 21 centuries. Moore\u2019s Law deals with a number of transistors in a most complex chip, density at minimum cost per transistor, performance per unit cost, power consumption, Ram storage capacity etc. The computer industry believes that Moore\u2019s law is the most successful observation till now, and it will work for centuries by doubling the time used in calculations. There are some limitations of Moore\u2019s law about transistor size and speed. Moore\u2019s law uses an integration of digital components in wireless, optical technology and sensor-based systems.\n\nMetcalfe\u2019s law\n\nMetcalfe\u2019s technology links computer systems and uses a mathematical formula for calculating the value of communication networks. It deals with communication between networks; it states the value of a communication network is directly proportional to the square of the number of connected users on the system. This law deals with the values and network effects of the internet, social networking and the World Wide Web. Mathematically it is expressed as n (n-1)/2. The best example of this law is a fax machine, a single fax machine is useless, but the addition of every single fax machine in the network adds value to the network and also increases its capabilities.\n\nCommunication\n\nCommunication is a trend of transmitting messages, documents from sender to receiver. Computer hardware plays an important role in communication. Over recent years, the need for communication over the network has increased a lot, and the number of hardware is specially designed keeping in mind communication and network needs.\n\nHistorical\n\nThe history of ICT has faced a number of challenges and unique trends in past decades. Martin Collins and Janet Abate retraced the historical context of the development of ICT in different dimensions in the post-war era (Jannet, 1978). In the early period, nobody knew the processing speed, cost over the unit, performance issues which created a lot of problems; with the passage of time number of scholars endorsed efforts in this domain and now its fast-growing field in the world.\n\nPresent Situation\n\nIn the UK, computer simulation systems based on ICT are working in different fields (Dera, 2001). In Malaysia, the online business portal is one of the best inventions based on ICT; in Australia, a number of monitoring systems are specially designed, keeping in mind the basics of ICT and Moors law. In underdeveloped countries, a number of applications based on ICT and Moore\u2019s law (1965) are working in different domains. Anecdotal evidence proves that there is a wide room for improvement in ICT, education and communication.\n\nFuture\n\nAt a glance, ICT has now become an important aspect of communication over the network. In 2028, this field will grow a lot and will reduce long distances via networks. Considering the future of ICT, there is no shortage of information and material. In upcoming years, wireless will become more important with the mobility of services, devices and people. Devices will evolve from independent and analogue to connected digital devices; there will be a tremendous increase of ICT in medicine, increase in the use of artificial intelligence, use of nanotechnology will increase, use of more robust and renewable materials will increase. ICT has a very bright growth in upcoming years, and it will change the shape of the world in 21 century.\n\nReferences\n\nDERA, 2001, Strategic futures thinking: meta-analysis of published material on drivers and trends, Performance and Innovation Unit, Cabinet Office, United Kingdom.\n\nJanet Abbate, 1978, History of ICT, Virginia Polytechnic Institute and State University,\n",
        "label": "human"
    },
    {
        "input": "Computer Network Types and Classification Research Paper\n\nIntroduction\n\nDefinition\n\nA network is defined as an \u201cinterconnection of three or more communicating entities\u201d.\n\nAccording to the glossary of telecommuting terms, a computer network is a network of data processing nodes which are interconnected for purposes of data communication\u2019. However, it is not yet clear whether an interconnection between two computers can be constituted as a computer network. Despite this definition, it is still generally accepted that a connection involving a computer or a set of computers to peripheral devices like printers using an Ethernet link or RJ-cables still constitute a computer network.\n\nFor a computer to be functional it must meet three basic requirements, which are it must provide services, communications and most of all a connection whether wireless or physical. the connection is generally the hardware in use, the communications refer to the way the devices talk whether in an analogue or digital manner while the services refer to the data or the things shared across the network.\n\nTyps of networks\n\nLocal area networks (LAN)\n\nA local area network refers to a network that is based on a small geographic area like an office home or building. A local area network does not necessarily require leased lines, it has higher data transfer rates compared to wide area networks as well as a small geographic radius.\n\nWide area networks (WAN)\n\nA wide area network in contrast to a local area network has a bigger geographical coverage. It could be town-based like a metropolitan network or it could even cover a whole country or continent. It is the kind of networks used for government ministries use, or it can also be used by special departments in ministries of different countries in a certain trading block.\n\nExtranet\n\nThis is a form of a specialized inter-network mostly used by a single organization, or an organization with its close and trusted affiliates. These kinds of networks may either be on a small or big geographical location. It is highly protected and in most cases, persons with permissions to access the site usually have their own usernames and passwords.\n\nPersonal area network (PAN)\n\nThis interconnection of computers and other peripheral devices like printers, scanners and communication devices like faxes and telephone lines is mostly available for the use of one person e.g. the operations manager of a certain firm. It is usually used for interpersonal communication and it could be within a radius of a few meters.\n\nClassification of computer networks\n\nBy use of network layers\n\nThere are two main models fop0r this is the four layers TCP/IP (transport control protocol/internet protocol) model and the seven-layer OSI (open systems interconnection) model. These are generally basic structural models used in the industry.\n\nOn a scale basis\n\nThis classification considers mainly the geographical location coverage of the network e.g., a personal area connection has a smaller scale than a local area network.\n\nConnection method model\n\nThis model involves classifying networks based on the types of hardware that is used to connect the communicating devices. Thus, we might have an Ethernet connection, a wireless connection or a power line communication.\n\nGeneral/ basic hardware necessary for making a computer network\n\nComputer connections are commonly referred to as nodes. Depending on the type and/or function of a computer network there are some basic hard wares required. These basic hardware does include, network interface cards (NICs), switches, routers and bridges.\n\nReferences\n\n 1. Bernard John Poole, essential Microsoft office 2003: Tutorials for Teachers.\n 2. Federal Standard 1037C . Web.\n 3. Larry L. Peterson, Bruce S. Davie (2007), Computer Networks: A Systems Approach, Elsevier.\n 4. Uyless D. Black (1993), Computer Networks: Protocols, Standards and Interfaces, Prentice Hall.",
        "label": "human"
    },
    {
        "input": "Computer Software and Wireless Information Systems Essay\n\nTable of Contents\n 1. Wireless Systems and Software used\n 2. Major issues in developing software\n 3. Conclusion\n 4. Bibliography\n\nPerceiving the constantly accelerating pace of technology adoption, one is forced to believe that wireless technology will fast replace the wired world. However, despite apparent advances in wireless technology, many are skeptical about its future with respect to the complete replacement of wired communication. Many IT specialists believe that the major barrier to the spread of wireless networks will be the little amount of innovation and investment in the industry of wireless systems support software (Orr, K., December 2001). However, many believe that organizations may be slow in responding to the growing needs of software for wireless systems, but they are not oblivious to them. Software development programs are being initiated by a number of companies to support wireless systems. Cutter Consortium conducted a survey in which 37% of IT professionals said that their companies were planning to develop wireless applications (Orr, K., Dec 2001). Therefore, we hope that in due time, there will be many issues regarding software support to wireless systems. The developers are only busy with that how far this technology will go and how big the market for its software will be in the future.\n\nThe first example of the wireless technology that comes to one\u2019s mind is the cell phone technology, and perhaps it is one of the most widely used network technologies in history. Cell phone networks make use of GSM and CDMA technologies in most countries. Mobile internet devices also include Personal Digital Assistants (PDAs). Wireless Local Area Networks (WLAN) is a relatively recent technology. In our paper, we will concentrate on the applications and operating software used by mobile networking devices and WLAN.\n\nWireless Systems and Software used\n\nWLAN is a technology that allows people in a limited geographical area to be linked to each other, just in the way a normal Local Area Network or LAN works \u2013 the only difference is that WLAN is wireless. WLAN could be used in organizations to interconnect the entire organization. We see examples of WLAN in shopping malls and hotels where hotspots or Wi-Fi systems allow people in the area with appropriate devices and software to access the services of the internet through this WLAN. As Goth (2006) notes, \u201cIn mid-August, Google launched Google Wi-Fi, a free wireless network for users in the city of Mountain View. In early September, a consortium including IBM, Cisco, Azulstar, and SeaKay won a contract for a 1,500-square-mile network intended to serve 42 entities and 2.4 million people in Silicon Valley, including every city in both San Mateo and Santa Clara counties.\u201d\n\nSoftware for mobile devices is more difficult to design because it will be developed and tested in a totally different environment, i.e., Solaris or MS Windows machine, from the one it will eventually be run on. Applications can be developed in several environments, for example, J2ME Wireless Toolkit. Software to download applications on cell phones includes Motorola iDEN Update Software Application/Java Application Loader. (Mahmoud, K. H. and Lorain N., May 2002).\n\nLLAN technology requires both operating software and applications. These applications must provide the system with security as well as efficiency. Thick and thin access point wireless solutions are provided by Cisco. They have recently acquired thin access point technology of Airespace, putting Cisco \u201cin the driver\u2019s seat for enterprise-grade wireless.\u201d Other networks are also in work: \u201cWhile not as big, but leveraging the benefits of thin access points are Aruba Wireless Networks and Trapeze Networks.\u201d (Gilliot, I.)\n\nMajor issues in developing software\n\nDevelopers of software for mobile internet devices face constraints from the limited memory space and processing power available for the application. The display screens of cell phones are also very small, and so there is very little information that can be made to appear on the screen at one time. Developers who may be used to working on large computer systems may find it hard to work with such limited power of the device. Besides the device the software is being designed for, there are limitations of the wireless environment as well. \u201cWireless networks are unreliable and expensive, and bandwidth is low. They tend to experience more network errors than wired networks.\n\nThe very mobility of wireless devices increases the risk that a connection will be lost or degraded.\u201d (Mahmoud, K. H. and Lorain N., May 2002)\n\nThere are further challenges posed by a wireless environment that developers have to face. Communication over a wireless system is prone to interference, creating \u201ctransmission errors.\u201d \u201cWireless network protocols may be able to detect and correct some errors, but you need to come up with error-handling strategies that address all the kinds of transmission errors that are likely to occur.\u201d (Mahmoud, K. H. and Lorain N., May 2002). This kind of interception could not only lead to inaccuracy of the message delivered but also insecure connections. If data is very sensitive, security is a high priority. Applications must ensure a secure environment for wireless communication. The time required to deliver a message depends on the processing speeds of both the devices involved. A good application must do away with processing delays.\n\nConclusion\n\nSoftware for all wireless technology must have a good architecture, must ensure accuracy, security, and speed of delivery of the message, and must be user-friendly. For the development of wireless software engineering, it is crucial to enhance the interoperability of different software platforms and develop middleware.\n\nModern software engineering is composed of several separate activities such as requirement analysis, test, and implementation. These ones are performed in sectors with not dynamic flowthrough of data. The segment approach in this way precludes the progress in the elaboration of sound wireless applications. That is why the manual process of engineering must be changed to create an effective automated system that will increase efficiency and create friendly interfaces for software applications.\n\nThe utilization of the most up-to-date approaches to software engineering will produce a hugely positive effect on the development of wireless technologies. The development of modern platforms for software engineering and adaptation of wireless hardware leaves no barriers for wireless software production.\n\nBibliography\n\n 1. Gilliot, I. (n.d.). The Business Case for Wireless Software Applications in the Enterprise .\n 2. Goth, G. (2006). It\u2019s a WLAN-derful Life.\n 3. Mahmoud, K. H. and Lorain N. (2002). Wireless Software Design Techniques . Sun Developer Network. Web.\n 4. Orr, K. (2001). Wireless: The Next Big Thing? Web.\n 5. Richards, K. (2005). Enterprise WLAN Growing up .",
        "label": "human"
    },
    {
        "input": "Computer Virus User Awareness Research Paper\n\nTable of Contents\n 1. What is a computer virus?\n 2. What does a virus do to a computer?\n 3. How can we protect ourselves from computer viruses?\n 4. References\n\nOne of the most frustrating things in the use of computer technology is being infected by computer viruses! Imagine yourself conveniently working on a computer on a very important task, and then all of a sudden, the file you have been slaving on for nights on end disappears! Most likely, it has been \u201ceaten up\u201d by an irritating, nerve-wracking computer virus.\n\nWhat is a computer virus?\n\nA computer virus \u201c is a software program that attaches itself to, overwrites or otherwise replaces another program in order to reproduce itself without the knowledge of the computer user ,\u201d as defined by Collette Dilly (2001). It is actually similar to a biological virus wherein both the computer and biological virus share the same characteristic of \u201cinfecting\u201d their hosts and have the ability to be passed on from one computer to another.\n\nJust like humans, a computer that is infected with a virus also becomes \u201csickly\u201d and thus, prone to suffer malfunction in its operation or running of computer programs and software.\n\nTrading programs with other people\u2019s computers may contaminate one\u2019s computer programs. The use of modems necessary to connect to the internet can likewise acquire viruses. Some unsuspecting users may get it from seemingly innocent emails.\n\nWhen this program enters your computer through your input device, it hides in your computer\u2019s memory and starts to duplicate itself like a disease. When you save your data, you also save the virus. Slowly but surely, the virus crowds out your data and causes major system problems. (Trickum Middle School, 1997)\n\nWhat does a virus do to a computer?\n\nThese computer viruses are actually created and developed by people using bits and codes designed to adapt to a computer\u2019s system or files and data. Depending on the particular type or kind of computer virus, the effects of viral infection on computers may range from a simple display of some sort of messages to a devastating crash of your computer system and programs.\n\nThe most common types of computer viruses are Trojan Horses, E-mail viruses, and Worms. A Trojan horse is simply a computer program. The program claims to do one thing (it may claim to be a game) but instead does damage when you run it (it may erase your hard disk). Trojan horses have no way to replicate automatically. An e-mail or network virus moves around in e-mail messages and usually replicates itself by automatically mailing itself to dozens of people in the victim\u2019s e-mail address book. A worm is a small piece of software that uses computer networks and security holes to replicate itself. A copy of the worm scans the network for another machine that has a specific security hole. It copies itself to the new machine using the security hole and then starts replicating from there, as well.\n\nHow can we protect ourselves from computer viruses?\n\nUsers should also consider the variety of anti-virus products currently available to protect their computers.. There are three classes of anti-virus products: detection tools, identification tools, and removal tools. Scanners are an example of both detection and identification tools. Vulnerability monitors and modification detection programs are both examples of detection tools. Disinfectors are examples of removal tools.\n\n  * Such anti-virus programs must be used to scan the computer of probable existing viruses. Never insert floppy disks or CDs from unreliable and unknown sources. Scan them with the antivirus before running them on the computer.\n  * Avoid opening emails from unknown sources. They may contain malicious information as well as destructive viruses. Download mails with care. Scan attachment with anti-virus software. Big email companies like Yahoo! usually have their own reliable default anti-virus software that automatically scans the attachments.\n  * Back up files for security reasons. In case the computer gets infected and needs to be reformatted, then it is ensured that important files have been saved in a separate folder or CD.\n\nNow that awareness of the management of a computer virus is widespread; it is a comfort to know that it is not a hopeless case! The important thing is to maintain a clean and virus-free computer to save one\u2019s files\u2026 and one\u2019s sanity!\n\nReferences\n\n 1. Dilly, C. (2001) \u201c Computer Viruses, Hoaxes and Protection \u201d Web.\n 2. Trickum Middle School (1997) \u201cWill a Computer Virus Strike Your Computer?\u201d Computer Viruses? What Really Is It? Vol.1 Issue 1, 1997\n 3. Brain, M. (n.d.) \u201c How Computer Viruses Work \u201d Web.\n 4. Burgess Forensics (2006) \u201cWhat are Computer Viruses, Trojans and Zombies?\u201d Web.",
        "label": "human"
    },
    {
        "input": "Computer Vision: Tracking the Hand Using Bayesian Models Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Theory of Bayesian Model\n 3. The Model\n 4. Conclusion\n 5. References\n\nIntroduction\n\nThe visual tracking has been a major challenge for the researchers for decades. The visual tracking is a projection of the movement of an object over an extended time frame. Based on the history of the movement, the moving object is checked and projected to identify the future positions it would occupy. This work has brought in a number of theories and working models into fore. The research has existed, particularly, on recognizing hand movement (D Hogg, 1983).\n\nDetection of the movement of hand, recording of the history of such movements, depends on a number of factors. This includes background (V. Athitsos and S. Sclaroff) for the movement, the skin color and even wrist delimitation (R. Rosales, V. Athitsos, L. Sigal, and S. Scarloff) apart from a number of other factors. The speed of the hand movement also plays an important role in identifying or tracking the hand movement (J. M. Rehg and T. Kanade). After the movement has occurred the pose of the hand can be reconstructed using a Kinematic model reconstruction as shown by (Y. Wu and T. S. Huang). In this case also, it can be found that edge conditions, contours and color have to be taken into account when simulating the movement of the hand (V. Athitsos and S. Sclaroff). There had been studies based on both two dimensional (McCormick & Isard) as well as three dimensional movement of hand. In case of two dimensional it is easier because of the limitations in the degrees of freedom. In the case of three dimensional studies, the degrees of freedom are six, whereas it is only four for two dimensions, taking into consideration rigid objects like that of a hand.\n\nIn order to bring into focus the entire tracking of the hand, it has been a practice to simulate or build a model of the hand movement. Models are reconstructed based on either planar patches (J. Yang, W. Lu, and A. Waibel), polygon meshes or generalized cylinders. Any of these methods can be employed to bring in a model while simulating the entire track of the hand. In order to mathematically identify the geometric location of the hand, a Bayesian filter is made use of. Many researchers have employed recursive Bayesian filter to realize the model. In many cases, this has been influenced by the Kalman filter and a combination of the two can be employed for the purpose. In the following paragraphs, the Bayesian filter has been adopted for the purpose and is presented below.\n\nTheory of Bayesian Model\n\nModel based studies of the hand movement are based on the following standard processes.\n\nFigure 1. Standard components of a model-based tracking system\n\nThe model collects the input image and does a feature extraction of the inputs. Based on this, it generates a new position and projects the image. A routine is planned that would identify the changes or differences between the actual image and the realized one subsequently. This would, in turn, would bring out the error between the two. The error thus identified is used to bring out a better image using the feedback loop created in the model. This model would provide a closed loop control of the entire work and thereby, enable the model to improve upon itself over a period of time.\n\nBayesian model is based on the similar structure with a feedback loop in it. Rehg and Kanade, first employed this model for the purpose of building the track of a moving hand. They employed 27 degrees of freedom and reduced the error using the square differences method and reduction using the Gauss-Newton algorithm. The mathematical model below is built based on these concepts.\n\nThe Model\n\nLet P t be the position of the hand at time t in a plane defined by coordinates x and y. In this case let us consider only two dimensional visualizing and a degree of freedom of four.\n\nTherefore, P t = f(Px t , Py t )\n\nPosition of the hand at time t-1 will be:\n\nP t-1 = f(Px (t-1) , Py (t-1) )\n\nThe velocity vectors at this point of time t are:\n\nV t = f(Vx t , Vy t )\n\nAnd at time t-1 it is:\n\nV t-1 = f(Vx (t-1) , Vy (t-1) )\n\nBased on the probabilistic approach of the recursive Bayesian method, the probability of a specific movement occurring after the current point of time is estimated as Probability Pr. Pr is expected to be any of the degrees of freedom allowed. In this case of two dimensional studies, it is taken to be four or a eight depending upon the need. In this case, we will consider this to be eight. The following probabilities are assumed for this purpose. Pr(left), Pr(right), Pr(up), Pr(down), Pr(left-up), Pr(right-up), Pr(left-down) and Pr(right-down).\n\nAs per the first order Markov assumption, the velocity and position at time t is dependent only on the same at t-1.\n\ni.e. V t = f(V t-1 )\n\nand the second assumption states that the velocity at t will be conditionally independent of all previous velocities. In order to track the movement of a hand, the actual position at time t is taken to be X t and that of the projected position and velocity at time t as Z t where X t and Z t are the functions of V t and the error correction or recursive update is a function of both these variables.\n\nThe distribution can be taken from the Bayesian rule which indicates for the above variables:\n\nThis projection is also called the Chapman-Kolmogorov equation (A. H. Jazwinski).\n\nConclusion\n\nThe \u2018slight\u2019 of hand has always been an intriguing prediction problem for the mathematicians and researchers. This probability based prediction bases itself in correcting and learning through the process. This will ensure that the model presented will have the option of becoming better as it progresses and would be able to present a much better result over a period of time. The fitment and the estimation of the probability itself has been varied starting from Gauss Newton algorithm to other normalization distributions.\n\nReferences\n\nA. H. Jazwinski. Stochastic Processes and Filtering Theory . Academic Press, New York, 1970.\n\nD. Hogg. Model-based vision: a program to see a walking person. Image and Vision Computing , 1(1):5.20, 1983.\n\nV. Athitsos and S. Sclaroff. An appearance-based framework for 3D hand shape classi_cation and camera viewpoint estimation. In IEEE Conference on Face and Gesture Recognition , 45.50, Washington DC, 2002.\n\nR. Rosales, V. Athitsos, L. Sigal, and S. Scarloff. 3D hand pose reconstruction using specialized mappings. In Proc. 8th Int. Conf. on Computer Vision , volume I, 378.385, Vancouver, Canada, 2001.\n\nJ. M. Rehg and T. Kanade. Visual tracking of high DOF articulated structures: an application to human hand tracking. In Proc. 3rd European Conf. on Computer Vision , volume II, 35.46, 1994.\n\nJ. MacCormick and M. Isard. Partitioned sampling, articulated objects, and interface-quality hand tracking. In Proc. 6th European Conf. on Computer Vision , volume 2, 3.19, Dublin, Ireland, 2000.\n\nY. Wu and T. S. Huang. Capturing articulated human hand motion: A divide-and conquer approach. In Proc. 7th Int. Conf. on Computer Vision , volume I, 606.611, Corfu, Greece, 1999.\n\nJ. Yang, W. Lu, and A. Waibel. Skin-color modeling and adaptation. In Proc. 3 rd Asian Conf. on Computer Vision , 687.694, Hong Kong, China, 1998.\n",
        "label": "human"
    },
    {
        "input": "Growing Compatibility Issues: Computers and User Privacy Essay\n\nTable of Contents\n 1. Introduction\n 2. ISP\u2019s Role in Cyber Ethics\n 3. Anonymity And Cyber Ethics\n 4. Conclusion\n 5. Works Cited\n\nIntroduction\n\nA crucial part of the ethics of business is computer ethics or information ethics. Most corporations today are teetering on whether computer improprieties are a violation of professional ethic rather than a legal ethics issue. The purpose of this paper will be to examine some of the ethical issues of the Internet as it relates to the theft of private or personal information from the material sent over the Internet. Professional ethics can best be defined as learning what is right or wrong as it relates to the workplace and then doing the right thing. This code of professional ethics lays down the standards of integrity, professionalism and confidentiality which all members of that particular profession shall be bound to respect in their work. Where as legal ethics is best defined as principles of conduct that members of the profession are expected to observe in the constraints of the governing laws.\n\nThe right to privacy in Internet activity, especially in creating databases out of personal information, is a serious issue facing society. As such it raises serious ethical issues. An additional example is of the people on the Internet who use anonymous servers as a way to avoid responsibility for controversial and inappropriate behavior. Cases of harassment and abuse have become increasingly frequent, aided by a cloak of anonymity. There are also problems with fraud and scam artists who elude law enforcement authorities through anonymous mailings and postings. These types of examples describe the ethical issues created by technology and the people or corporations that control them (Tavani, pp. 179-85).\n\nISP\u2019s Role in Cyber Ethics\n\nISPs have rightfully asserted that prescreening would be especially burdensome, they have not made the same argument that post screening would impose similar economic or administrative burdens. ISPs act like publishers when they sponsor or operate newsletters or other online publications over which they exercise editorial control. At other times, when they are simply functioning as a conduit for other information content providers, their role is equivalent to a distributor. Clearly there must be a higher standard of liability when they assume a publisher like role. If an ISP functions as a publisher it must be held to a higher standard of liability; that is, it must be held accountable for defamatory remarks in the same way that the New York Times or other media would be held accountable.\n\nIn most situations, however, ISPs will not be acting as publishers but as distributors, passive conduits for the exchange of information by their legions of subscribers. In this context, ISPs should assume responsibility for post screening even if the law allows them to do otherwise. They should not take refuge in misguided policies and questionable legal precedents. But the policy should also be changed so that no one is victimized by an intransigent ISP that fails to live up to its moral obligations. Unless we abandon blanket immunity for ISPs and reach the type of compromise sketched out here, it is likely that ISPs will become the unwitting accomplices of many Internet defamers, who are often hiding behind the cloak of anonymity. Libelous speech is different from pornography and hate speech; it cannot be regulated from the bottom up through code. It requires some regulation from the top down through carefully crafted statutes. Unfortunately, the current statute is inimical to the interests of the Internet community.\n\nLessig (1999) argues that both fair use and the entry of works into the public domain will be jeopardized by these systems, since nothing requires that the balance now provided by copyright law be preserved. The problem is that those writing the rights-management code can embed their own intellectual property regulations into that code: They can program the system to charge a fee for any use and ignore any fair use or first-sale considerations by anchoring the content to a specific user. It is possible, of course, that rights-management systems will be constructed that will at least try to strike the right balance. (Lessing, pp. 133-36) Lessig seems to deny this, but some developers of DRM may realize that there are moral and social issues at stake here and will work to preserve fair use or its equivalent in their systems.\n\nIt would be irresponsible and imprudent to design these rights-management systems without allowing for fair use and without respecting other safety values such as first-sale. There is a lively debate about the technological feasibility of developing a system that would include a realistic provision for fair use, but that discussion is beyond the scope of this analysis. Suffice it to say that doing so will be a challenge, since system developers will need to anticipate the myriad array of fair use requests without being duped by those trying to manipulate the system. According to industry analysts Ashish Singh, \u201cFair use algorithms could be written into the code and that would become the hook, the attractiveness of the product\u201d (Howe, pp. 10-11).\n\nAnonymity And Cyber Ethics\n\nOne of the biggest security problems for the Net is the fact that individuals and organizations can still misrepresent themselves with impunity. At present, there is no uniform system or mechanism for identifying users who frequent cyberspace. The Internet does support architectures that facilitate identification, such as password protections, biometric systems, and digital certificates. It is still quite possible for users to interact in cyberspace anonymously, and it can be difficult to trace the real identity of users who are deliberately trying to conceal their identity. While anonymity supports privacy and free-speech rights, it also interferes with security and the curtailment of cyber crime. Hence, the lack of an identifying infrastructure has sometimes been detrimental for electronic commerce and for effective law enforcement in the realm of cyberspace.\n\nThe interconnected issues of digital identity and anonymity are highly charged ones that stir deep emotions. This was evidenced by the heated response to Intel Corporation\u2019s announcement in February 1999 about its plan to embed identification numbers in its next generation of computer chips, the Pentium III. The primary purpose of the embedded serial numbers was to authenticate a user\u2019s identity in e-mail communications, enhance security for electronic commerce by reducing the risk of fraud, and allow organizations to better track their computer equipment. Privacy advocates, on the other hand, argued that this unique identifier would enable direct marketers and others to surreptitiously track a user\u2019s meandering through various Web sites. While Intel capitulated to this intense pressure and agreed to ship its products with the serial numbers turned off, the incident seemed to elevate awareness about the tenuous future of electronic anonymity. Also, the incorporation of identity features into chip technology has not been abandoned by Intel.\n\nDespite Intel\u2019s quick response, there is growing anxiety that these serial numbers are \u201charbingers of a trend toward ever more invasive surveillance networks\u201d (Markoff, C1). What if governments throughout the world attempt to mandate the deployment of such invasive identifying numbers in order to keep better track of their respective citizens? According to security expert Vernor Vinge, \u201cThe ultimate danger is that the government will mandate that each chip will have a special logic added to track identities in cyberspace\u201d (Markoff, C1).\n\nThe Intel incident also graphically illustrates the difficult dilemma faced by policy makers involving an unavoidable trade-off between security and anonymity or privacy. Security and anonymity seem to be mutually exclusive goods. If we really want to make the Internet a more secure environment it is necessary to suppress anonymous transactions and hold users accountable for what they say and what they do. For example, thanks to a tracking mechanism installed in Microsoft\u2019s Office software, the rogue programmer who released the destructive Melissa virus was swiftly apprehended. But the cost of security and better accountability is a loss of privacy and perhaps the termination of untraceable Internet communications.\n\nThere is a cost to preserving anonymity; its central importance in human affairs is certainly beyond dispute. From a moral perspective, it is a positive good, and it is valued as highly instrumental in helping to realize two other goods that are vital for human fulfillment, freedom and privacy. Anonymous communication in cyberspace is enabled largely through the use of anonymous remailers in conjunction with cryptography. A brief word about these is in order. According to Lohr (1999), an anonymous remailer functions like a \u201ctechnological buffer.\u201d It strips off the identifying information on an e-mail message and substitutes an anonymous code or a random number. By encrypting a message and then routing that message through a series of anonymous remailers a user can rest assured that his or her message will remain anonymous and confidential. This process is known as \u201cchained remailing.\u201d (Lohr, 5-6) The process is quite effective, because none of the remailers will have the key to read the encrypted message, neither the recipient nor any remailers (except the first) in the chain can identify the sender, and the recipient cannot connect the sender to the message unless every single remailer in the chain cooperates. This would assume that each remailer kept a log of their ingoing and outgoing mail, and that is highly unlikely.\n\nAccording to Froomkin, this technique of chained remailing is about as close as we can come on the Internet to \u201cuntraceable anonymity\u201d; that is, \u201ca communication for which the author is simply not identifiable at all.\u201d (Froomkin, p. 245) If someone clandestinely leaves a bunch of political pamphlets in the town square with no identifying marks or signatures, that communication is also characterized by untraceable anonymity. In cyberspace things are a bit more complicated, and even the method of chained remailing is not foolproof: If the anonymous remailers join together in some sort of conspiracy to reveal someone\u2019s identity there is not much anyone can do to preserve anonymity. Anonymity can also be useful for revealing trade secrets or violating other intellectual property protections. In general, secrecy and anonymity are not beneficial for society if they are used improperly.\n\nThus, anonymity can be exploited for many forms of mischief. There are concerns about anonymity abuses from both the private and public sectors. In the private sector there are worries about libel, fraud, and theft, while the state is worried about the use of anonymity to launder money, to evade taxes, to manipulate securities markets, and so forth. Hence, there is the temptation for governments or even digital infrastructure providers, such as ISPs or companies like Intel and Microsoft, to develop and utilize architectures that will make Internet users more accountable and less able to hide behind the shield of anonymity (Nissenbaum, pp. 141-44).\n\nIn response to these concerns, there are several options available for more comprehensive digital identity systems. The most thorough system would ensure that there is always an indissoluble link between one\u2019s cyberspace identity and one\u2019s real identity. This is accomplished by somehow mandating the traceability of all Internet transactions. The use of technologies such as chained remailers helps protect anonymous communications so that they are untraceable. But what mechanisms might be adopted to mandate traceability; that is, to make the untraceable traceable? One way to achieve mandatory traceability is to demand a user\u2019s identification as a precondition of Internet access. The government might also implement such a system by law, by requiring that all ISPs demand verifiable identification as a prerequisite for access to the Internet. There are many variations on these two broad approaches.\n\nThe basic idea behind any system of mandatory traceability is that speakers entering cyberspace would be required to deposit (e.g., with the ISP), or attach to their communications, a means of tracing their identities. One can conceptualize mandatory traceability by positing a regime in which an encrypted fingerprint automatically would be attached to every transaction in cyberspace. In such a regime, the fingerprint could be encrypted with the government\u2019s public key such that properly authorized law enforcement officials could access the private key necessary for decryption while participants in the cyber-transaction would not be able to strip away the speaker\u2019s anonymity. Digital certificate technology could also play some role in the development of such an identity infrastructure. (Fitzgerald, pp. 77-80) It would provide a feasible method of authenticating individuals and verifying the integrity of their transmissions. Recall that digital certificates allow individuals or organizations that use the Internet to verify each other\u2019s identity.\n\nConclusion\n\nThe Net\u2019s inherent vulnerabilities have been exploited by hackers and other miscreants who frequent cyberspace. Among the architectures that accomplish this goal, there are firewalls, powerful encryption software and digital identity systems. Paradoxically, some of the architectures like encryption code that secure the Net and safeguard privacy obstruct the efforts of law-enforcement authorities to deal with cyber crime and computer-related crime. The protection of public safety can sometimes conflict with respect for basic civil liberties. There is a tension between a perfectly secure Internet, where all transactions are traceable and users are held accountable, and an Internet that respects the values of privacy and anonymous free expression. Anonymous expression is an important social value worth preserving in cyberspace. Therefore, architectures and technical standards utilized to implement a digital identity system or other security mechanisms must not preclude the possibility for anonymous expression or create serious new privacy hazards.\n\nWe can preserve the integrity of the Internet as a tool of autonomy and a forum for creativity if we comport ourselves in cyberspace in an ethical manner. Each member of the cyber community must engage in activities that support the collective values of that community and refrain from activities that regard it as a commodity. The Internet, like all public goods, is vulnerable to abuses and excesses that can endanger its fragile ecology and some of those abuses can be the result of poorly crafted code. The global nature of the Internet makes addressing the legal issues associated with Internet or information privacy daunting and complex. It is a legal arena without walls or physical boundaries, where the laws vary from country to country. Even within the United States there is dissent and disagreement about the definitions of Internet Privacy, who owns that information and what constitutes appropriate or inappropriate use of that information. Perhaps the issues that have been unsuccessfully resolved through the law can be resolved through the creation of moral and ethical guidelines that will frame the issues, at which point legal protections can be put in place.\n\nWorks Cited\n\n 1. Fitzgerald, A. (2000). Going Digital 2000: Legal Issues for E-Commerce, Software and the Internet. St. Leonard\u2019s, Australia: Prospect Media. 77-80\n 2. Froomkin, M. (1996). \u201cFlood Control on the Information Ocean: Living with Anonymity, Digital Cash, and Distributed Data Bases.\u201d University of Pittsburgh Journal of Law and Commerce 395:245.\n 3. Howe, Jacob and Andy King. Three Optimisations for Sharing. Technical Report, Computing Laboratory, University of Kent at Canterbury, 2001. 10-11\n 4. Lessig, L. (1999), Code and Other Laws of Cyberspace, New York: Basic Books.133-36\n 5. Lohr, S. (1999). \u201cPrivacy on the Internet Poses Legal Puzzle.\u201d New York Times. 5-6\n 6. Markoff, J. (1999). \u201cGrowing Compatibility Issues: Computers and User Privacy.\u201d New York Times, C1.\n 7. Nissenbaum, H. \u201cThe Meaning of Anonymity in an Information Age\u201d The Information Society 15: 1999; 141-144\n 8. Tavani, H. (2001). \u201cDefining the Boundaries of Computer Crime: Piracy, Break-Ins, and Sabotage in Cyberspace.\u201d In Readings in Cyberethics, edited by R.Spinello and H.Tavani. Sudbury, Mass.: Jones and Bartlett.179-85",
        "label": "human"
    },
    {
        "input": "The Concept of Computer Hardware and Software Essay\n\nTable of Contents\n 1. Computer Hardware\n 2. Computer Software\n 3. Seworld IT system\n 4. References\n\nComputer Hardware\n\nThese are the physical components of a computer system. They include the input and output devices such as the mouse keyboard or the monitor. They can also be the storage devices such as Hard disk or RAM. The physical devices can still be the components that are responsible for the execution of the programs in a computer such as a microprocessor.\n\nComputer Software\n\nThese are set of instructions that a computer uses to execute a given task. They can be classified as:\n\n  * System Software-These is the programs that help to run and manage the computer hardware. They regulate the execution processes and the allocation of memory in a computer. They include programs like operating systems such as XP and Linux. In addition, they can be the software that manages a computer network such as the OSI or the IP/IPT.\n  * Application Software-These are programs that are designed to enable a user to accomplish a certain task. They include business programs, games, or database programs that collect and organize data in a systematic and logical order for quick and easy retrials.\n\nSeworld IT system\n\nAn efficient information system is very vital to any business firm. This is because it ensures effective communication of all the organization\u2019s departments. This will in return boost the performance in the organization for quality and quantity production and marketing of the firm\u2019s products or services. (Ronald J. 1998).\n\nThis is not the case with SewWorld because we can see that communication in this organization is very poor. There is no good coordination for the various departments present and also among their branches. This is why we have cases, where some branches are overstocked with a certain product while others do not have even a single one. The reporting system is also very poor and unreliable. Therefore the company needs to establish a local area network that will enable the various departments and branches to coordinate well. It will also help to centralize the managerial to reduce the number of managers supervising each department. This is because with a good LAN all the departments can feed their data online to a central system for easy analyses and helps also genera rate reliable reports as data is collected from all the departments. (Ronald J. 1998).\n\nThere is also a need for the company to build a wide area network that will link its branches in all the states. This will enhance communication among the branches and help them to work in harmony for high performance. This network will also enable SewWorld to market and sell its product through the Internet, a way that will boost its performance very much. (Web.).\n\nThe other thing that should be looked at in SewWorld is the installation of security systems in the network established. This will guarantee that unauthorized users, which can alter some data in the organization, that can turn to be very detrimental later, will not access its network. If these factors are put into consideration in SewWorld Company, its performance will rise up due to the enhanced communication as a result of the established LAN and WAN networks. (Ronald J. 1998).\n\nReferences\n\nRonald J. Tooci (1998) Microcomputer Network Prentice-Hall Publishing Press NewYork.\n",
        "label": "human"
    },
    {
        "input": "Computer Financial Systems and the Labor Market Essay\n\nIntroduction\n\nThe rapid and stable growth of the innovation computer base in various fields indicates that modern technologies have become an integral part of almost every industry\u2019s work. Specialized equipment and software developed for solving the problems of different profiles make it possible to simplify operating processes and, in many respects, speed up professional activities. With regard to the field of finance and accounting, computer systems are indispensable since digital technologies contribute to conducting complex computational operations and even planning certain stages of working with cash flows.\n\nNevertheless, when analyzing this topic from a different angle, one can note a controversial aspect of computers\u2019 total distribution \u2013 the level of employment and job opportunities. Performing a number of functions on computing equipment reduces the need for human activities significantly, which is a potential problem of modern times. This paper aims to describe the trend of technological progress, the causes and advantages of developments in computer financial systems, as well as the implications of the transition to digital tools for the labor market. Combining artificial intelligence with human work may help avoid negative consequences and achieve providing jobs to those who need them.\n\nModern Advanced Technology Development\n\nIn a modern technological environment, the tasks of financial management and control over profits and expenses are largely carried out through the introduction of highly efficient computer systems and software. According to Radu and Marius (2012), accounting programs based on the use of advances in the development of artificial intelligence, bioengineering, and other related areas provide an opportunity to establish a stable mode of operation. In addition, as Tan and Laswad (2018) note, the knowledge of the specifics of corresponding software opens up substantial prospects for those who intend to work in the accounting field. Project planning and analysis systems allow minimizing financial and other risks, which is an absolute plus and rationale to support the development trend and its logical background.\n\nA large number of start-up companies working on creating relevant business software are the evidence of a course for innovation. P\u00e9ralte (2018) argues that the stages of monitoring a specific environment, analyzing interventions, and launching specific innovations may be performed by using off-the-shelf software. Such an approach is common and popular due to reduced responsibility for the decision-making process and simplified accounting. According to Tsai, Lee, Shen, and Lin (2012), the range of diversity of these computerized systems is extensive, and preferences for a particular application are given due to the functionality of the programs and their relevance in a particular financial environment. Therefore, competition among the manufacturers of such software is high and tense.\n\nThe trend of technological advancement is widespread, which explains the constant appearance of new products in specialized markets. In the field of finance and accounting, those programs and applications that allow controlling any changes and predicting risks serve as standard operating tools. The use of such systems is not a desire to demonstrate the high status of a company but a necessity since many procedures may be performed by utilizing appropriate software. As Adams and Link (2018) state, intellectual property is a valuable component in an increasingly competitive environment, and ensuring sustainable working principles to promote modern technology is a step towards its preservation. Therefore, the modern trend of innovation is natural and reasonable in the current market conditions.\n\nReasons for Introducing Advanced Computer Systems and Software\n\nCreating professional computer systems designed to work with financial reporting is a necessary practice in the context of dynamic economic indicators and the use of various planning tools. According to F\u00fcssl, Streitferdt, Shang, and Triebel (2015), \u201csoftware development teams have to adapt continuously for fitting newest stakeholder needs and finding success in the market\u201d (p. 1). As a result, updating the existing software components and making innovative computer systems are the consequences of the growing interest in improving the performance of the digital base. Therefore, one of the key reasons for introducing advanced technological mechanisms is the need to conduct operations that meet contemporary quality standards.\n\nThe financial sector is not the only one where involving the latest digital developments is a relevant practice. For instance, Marks (2010) mentions the medical sphere and notes that progress in diagnostics and screening testing is largely due to the development of appropriate computer systems. The analysis of academic literature proves that engaging special software is essential to not only avoid mistakes when planning activities but also not to lose market positions. As Monostori et al. (2016) remark, competitive advantage, as a factor associated with profit, is achieved due to smart cyber-innovations that allow maintaining operating activities at the level that satisfies consumers. Therefore, the introduction of advanced software is relevant not only in the field of financial reporting but also in other sectors.\n\nThe evaluation of the effectiveness and working performance are the aspects of planning that encourage stakeholders to utilize high-tech computer systems for accounting. Romero and Vernadat (2016) state that the promotion of corporate value as one of the management strategies is the principle that may be implemented through the use of specialized monitoring software. Despite the potential challenges that may arise in the course of work, the need to apply for such systems outweighs the fears of their uselessness. Therefore, the trend of involving digital systems is strategic and logical due to ever-growing competition conditions and the importance of control over all processes within companies.\n\nAdvantages of Developing Computer Financial Systems\n\nThe development of computer systems and high-performance software in the financial and accounting sectors has a number of advantages achieved due to the high functionality of these tools. Coe and Yeung (2015) argue that one of the main merits is the price benefit that advanced digital programs provide. Interested parties do not have to spend substantial funds on the activities of analysts who monitor all the changes in the workflow and make development forecasts. In addition, as Wuest, Weimer, Irgens, and Thoben (2016) note, these systems make it possible to optimize quality control and maintain consistently high productivity. These properties are the factors in support of the use of innovative software in the financial sector.\n\nAnother indisputable and aforementioned benefit is a competitive advantage that is acquired through the introduction of appropriate digital systems. Hsu, Tian, and Xu (2014) state that an opportunity to perform all accounting calculations competently and quickly through the use of high-precision functionality enhances the status of a certain company and makes its activities credible. Accordingly, the interest of consumers, investors, and other stakeholders increases, which has a positive effect on profits and allows maintaining a high corporate value. This outcome is essential for both start-up firms and experienced companies. Therefore, the involvement of specialized software has positive implications for the authority of organizations.\n\nFinally, the advantage expressed in strategic success is one of the factors for introducing computer systems into financial and reporting activities. According to Nwankpa (2015), the standardization and orderliness of management are the outcomes of work on promoting the innovative development of the accounting sector due to the ability to control all the income and expense procedures. The absence of errors due to the human factor allows avoiding undesirable results and, consequently, increases productivity. Moreover, as Nwankpa (2015) remarks, the entire information infrastructure is available for reorganization, which allows for adjusting the current mode of work. All the considered advantages prove the relevance and benefits of computer systems in the financial and accounting fields and may serve as justifications in support of the acquisition of appropriate software based on artificial intelligence.\n\nDecreasing Job Opportunities Due to Computer Systems and Software\n\nDespite the advantages that may be achieved through the introduction of computer systems and software in financial activities, there is an issue that is acute because of the global spread of such functionality. This is the problem of the labor market and job opportunities for people whose duties are increasingly being performed by digital programs. Frey and Osborne (2017) note that \u201cthe poor performance of labor markets across advanced economies has intensified the debate about technological unemployment among economists\u201d (p. 254). Technological progress that proceeds rapidly entails a reduction in opportunities for applicants due to the fact that artificial intelligence is able to perform similar functions while avoiding errors. Chamoso, Rivas, Rodr\u00edguez, and Bajo (2018) remark that there are communities for those who are willing to combine their activities with the work of digital equipment. Nevertheless, despite this possibility, the risk of reducing the number of jobs is objective, and specialists with financial and economic education may feel the negative consequences of the transition to computerized practices.\n\nConclusions about the potential reduction of jobs are reasonable and logical. According to Peng, Wang, and Han (2018), some procedures, in particular, routine tasks for standardized calculations and budget planning, are increasingly performed by digital programs rather than people. At the same time, the motives of individual companies\u2019 managers are understandable and explainable since they do not need to pay subordinates for the work that special applications do stably and, in addition, for free. As Prasad and Green (2015) state, modernized accounting systems provide decision-makers with valuable information, and engaging additional labor is unjustified. Applicants, in turn, are unhappy with this situation, and Willison, Warkentin, and Johnston (2016) note that the attitude towards technological progress becomes ambiguous and sometimes negative. In order to avoid this, it is crucial to integrate the software into the workflow harmoniously. Otherwise, computer systems will force out human labor and create a severe problem of unemployment in the economic sector, which can be even a more challenging issue for this sphere than insufficient productivity.\n\nCombining Artificial Intelligence and Human Operating\n\nIn order to prevent an unemployment crisis and provide jobs for all applicants, it is essential to integrate modern technologies wisely and not to use them to perform all the tasks without exception. Managers should understand and distinguish among those functions that people and programs cope with successfully. For instance, according to Agrawal, Gans, and Goldfarb (2019), today, forecasting is the area of \u200b\u200bfinancial activity that special computer systems do better than analysts. However, the work of ordinary employees can also be valuable and useful. Davenport and Ronanki (2018) note that the characteristics of human nature make it possible to maintain a more creative environment for the implementation of the tasks set. Routine procedures that artificial intelligence copes with are sometimes combined with situational approaches. In case this function is assigned to personnel, productivity may be improved, and valuable and talented employees will not lose their jobs.\n\nThe negative consequences of the widespread introduction of computer systems in financial activities can manifest themselves in an insufficiently high corporate culture. As Wright and Schultz (2018) state, \u201cautomation could decrease the regularity and quality of human interactions, which may contribute to greater feelings of isolation and disconnection\u201d (p. 826). In order to prevent this, it is necessary to integrate modern software into the workflow of companies so that it could contribute to increased work efficiency but not its decline. According to Pueyo (2018), people tend to take the initiative and motivate colleagues to improve performance. If managers make efforts to enhance the communicative competence of employees, this will prove the viability of human labor and prevent computer systems from crowding it out completely.\n\nConclusion\n\nCombining the opportunities achieved by introducing innovative computer systems in the financial and accounting industries with human resources is a good solution for reducing the likelihood of an unemployment crisis. The advantages of modern software aimed at predicting operating activities and control over budget funds are undeniable. However, employees\u2019 activities also have some strengths, in particular, the ability to adapt to specific working conditions and situations. Today, the tendency to transition to a computerized model is relevant. At the same time, in order to save job opportunities for applicants and maintain a high level of efficiency, it is crucial to integrate digital systems and programs into the work process but not to give preferences to them exclusively. The proper combination of computer technology and employee incentive strategies may help maintain a high level of corporate culture and withstand market competition.\n\nReferences\n\nAdams, J. D., & Link, A. N. (2018). The structure and performance of US research joint ventures: Inferences and implications from the Advanced Technology Program. Economics of Innovation and New Technology , 27 (5-6), 551-575. Web.\n\nAgrawal, A., Gans, J. S., & Goldfarb, A. (2019). Exploring the impact of artificial intelligence: Prediction versus judgment. Information Economics and Policy . Web.\n\nChamoso, P., Rivas, A., Rodr\u00edguez, S., & Bajo, J. (2018). Relationship recommender system in a business and employment-oriented social network. Information Sciences , 433 , 204-220. Web.\n\nCoe, N. M., & Yeung, H. W. C. (2015). Global production networks: Theorizing economic development in an interconnected world . Oxford, UK: Oxford University Press.\n\nDavenport, T. H., & Ronanki, R. (2018). Artificial intelligence for the real world. Harvard Business Review , 96 (1), 108-116.\n\nFrey, C. B., & Osborne, M. A. (2017). The future of employment: How susceptible are jobs to computerisation? Technological Forecasting and Social Change , 114 , 254-280. Web.\n\nF\u00fcssl, F. F., Streitferdt, D., Shang, W., & Triebel, A. (2015). Introducing a method for modeling knowledge bases in expert systems using the example of large software development projects. International Journal of Advanced Computer Science and Applications , 6 (12), 1-17. Web.\n\nHsu, P. H., Tian, X., & Xu, Y. (2014). Financial development and innovation: Cross-country evidence. Journal of Financial Economics , 112 (1), 116-135. Web.\n\nMarks, A. (2010). The professional status of massage therapists: Experience, employability, and evolution. Journal of Human Resource Costing & Accounting , 14 (2), 129-150. Web.\n\nMonostori, L., K\u00e1d\u00e1r, B., Bauernhansl, T., Kondoh, S., Kumara, S., Reinhart, G.,\u2026 Ueda, K. (2016). Cyber-physical systems in manufacturing. CIRP Annals \u2013 Manufacturing Technology , 65 (2), 621-641. Web.\n\nNwankpa, J. K. (2015). ERP system usage and benefit: A model of antecedents and outcomes. Computers in Human Behavior , 45 , 335-344. Web.\n\nPeng, G., Wang, Y., & Han, G. (2018). Information technology and employment: The impact of job tasks and worker skills. Journal of Industrial Relations , 60 (2), 201-223. Web.\n\nP\u00e9ralte, P. (2018). Elavon invests in FinTech Innovation with Georgia Tech\u2019s Advanced Technology Development Center. Georgia Tech . Web.\n\nPrasad, A., & Green, P. (2015). Organizational competencies and dynamic accounting information system capability: Impact on AIS processes and firm performance. Journal of Information Systems , 29 (3), 123-149. Web.\n\nPueyo, S. (2018). Growth, degrowth, and the challenge of artificial superintelligence. Journal of Cleaner Production , 197 , 1731-1736. Web.\n\nRadu, D., & Marius, D. (2012). Changes caused by computerization in accounting management. Annals of the University of Oradea, Economic Science Series , 21 (2), 655-661.\n\nRomero, D., & Vernadat, F. (2016). Enterprise information systems state of the art: Past, present and future trends. Computers in Industry , 79 , 3-13. Web.\n\nTan, L. M., & Laswad, F. (2018). Professional skills required of accountants: What do job advertisements tell us? Accounting Education , 27 (4), 403-432. Web.\n\nTsai, W. H., Lee, P. L., Shen, Y. S., & Lin, H. L. (2012). A comprehensive study of the relationship between enterprise resource planning selection criteria and enterprise resource planning system success. Information & Management , 49 (1), 36-46. Web.\n\nWillison, R., Warkentin, M., & Johnston, A. C. (2016). Examining employee computer abuse intentions: Insights from justice, deterrence and neutralization perspectives. Information Systems Journal , 28 (2), 266-293. Web.\n\nWright, S. A., & Schultz, A. E. (2018). The rising tide of artificial intelligence and business automation: Developing an ethical framework. Business Horizons , 61 (6), 823-832. Web.\n\nWuest, T., Weimer, D., Irgens, C., & Thoben, K. D. (2016). Machine learning in manufacturing: Advantages, challenges, and applications. Production & Manufacturing Research , 4 (1), 23-45. Web.\n",
        "label": "human"
    },
    {
        "input": "Firewalls in Computer Security Term Paper\n\nTable of Contents\n 1. Introduction\n 2. Main text\n 3. Conclusion\n 4. Bibliography\n\nIntroduction\n\nComputer security is one of the branches of technology, and as far as it is applied in the field of computer, it is known as information security. The aims of computer security are far fetched but mostly encompass shielding information from theft, distortion or making the preservation of information possible. Computer security instill conditions in the computer that are different from most conditions in\n\nother systems as far as what computers are supposed to do is concerned. However, these conditions make computer security a challenging issue since it makes computer programs to carry out only that which is required of them and in a specific manner. This limits the scope and the speed with which the computer program is supposed to operate. Computer security programs aim at lessening these inhibitions by transforming these negative constrains into positive enforceable principles (Layton, 55). Therefore, computer security can be said to be more technical and mathematical in comparison to other computer science related fields. However, it must be noted that the main issue of concern in information security and /or computer security is the protection of information that is stored, or processed or worked on by the computer. This is true whether it is the protection of the computer hardware or the systems software that is involved.\n\nMain text\n\nMuch developments and evolution has taken place in the field of computer security so that presently it is globally held that there are four most common typical approaches to attaining computer security. The first approach involves the imposition of the physical barring of access to computers to computer security compromisers. The second and third approaches involve the use of hardware programs to set in rules on different programs to avoid susceptible computer programs, and third approach being the use of operating systems mechanisms that entrench rules on programs to avoid gullible computer programs, respectively (Peltier, 256). Much of the operating system security technology is based on the 1980\u2019s science which has been used to produce the most impenetrable operating systems. However, presently, they have been put into limited use since they are both laborious and very technical (and therefore, little understood about them for efficient and maximum exploitation). An example of this is the Bell- La Padula Model. The fourth approach involves the use of program strategies to make computer programs highly reliable and able to withstand subversion.\n\nA firewall is a configured device designed to allow, reject, encrypt, or proxy all the traffic of the computers between diverse domain sections, based upon a set of specific rules. Conversely, it may be defined as a dedicated appliance equivalent to a software connection running on another computer which checks the network traffic passing through it and rejects or allows passage following a set of rules. The basic function of the firewall is to control the flow of traffic between or among computer networks depending on the different trust level. There are three different trust levels, and these are the Internet, which is a total no trust zone (this is so since the Internet is highly porous to all materials that can be sent through the web), the Internet link which is a higher trust zone, and the demilitarized zone (the DMZ), an intermediate trust level which is located between the Internet and the perimeter network which is the trusted internal network. The firewall operates on a rule of \u201cdefault deny\u201d as a methodology of allowing in only the designated network connections for entry and locking out the rest. Without proper configuration, firewall can be almost useless.\n\nHistorically, the term \u201cfirewall\u201d was used to refer to measures taken to ward off fire from reaching buildings. With later developments,the term came to be used to refer to the structure or metal sheet that severs the engine compartment from the passenger\u2019s cabin in a vehicle or an aircraft. In the 1980\u2019s, the firewall computer technology emerged when the Internet was still a fledgling in terms of global use and connectivity. The antecedent factors that pushed for firewall\u2019s introduction were the discovery of German spies trifling with the Internet system by Clifford Stoll, Bill Cheswick\u2019s 1992 manipulation of electronic devices in jails to observe attacks (this demonstrated clearly that Internet users were not safe but were susceptible to spying and unwarranted interference either by online criminals with vast computer and Internet acumen, or by computer bugs), the 1988 viral invasion on the NASA\u2019s American Research Center in California, and the first large scale Internet attack by the Morris Worm.\n\nThere are diverse types of firewall classification and these are based according to qualities and characteristics such as speed, flexibility, and simplicity, greater authentication and higher logging speed. Types of firewalls that fall under the rubric of speed, flexibility, and simplicity include the Packet filter firewall and the Stateful inspection firewall (whose modi operandi have been discussed in the succeeding paragraphs). On the other hand, those that are classified under greater authentication and higher logging capacity include the Application Proxy Gateway firewall and the Dedicated Proxy Servers. These too have been delved on in the succeeding paragraphs.\n\nIn 1988, the first paper on firewall technology appeared following the Digital Equipment Corporation conducted a series of researches and subsequently came up with a filter system known as the Packet Filter Firewall. This was the first generation of the highly evolved technical Internet security system. Later improvements came, courtesy of Bill Cheswick and Steve Bellovin of the AT & T Bell Labs on packet filtering.\n\nPacket filtering works by inspecting packets which are the basic units of data transfer betwixt the inter connected computers. The packet filters by following a set of rules, achieve this feat by dropping the packet and sending the error responses back to the source if the packet corresponds the packet filter (Zhang and Zheng, 300). This type of packet filtering filters out every packet depending on the information in the packet, and pays no attention to whether or not the packet falls within the rubric of the already existing string of traffic.\n\nThe packet filters work at a network layer of 1-3 and operates very efficiently since they only inspect the header of the packet. Initially, there was the stateless firewall which lacked the capacity to detect whether or not a part of a packet was already inside an already existing connection or not. An example of this is the File Transfer Protocol which opens opens up itself to arbitrary ports by design. This type of firewall operates by maintaining a table having open connections which are associated with new connection requests and already existing connections that are held as legitimate.\n\nThe packet filters and the stateless firewall work efficiently also because they check against the IP address and other involved ports in relation to the connections and the sequence of the packets encircling the connections. This, the stateful firewall is able to achieve since it has the capacity to contain memory in significant range of every connection from the beginning to the end.\n\nThe packet filters, when the client starts a new connection,sends a set of SYN bit in the header of the packet. The firewall in turn deems all sets with SYN bits, new connections. The service replies the SYN packet upon the service being asked by the client being available. Should the client respond with an ACK bit packet,the connection enters an established state. Having by passed all outgoing packet, the firewall accepts incoming packets that have been established as an already existing o r established connections. Thus, the hackers are kept from being able to start unwanted connections in the protected machine. If there is no traffic that has passed, stale connections are deleted from the state table to keep the latter from overflowing. To prevent dropping connections, the firewall sends periodic updating messages to the user. Thus the packet filters work.\n\nOne of the side effects of the pure packet filters is that they are highly susceptible to spoofing attacks. This is so because packet filters do not posses the concept of of a state as dwelt on in the field of computer science and computer security. On this premise, the pure application firewalls are also vulnerable to exploits especially at the hands of online criminals.\n\nA layer is a collection of functions that are interrelated to offer and receive services to other layers above it and from those below it respectively. The history of layers stretch back from 1977 when work under the American National Standard Institute working group, was carried out on a layered model; to become the Operational Standard model to come up with the Distributed Systems.\n\nAlthough the ISO has influenced the Internet protocol, yet none has done so heavily as the concrete operational system model.\n\nThe application layer explicitly interfaces and discharges application services to facilitate the application process and also forwards requests to the presentation layers. The application layer exists to offer services to the user- defined processes of application, and not to the end user. For instance, the application layer defines the transfer protocol file, but still refers the end user to follow the application process to facilitate the transfer of files.\n\nThe primary functions performed by the application layer include the the facilitation of the the applications and the end user process, and the identification of the of the communication partners. Application layer facilitates services for the transfer of of files, e- mailing, and the running of other network software services (Abrams, Jajodia, Podell, 199). In addition to all these, the application layer bolsters the aspect of privacy and establishes and authenticates the application user and marks and identifies the quality of services that are offered. Examples of application layer include, the Telnet, the FTP and Tiered. The application layer also marks constrains on the data syntax.\n\nUnder the application layer, there is the senior sub layer which offers functional services which entail association control, remote operations on the servicing elements and the facilitation of all the transactional processes. Above the common sub layer of the common application services, there are important application programs such as; File transfer (FTAM), Directory (*500), Messaging (*400) and Batch job manipulation.\n\nIn computer networking, proxy servers are computer application systems or programs which by forwarding clients\u2019 requests to ether servers, serve the clients\u2019 interests. On the client connecting to the proxy server in order to request for some servicer (for example, connecting to a web page or retrieving a file), the proxy server will respond by providing resources by linking to the server and inquiring the services on the client\u2019s behalf. Sometimes, the proxy server may change the client\u2019s requests or the feedback from the server without having notified the particular server. Herein, there is a proxy server that transcends all the requests and issues general responses- and this being called the gateway or the tunneling point. Proxies are connectible with the user\u2019s local computers or alternatively at particular main points between the Internet and the destination server. There are myriad types of and functions of proxies as discussed forthwith.\n\nThe caching proxy server which caters for requests without corresponding with the particular server by tracing the previously saved client request. Whereas this process is known as caching, it is worth noting that caching proxies maintain domestic copes of sources that are most frequently requested, and thus allowing big organizations to plummet their the company\u2019s upstream bandwidth dispensation expense while increasing the performance at the same time.\n\nThere is also the content filtering web proxy that conducts the administrative regulation over the relayable content through the proxy. This type of proxy is used in both commercial and non commercial agencies for example in schools to instill conformity in use. The common approaches include the URL regex filtering, DNS blacklist, the URL, and the content filtering. This technique is mostly employed because it facilitates authentication to allow the web page control. Saperate from this, there is also the web proxy which majors on the WWW traffic and commonly acts as the web cache. This method is vastly employed in the corporate domains and this has been highly evidenced by the increasing application of the Linux in both small and large enterprises, and at homes. Examples of this include the Squid and the Net Cache which allows the filtering of the content and thus providing ways to refuse access to some URLs specified in the blacklist.\n\nAnonymizing proxy servers otherwise known as a web proxy anonymizes the web. However, this facility is susceptible to being overridden by the administrators of sites and thus remaining useless. Nevertheless, this form of proxy is able to facilitate the control accessibility since it implements log on requirements. This helps in the organizations being able to limit the web access to authorized users only and also helps keep abreast on how the web is being used by employees.\n\nIntercepting or the transparent proxies integrates a gateway with a proxy server. This type of proxy has been vastly used in businesses to increase the use of policies, and to ease the administrative load on the premise that the configuration of the client users browser is not a prerequisite. Intercepting proxies are detectable through the comparison of the HTTP header and the IP address. Hostile proxy as the name suggests is normally set up by cyber criminals to access the flow of data between the client and the web. The panacea to this is arrived at by changing the pass word that is used to access online services on detecting proxy that is unauthorized.\n\nWhile on the one hand transparent proxy leaves open the response beyond the proxy authentication, non transparent proxy on the other hand modifies responses to bring more services to the group of users. Open proxy is a server set by administrators to counter abuse so as to deny access to open proxies services. Another form of countering this problem is by testing the system of the client to detect open proxy.\n\nForced proxy takes away the traffic on the accessible pathway to the Internet.. Besides this, it also configures proxies to facilitate entrance into the Internet. This operation is expedient for the interception of the TCP connectivity and the HTTP. For instance, the HTTP interceptions affect the usefulness of proxy cache and can therefore impact on the mechanisms of authentication.\n\nThe reverse proxy server is one that is installed within the proximity of a single or multiple web servers. Herein, all the traffic emanating from the Internet into one of the servers web page passes through the reverse proxy server. The reverse proxy server has multiple functions such as the acceleration or the encrypting of the SSL when designing secure websites,compression and the expansion of the web content to catalyze the loading time, and the serving of the static content of the cache so as to offload servers.\n\nThe server proxy is also able to abate resource usage that is incurred due to slowness on the client\u2019s side. This feat is achieved by caching the web server content that has been sent and issuing it to the client in dribs and drabs. This undertaking is known as spoon feeding. The proxy server also beefs up security as an extra stratum of safety and can therefore shield against attacks that are known to be web server specific.\n\nThere are also special kinds of proxies- the extra net publishing, a reverse type of proxy server used for communicating to an internal server that has been firewalled, and issuing extranet services as the server remains behind firewalls.\n\nIn computer networking, the network address translation (NAT) which is also known as the network masquerading, IP masquerading, and the native address translation, is a technique that works towards the transception of the network traffic by using the router that entails re- encoding the IP address destination, the TCP or the UDP\u2019s IP packets port number on passing through. The methodology also entails using the check sums which are written down to take note of the changes. Most NAT systems do this to capacitate a myriad hosts on undisclosed networks to reach the Internet via the use of IP address.\n\nNAT as a technique first came in as a method of countering the IPV4 shortage and lessening difficulty the IP Address reservation. Recently, there has been a widespread adaption of the technique by countries with less allocation of address book per capita (Bragg, Rhodes- Ousley and Strassberg).\n\nConclusion\n\nNAT adds security by disguising the structure of internal network. This it does by letting the the traffic pass through to the Internet from the local network. As the translation of the source address in every packet is done, the router keeps track of on each connection that is active, and the basic data. Afterwards, the TCP or the UDP port numbers in the case of the overloading of the NAT are used to enable the demultiplexing of the packets. As the source for the traffic, the router reveals itself.\n\nThe merits of using NAT are very far fetched. One of them is that it allows convenience and entrenches minimal costs. The fact that NAT is devoid of full bidirectional connectivity means that it keeps away malicious activities which are carried out by external hosts from permeating local connections. This keeps at bay worms and abates scanning and thus enhancing privacy.\n\nPerhaps the greatest of all these benefits is the fact that NAT solves the problems that result from the exhaustion of the space in the IPV4 Address.\n\nThe drawbacks of the Network Address Translation are also clear. For instance, there are no end- to- end connections at the back of the Network Address Translation- supported routers. This makes it impossible for the system to accommodate the Internet protocol which is very useful. In the same wavelength, this makes it mandatory that servicing receive the TCP connection initiation from external networks. Conversely, to curtail this problem, there will have to be the use of the stateless protocol- the problem with this being that some of these stateless protocol systems such as the UDP are not impregnable to interference or to disruption.\n\nBibliography\n\nAbrams, D. Marshall, Jajodia, Sushil and Podell Harold. Some integrated essays on information security. US: IEEE Computer Society Press, 1994.\n\nBragg, Roberta, Rhodes- Ousley, Mark and Strassberg, Keith. A complete reference of network security. US: Mc Graw Hill Professional, 2003.\n\nLayton, P. Timothy. Information security: measurements and compliance. US: CRC Press, 2006\n\nPeltier, R. Thomas. Guidelines to information security policies. US: CRC Press, 2001.\n\nZhang, Kan and Zheng, Yuliang. The seventh information security conference. US: Springer Press, 2004.\n",
        "label": "human"
    },
    {
        "input": "Modern Portable Computer \u2013 Battery Technology, LCD Displays, Low-Power CPUs Report\n\nTable of Contents\n 1. Introduction\n 2. Laptop Batteries\n 3. Laptop Displays and Energy-saving CPUs\n 4. References\n\nIntroduction\n\nLaptops, palmtops, tablets and PDAs are computing devices that are being increasingly adopted for a variety of uses having an acute need of mobility while computing. These devices work with different hardware configurations than a conventional desktop computer and it is very important for any user of such devices to know about the technicals and working of the main hardware parts of these mobile computing devices in order to ensure uninterrupted and continuous usage. This paper focuses on the batteries, display and some power-saving CPU developments in the laptops of today.\n\nLaptop Batteries\n\nBatteries of the laptop are devices that enable a laptop to function without the main AC power supply. This working is supported for all laptop operations for anywhere between 1-4 hours depending upon the quality and the technology of the battery. A laptop like a desktop has CMOS & Clock Backup batteries which keeps u the date and the time when the laptop is turned off. This ensures that the restarted laptop carries on with the correct time and date. These are internal batteries of the laptop and these have another important function of saving the BIOS setup configuration, a program that is critically needed to reboot the system when restarted. Usually, the CMOS batteries are based on Lithium, Nickel Cadmium (NiCad) and alkaline technology and these are found fixed at one place on the motherboard. These batteries work with a current ranging between 3-7.2 volts.\n\nRegarding the main laptop batteries, Wikipedia (2007) states as follows, \u201cLaptops usually run on a single main battery or from an external AC/DC adapter which can charge the battery while also supplying power to the computer itself. Many computers also have a 3volt cell to run the clock and other processes in the event of a power failure [which confirms above narration]\u201d. These main batteries have undergone rapid technology change and revolution over the years. Earlier alkaline, Lithium and NiCad batteries. While the former two were not chargeable and had to be replaced by equivalent new batteries; NiCad batteries were chargeable. An improvement over NiCad batteries came in the form of NiMH batteries (which was based on evolved and refined technology).NiMH batteries gave outputs of higher energy densities in comparison to NiCad batteries. For the same battery weight, a NiMH supplied twice the amount of power of a NiCad. This means that a leaner laptop can work for more hours with NiMH. NiMH batteries also do not have memory capture which means that NiMH clearly recognizes a full charge from a partial charge, unlike NiCads which were unable to distinguish this. Thus NiMH is more reliable and maintenance-easy. The latest development in battery technology is the Li-Ion battery technology. This has now become the benchmark battery technology. It has some advantages over the NiMH which the NiMH enjoyed over the NiCad batteries i.e. Li-Ion batteries have the same output as NiMH batteries but weigh nearly 35% less. This helps laptops turn leaner and less weight; in addition, Li-Ion batteries do not have any memory effect and are green products as they are not based on toxic chemicals such as Mercury or Cadmium. Random Access Memory (RAM) of some laptops are fitted with a backup battery which is a bridge or auxiliary battery. This makes the device switch on to such a battery in case the main battery fails. This helps the user with time to change the main battery as well as saves the settings and configuration. Frequently rechargeable NiCad or NiMH batteries are used as RAM backups. Smart batteries are equipped with chips that help them communicate with the laptop on battery performance, output voltage, temperature etc. These perform about 15% more than their non-smart counterparts. Kamal et al (2007) write about the latest development in battery technology, in a PowerPoint presentation at the Intel Development Forum, as follows, \u201cHigh capacity battery technology has potential to High capacity battery technology has potential to support the vision of 8 hours of battery life-support the vision of 8 hours of battery life; fuel cell technology holds promise but vendors Fuel cell technology holds promise but vendors need to continue to pay attention to cost and form need to continue to pay attention to cost and form factor, and Intel AMPS allows support for new battery chemistries as well as fuel cells chemistries as well as fuel cells.\u201d\n\nThe most critical power measurement used in relation to the laptop batteries is the Ah (Ampere hours) which can be then combined with the power consumption or power load of the laptop to calculate the run time of the battery. Several formulae have been devised to calculate such a run time. For instance, for a 12 v battery the following formula calculates the battery run time: (10 X (Battery capacity in Ampere hours))/ (Load Power in Watts)\n\nUsing the above formula one can calculate the run time of the chosen battery and go in for one or more batteries depending upon the requirements of computing. More battery cells simply augment the run time in any laptop as it improves the AH.\n\nLaptop Displays and Energy-saving CPUs\n\nThe laptop is critically dependent on the quality of its display and the capability of such a display to handle high-resolution graphics that are based on substantial data. The display technology also interacts strongly with the power consumption of the laptop and thus affects the backup time of the battery. Keeping these aspects in mind the laptop display technology stabilized around two main streams.Wikipedia(2007) explains the history and technicals as follows, \u201cBy about 1991, two new colour LCD technologies hit the mainstream market in a big way; Dual STN and TFT. The Dual STN screens solved many of the viewing problems of STN at a very affordable price and the TFT screens offered excellent viewing quality although initially at a steep price. DSTN continued to offer a significant cost advantage over TFT until the mid-90s before the cost delta dropped to the point that DSTN was no longer used in notebooks. Improvements in production technology meant displays became larger, sharper, had higher native resolutions, faster response time and could display colour with great accuracy, making them an acceptable substitute for a traditional CRT monitor\u201d.\n\nThe screen resolution is decided by the number of pixels a particular display is capable of handling. As Wikipedia (2007-i) explains, \u201cA pixel (short for pi c ture element, using the common abbreviation \u201cpix\u201d for \u201cpicture\u201d) is a single point in a graphic image. The intensity of each pixel is variable; in colour systems, each pixel has typically three or four dimensions of variability such as red, green, and blue, or cyan, magenta, yellow, and black\u2026.. A pixel is generally thought of as the smallest complete sample of an image. The more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution , though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a \u201cthree-megapixel\u201d digital camera , which has a nominal three million pixels, or as a pair of numbers, as in a \u201c640 by 480 display\u201d, which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display), and therefore has a total number of 640 \u00d7 480 = 307,200 pixels or 0.3 megapixels\u201d.Similarly, all other screen resolutions can give out the pixels, in megapixels(one million pixels) which they can handle by simply multiplying the two measures of the screen resolution.. Recently LED-based LCD standardization is being attempted to improve the battery life and reduce the power consumed in laptop displays. As Kamal et al (2007-i) explain in an Intel Development forum presentation,\u201d Reducing display backlight power is essential to extending the battery life of the platform\u201d. They correlate the energy consumption with the display by reducing the statistics that the display subsystem(panel electronics, lighting system and the inverter) may consume anywhere between 33-40 % of the total power. They go on to explain that the new LED-based backlighting system will have the following major benefits in terms of energy saving, \u201c Support for thin and light form factor; RoHS compliant (mercury-free, lead-free); Low Voltage DC, no inverter; Vibration and shock resistant; Instant light on and off (less than 100 ns); Temperature range (-40 ????85\u00b0C); Luminance, uniformity & colour gamut same or better; Scalability; Easy LED driving & unlimited dimming, and direct CCFL replacement possible, no additional system cost!\u201d\n\nThe above development had the parallel development in energy saving CPUs by major manufacturers like Intel. As Shultz (2007) explains, \u201cThe road to dual-core processors really began when Intel and AMD discovered that the goal of improving processor efficiency and performance by increasing the clock speeds towards the 3-GHz mark and beyond was actually having the opposite effect. The faster clock speeds were producing more heat and consuming more energy, both of which were hindering the efficiency and performance of the processors. Both companies realized that they needed to move towards dual-core processor technology in order to continue to improve processor performance. Dual-core processors contain two processor cores on one chip; consequently, they can simultaneously perform calculations on two streams of data, which increases efficiency and speed when running multiple programs. This is especially true when running new, multi-threaded software, such as video and audio editing applications\u201d Shultz pinpoints the energy-saving aspect of the dual-core technology as follows, \u201cIn developing the Core brand of processors, Intel took the low heat and low voltage technologies developed for its recent mobile processor, the Pentium M, along with the EM64T 64-bit architecture technology, and combined that with its newly developed dual-core technology\u201d.\n\nReferences\n\n 1. Laptop . Wikipedia.(2007). Web.\n 2. Kamal Shah ,Shinichi Itoh,Will Hill &Calvin Shu.(2007). Laptop PC Power Sources:\n 3. An Update. Intel Development Forum.\n 4. Pixel . Wikipedia.(2007-i). Web.\n 5. Kamal Shah,Kevin Bosse ,K. Sugimoto,Bill Densham,Francis Nguyen, &.Ran Ghoman.(2007-i). LED As Laptop PC LCD Backlight: A Panel Discussion. Intel Development Forum.\n 6. Shultz,Greg.(2007). SolutionBase: Surveying the dual-core processor landscape . Web.",
        "label": "human"
    },
    {
        "input": "Use and Benefit of Computer Analysis Essay\n\nAbstract\n\nA computer is an important device for managing huge data. The introduction of computers, therefore, has improved the level of service delivery and thus enhances convenience and comfort. This has also helped governments to achieve economic benefits.\n\nUse and Benefit of Computer\n\nThe computer is a scientific device that accepts log-in information and thereafter manipulates it to produce the desired result based on the program of instruction on how data is to be processed. Computer technology has been around for hundreds of years, \u201cfor instance during the 17th century, Blaise Pascal invented the adding machine that operated as a digital calculator, but was only made up of mechanical gears, levers and counting wheels.\u201d (Garson 1999, p, 43)\n\nThe uses and benefits of computers are diverse and paramount for socio-economic development. Public administration offices were known for the mismanagement of public information. With the invention of the computer, today there is improved safety with the information in both public and private hands. This is therefore restoring the public trust in the ability of institutions to handle sensitive information.\n\nNext, organizations and public offices are known to handle a large volume of data, have been salvaged from huge workloads by the introduction of computers. Such large amounts of information include; benefit data like pensions, unemployment benefits, and other social services. Today, the huge crowd that used to gather by the government offices has considerably reduced.\n\nAnother benefit accruing from the introduction of computers is the ability of the world to manage networks of communication. This has greatly improved the level of interaction of the world. Thus fostering friendship and economic improvements. This in conjunction with the world wide web and other internet services promotes globalization thereby making the world to be like a village.\n\nBefore the introduction of computers, productivity and efficiency particularly in the government sector were questionable. Like a tortoise, many organizations and government institutions crumpled to beat deadlines yet it was never easy. Garsen shares the same sentiment when he says that, \u201cCitizen and businesses increasingly demanded that governments programs made sense and worked efficiently. They began to expect better levels of services instead of the fragmented, duplicative and lengthy processes that had come to characterize government operations.\u2019 (Garson, 1999, p.44)\n\nAs the discussion come to an end, the introduction of computer has availed a bundle of devices that promote luxuriant living. These promote individual comfort and convenience. For instance, the barcoding machine has enabled instant register pricing, updating of stock levels, and recording personal buying patterns. Not forgetting devices such s the use of ATM \u2013 machines; GPs machines; internet connecting for shopping, education, and productivity; and office network and robotic transforming the workplace and productivity.\n\nDespite the many uses and benefits the world has gained from this introduction of the computer, there are still a number of challenges that threaten strain and strangle its importance. One of the challenges is the lack of basic support systems. For example, \u201cmost clinicians continue to practice, \u201cmemory-based medicine with inexplicably wide variances diagnosis, treatment, and outcome\u201d(Ceruzi 2003, p,40)\n\nThe introduction of the computer has therefore come with a great array of uses and benefits. This is a great invention to transform the world. Even though challenges threaten to bottle-neck its importance, restructuring of the organization will afford its success.\n\nReferences\n\nCeruzz, (2003f). A History of Modern computing.Londo:MIT press.\n\nGarson G.D. (1999) Information Technology and Computer Application: USA: Idea Group Publishing.\n",
        "label": "human"
    },
    {
        "input": "Computer Engineer Stephen Wozniak Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Early Life\n 3. Birth of Apple Computers Inc\n 4. Apple II Computer\n 5. Departure from Apple Computer Inc\n 6. Return to Apple Computers Inc\n 7. Conclusion\n 8. Reference\n\nIntroduction\n\nStephen Wozniak is a Computer Engineer and a co-founder of Apple Computer, currently known as Apple Inc. He is credited with the invention of Apple I and Apple II computers in the 1970s. His inventions made significant contributions to the revolution of the Personal Computer. The Apple II computer was the most popular and best-selling personal computer in the late 1970s through early 1980s.\n\nEarly Life\n\nStephen Wozniak was born on 11th August 1950 in Santa Clara Valley, California; this place is currently known as Silicon Valley. His mother was the president of the California Women Republican Club and his father was an engineer at Lockheed Corporation, A company that manufactured aircraft. It is important to note that at the time of Wozniak\u2019s birth, Santa Clara Valley was a technological hub with all sorts of scientific inventions.\n\nAs a child, Wozniak was so much engrossed with electronics and mathematics. His love for mathematics as a child contributed greatly to his ambitions to become an engineer. As a young boy, Wozniak was very bright, that he would make electronics devices such as games, radios, etc from scratch. School bored him and this is demonstrated by his dropping out of Colorado and California University in two years, between 1970 and 1971.\n\nAfter dropping from the University of California in Berkeley, he started working for Hewlett-Packard. It was during this period that he met John Draper, another smart young man, enthralled by electronics. At the time, Draper was working on an illegal telephone accessory known as a Blue box that would let users make free phone calls. It was from his close contact with Draper that Wozniak was introduced to the world of circuit board modifications and personal computer platforms.\n\nAs years went by and continued to work for Hewlett Packard, he met another employee known as Steve Jobs. Steve was another young man who was so much into computers and electronics. This was the same period when video game lovers and computer hobbyists were teeming up in Silicon Valley. There was this computer club known as Homebrew computer Club, in which Wozniak and Steve Jobs joined. This Club and many other can be credited with the development of the first computer microprocessor. It was around that time in 1975, when the first computer components, known as Altair 8800 was introduced to the market.\n\nBirth of Apple Computers Inc\n\nAltair 8800 was expensive and very slow. Wozniak couldn\u2019t afford to buy one, but he was quick to learn on the model, and together with Jobs, they settled down to design their computer on a single board using a cheaper microprocessor with additional memory chips. The computer had a video interface integrated into the computer and an On-board Random Access Memory (ROM) which controlled the loading of external programs. It was faster than Altair. This first computer is what they called Apple I.\n\nWith Apple I reality, it was agreed that the job would engage in marketing the computer with a price tag of $666, while Wozniak works toward improving it. Over 600 units of Apple, I was sold, mainly to computer hobbyists, generating more than $774,000. This was a huge score for Wozniak and Jobs.\n\nWozniak was by now working harder to improve Apple I, maintaining the basic model, and in 1977; he succeeded in building Apple II. It was at this time he resigned from Hewlett Packard and together with Jobs formed Apple Computer Inc.\n\nApple II Computer\n\nA game called Breakout contributed greatly to the success of the Apple II computer. Wozniak had written the basic program for translating instructions to machine language, known as Basin interpreter for this particular game. In a bid to improve it, he would join hands with another colleague known as Randy Wigginton to design chip-based paddle circuits and sound systems in Apple II computers. Next, they made simple Operating software that would be commanded by one letter keyboard command to load files from a specific location of the disk. It was after much improvement on this model that they came up with DOS 3.1. It is important to note that Apple II, unlike Apple I had a built-in circuit that could allow it to be interfaced with a color monitor or even a television, with simple modifications and adds.\n\nMost of the software that was provided with Apple computers was written by Wozniak. To promote the sale and use of Apple II, Wozniak invited computer programmers to write a program for Apple II, from which a lot of programs were generated. For five consecutive years, Apple II was the best-selling computer, raking over $139 million. By the time Apple Computer Inc. was going public in 1980, it had grown by 700 percent although a later version of apple personal computers, Apple II Plus and Apple III, were not as popular as Apple II.\n\n1981 would see a decline in the performance of Apple computers. Apple III computers for example had to recall after some technicalities were detected. This incidence caused massive layoffs. At this time round, Wozniak was deeply involved in writing some important math formulas for Spreadsheet that apple was to developing to compete against other personal computer companies that were coming up. This was the same time that he was working on the Macintosh project.\n\nDeparture from Apple Computer Inc\n\nStephen Wozniak\u2019s first departure from Apple was fateful. In February 1982, he was involved in plane accidents that show him lose his memories. Wozniak could hardly recall anything. On recovery, he decided to leave Apple computer briefly to concentrate on personal interests unrelated to computers and technology. This period of departure from Apple computer shows Wozniak marrying and returning to college. He would later graduate with a degree in Computer Science and Electrical Engineering from California University, Berkeley College.\n\nReturn to Apple Computers Inc\n\nUpon graduation, Wozniak formed UNISON Corporation, a company that specialized in producing computer educational material aimed at making learning computers easier for students. In 1983, Wozniak returned to Apple computer, by this time, the company had forged ahead and produced many new products. Apple IIe and Apple Lisa were some of the new personal computers that the company had managed to introduce to the market in January 1983. It is important to note that Macintosh was developed successfully and was one product that was a success since its introduction to the market.\n\nStephen Wozniak and Steve Jobs\u2019s contribution to technological advancement did not go unnoticed by a top government official. In 1986, Wozniak and Jobs received the National Technology Medal from US president Ronald Regan. It was at this juncture that Wozniak decided to leave apple computers Inc. to concentrate his effort on services toward humanity. By the time he was leaving Apple Computers, Wozniak had a personal fortune of $45 million. He used a considerable percentage of the money as a donation to various charities.\n\nUnable to let go of his talents, Wozniak joined a Home Electronic Appliances company going by the name Core , where he specialized in making home appliances remote control devices. Later he would leave the company to volunteer as a teacher at Silicon Valley Kindergarten volunteering his services to both the teachers and students.\n\nConclusion\n\nStephen Wozniak has beyond reasonable doubt pioneered an industry that has become part and parcel of our everyday life. His hard works from childhood through adulthood have laid the foundation for future technological advancement for generations to come in the field of computers. His work on Apple I and Apple II computer showed personal commitment to define and set standards in the world of personal computers. Wozniak has been consistent from his childhood through the time of his retirement from active technological involvement. He is one of the few individuals who can be said to have lived life to the fullest.\n\nReference\n\nKeith Elliot Greenberg (1994) Steve Jobs & Stephen Wozniak: Creating the Apple Computer, Blackbirch Publishers, United States.\n\nSteve Wozniak, Co-founder, Apple Computer, Web.\n\nStephen Wozniak Biography, Web.\n",
        "label": "human"
    },
    {
        "input": "Gaming System for Dell Computer: Media Campaign Issues Report\n\nIn the competitive environment it takes huge sums of money, sometimes over long periods, for media campaigns to be really effective. DGS (Gaming System for Dell Computer) is an innovative product which needs creative and unique media campaign to attract potential consumers. Advertisers, on the other hand, should take the attitude that it is indeed possible to know what works, and what doesn\u2019t and why. As a result they can confidently reject attempts to obscure their inquiries or fob off their concerns about whether their advertising is working. Belief in the exclusive power and province of the tribal medicine man belongs to an era that has passed (Evans et al, 2004).\n\nThe media campaign budget will be based on purchase of media time and space. The main media selected for campaign are TV, press and the Internet. Also, the budget will involve all direct and indirect costs of the campaign. The budget should be aligned with the goals and aims of the campaign. The budget process sets up procedures (financial regulations) for authorizing expenditure and the headings under which money can be expended. The accounting system provides information on the levels of actual expenditure against budgeted expenditure. Taking into account experience and success of Dell\u2019s competitor Sony, it will require $127M for media campaign. Dell should be able to get much more effectiveness out of their media campaign budget and out of working with their advertising agency And consumers should be able to accept, without necessarily feeling threatened, that advertising does influence which brands they choose, especially when it does not matter to them personally which brands they choose. Indirect costs will involve personal and administration costs. Usually, it takes 25% of the budget ($31,75M for Dell).\n\nThe appropriate media for the selected strategies and budget will be TV, press and the Internet. The main advantage of TV and commercials is that moving color pictures of actual scenes, people and products have a realism which his not possible via other media. Any effort is necessary for a viewer to absorb television advertising. Commercials will help to reach wide target audience and inform potential buyers about the new product. A television advertisement can be timed to the hour, day, week or program. The advertisement can be repeated every few hours, daily, weekly, etc. A special attention should be given to TV ads. One of the seductive attractions of 15-second commercials is that advertisers can get more repeat exposure for the same ad budget than they can with a 30-second ad (Evans et al, 2004).\n\nPress is another powerful medium for a new product. This media is cost-effective where offering has broad appeal. It can create an impact and be persuasive. It allows more scope for testing than TV and can use advertorials \u2013 ad and editorial combined. The advantage of this medium is that it supports TV and the Internet and persuades potential buyers to try a new product. It can also offer postal response route.\n\nThe Internet medium is important for such product as a gaming system because many potential consumers are advanced Internet users looking for innovative products and solution via this channel. Web banner require low production costs; can use free banner ad exchange schemes or negotiate charges per click through; get more than a response (Evans et al, 2004).\n\nBillboards and posters will be used in big cities around the country in order to ensure high response rate. Also, it is necessary to use repetition in promotion campaign because it is as important as the promotion itself. Research has shown that over 95 percent of people forget the exact message within six weeks of seeing it.\n\nBudget for Dell Gaming System (Total budget \u2013 $127M)\n\nspending                        Media    Million, dollars  % of the budget\nDirect Costs                    $89,25M  75%             \n                                TV       $38,1M            30%            \nPress (newspapers, magazines )  $37,75M  25%             \nOutdoor                         $12,7M   10%             \nInternet                        $12,7M   10%             \nIndirect Costs                  $37,75M  25%             \n\n\nReferences\n\nEvans, M., O\u2019Malley, L., and Patterson, M., (2004), E xploring Direct & Customer Relationship Marketing , 2nd edition, London: Thomson\n",
        "label": "human"
    },
    {
        "input": "Bill Gates\u2019 Contributions to Computer Technology Essay\n\nTable of Contents\n 1. Introduction\n 2. The Child Prodigy\n 3. The Complex Genius\n 4. Gates as a Visionary\n 5. Gates as a Flawed Hero\n 6. Bibliography\n 7. Footnotes\n\nIntroduction\n\nWithout a doubt, the single most influential figure within the field of microcomputer technology is Bill Gates. He was one of the pioneers of microcomputer technology and since his departure from IBM in the early 1980s, he has gone on to dominate the microcomputer software industry. Today, outside of the Apple brand of microcomputers Microsoft\u2019s operating system can be found on each and every new computer sold. In order to examine the contributions made by Mr. Gates, it is prudent that we examine his life as a whole. This examination will begin with his childhood and culminate with his present life.\n\nThe Child Prodigy\n\nUpon examination of articles written about Gates and quotations from Gates recounting his early childhood, several events stand out in significance as key to depicting the future potential of Gates to transform the world with his view of technology. These childhood stories prominently feature Gates\u2019 tenacity and relentlessness, crafting a portrait of a child destined for greatness. The repetition of these stories in the mass media helps form our first portrait of Gates as the child prodigy.\n\nFor example, Gates\u2019 mother, Mary Gates, as a member of the Lakeside School Mothers Club, used rummage sale proceeds to install a computer terminal at the school. Gates became immediately engrossed with it. By eighth grade, Gates was actually earning money doing computer programming. In tenth grade, he wrote a class-scheduling program for the school and began teaching computer classes. The Lakeside Programmers Group offers a first glimpse into Gates\u2019 managerial skills, programming talent, and drive to succeed.\n\nThe Complex Genius\n\nThis multi-faceted portrait of Gates is richly textured, supported by several characterizations of Gates that when combined form the portrait of the complex genius. The constellation of characterizations of this portrait include borderline autism, a demanding/explosive nature, and an uncanny tendency for \u201cbeing right.\u201d The year 1983 includes a significant moment in the public shaping of Gates\u2019 reputation as a complex genius.\n\nIn 1983, at 28 years of age, Gates was named by People magazine as number 6 of the 25 most intriguing people of the year, spawning one of the most frequently printed and repeated summaries of his reputation: \u201cGates is to software what Edison was to the light bulb\u2014part innovator, part entrepreneur and full-time genius.\u201d 1 Gates was the first representative of the computer industry ever chosen for inclusion on People \u2018s list.\n\nPersonal qualities of Gates highlighted in the article include his youth\u2014\u201dthere is a hint of Andy Hardy in his boyish grin and unruly cowlick\u201d\u2014and his genius\u2014\u201dGates got there by writing truly elegant, bug-free computer programs\u2026 some people never do\u2026 slick tight code must be intuitive, a bold leap of microchismo. \u201c 2 When asked by Seattle Times reporter Kathleen Brooks about his reaction to being included in People\u2019s listing, Gates responded: \u201cI was happy that the article told what the company was doing instead of just focusing on me.\u201d 3\n\nWhile that may be true of the article\u2019s text, the editors of People portrayed Gates in a visually compelling light that reinforced the portrait of the complex genius, focusing on the incongruence of the highly successful, yet overly casual and youthful corporate leader. Gates is posed on a sofa resembling a giant computer created for the photo shoot, and was instructed to wear inexpensive white sneakers displayed prominently in the picture, as opposed to the loafers he had come to the studio wearing. Both the sneakers and the couch were paid for by People magazine.\n\nGates as a Visionary\n\nIn his own books and speeches, and in excerpts from media interviews, Gates has sold not only his products, but also his vision of technology\u2019s place in society. This self-portrayal as a Utopian visionary depicts leadership strategies commonly associated with social movement leaders. Hartog and Verburg contend that when communicating a vision, charismatic leaders engage in framing, \u201cplacing the vision in a certain context, interpreting reality for listeners and giving meaning to events.\u201d 4 Journalists were quick to afford Gates a seat at the table of major technology players; only in later years would Gates appear to be dining alone.\n\nIn a 1980 article in On Computing magazine, author Chris Morgan writes that, \u201cWe recently took some time to look into a crystal ball with two of the leading experts in personal computing software, Bill Gates and Dan Flystra.\u201d 5 Gates is characterized simply as \u201cthe young President of Microsoft, Inc.\u201d Gates speaks authoritatively about the future direction of the computer industry. In response to the question, \u201cdo you have a general feel for where the market is going in the 1980s?\u201d Gates responds that in the future, software will define the personal computer. He predicted that the cost of computer hardware would plummet, and that software would become the determining factor in the computer industry\u2019s success.\n\nThe 1981 Seattle Business Journal article attributes Gates\u2019 success to his childhood fascination with computers and his uncanny timing. It summarizes the highlights of Gates\u2019 childhood, beginning with the story of the Lakeside school and its early exposure to computers for the students, and reaching its apex in Gates\u2019 and Allen\u2019s ability to accurately predict the future: \u201cA tiny silicon chip with microscopic circuits was responsible for a technological revolution\u2014a revolution Gates and Allen could anticipate.\u201d 6 Once again, we see how the reputation portraits are not distinct, but rather work together to reinforce Gates\u2019 various public images. Yet in combination, the childhood prodigy, complex genius and secular prophet reputation-portraits function to portray Gates as a uniquely qualified technology industry leader.\n\nGates as a Flawed Hero\n\nNo one can doubt the propensity of Mr. Gates to appear as an imperfect individual. In fact his moral character has been called to question when he obtained the work product of IBM, the company he worked for at the time when IBM dominated the personal computer operating system market. The first instance of Gates as a flawed hero is found in the delay of the highly-hyped product, Microsoft Windows. At New York\u2019s Plaza hotel on November 10, 1983 Gates announced the newest addition to the Microsoft family of software products: Microsoft Windows. The delivery date was to be May 1984.\n\nWith its graphical interface and capability to see multiple \u201cwindows\u201d open simultaneously, this new operating system was hyped as being a milestone for ushering computers into the non-specialist arena. However, the product did not actually make it to market until July 1985. The press characterized this delay as the first major embarrassment for Microsoft. In a 1984 InfoWorld editorial, computer industry professional Stewart Alsop wrote that Microsoft \u201cseems to have fallen into a bad habit with Windows. Announce, show, promise and delay. Then delay some more.\u201d 7\n\nA second embarrassment for Gates and Microsoft comes from their lackluster performance in the computer games market. MSX, a Microsoft-designed operating system made especially for game-oriented computer enthusiasts was poorly received at the Winter Consumer Electronics show in January 1985. A Seattle Times article characterized MSX as \u201cdead technology,\u201d and quoted an Info World column labeling MSX \u201cthe Pong of the 1980s.\u201d 8 Though this article did not mention Gates specifically, still it had a negative impact on his image. After all, Gates and Microsoft are so intimately connected that the tarnishing of one reputation necessarily impacts the other.\n\nFurther, when a company has experienced the phenomenal business success Microsoft enjoyed in its first few years, the media is always ready to find fault. For example, an August 1985 article in The Seattle Times detailed Microsoft\u2019s fiscal sales as growing 44 percent to $140 million, but characterized this growth as \u201ca sign that the company isn\u2019t growing quite as fast as it did in the past eight years.\u201d 9 At the same time as the company began experiencing its first tastes of failure, they also began reaching out directly to consumers\u2014perhaps in response to their brush with negative publicity.\n\nBibliography\n\nBrooks, Kathleen. \u201cMicrosoft\u2019s Bill Gates Is People\u2019s Choice.\u201d The Seattle Times, 1984.\n\nBuck, Richard. \u201cHard Going for Microsoft. Trade Show Is Cool toward Machines with Microsoft Systems,\u201d The Seattle Times, 1985.\n\n\u201cDropping out of Harvard Pays Off for a Computer Whiz Kid Who\u2019s Making Hard Cash from Software.\u201d People Weekly, 1984, 36-37.\n\nHartog, Deanne. \u201cCharisma and Rhetoric: Communicative Techniques of International Business Leaders.\u201d Leadership Quarterly 8 (1997): 355-391.\n\nManes, Stephen, and Paul Andrews. Gates: How Microsoft\u2019s Mogul Reinvented an Industry and Made Himself the Richest Man in America. New York: Simon and Schuster, 1994.\n\nMorgan, Chris. \u201cSoftware in the 80s: Two Interviews.\u201d On Computing, Fall 1980, 36-39.\n\nFootnotes\n\n 1. \u201cDropping Out Of Harvard,\u201d 36.\n 2. Ibid.\n 3. Kathleen Brooks, \u201cMicrosoft\u2019s Bill Gates Is People\u2019s Choice,\u201d The Seattle Times, 1984.\n 4. Deanne Hartog, \u201cCharisma and Rhetoric: Communicative Techniques of International Business Leaders,\u201d Leadership Quarterly 8 (1997): 6\n 5. Chris Morgan, \u201cSoftware in the 80s: Two Interviews,\u201d On Computing, Fall 1980, 36. Dan Flystra was the founder editor of BYTE magazine in 1975 and the founder of the VisiCorp computer corporation. He is currently president of Frontline Systems, Inc., a PC software vendor.\n 6. Chris Morgan, \u201cSoftware in the 80s: Two Interviews,\u201d On Computing, Fall 1980, 36. Dan Flystra was the founder editor of BYTE magazine in 1975 and the founder of the VisiCorp computer corporation. He is currently president of Frontline Systems, Inc., a PC software vendor.\n 7. Stephen Manes and Paul Andrews, Gates: How Microsoft\u2019s Mogul Reinvented an Industry and Made Himself the Richest Man in America (New York: Simon and Schuster, 1994), 274.\n 8. Richard Buck, \u201cHard Going for Microsoft. Trade Show Is Cool toward Machines with Microsoft Systems,\u201d The Seattle Times, 1985.\n 9. Ibid.",
        "label": "human"
    },
    {
        "input": "Computers: Science and Scientists Review Essay\n\nIntroduction\n\nThis essay addresses several important issues concerning career possibilities for computer engineers. It describes the levels of programming skills needed for different job concentrations and is required by employing agencies, additional skills of non-programming origin needed in these areas, describing of past work experience and its relevance to the chosen job concentration and finally the level computer programming experiences and other knowledge.\n\nSoftware engineering\n\nThe programming skills in this job concentration should involve programming fundamentals of computer science.\n\nBasic skills should include problem analysis and modeling, software design, validation and verification, software process, software quality and management, and the ability to work in a software construction team.\n\nBesides software programming skills software engineering goes far beyond programming and includes such knowledge as an ability to meet customers\u2019 requirements, testing, and designing software. The candidates for software engineering programs are to assess customers\u2019 need and develop software which meets these needs. Moreover, frequently a solid knowledge of technical peculiarities of \u2018a real world\u2019 conditions of work is required in this job concentration.\n\nComputer science education with the level of the master is the first precondition for employing in this job concentration. Past work and experience in building high-speed, reliable, and large-scale communication, trading, banking, and other activities are usually needed and should be systematic. Moreover, a commitment to quality and sound engineering should be evidenced by real past experience and activities patterns and a high level of motivation is required.\n\nThe experience level needed for this job concentration differs considerably depending on the job position. But generally professional knowledge and experience in such programming languages as Lisp, C++, Python, and Java are needed.\n\nOften an experience in implementing large-scale software systems is required for important job positions as well as in Internet languages and software programming XSLT, Web services, databases (Oracle), etc.\n\nFor networking software engineering it is necessary that job applicants be proficient in developing various compilers, networking software, and application servers, and operating systems for working with networks. An experience in reading, comprehending, and implementing complex specific domain documents and design materials is often required as a condition for successful application.\n\nBesides these requirements candidates are to take full responsibility for implemented software finished products for which it is important that an applicant be a strong contributor to the company\u2019s performance.\n\nTest engineering\n\nGeneral skills required for this job concentration are experienced in the software life cycle testing methods, testing life cycle and its development, OOP and Windows development, experience in developing software and knowledge of different testing methodologies, knowledge in the object-oriented and object-based programming, knowledge of XML, RDBMS interfaces, etc. Among additional skills required one should mention such as specification documentation (for instance, Dataflow diagramming, transition diagramming, etc.), knowledge of certain industry data processing subsystems, sensors and RFID, and various test automation tools, experience in Wireless LANs.\n\nThe applicants in this job concentration should have past experience in testing software kits, sensor platforms, providing technical support for an installed computing system, developing the total life cycle, and also should have full experience in writing various test cases, designing test scripts.\n",
        "label": "human"
    },
    {
        "input": "How Computers Negatively Affect Student Growth Essay\n\nTechnology is becoming an ever-present entity in the lives of students today. Since today\u2019s students are the potential workforce of tomorrow, they will need skills for problem-solving, which originates from computer technology. Since the computer is a potent tool for information processing, it has become an enormous part of our daily life. Lowell (2004) observed that although computers and other related technology have been emphasized in the learning process, the amount of technology currently used in the classroom is the main difficulty. Some examples of the computer technology adopted in the school are computer-assisted learning, open learning, connected learning community, and anywhere, any time learning program, among others. In this paper, an argument for the negative impact of the computer on student growth is presented.\n\nAccessibility and suitability: most of the school and student do not have computers that imply that they cannot use computer programs for learning, lack of availability of internet facilities\u2019 availability also makes the students lack information and content required for academic purposes. Those lucky to have access may not understand the range due to language deficiency or cultural differences. (Veasey, 1999)\n\nInterfering with natural development- students, primarily from lower grades, when subjected to a computer for the learning process, do not utilize their propensity for physically-based activity since they spend a lot of time with the computers. According to researchers, where a student spends most of the time on the computer, his or her development is impaired. This may interfere with cognitive development since psychologist claims that, student or children should socialize with peers or adults in order to acquire new concepts.\n\nLacks depth- Computer content does not offer in-depth and flexible content, i.e., the content usually is shallow and not dynamic. According to researchers, a trained and dedicated teacher can provide more profound and more flexible range full of examples than a computer can offer to the student. This, in turn, implies that a student can have a vast knowledge and skills of tackling a problem which in turn positively improves his grade rather than learning from a computer.\n\nQuality of content: Digitalized content is overly simplistic in its structure; for instance, a sum can only be wrong or right. The content does not explain why the sum was terrible, but a natural teacher will make a piece of work and offer the essential logical reasoning for the decision. This makes the student gain a fundamental understanding of the system behind what constitutes correct or incorrect.\n\nHealth hazards: computers are hazardous to the health of children in that they can lead to; repetitive stress injuries, eyestrain, obesity, social isolation, and long-term physical, emotional, and intellectual development damage which affects academic progress.\n\nSafety: internet poses a lot of danger to the student, which affects his or her academic performance. This danger includes; stalkers, hate and violence, pornography materials, etc.\n\nTechnology is not absolutely essential for meaningful learning; as portrayed by Lowell Monke that it has led to sacrifices in intellectual growth and creativity. The use of computers in education lends the student to be lazy and less innovative. The laptop does not offer a conducive environment for discussion, illustration, debate, etc., which is well provided by a natural teacher.\n\nDue to the fact that computers and associated programs such as the internet are costly as well as the shortcoming of the technology discussed above, the money and funds directed to technology should be used in other fields. (James, 2004)\n\nIn conclusion, for effective learning to take place, the real teacher should be encouraged, especially in lower grades of education. Computer-based learning should be advocated for students in higher levels of education such as colleges and universities.\n\nReferences\n\nJames, W. (2004). Taking Sides; Clashing Views on Educational Issues . Newyork, McGraw-Hill.pp.36-78.\n\nLowell, M. (2004). \u201cThe Ecological Impact of Technology,\u201d the journal of opinion and research, Vol.4 pp. 23-30.\n\nLowell M.(2004).The human touch: in a rush to place a computer on every desk, schools are neglecting intellectual creativity and personal growth. vol.2 pp. 57-65.\n\nVeasey D\u2019Souza, P. (1999). \u201cThe Use of Electronic Mail as an Instructional Aid: An Exploratory Study .\u201d Journal of Computer-Based Instruction 18, 1, 106-110.\n\nStudent Growth: The Development of Enhanced Practices for Computer Technology. Web.\n",
        "label": "human"
    },
    {
        "input": "The American Military and the Evolution of Computer Technology From the Early 1940s to Early 1960s Essay\n\nThe history of computer technology goes back to the late 1930s \u2013 early 1940s, when the first computer was invented. American military used computer technology for different purposes, including strategic decisions and control functions. The technological change in information technology creates the possibility of change in the conduct of military operations. Computer technology constitutes a potentially powerful source in military affairs. The problems that military organizations have experienced in formulating appropriate responses to new technologies have traditionally been viewed as the result of a conservative and stability-seeking institution\u2019s attempts to accommodate a phenomenon whose developmental trajectories are inherently difficult to predict. The age of experimental computers lasted into the fifties when all of the basic ideas and technological inventions became available to create the first generation of commercial general-purpose computers 1 . During the 1940s-1960, the American military was the only \u2018driver\u2019 of computer development and innovations. \u201cThough most of the research work took place at universities and in commercial firms, military research organizations such as the Office of Naval Research, the Communications Security Group (known by its code name OP-20-G), and the Air Comptroller\u2019s Office paid for it. Military users became the proving ground for initial concepts and prototype machines\u201d 2 .\n\nBy the mid- 1930s, various developments had occurred, which made possible the realization of Babbage\u2019s ideas (concerning computer technologies) without having to solve the mechanical problems which had defeated him. 3 . These were, in particular, the punch-card system just described and a variety of automatic selecting devices developed primarily for electrical communication systems. These possibilities were realized by Howard Aiken, of Harvard University, in 1937 and he invited International Business Machines to collaborate with him in their development. The result, in August 1944, was the IBM Automatic Sequence Controlled Calculator (ASCC). This is commonly regarded as the prototype of modern electronic computers, and it is certainly true that it initiated an entirely new line of calculators and information processors. Nevertheless, it was not strictly an electronic computer, for it was based on electrically driven number wheels, controlled by an elaborate system of electro-magnetic clutches and programmed by punched tape. Although less ambitious than Babbage\u2019s analytical engine, it was 50 ft long, weighed 5 tons, and included nearly a million components and 500 miles of wire 4 . Although it could multiply two eleven-digit numbers in three seconds, this was impossibly slow by the standards of only a few years later. Nevertheless, ASCC remained in use at Harvard for some fifteen years, and three later models were built in which, among other improvements, the magnetic clutches were replaced by electrical relays of the type used in communications engineering. These computers were not widely used by the military because their main functions were limited to \u201ccalculations, communication, and control\u201d. 5\n\nThe first of the true electronic computers was the Electronic Numerical Integrator and Calculator (ENIAC), completed in 1946. It was designed and built for the US government by J. W. Mauchly and J. P. Eckert of the University of Pennsylvania. Originally called for to prepare ballistic tables for wartime use, it was in fact not completed until 1946 when it was sent to the Ballistic Research Laboratory in Maryland. Although input and output were still in the form of punched cards \u2014 which necessarily limited speed of operation \u2014 mechanical parts were eliminated except for switches to control certain sections of the circuitry used for special programming purposes 6 . The moving part was, in effect, a train of electrical impulses generated at the rate of 5000 per second and controlled by electronic gates. The demands of radar for pulsed signals had already generated much expertise in this field of electrical engineering. Leibniz\u2019s stepped wheels or Ohdner\u2019s pin-wheels were represented by groups of ten electronic valves. With ENIAC, two ten-digit numbers could be multiplied in little more than two-thousandths of a second. 7\n\nComputer engineers, notably J. von Neumann of the University of Pennsylvania, also went back to a proposal by Leibniz that mechanical calculation could be better performed by the use of a binary rather than a decimal system of notation. This simply means that numbers are built up from only two digits, 0 and 1, rather than ten as in common usage (0-9). Any number of digits can, of course, be used to express numbers; the ancient Japanese soroban, for example, operates on a quinary system 8 . The ideas of von Neumann were incorporated in a machine known as EDVAC (Electronic Discrete Variable Automatic Computer), work on which started at the University of Pennsylvania just before ENIAC was finished. It constructed a new form of the memory device based on the circulation of electrically generated sonic pulses through a long tube of mercury. Meanwhile, computer research was being undertaken in Britain and elsewhere in Europe, with emphasis on the development of storage capacity 9 .\n\nDuring WWII, computer technology constitutes a potentially powerful source of change in military affairs, there exist obstacles to the optimal exploitation of this form of innovation. The problems that military organizations have experienced in formulating appropriate responses to new technologies have traditionally been viewed as the result of a conservative and stability-seeking institution\u2019s attempts to accommodate a phenomenon whose developmental trajectories are inherently difficult to predict 10 . If the US Army\u2019s historical relationship with the tank can provide any measure, this task can be rendered still more difficult by developments in the broader context within which organizational responses to technology-related change are forged. Even when an institution provides the tactical and operational environment within which the full potential of technological innovation might be realized, the overarching strategic and political environment can generate constraining influences of its own. More work remains to be done on this issue, but it is likely that many suboptimal responses to technology-related change were subjected to a broader set of structural constraints than is commonly realized. 11 . \u201cComputers thus improved military systems by \u201cgetting the man out of the loop\u201d of critical tasks. Built directly into weapons systems, computers assisted or replaced human skill in aiming and operating advanced weapons, such as antiaircraft guns and missiles\u201d 12 .\n\nThe next stage of military power was connected with the development of the magnetic core storage system incorporated in the Remington-Rand UNIVAC (Model 1103A) in 1956. It is highly sophisticated but basically depends on the fact that magnetic rings are arranged at the intersection of a matrix of wires carrying electrical pulses. An electrical pulse in one wire is insufficient to reverse the direction of magnetization in the ring, but if two pulses arrive simultaneously, a reversal occurs 13 . This provided the input. The output was provided by the third set of wires passing through the rings and responding to changes in the direction of magnetization. By this time, computers were experiencing conditions comparable with those developing in the air transport business 14 . There, the higher speeds of aircraft made the journey itself quicker, but this was largely offset by delays caused by traffic congestion on the roads leading to and from airports; the need to register well in advance of take-off so that formalities could be completed; delay on arrival while luggage was unloaded and examined; and so on. In the computer field, the computing process itself had been enormously accelerated, but the continuing use of punched cards or tape for programming and output made it impossible greatly to speed up the process as a whole. The UNIVAC was important in that instead of paper tape, it made use of magnetic tape for programming; as we have noted earlier, this had been developed during the war in Germany and had subsequently been adopted for sound recording, especially in the motion picture industry. The original UNIVAC was designed for the US Census and delivered in 1951. In the following year, it was used to predict the outcome of the Presidential election, and its correct identification of Eisenhower as the successful candidate was important in directing public attention to the potentialities of computers. Up to this time, electronic computers had been based on thermionic valves; in the late 1950s, the replacement of these by transistors represented a major step forward. This takes us rather beyond 1950, but it is an appropriate point at which to close the history of computers nominally ending at mid-century. \u201cBell Laboratories, the largest independent electronics research laboratory in the country, saw the percentage of its peacetime budget allocated to military projects swell from zero (prewar) to upwards of 10 percent as it continued work on the Nike missile and other systems, many of them involving analog computers\u201d 15 .\n\nThe real computer revolution had to wait until the end of World War II, when a much faster and more reliable solid-state device known as the transistor was invented. This was the age when American military and economic power was at its peak when high technology was strategic to America\u2019s military efforts, and developers and manufacturers could be guaranteed hundreds of millions of dollars for participating in such ventures as the ICBM Minuteman program, the Apollo lunar mission, and the Vietnam War 16 . According to Michael Borrus and John Sysman of the Berkeley Roundtable on the International Economy, for a time in the fifties, about one-half of the research budgets of IBM and AT&T were paid for by defense contracts, and from 1962 to 1965, military and space procurement for integrated circuits accounted for more than 75 percent of total industry sales. It was this cultural and institutional setting in combination that enabled America to gain a leadership role in high technology and maintain it over the decades until the late eighties when other countries, especially Japan, would begin to seriously challenge this leadership in key segments of the industry 17 .\n\nThe communications satellite was another computer innovation that gave tremendous opportunities and potential to the army after WWII. Along with the transistor, the microprocessor, the laser, and the optical fiber, the communications satellite must rate as one of the most important technological developments in the postwar period. Because it can carry telephone and television signals as well as information and news, and because a single satellite can cover an entire hemisphere, it had dramatic military as well as economic, industrial, cultural, and political implications. The satellite communications revolution began with the launching of Sputnik I by the Soviet Union in 1957, and it launched both the United States and the Soviet Union into a race to exploit space and space technology for military as well as commercial purposes 18 . In July 1958, the U.S. Congress passed the National Aeronautics and Space Act establishing NASA as a civilian agency to lead America\u2019s space development activities. But it was the leadership of the United States in microelectronics and computer technology that proved critical to eventually surpassing the Soviet Union in the development of satellite technology and led to its landing a man on the moon in 1969. In the late fifties, the United States tested several scientific communications satellites, including Explorer I, launched in 1958, and various military satellites as well as passive communications satellites, including ECHO I and ECHO 11, which simply reflected radio signals. Those that followed were active in the sense that they had electronics onboard to amplify signals 19 .\n\nA number of developments in the history of computer-mediated com\u00admunication were fundamental to the success of the ARPANET project. The first was the idea of communicating with computers at a distance. In September 1940, George Stibitz had decided to demonstrate a calculator to a meeting of the American Mathematical Society. The complex machine took up lots of space, so rather than transporting it to the meeting with the risk of it getting damaged, Stibitz set up a teletype terminal so that the calculator could be used remotely via a telegraph connection. A second development was that computers had to be seen as more than just devices for solving mathematical problems 20 . In 1945, Vannevar Bush published an article, \u2018As we may think, in which he described the \u2018Memex,\u2019 a communication system for storing and retrieving information. A third and crucial development for setting up the ARPANET was the invention of packet-switching communication technology. Packet-switching involves the breaking down of digitized information into packets or blocks that are labeled to indicate both their origin and their destina\u00adtion, and the sending of these from one computer to another. The advant\u00adages of packet-switching are twofold. First, network resources are used more efficiently because a single channel can carry more than one transmission simultaneously. 21 .\n\nAfter WWII, the technology necessary for packet-switching was developed inde\u00adpendently at a number of research centers around the world, for example, at the National Physical Laboratory in Great Britain and the Massachu\u00adsetts Institute of Technology and the RAND Corporation in the United States. Of these, the research done by Paul Baran at the RAND Cor\u00adporation deserves particular attention. Baran had been commissioned by the United States Air Force to do a study on how the military could maintain control over its missiles and bombers in the aftermath of a nuclear attack. In 1964 he proposed a communication network with no central command or control point 22 . In the event of an attack on anyone point, all surviving points would be able to re-establish contact with each other. 6 He called this kind of network a distributed network. It was from this RAND Corporation study that the false rumor started that the ARPANET was somehow directly and primarily related to the building of a communication network that would be resistant to nuclear attack.\n\nComputing facilities especially represented particularly delicate operational environments, characterized by large workforces. In an era when air conditioning was still comparatively rare, computer centers were generally arctic-like environments. Communications networks likewise consisted of extremely dense hardwired arrangements linked to fixed location detector sites for battle-space management. Space-based detectors made the initial contact, but the existing BMD technology was a terminal point or limited area defensive arrangement, which meant the detectors handling the actual interception must be placed comparatively close at hand for ease and speed of operation 23 . The vulnerability of these facilities to destruction or electronic disruption was a particularly thorny problem but was not in itself disabling since larger policy questions drove the debate. Firing missiles over population centers is not considered a prudent thing to do since flight failures do occur even under the best of circumstances. Therefore, missiles are transported to Vandenberg Air Force Base for testing, a process during which the missile moves completely outside its operational milieu with new parts substituted for existing computer hardware in order to run the test. How reliable or accurate an evaluation this process is best, in fact, proved troubling to many, but no realistic alternative has ever been developed.\n\nIn sum, computer technologies played a crucial role in the development of military forces during WWII. Concepts such as the stored program control, binary representation of data and programs, and the use of punched cards were becoming standard only in the fifties. In a short period of time, electromechanical and fully electronic and magnetic storage technologies were also becoming commercially available. The triode became a natural means of fast switching in the central processing unit. It was a significant improvement over electromechanical relays in terms of speed and reliability, and it reduced the switching time from seconds to milliseconds. Further, the heart of the American strategic nuclear forces rested upon a series of assumptions that the systems would work when needed. Given the high failure rate of military equipment during routine operations, this remains an interesting assumption.\n\nFootnotes\n\n 1. Campbell-Kelly, M., Aspray, W. Computer: A History of the Information Machine. (HarperCollins Publishers, 1997): 25.\n 2. Why Build Computers? The Military Role in Computer Research . N.d. 2007.\n 3. Campbell-Kelly, M., Aspray, W. Computer: A History of the Information Machine. (HarperCollins Publishers, 1997): 29-30.\n 4. Geoffrey J. E. Rawlins, Moths to the Flame: The Seductions of Computer Technology (Cambridge: MIT Press, 1996, p.67\n 5. Why Build Computers? The Military Role in Computer Research . N.d. 2007.\n 6. Geoffrey J. E. Rawlins, Moths to the Flame: The Seductions of Computer Technology Cambridge: (MIT Press, 1996): 38.\n 7. Ibid, 39.\n 8. Campbell-Kelly, M., Aspray, W. Computer: A History of the Information Machine. (HarperCollins Publishers, 1997): 1.\n 9. Ibid, 1.\n10. Geoffrey J. E. Rawlins, Moths to the Flame: The Seductions of Computer Technology (Cambridge: MIT Press, 1996): 120.\n11. Walker, G.K., Information Warfare, and Neutrality. Vanderbilt Journal of Transnational Law 33 (2000): 1079.\n12. Why Build Computers? The Military Role in Computer Research . N.d. 2007.\n13. Campbell-Kelly, M., Aspray, W. Computer: A History of the Information Machine. (HarperCollins Publishers, 1997): 87.\n14. Ibid, 88.\n15. Why Build Computers? The Military Role in Computer Research . N.d. 2007.\n16. Ceruzzi, P.E. A History of Modern Computing, 2nd Edition. (MIT Press, 1998): 65.\n17. Ibid, 66\n18. Campbell-Kelly, M., Aspray, W. Computer: A History of the Information Machine. (HarperCollins Publishers, 1997): 299.\n19. Ibid, 288.\n20. Ibid, 300\n21. Ibid, 300\n22. Pearson, A.W. Allied Military Model Making during World War II. C artography and Geographic Information Science 29 (2002): 227.\n23. Campbell-Kelly, M., Aspray, W. Computer: A History of the Information Machine. (HarperCollins Publishers, 1997): 305.",
        "label": "human"
    },
    {
        "input": "Uniform Law for Computer Information Transactions Essay\n\nBefore the UCC and the UCITA the first, and most important, of the U.S. government\u2019s attempts to promote uniformity in commercial laws from state to state, was the establishment of \u2018The Commerce Clause\u2019 of the U.S. Constitution which empowered the federal government and the Congress with the authority to control the commerce activities with not only the foreign countries but also between the states of America including the Indian tribes, thereby having a large influence on business and aiming to promote the progress of a nationalized market and enabling free inter-state trade.\n\nAccording to this clause, it was not necessary for the regulated activity to be an interstate commerce activity per-se and could include any local, intrastate activity having an effect on interstate commerce. (Cheesman H R., Contemporary Business and Online Commerce Law 5th Edition).\n\nUniform Law for Computer Info Transactions is Offered National Law Journal and National Conference of Commissioners on Uniform State Laws (NCCUSL)1999.\n\nThe Uniform Commercial Code (UCC or the Code) applies to sales of goods, and includes the sales law for all the states of America. (Cheesman H R., Contemporary Business and Online Commerce Law 5th Edition).\n\nA sale of goods includes the transfer of title from a seller to a buyer for a pre-determined price (UCC 2-106-1). (Cheesman H R., Contemporary Business and Online Commerce Law 5th Edition)\n\nGoods are therefore substantial objects that can be moved or exchanged when there is a contract involving their transfer from one party to another. (UCC 2-105-1).\n\nThe Uniform Computer Information Transactions Act (UCITA) on the other hand, comprises rules which are necessary for the licensing of computer-based information particularly computer software or any other computer information. Examples of these are databases of companies on computers and music software. The UCITA is also responsible for the supervision of the admission contracts, whether on the internet or offline to the numerous and additionally influences the information in the storage devices such as disks and CDs. This involves licensing and is absolutely different from selling a product which entails an exchange of goods or transferable property in return for money or another recompense.\n\nSelling a product basically involves an exchange of goods or products in return for money or any other reward and ends the trade with this activity of exchange. (Cheesman H R., Contemporary Business and Online Commerce Law 5th Edition)\n\nLicensing contracts on the other hand involve the safeguarding of computer information to avoid replication and the UCITA, therefore, functions to control this illegal replication thereby involving a transfer of a copy of protected and secure. A crucial aspect of a license is that it only offers the purchaser the right to use the item implying that the \u2018title\u2019 or information is still the property of the owner which is radically different from a sale where title or ownership is clearly transferred from the seller to the buyer.\n\nThe UCITA clearly functions to separate this transfer of a copy of information from the transfer of ownership.\n\nThe UCITA is necessary to grant licensing activity which is continuously growing with the increase in commercial activity in the computer business. This expansion is the most important source of economic development to America and has the potential to be a crucial financial base for the future. Since the rules of the UCITA clearly aid the economic activities and ratify the mode of contract, commerce in the computer information industry can proceed without apprehension. Article 2 of the Uniform Commercial Code involves contracts for the sales of goods whereas the UCITA functions to safeguard the crucial computer information by licensing it, thereby ensuring that the sole rights remain with the creator and the owner who ultimately has the authority to license it at his will. The communication of computer information in the electronic form is faceless, which the UCITA governs.\n\nReferences\n\nCheesman H R., Contemporary Business and Online Commerce Law 5th Edition National Law Journal. Web.\n",
        "label": "human"
    },
    {
        "input": "Apex Computers: Problems of Motivation Among Subordinates Case Study\n\nThe given case study illustrates how highly enthusiastic employees can lose motivation under management that does not praise creativity. Rohit, after joining Apex Computers found himself in a situation where his boss Aparna does not show any interest in the opinions of her subordinates. Initially, Rohit was working for Suresh, who would reward every attempt and innovative solution. He would also allow mistakes to occur without severe punishment. In the end, Rohit loses all of his motivation due to the lack of self-expression.\n\nDiscussion\n\nThe case study demonstrates a motivation loss of Rohit who used to be highly enthusiastic and energetic. The absence of positive reinforcement led to his discouragement and decreased performance (Giancola 30). Suresh was highly positive in terms of letting the subordinates come up with their ideas and solutions for the problems. Although both Suresh and Aparna were completely disengaged from their employees\u2019 activities, they possessed different attitudes towards giving feedback. Suresh gave constant input in terms of appraisals and recognition, whereas Aparna was simply disinterested in her subordinates. She did not provide them a creative playground, but only correct answers to the issues.\n\nIn the process of using intangible incentives, it is necessary to use, first of all, recognition of the merits of employees. It stimulates everyone and always aids the management when no resources are available for material rewards (Chlpekov\u00e1 et al. 26). Therefore, companies, such as Zen, often have the practice of awarding the best workers of labor. However, it must be remembered that this practice should apply to all levels, and the excellent work of a department employee should not go unnoticed (Chlpekov\u00e1 et al. 41). Another approach that effectively motivates employees is the introduction of a floating ward-to-office schedule to and from the office or flexible working hours. In addition, advanced training courses can develop personal and business skills and strengthen motivation (Chlpekov\u00e1 et al. 38). This is made to directly satisfy Maslow\u2019s self-actualization need, which is located at the top of the pyramid of necessities.\n\nFurthermore, people with the need for self-development, such as Rohit, must be prepared in advance for taking managerial positions. The requirement for success is satisfied by the process of bringing work to a fulfilling conclusion (Chlpekov\u00e1 et al. 35). Such people risk moderately, like to take personal responsibility for finding a solution to a problem. Therefore, it is necessary to motivate these people by giving them tasks with a moderate degree of risk or the possibility of failure. The bosses can also delegate to them sufficient powers, and encourage them regularly and specifically by the results achieved.\n\nKey Concepts\n\nThe key terms of the given case study are team motivation and achievement recognition. Team motivation and environment play an essential role in determining the overall team effectiveness. However, achievement recognition is important for individual motivation because a person cannot be constantly energetic and enthusiastic for an activity without regular rewards in the form of appraisals (Giancola 29). In addition, the case study shows how the most engaged employees can lose interest in their jobs due to the incorrect management style. It is critical to allow the subordinates to learn through their mistakes and creativity.\n\nAnswer 1\n\nThe main reason for Rohit\u2019s disillusionment was his expectation that his new boss would be similar to the previous one. According to Maslow\u2019s Hierarchy of Needs, Suresh was providing Rohit with basic needs, psychological needs, and self-fulfillment needs (Bland and DeRobertis 4). The entire pyramid of human motivational requirements was met under the management of Suresh. However, Rohit under the governance of Aparna could not satisfy his needs for esteem and self-actualization. He was highly unmotivated, although the payment was significantly higher and working conditions were far better. In addition, the case study does not inform on Rohit\u2019s family and friends, but it is clear that his needs for belongingness and love at the workplace are not satisfied too. The main reason is that all of the subordinates were unmotivated and disinterested in their jobs due to Aparna\u2019s management style.\n\nAnswer 2\n\nTo solve the lack of motivation problem, Rohit should satisfy his needs in other available ways. For instance, he can self-actualize himself by setting long-term goals for international projects, where he can get an opportunity to travel to the USA. It can result in the relocation within the company, which will change Rohit\u2019s current team and boss. A team leader can motivate subordinates by offering them autonomy at the job and allowing them to learn from their mistakes (Giancola 27). According to Two-Factor Theory, the motivators are achievement, recognition, work, responsibility, advancement, and growth (Bland and DeRobertis 5). Although Rohit cannot derive his motivation from success and recognition, there are still available sources for enthusiastic behavior, such as advancement and growth. According to Alderfer\u2019s ERG Theory, Maslow\u2019s Pyramid can be divided into existence, relatedness, and growth needs (Bland and DeRobertis 7). A team leader can also satisfy the growth needs of his/her subordinates by showing them the methods of the correct solutions and allowing them to test these ideas.\n\nWhen solving the problems of effective stimulation of young workers, it is necessary to take into account certain features. Usually, young people do not imagine the development of their careers, with rare exceptions, they are energetic and often do not think about any motivational issues (Chlpekov\u00e1 et al. 45). The opportunities in various fields and the system of rotation within the company are examples of outstanding solutions for Rohit\u2019s case. To preserve the young employee\u2019s enthusiasm and motivation, the incentive for personal growth must be provided. The main reason is that it is the highest and most essential Maslow\u2019s need of any person.\n\nFor young professionals, it should be noted that this category of people is ambitious and aimed primarily at moving up the career ladder. They have a great deal of need for independence, the desire to try their strength, and to manage projects with full responsibility for the result (Chlpekov\u00e1 et al. 17). Therefore, the delegation of serious tasks, involvement in decision making, responsibility for a whole block of functions, recognition of the result obtained at the management level will serve as a significant motivating factor for further activities.\n\nConclusion\n\nIt is important to note that the given case study represents the problems of motivation among subordinates, which is highly important for both managers and employees. The organization\u2019s motivation system must be constantly improved since changes in the external environment lead to intra-organizational changes. In turn, intra-organizational alterations, in combination with shifts in the external environment, can cause an improvement in the motivation of employees. In addition, with time and with age, the needs of people tend to change, which is why Rohit is more ambitious and energetic. The content of the motivation system in a particular organization depends on its leader, who must skillfully combine material and non-material incentive methods. He/she also needs to take into account the motivation theories, such as Maslow\u2019s Hierarchy of Needs and Two-Factor Theory.\n\nWorks Cited\n\nBland, Andrew M., and Eugene M. DeRobertis. \u201cMaslow\u2019s Unacknowledged Contributions to Developmental Psychology.\u201d Journal of Humanistic Psychology , vol. 1, no. 1, 2017, pp. 2-8.\n\nChlpekov\u00e1, Andrea, et al. \u201cEnhancing the Effectiveness of Problem-Solving Processes Through Employee Motivation and Involvement.\u201d International Journal of Engineering Business Management , vol. 2, no. 5, 2014, pp. 13-47.\n\nGiancola, Frank L. \u201cShould HR Professionals Devote More Time to Intrinsic Rewards?\u201d Compensation & Benefits Review , vol. 46, no. 1, 2014, pp. 25-31.\n",
        "label": "human"
    },
    {
        "input": "Computer Science. Open Systems Interconnection Model Essay\n\nTable of Contents\n 1. Introduction\n 2. Main text\n 3. Conclusion\n 4. References\n\nIntroduction\n\nOSI was done by a group at Honeywell Information Systems with Charlie Bachman and Mike Canepa as the principal technical member. This group was contracted within Honeywell, with sophisticated product planning and with the development and design of prototype systems.\n\nMain text\n\nThe OSI stands for Open Systems Interconnection. This model was originally created as the basis for crafting a universal set of protocols called the OSI Protocol Suite. This suite hardly achieved extensive success. However the model became a very handy tool for both development and education. The model defines a set of layers and a number of concepts for their use that make the understanding of the networks easier. The idea behind the development of networking standards is to classify widely recognized approaches of setting up networks and connecting them collectively. The OSI Reference Model represents an early effort to get all of the different software and hardware manufacturers to have the same opinion on a structure for developing different networking technologies.\n\nTwo projects were started in late seventies with a sole aim of defining a unifying standard for the structural design of the networking systems. One was performed by the International Telephone and Telegraph Consultative Committee (CCITT) and the other was administered by the International Organization for Standardization (ISO). The two international standards bodies developed a document that explained same networking models.\n\nThe two documents were put together in 1983 and gave rise to The Basic identified as OSI model. It was then published by both the ISO, as standard ISO 7498, and the renamed CCITT (now called the Telecommunication s Standardization Sector of the International Telecommunication Union or ITU-T) as standard X.200.\n\nThe OSI Reference model was not initially created for educational intentions but to serve as the building blocks of a widely used suite of protocols that would be utilized by the international internet works. Incidentally nothing worked out as planned. There were rise in the fame of Internet and IP/TCP protocols that met the OSI suite and TCP/IP won. This led to the implementation of the OSI protocols. However, when the internet started to expand, the OSI protocols lost out to IP/TCP protocol.\n\nConclusion\n\nThe OSI then became a device for defining the networking and functioning of the OSI protocol. In the long run it became widely used as an educational tool and also assists in describing relations between other protocol suites and the components and hardware devices. The model is also important to software developers in that it makes clear the duties carried by each component in the network system.\n\nReferences\n\nJeffery C.M. 1989. Computer Communication Review: New York. ACM. Web.\n",
        "label": "human"
    },
    {
        "input": "Computers Will Not Replace Teachers Essay (Critical Writing)\n\nThere is no shortage of debates on whether rapidly developing technology might replace people in teaching and pedagogy. First and foremost, it is undeniable that technological advancements are used in all of the spheres of human activities nowadays. On the one hand, \u201cRobotics has progressed to a point where there is a real possibility of robots taking on social roles\u201d (Sharkley, 2016, p. 284). Indeed, Artificial Intelligence (AI) can systemize information, talk, move, and do everything that human beings are capable of doing in a faster and more precise manner. On the other hand, real teachers can emotionally connect and relate to their students; in contrast, computers do not possess feeling and lack of empathy. As stated by Purewall (2016), only people are emotionally responsive, truly compassionate, intuitive and understanding, what enables them to effectively support students in struggles with academics as well as peer relationships. For instance, humans tend to individualize their approach to students regarding a given situation and setting, which improves the quality of a learning process. Henceforth, while modern technology is capable of accomplishing various tasks effectively, the human factor is crucial for creating a comfortable environment for students, meaning that computers will never replace teachers.\n\nReferences\n\nPurewal, H. (2016). Can technology replace teachers? You asked Google \u2013 Here\u2019s the answer. The Guardian. Web.\n\nSharkley, A. J. (2016). Should we welcome robot teachers?. Ethics and Information Technology, 18 (4), 283-297. Web.\n",
        "label": "human"
    },
    {
        "input": "Analyzing the Usability of the TED Website Essay\n\nTable of Contents\n 1. Introduction\n 2. Planning and Data Collection\n 3. Results\n 4. Discussion\n 5. Conclusion\n 6. Recommendations\n 7. References\n\nIntroduction\n\nInterface usability is one of the priorities of software developers who try to provide easy-to-use products that could attract certain audiences. Xiao-Jun, Zhong-Dong, Tao, and Bao-Cheng (2017) claim that usability is one of the areas that have been explored in detail in recent years. Although several definitions of usability exist, developers and researchers tend to concentrate on such aspects as attractiveness, functionality, understandability, and learnability (Hentati, Ammar, Trabelsi, & Mahfoudhi, 2016).\n\nThis report includes an analysis of the usability of the TED website that will be based on the four concepts mentioned above. In order to assess the website\u2019s usability, three people will be observed while completing the tasks given by the researcher. This report includes a brief description of the methods of the evaluation, its results interpretation, as well as recommendations regarding improvements associated with usability.\n\nPlanning and Data Collection\n\nWhen planning the evaluation process, it was essential to consider the elements of usability. The questionnaires and the tasks the participants completed were designed so that it could be possible to assess the attractiveness, functionality, understandability, and learnability of the website under analysis. The website has a very wide audience as TED emphasizes its goal to help people share ideas and enhance their understanding of the world (\u201cAbout,\u201d n.d.). The primary users of the website in question are teenagers and young adults, as well as students and educators (irrespective of their age). This evaluation targets quite a specific group: three elderly people (aged 65, 66, and 70) took part in the study and evaluated the TED website.\n\nVroman, Arthanat, and Lysack (2015) note that older adults are becoming common users of various social networks and websites, so it is but natural to address their needs and expectations when designing digital products. This population is growing very fast, so the number of elderly users is likely to increase soon. Vroman et al. (2015) argue that elderly users\u2019 educational background and personality have a considerable impact on their overall attitude toward technology and certain digital products.\n\nTherefore, the participants of this research noted this information in the questionnaires that consisted of some demographic section, Likert scale questions section, and one open-ended question that was not compulsory. The open-ended question was added as it could provide qualitative data and shed light on the extent to which the participants liked the experience.\n\nAfter signing consent forms (an example is provided in Appendix), the participants chose the time they could take part in the research. The session lasted one hour, during which participants completed certain tasks and responded to the questionnaires. Although the older people had a set of tasks, they were free to choose any of them and take their time. The tasks included searching for any video on a specific topic, a specific video or speaker, undertake certain actions with videos (rate, like, or share), and review the commentaries. The optional tasks included commenting (which required logging in), reading a blog, and some others.\n\nThe questionnaires included ten questions divided into three sections: demographic data (age, gender, education, profession/job), technology-related section, and the open-ended question regarding the experience. The questions were quite simple and straightforward as the focus was on the participants\u2019 behavior during the session. The samples were encouraged to think aloud, and the notes taken during these talks were analyzed with the elements of content analysis. The participants were informed about the purpose, and some components of the research, as well as their ability to withdraw from the study at any point. The primary measurements to evaluate the website were the time spent on task completion and the overall impression as well as the participants\u2019 emotional state during the process.\n\nResults\n\nTwo males (aged 66 and 70) and one female (aged 65) took part in the study. One of the male participants had higher education and had been employed as an IT professional before his retirement. He noted that he spent less than 2 hours a day using the Internet. The other male participant had a high-school education, had worked as a blue-collar worker before he retired, and spent approximately 3-4 hours a day online. The female participant had a high-school education and had worked as a blue-collar worker before her retirement. She noted that she spent less than one hour a day online.\n\nAs far as the questionnaires are concerned, the male participants agreed that the website was attractive, understandable, and easy to learn how to use it. The female participant disagreed that it was understandable or easy to learn how to use it, but she chose \u201cundecided\u201d as to the website\u2019s attractiveness. All of the participants strongly agreed that the website was functional. All of them responded to the open-ended question quite briefly. The overall impression of using the website was positive. The participants stressed the informativity of this digital product.\n\nThe male participants completed the required tasks within a short period of time. They had some difficulty returning to the home page, but they soon learned how to do that. They also completed some of the optional tasks as they still had time and were willing to try. As for their attitude and their emotional state, the males were positive and interested in using the website. The female participant was quite willing to complete the tasks, but she spent almost twice as much time as the male participants did. During the completion of the tasks, she noted that there were too many videos and a lot of interesting information. She repeated that she was constantly distracted.\n\nThis participant also found the section \u201cGet Started\u201d (that was on the Home Page of the website) very helpful. The male participants revealed some degree of frustration when they could not go back to previous pages or to the home page as easily as they were accustomed to. At the same time, they revealed their satisfaction and positive feelings throughout the session.\n\nDiscussion\n\nIt is possible to note that the results of this research are consistent with the existing data on the matter. The educational backgrounds and former professions, as well as personalities, could be the factors affecting the way older people use digital products, which was suggested by Vroman et al. (2015). The findings of the present study show that people with higher education or certain personality traits have fewer difficulties when using some online services.\n\nIt is possible to assume that these people have quite extensive experience enabling them to be more effective users. The female participants did not use the Internet in her career life and use it quite occasionally in her present life. The lack of experience can be the reason behind her slow completion of tasks as well as certain frustration.\n\nIt is noteworthy that all the participants mentioned the fact that the website contained a lot of or too much information. The tasks related to using the videos were completed with no difficulties, and all the participants were willing to rate or \u201clike\u201d the items. One of the older users shared the video and expressed his readiness to log in later in order to take part in the discussion. He also noted that the login procedure was always associated with certain discomfort as there was the need to reveal some information (including social network accounts) and create a password.\n\nAt the same time, there were no highly negative commentaries during the completion of the tasks. The commentaries the participants made were quite helpful in identifying the weaknesses of the website\u2019s usability. One of the major issues was the presentation of the information as it is quite confusing for older people, especially those who have limited experience of using the Internet. Yamaura, Tamura, and Nakamura (2018) also state that many people feel uncomfortable when they are exposed to this kind of abundance of visual items.\n\nThe researchers suggest that adding the blurring effect to peripheral images or videos could minimize the discomfort users can experience. The developers could consider using this method as the information available from TED is varied and can be quite overwhelming for older people. One of the participants emphasized that she was often distracted and tended to watch things she did not intend to.\n\nThe blurring effect could also be used to highlight the most relevant data. Brasel and Gips (2017) note that multitasking is a natural behavior for Internet users as they tend to pay attention to different details and can be distracted in certain ways. This feature of users is often used in marketing as advertisements are present in the vast majority of digital products including the TED website. This type of content can be emphasized with the help of blurring. Nevertheless, although the participants did not comment on the advertisements they encountered when completing tasks, this content should be incorporated with caution as many people feel negative about online advertisements.\n\nThe participants noted that it was easy to learn how to use the website, but they all had some difficulties related to going back or going to the website\u2019s homepage. This task was not included in the scope of this research, but it proved to be important for the participants who needed to view some videos or simply start the entire process from the homepage. In addition, the female participant noted that it could be easier for her to find the content she might be interested in if she could access the speakers\u2019 details.\n\nThe offered categories were quite effective but lacked the focus on speakers\u2019 backgrounds. It is also important to add that older people are becoming quite active users of the Internet. Although the sample size is very limited, two out of three people turned out to be quite experienced users of digital products. Therefore, this population deserves more attention, and these people\u2019s peculiarities and needs should be explored in detail.\n\nConclusion\n\nOn balance, this brief research indicates that older people have a positive attitude toward the website in question. The usability of this website is quite high for the target population although some weaknesses are also apparent. The strengths of the website include its functionality and attractiveness. Older users find the website informative and interesting as they learn about many things that are relevant to them.\n\nThe associated learnability and understandability are limited, but a set of solutions are available and presented in the following section of this paper. It is necessary to stress that this study involved only three participants, but their insights could be utilized to develop large-scale research. Although students and educators are the primary users of the website, the audience can be extended to include people older than 65 years old.\n\nRecommendations\n\nBased on the conclusions provided above, it is possible to identify several recommendations that could improve the website.\n\n 1. A quick reference to the homepage could make older users\u2019 experiences more pleasant.\n 2. By adding the blurring effect to the peripheral information or less relevant data, the developers could enhance elderly users\u2019 experiences and help them view the videos they need.\n 3. The website could be improved by refining its searching options. It could be beneficial to add such categories as speakers\u2019 backgrounds, age, profession, or other details.\n 4. It is also possible to introduce options for those willing to comment as the existing login procedure is an obstacle to start the communication through participation in discussions.\n\nReferences\n\nAbout . (n.d.). Web.\n\nBrasel, S. A., & Gips, J. (2017). Media multitasking: How visual cues affect switching behavior. Computers in Human Behavior, 77 , 258-265. Web.\n\nHentati, M., Ammar, L. B., Trabelsi, A., & Mahfoudhi, A. (2016). A fuzzy-logic system for the user interface usability measurement. In Conference Proceedings: 2016 17Th IEEE/ACIS international conference on software engineering, artificial intelligence, networking and parallel/distributed computing (SNPD) (pp. 133-138). Washington, DC: IEEE Computer Society. Web.\n\nVroman, K. G., Arthanat, S., & Lysack, C. (2015). \u201cWho over 65 is online?\u201d Older adults\u2019 dispositions toward information communication technology. Computers in Human Behavior, 43 , 156-166. Web.\n\nXiao-Jun, L., Zhong-Dong, X., Tao, S., & Bao-Cheng, W. (2017). Mapping the intellectual structure of relationship between usability of information system and user emotion. In Conference proceedings: 2017 4th international conference on information science and control engineering (ICISCE) (pp. 438-442). Washington, DC: IEEE Computer Society. Web.\n\nYamaura, H., Tamura, M., & Nakamura, S. (2018). Image blurring method for enhancing digital content viewing experience. In M. Kurosu (Ed.), H uman-computer interaction. theories, methods, and human issues (pp. 355-370). Cham, Switzerland: Springer. Web.\n",
        "label": "human"
    },
    {
        "input": "Use of Robots in Computer Science Essay\n\nTechnological development has facilitated the use of robots to advance learning. According to Burbaite, Bespalova, Damasevicius, and Stuikys (2014), robots motivate students, encourage engagement, and enable them to acquire practical skills. Robots are effective in delivering large skill sets to learners. Burbaite et al. (2014) say, \u201cThe positive effect is gained from the \u201cembodiment\u201d and physical presence of robots, which make the outcomes of programming very vivid and immediately accessible\u201d (p. 931).\n\nToday, many institutions are moving away from the conventional methods of teaching and adopting robotic activities to augment learning. Currently, the most significant development in the field of computer science is the inclusion of robots as teaching tools. Initially, most computer science departments used robots as a strategy to increase the number of learners in computer science courses. Nevertheless, with time, educators realized that robots could be important teaching instruments. That is when they decided to look for ways to integrate the technology into learning activities. This paper will discuss the strategy for using robots in teaching computer science.\n\nUse of Robots\n\nIn the United States, many higher education institutions have realized the importance of collaborative learning and practical exercise in computer science. Most lecturers argue that engaging students in classes enable them to remember what they learn (Berenguel, Rodriguez, Moreno, Guzman, & Gonzalez, 2016). Collaborative learning and hands-on exercises allow learners to internalize what is being taught. Consequently, most institutions have implemented collaborative learning as a means to integrate robots into the field of computer science. Educational theorists agree that robots have enormous power to improve classroom teaching (Berenguel et al., 2016).\n\nAccording to Berenguel et al. (2016), one of the significant values of robots in computer science is their concrete nature. The use of robots in teaching computer science has significantly helped to endow students with valuable skills in this field. In the past, it was difficult for learners to comprehend theoretical ideas. Toh, Causo, Tzuo, Chen, and Yeo (2016) assert, \u201cToday, students can understand abstract concepts and gain a more functional level of understanding when they learn with robots\u201d (p. 152).\n\nNonetheless, Toh et al. (2016) stress that teachers must regard robots as one of the teaching materials. The use of robots alone cannot help to boost learning amid computer science students. The educational theory that instructors apply plays a significant role in determining the success of robot application.\n\nRobots enable educators to use activities that are helpful in teaching different disciplines including mathematics, technology, and computer science. In the field of computer science, instructors have formulated practical activities with essential experimentation aspects to aid in teaching (Shiomi, Kanda, Howley, Hayashi, & Hagita, 2015). These activities aid lecturers to apply robots to create a dynamic, collaborative learning atmosphere that encourages students\u2019 participation. Indeed, integration of robotic technology into the field of computer science has augmented teaching practices. Teachers can now use inventive methods to meet diverse learning objectives (Shiomi et al., 2015).\n\nResearch shows that robots are connected to multiple disciplines. According to Shiomi et al. (2015), a robot comprises different components. They include software, sensors, and motors. Each of these components is manufactured using knowledge from disciplines such as computer science, electronics, and engineering. Therefore, using robots to teach computer science students has an added advantage. Learners get an opportunity to acquire skills in other disciplines that are connected to robotics.\n\nUniversities are coming up with frameworks to facilitate the use of robots to teach different subjects. For instance, in the United States, Carnegie Mellon University has developed an open source robot program dubbed Tekkotsu to facilitate teaching (Zaharija, Mladenovic, & Boljat, 2015). This application is developed based on C++ programming language (Zaharija et al., 2015). Zaharija et al. (2015) allege that Tekkotsu has been helpful in teaching mathematics topics like linear algebra, matrices, and vectors. In Brazil, some tertiary institutions use robots to teach physics (Zaharija et al., 2015).\n\nThe organizations have created model robots, which they use to teach electronics and electricity, especially showing learners how to assemble electrical circuits. In the field of computer science, robots have been useful, particularly in programming courses (Zaharija et al., 2015). For example, the University of Waterloo has a robot called Karel, which is used to teach Java programming (Zaharija et al., 2015). The university has developed an Introductory to Computer Science syllabus, which utilizes a robot to train learners in object-oriented programming.\n\nConclusion\n\nTechnological growth has enabled instructors to use robots to improve learning environment and encourage student participation. Today, robots are used to teach subjects like mathematics and physics. The field of computer science has greatly benefited from robots. Initially, computer science departments used robots as incentives to encourage many learners to register for their courses. Later, they realized that these tools were helpful in boosting learning.\n\nToday, the field of computer science has devised mechanisms to facilitate the use of robots to equip learners with different skill sets. The use of practical exercises and collaborative learning has enabled computer science departments to include robot as an essential teaching instrument. Robots help students to comprehend abstract concepts. Moreover, interacting with robots gives learners a chance to gain skills in other disciplines.\n\nReferences\n\nBerenguel, M., Rodriguez, F., Moreno, J. C., Guzman, J. L., & Gonzalez, R. (2016). Tools and methodologies for teaching robotics in computer science & engineering studies. Computer Applications in Engineering Education, 24 (2), 202-214.\n\nBurbaite, R., Bespalova, K., Damasevicius, R., & Stuikys, V. (2014). Context-aware generative learning objects for teaching computer science. International Journal of Engineering Education, 30 (4), 929-936.\n\nShiomi, M., Kanda, T., Howley, I., Hayashi, K., & Hagita, N. (2015). Can social robot stimulate science curiosity in classroom? International Journal of Social Robotics, 7 (5), 641-652.\n\nToh, L. P. E., Causo, A., Tzuo, P., Chen, I., & Yeo, S. H. (2016). A review of the use of robots in education and young children. Journal of Educational Technology & Society, 19 (2), 148-163.\n\nZaharija, G., Mladenovic, S., & Boljat, I. (2015). Use of robots and tangible programming for informal computer science introduction. Procedia \u2013 Social and behavioral Sciences, 174 (1), 3878-3884.\n",
        "label": "human"
    },
    {
        "input": "Apple Inc. and Computer Market Essay\n\nApple Inc. is a US multinational company that has its headquarters in California and designs, manufactures, and sells electronics, online services, and software. It was founded in early 1976 by Steve Wozniak, Ronald Wayne, and Steve Jobs and mainly dealt with the development and sale of personal computers (Kane, 2015). The company became incorporated in 1977 when it changed its name to Apple Computer, Inc., a process that gave it considerable momentum and increased profitability. Some of its hardware products are personal computers, smartphones, tablet computers, and media players (Kanagal, 2015). The corporation\u2019s software includes iTunes media player, iOS and macOS operating systems, and the Safari web browser, while its online services encompass iCloud and App Store. Nevertheless, the high cost of its products coupled with the struggle for power between executive directors caused problems for the corporation. Because of the arising challenges, Steve jobs decided to resign and start his company referred to as NeXT.\n\nWith the expansion of the computer market, Apple experienced decreased sales attributable to low-priced products from its competitors, especially Microsoft. Job shuffles followed to the point that its CEO in 1997 chose to buy NeXT as a way of bringing back Steve Jobs to the corporation (Kao, 2018). After regaining the CEO position, Steve Jobs began the process of rebuilding Apple\u2019s reputation, which he succeeded in realizing until 2011 when he resigned as the Chief Executive Officer because of health problems. Apple is currently the largest information technology corporation across the globe with respect to revenue and the second-biggest cellphone manufacturer after Samsung. In 2015, Apple was the first American company to be valued at more than 700 billion US dollars. It employs over 120,000 full-time workers and runs about 500 retail stores in about 22 nations.\n\nReferences\n\nKanagal, N. B. (2015). Innovation and product innovation in marketing strategy. Journal of Management and Marketing Research , 18 , 1-25.\n\nKane, Y. I. (2015). Haunted empire: Apple after Steve Jobs . New York City, NY: Harper Business.\n\nKao, R. (2018). Disruptive leadership: Apple and the technology of caring deeply\u2013Nine keys to organizational excellence and global impact . New York City, NY: Productivity Press.\n",
        "label": "human"
    },
    {
        "input": "Personal Computer and Social Media Security Case Study\n\nCase 1\n\nToday, private users and businesses extensively use computers that are connected to networks for storing and sharing valuable data. Cybersecurity is now a concern of each individual as such data as private information, banking service access details, and information about accounts the person uses are the assets for attacks. Computers are vulnerable to the loss of data, so they should be protected by several means such as cryptography technology, password security policy, or network security technology. The suggested steps to personal computer security are firewall and antivirus protection, security settings on the browser, password management, two-factor authentification, and automatic updates. The lack of protection causes risks of virus or malware intrusion, hacker attack, or identity theft.\n\nFirewall protection is the first step to network security as its main aim is to prevent unauthorized access. According to Ming, Chen, and Guo (2019), it effectively controls and supervises access between different networks and automatically rejects data information that contains risk. The computer under discussion runs on Windows 10 OS, so it already has Windows Firewall, which gives the necessary protection. Still, system security can be compromised by a user\u2019s activity, and once it happens, the firewall is no more useful.\n\nThat is why additional software for identifying and dealing with viruses and malware is required. Windows Defender is the most appropriate anti-virus in the discussed case as it suits the OS, and does not impact performance.\n\nFor the aim of preventing networks from collecting user information, browser security settings should be applied. However, Easttom (2016) argues that too strong browser security settings unable access to many pages, so they need to be moderate. For example, Google Chrome browser security settings include such functions as a \u201cDo Not Track\u201d request for browser traffic. Additionally, one may use browser extensions, such as VPN, that provide anonymous surfing.\n\nThe security of computers that use protective software can still be compromised as there are security flaws. Hackers try to identify them and create codes that target these flaws. However, the process goes both ways, and the developers are continually improving their software. To be able to have the newest and safest versions of the software, one should enable automatic updates, because outdated programs are extremely vulnerable. Encryption is another method of information protection and confidentiality. Free basic encryption solutions, such as VeraCrypt, are often sufficient for private users.\n\nLogin information is a frequent target of attackers, and its theft causes catastrophic outcomes. That is why the passwords for all the services should be secure. The main recommendations for the passwords are to make them long and complex. Uniqueness means that each service a person uses must have a different password. People who use one password for all accounts put their security at considerable risk. Picking it once, attackers can access any information from any account a person uses. It is especially dangerous when a user has the same password for e-mail as for other services. With this information, hackers can access all the accounts and change their passwords. Remembering all the passwords, especially when they are complicated, is not easy, so password managing services are recommended.\n\nAccess to banking services is probably the most valuable information for private users. Its protection requires all the mentioned means, such as password strength policy or antivirus. However, in case of loss or theft of the PC, smartphone, or credit card, two-factor authentification is recommended. This type of authentification implies that two devices are necessary for login. Even when all the security software is running, a user must pay attention to their behavior. One should know how to detect phishing on e-mail, and to be careful when downloading programs. Neglecting these simple rules threatens virus attack, identity theft, or information breaches.\n\nCase 2\n\nIn the USA, children at the age of 13 are allowed to create accounts on Facebook and several other social media services. If teenagers above this age want to use social networks, they should be supervised and adequately instructed about security issues. Full access to a child\u2019s account and messages is unethical, especially without their consent, but it is recommended that adults check their privacy settings once in a while and teach them about online security.\n\nTeenagers rarely have valuable information, such as banking account access details, but they are vulnerable to identity theft and online child predatory activity. Moreover, according to Schaik, Jansen, Onibokun, Camp, and Kusev (2018), teenagers are at risk of cyberbullying and the theft of information concerning their e-mail and phone number. That is why children\u2019s safety on the internet is more about behavior guidance than technical solutions.\n\nBasic privacy settings for such social networks as Facebook and Twitter include personal information and messaging activity protection. To avoid identity theft, the majority of the account content should be invisible to strangers. Facebook posts visibility settings should be switched to \u201cFriends only\u201d mode, and tweets have to be visible only for the approved users. Failing to do so may result in creating cloned pages that would act under one\u2019s identity.\n\nOther essential parameters of Facebook and Twitter settings are making one\u2019s personal information private, disallowing location sharing, and creating a strong password. In addition to this, two-factor authentification is necessary to make sure that nobody else uses the account. If a user sees suspicious behavior from friends, asking to click on some links, it is recommended to use another way of communication, for example, a phone, to ask whether their account has not been hacked.\n\nAfter checking for all the necessary security settings, adults should instruct teenagers in their behavior online, guiding texting, teaching them to identify cyberbullying, child predatory behavior, and identity theft. According to Easttom (2016), 19% of cyberstalking cases escalate to attacks in the real world. That is why clear safety rules must be presented to teenagers:\n\n  * Do not accept any follow or friendship requests from strangers.\n  * Do not send any of your photos or personal information to people you do not know offline.\n  * Text only to people you know in real life.\n  * In real life, never meet with anybody you do not know offline.\n  * Never respond to cyberbullying.\n  * Report identity theft through special forms.\n\nTo ensure that these rules are followed, they must be justified to teenagers. In other words, they must understand what threatens them if the security prescriptions are not followed. According to Kayes and Iamnitchi (2017), the most security and privacy risks are related to two types of threat \u2013 \u201cattacks that exploit the implicit trust \u2026 attacks that harvest user\u2019s personal information for ill-intended use\u201d (p. 1). That is why the most severe outcome is child predatory behavior in the real world when children agree to meet with strangers. Secondly, information and page content may be used for identity theft online, causing cyberbullying and information misuse.\n\nReferences\n\nEasttom, C. (2016). Computer security fundamentals (3rd ed.). Indianapolis, IN: Pearson.\n\nKayes, I., & Iamnitchi, A. (2017). Privacy and security in online social networks: A survey. Online Social Networks and Media, 3-4 , 1\u201321. Web.\n\nMing, X., Chen, Y., & Guo, J. (2019). Analysis of computer network information security and protection strategy. MATEC Web of Conferences , 267, 02013. Web.\n\nSchaik, P. V., Jansen, J., Onibokun, J., Camp, J., & Kusev, P. (2018). Security and privacy in online social networking: Risk perceptions and precautionary behavior. Computers in Human Behavior, 78 , 283-297. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics in Criminal Investigation Research Paper\n\nAccording to the International Telecommunication Union (2012), the term computer forensics in its usage portrays the orderly gathering of information and examination of computer-based innovations and technologies to scan for digital evidence. When handling a crime scene, investigators should remember that the documentation of the scene builds a record of the investigation for prosecution purposes. Computer Forensics falls within the domain of digital forensic science, and the goal of an investigator in the realm of computer forensics is to forensically examine a computer to identify, obtain, and analyze digital information found on a computer and its components. The US Department of Justice defines digital evidence as data and information of significant worth to an examination that is domiciled on, got, or transmitted by an electronic gadget. This proof is gained when information or electronic gadgets are seized and secured for assessment (Cole, Gupta, Gurugubelli, & Rodgers, 2015). In this section, this paper will address the components of a computer to photograph during forensic photography, the most emergent action an investigating officer should take upon arriving at a cyber-crime scene, the value of computer\u2019s hard disk in computer forensics, the concern of encryption (Secure Hash Algorithms) in a cybercrime scene investigation, and the restrictions that are there for cybercrime scene investigators on the data they seize while executing a warrant for the contents of hard drive during a crime scene investigation.\n\nCriminological photography is an irreplaceable device in present-day measurable odontological convention, which helps in insightful systems, support of documented information, and to give proof that can enhance lawful issues in court. Investigators at a crime scene set many goals in their quest to achieve the most desirable outcomes in an investigation. One such goal the investigators set is incorporating photography into detective work. For this reason, the proper determination, selection, and execution of the appropriate photography and computer components coupled with proper training as well as correct workflow operations make fusing photography into the field of crime scene investigation an effectively reachable objective (Gouse, Karnam, Girish, & Murgod, 2018). The job of the forensic picture taker is essential as decent expertise in photography together with continued learning of the mechanics and procedures involved is requisite for legal documentation of proof. According to the US National Institute of Justice\u2019s \u201cGuide for First Responders\u201d (2008), other than accounting for the area of crime commission, there is a requirement for an agent to record not just the scene itself, but also the condition of, the power statuses, and the conditions of the computers together with its attached components and accessories such as storage media, portable components such as thumbnails, PDAs, and Internet access devices.\n\nIn a crime scene, while searching for digital evidence, an officer should, therefore, photograph the mentioned devices associated with a computer, before and after marking the scene. Computers may be the portable type \u2013 the laptop and handheld devices such as tablets and smartphones \u2013 or the non-portable type \u2013 the desktop computer. The detective should capture images of the computer\u2019s monitor, as is, whether powered on or not. The desktop computer has a monitor (screen) which is detached from the computer\u2019s central processing unit (CPU). If not available in plain view of the officer, they should locate the separate CPU of a desktop computer and photograph it as well. The keyboard for a desktop computer is also detached hardware, although some computer operating systems provide for an on-screen keyboard.\n\nThe investigator should locate and capture an image of the keyboard, mouse (pointer), and other equipment connected to the computer\u2019s CPU, including external speakers and recording devices, if any. With a laptop computer, most of the abovementioned components are a single integrated unit. If flapped, the officer should capture an image of the computer in the flapped condition and capture images of the laptop computer when not flapped. The investigating officer should also capture any other devices peripherally attached to the laptop. For handheld computers, the integration of components is even higher with minimal peripheral components. In the case of handheld computers, the forensic photographer should capture the devices wholly from their front and rear.\n\nThere are also other non-component parts of value on a computer; these are serial numbers, make (name of the computer e.g., HP), model (e.g., HP ProBook), and model number (e.g., 4440s). Manufacturers usually inscribe the make, model, and model numbers of desktop and laptop computers on the exterior of the devices. The photographing officer should capture the computer\u2019s make, model, and model number as well. For handheld devices, only the make is available in plain view, the model and model number are usually part of the devices metadata and are only obtainable from the devices\u2019s manifest once powered on. Additionally, the investigators should take images of the devices\u2019 serialization on the computer or its peripheral devices. Serial numbers are unique to each device and the devices\u2019 identities. Manufacturers inscribe serial numbers both exteriorly and in the metadata manifest of the device.\n\nIf the photographer cannot locate a serial number on the device, they should do so from the device\u2019s metadata and capture it too. Other than serial numbers, computers have light signals, for instance, power-on light, wireless connection light, web connection light, among others. The investigator should locate and capture images of the light signals on the computer for purposes such as showing the power status, the Internet connection status, among others. Another critical aspect of a computer is networking. Networking computers utilize many different cables and transfer equipment such as LAN and WAN cables, power cables, USB cables, VGA cables, HDMI cables, and the like. For an investigating officer, they should photograph such connectors found on and with a computer and take a keen interest in any information printed on the exterior of these connectors. The officer should capture the connectors in situ and then label them appropriately before proceeding to capture images of the connectors with the appropriate labels.\n\nFor an officer, as a first responder, the most emergent action upon arriving at a cyber-crime scene is to secure the scene and evaluate it (National Institute of Justice, 2008). In this preliminary phase, the officer should account for their self-safety as well as the safety of all persons at the scene. The investigator should ensure that, in so doing, they remain within the confines of the Police Department\u2019s policy as well as the federal, state, and local laws. For instance, if the first responder feels unsafe, they should call in for back and detest any manner of action that is provocative or that endangers their lives and the safety of the scene. The investigating officer should secure all electronic devices, and these include personal as well as portable devices (National Institute of Justice, 2008). Another immediate act of great importance is barring unauthorized persons from accessing the scene and rejecting any help volunteered by unauthorized persons.\n\nIt is the investigating officer\u2019s duty obligation to secure and seal off the scene of felony commission, and partly the fulfillment of this obligation incorporates expelling all people from the scene of a crime and the area in close proximity to the area from which they plan to gather proof. The official, first on the scene, has an obligation to guarantee that the state of the PC and every single electronic gadget stay unaltered. To this end, the officer may draw up a sketch of the scene as they found it with all devices intact and-or take a photograph of the scene from different angles to ensure they capture all the relevant details. Failure to secure the scene can get the scene compromised, and so will the digital evidence be. Evidence that is compromised may be inadmissible in court or it may lead to an undesired outcome of litigation. Also, failing to secure the scene can lead to the alteration of evidence and probably lead to the officer\u2019s harm. Eventually, first responders should ensure that they leave the computer and other electronic appliances powered off according to the National Institute of Justice guidelines if they found it off. Another huge obligation on the shoulders of a first responder immediately they get to the scene is ensuring that any physical evidence that a scene can offer does not get compromised in any way during documentation.\n\nThere is an abundance of potential digital evidence on a PC. A large number of these things are obtainable through a manual or logical/computational extraction procedure. While a portion of the proof overlaps with data found on the web, there are a couple of essential sources that are obtainable from the physical gadget instead of on the Internet (Goodison, Davis, & Jackson, 2015). The latter will typically arise from the computer\u2019s hard disk. The hard disk drive of a computer contains the data stored in and by that computer\u2019s use. Some people call the computer\u2019s hard disk its memory, without which, the computer is unusable. The hard disk is indispensable in the functioning of the computer, and it is, therefore, a component of the most value in a cyber-crime scene. All systems and software used on and by a computer coordinate their actions on the hard disk, and as such, even software, networking applications, and IT networks leave vast amounts of data on the computer\u2019s memory (Goodison et al., 2015). The National Institute of Justice (2008) affirms that a computer\u2019s hard drive indeed contains information like email messages, image (and photograph) files, databases, Internet browsing history, financial records, Internet Chat logs, event logs, as well friend lists and itineraries that would be valuable as evidence during investigation and the prosecution of a crime. These facts affirm the top value that hard disks have in a cyber-crime scene.\n\nFor instance, when surfing on the Internet, programs and software will frequently keep up transitory Internet documents, cookies, and browsing history (Goodison et al., 2015). Every one of these things is usable in an investigation to decide the user\u2019s online behavior. Intermittent files and records and cookies are regularly utilized by sites themselves to follow Internet users\u2019 activity and store data. Email and different messages might be found on the physical hard disk of the PC too. Even though most email messages remain in the custody of Internet servers, some messaging programs, and software stores earlier messages onto a PC hard drive. All the information above points out that the computer\u2019s hard disk drive archives almost everything that happens on that particular computer, even if it is in the form of caches, temporary files, or shadow copies.\n\nEncryption is a perfect example of privacy-enhancing technology (PET). PET aims at protecting and preserving the privacy of individuals and the confidentiality of personal information (United Nations Office on Drugs and Crime, 2019). Encryption is, therefore, a way of hiding data by locking out any individual who lacks the encryption code that concealed the data. The International Telecommunication Union, in a 2012 report, defines encryption as \u201ca technique of turning a plain text into an obscured format by using an algorithm\u201d (p. 81). The security of many application software today relies on Hash Functions (or Hash Algorithms) to secure user data. Out of different hash functions stems different security properties depending on the individual security requirements of the application software. There are three fundamental security characteristics of hash algorithms; \u201cpre-image resistance, second pre-image resistance, and collision resistance\u201d (AlAhmad & Alshaikhli, 2013, p. 240).\n\nThe pre-image resistance is the lack of ability to learn or know the contents of the data input from the data\u2019s hash digest; \u201cFor any given code h, it is computationally infeasible to find x such that H(x) = h\u201d (AlAhmad & Alshaikhli, 2013, p. 240). The second pre-image resistance generates similar hash digests by ensuring that there is an inability to learn or to know about the contents of the subsequent pre-image from the given initial pre-image; \u201cfor any given input m, it is computationally infeasible to find y \u2260m with H(y) = H(m)\u201d (AlAhmad & Alshaikhli, 2013, p.240). The interpretation of the collision resistance arises when two independent and varying input contents result in a similar hash digest; \u201cit is computationally infeasible to find any pair (m, y) such that H(y) = H(m)\u201d (AlAhmad & Alshaikhli, 2013, p. 240). Novak, Grier, and Gonzales (2018) assert that hash verification is a potential hindrance for sifting collectors during the collection of digital evidence. Owing to the said reason, hash verification (and encryption in general) is a significant concern in a cyber-crime scene investigation.\n\nHash verification involves the use of an electronic or computational signature otherwise called a verification code or a hash to ensure that a disk image is a match of the original evidence disk as postulated above. In the event of cyber-crime scene investigation, a problem arises with disks that have hash algorithms encryption. Existing techniques for hash check rely upon confirming the whole disk and in this way, are incompatible with Sifting Collectors (Novak et al., 2018). Be that as it may, this issue is not restricted to Sifting Collectors; present-day, solid-state drives (SSDs) are frequently incompatible with a hash check because specific SSD locales are precarious and unstable because of maintenance tasks. However, if there were to be a break between sifting collection and modern practice, the drawback of hash verification (and encryption) could get overcome.\n\nWarrants for the contents of a hard drive are typically restricted to the relationship between the evidence and the crime under investigation. Before starting a search, specialists and investigators must guarantee that they submit to material laws or stand the risk of having held onto evidence proclaimed unacceptable at preliminary trials for inadmissibility. There are certain jurisdictions in which exceptional cases may legitimize search and seizure exercises devoid of a warrant, for example, in case of consent/assent, \u2018crisis\u2019 fear-based oppressor and terrorist circumstances, plain view principle, searches related with lawful arrests, among others (Brown, 2015). However, the practical search of the information put away on a gadget, as a rule, requires that an investigator produces a warrant in common law nations/jurisdictions.\n\nIn conditions where there is a considerable danger of losing proof, for example, where information sanitization and other anti-crime scene investigation measures are active or imminent, a few jurisdictions license law enforcers to play out a restricted hunt of gadgets without a warrant because of the apparent susceptibility of the information and data in these devices (Brown, 2015; Cole et al., 2015). Remote cleaning and erasure apparatuses are packaged preinstalled on numerous mobile devices and accessible for buy as business software or freeware. During warranted action, examiners may likewise find lawfully ensured sources of ESI, for instance, the principle of lawful expert benefit, open intrigue insusceptibility, among others (Brown, 2015). Such legally protected sources of ESI add a layer of unpredictability to the procedure of proof handling, search, and seizure. Numerous specialist examiners experience authoritative postponements, delays, and adjournments in acquiring constitutional power to direct police examinations because of legal uncertainty about cybercrime offenses.\n\nIn most western popular governments, authoritative national legislations exist to implement compliance with universal human rights law, such as the rights to privacy and the freedom of expression. In the United States, for instance, there is still equivocalness about the translation of the Fourth Amendment assurances to the digital world realm (Cole et al., 2015). Concerning the Fourth Amendment and digital proof ventures, the plain view exemption and the closed-container guideline has raised substantial attention (Cole et al., 2015). At the point when an agent is leading a pursuit inside the extent of a warrant and runs over contraband material in plain view, the official has the authority to hold onto it. The issue with digital proof is that the degree is at times overbroad (Brown, 2015). With a substantial warrant, the specialist can look through the entire hard drive as though it were a container, and in this way, the majority of its substance is in \u2018plain view\u2019 of the officer.\n\nContingent upon the judge and proof submitted, courts may constrain the extent of such searches. Legitimate position and best practices for enforcing warrants of search and seizure vary altogether across locales and criminal justice systems, including enactment and standards regulating the treatment of electronic evidence during litigation. At the point when police lead search exercises, equipment (hardware components), programs and software, external storage media, and data in binary and printed structure might be seized. It is occupant for examiners to think about the appropriateness of viewing and forensically obtaining information at the scene, i.e., \u2018in situ\u2019 and whether the conditions may legitimize physically holding onto the material for further investigation in a research facility. Cole et al. (2015) contend that, ordinarily, evidence from a warranted search is admissible if the testifying witness had firsthand information of the proof if the proof obtained resulted from an automated process or framework, and if the computerized record(s) meet the business records exemption to the Hearsay Rule.\n\nReferences\n\nAlAhmad, M., & Alshaikhli, I. (2013). Broad view of cryptographic hash functions. International Journal of Computer Science Issues, 10(4 No.1), 239-246. Web.\n\nBrown, C. (2015). Investigating and prosecuting cybercrime: Forensic dependencies and barriers to justice. International Journal of Cyber Criminology, 9(1), 55-119. Web.\n\nCole, K., Gupta, S., Gurugubelli, D., & Rodgers, M. (2015). A review of recent case law related to digital forensics: The current issues. In Annual ADFSL Conference on Digital Forensics, Security and Law (pp. 95-104). Daytona Beach, FL: Embry-Riddle Aeronautical University, Scholarly Commons. Web.\n\nGoodison, S., Davis, R., & Jackson, B. (2015). Digital evidence and the US criminal justice system: Identifying technology and other needs to more effectively acquire and utilize digital evidence. Web.\n\nGouse, S., Karnam, S., Girish, H., & Murgod, S. (2018). Forensic photography: Prospect through the lens. Journal Of Forensic Dental Sciences, 10(1), 2-4. Web.\n\nInternational Telecommunication Union. (2012). Understanding cybercrime: Phenomena, challenges, and legal response. Web.\n\nNational Institute of Justice. (2008). Electronic crime scene investigation: A guide for first responders, Second Edition. Web.\n\nNovak, M., Grier, J., & Gonzales, D. (2018). New approaches to digital evidence acquisition and analysis. Web.\n\nUnited Nations Office on Drugs and Crime. (2019). Cybercrime module 10 key issues: Enforcement of privacy and data protection laws. Web.\n",
        "label": "human"
    },
    {
        "input": "Keystone Computers & Networks Inc.\u2019s Audit Plan Report\n\nPurpose of Audit Plan\n\nKeystone Computers & Networks, Inc. (KCN) is an information technology company that develops networking software products and sells them to its clients (Whittington & Pany, 2012). The company had hired an external audit firm to carry out its audit according to the US Generally Acceptable Accounting Principles (GAAP). The external auditor is required to carry out the audit of the company\u2019s financial statements and provide its professional opinion based on the fundamental principles of independence, responsibility, integrity, and verified disclosure of business accounts and financial transactions.\n\nMoreover, the auditor has to perform the review of the internal controls of KCN and determine the degree of risks associated with different trades and their reporting. The current report describes the audit plan that has been proposed to investigate various accounts and transactions of the company incurred during the year ended on December 31, 20X5 (Whittington & Pany, 2012).\n\nAudit Plan for KCN\n\nConsiderations\n\nIt is reported that the last three years\u2019 audit of KCN was performed by Adams, Barnes & Co. Therefore, the auditor has the experience and understanding of the company\u2019s accounting system and internal controls. Moreover, it will be easier to perform the audit of 20X5 accounts and financial statements as the firm has previous years\u2019 documentation. The objectives of the audit are (1) to identify and describe the objectives of the audit engagement and services that will be provided to the client by the audit team and (2) to define the responsibilities of the audit team members (Whittington & Pany, 2012).\n\nThe contents of the audit engagement are as follows.\n\n  * To perform an audit of the company\u2019s financial statements, including the statement of financial position, statement of income, statement of total earnings, and statement of cash flows for the year ended on December 31, 20X5.\n  * To issue a letter confirming whether the covenants of the letter of credit agreement with Western Financial services are in full compliance or not.\n\nAssessment of Internal Controls\n\nThe internal control related to the client\u2019s CIT will be selected for evaluation. The reason is that its manager was involved in fraudulent activity, which caused a significant financial loss to the company. Moreover, internal controls over deliveries of products in and out of the client\u2019s warehouse will be carefully evaluated. The audit will execute the following steps to assess the current status of the company\u2019s internal controls.\n\n  * Assessment of the correctness of journal entries by using Computer Assisted Audit Techniques.\n  * Determination of the risk of a material misstatement by reviewing the basis for estimates used by the client.\n  * Identification of significant transactions and seek the management\u2019s rationale for them.\n  * Evaluation of the segregation of duties related to the access to inventory, authorization of trades, record keeping, and verification (Whittington & Pany, 2012).\n\nMeasurement and Review of Financial Performance\n\nThe management of KCN uses various performance measures, including inventories and receivables turnover, aging of A/R, sales and gross margins by type of revenue, net income, and total inventory balance (Whittington & Pany, 2012).\n\nInventory\n\nThe audit will investigate KCN\u2019s list as follows.\n\n  * The inventory of KCN will be observed physically by performing an inventory count at its warehouse.\n  * A cutoff analysis will be performed by halting delivery of merchandise in and out of the company\u2019s warehouse and then analyzing the deliveries before and after the physical count (Whittington & Pany, 2012).\n  * The inventory count will be reconciled with the general ledger balance and cost of goods sold.\n  * Review of freight costs and also determining inventory ownership and inventory in transit to compare with the physical count.\n  * Assessment of the basis for the change in the inventory policy and consistency of the inventory valuation method (Whittington & Pany, 2012).\n\nReceivable\n\nThe following procedures will be performed.\n\n  * Reconciliation of the accounts receivable report to the general ledger.\n  * Reconciliation of the allowance for doubtful accounts to the wrong debt expense account and also perform a consistent comparison with the method and amount/percentage used in the previous year (Whittington & Pany, 2012).\n  * Review of the accuracy of sample invoices, shipping documents, cash receipts, customer return records, credit memos.\n  * Reconciliation of sales ledger with the aged receivables\u2019 listing.\n  * Comparison of the level of prepayments with the previous year\u2019s data and also check the amount prepaid.\n  * Calculate the gross profit of each product and compare it with the company\u2019s last year\u2019s data and industry average (Whittington & Pany, 2012).\n  * A cutoff analysis similar to the inventory to investigate accounts receivable before and after the cutoff time.\n\nRevenue\n\nThe audit will investigate revenue recognition by KCN as follows.\n\n  * Obtain information and evaluating controls over the client\u2019s revenue measurement, recognition, and reporting.\n  * Assessment of the compliance of revenue recognition procedures with the relevant financial accounting standard will be checked (Whittington & Pany, 2012).\n  * Detailed testing of revenue transactions and reconciliation to the general ledger.\n  * The quarterly sales data will be checked for overstatement of figures by performing a substantive testing procedure.\n  * Review of the management\u2019s policy regarding the change in product pricing.\n  * Assessment of the percentage of planning materiality for sales and highlight significant variations, if recorded in the audit of sales and income.\n\nPayables\n\nThe audit will involve the following substantive procedures.\n\n  * Reconciliation of the accounts payable report to the general ledger, suppliers\u2019 accounts, and cost of goods sold.\n  * Verification of transactions for accuracy, completeness, and legitimacy by selecting an adequate sample of supplier transactions (Whittington & Pany, 2012).\n  * A cutoff analysis will be performed to check the recording of accounts payable before and after the cutoff time.\n  * Examination of documents for any unrecorded liabilities.\n  * Comparison of versions payable balances of the current year with the previous year and industry data.\n\nIntangible Assets\n\nThe audit will perform the following tests.\n\n  * Review of the accounting of receipt and creation of intangible assets and reconcile their accounts with general ledger.\n  * Assessment of the carrying value of intangible assets and determining their market value or specialist valuation (Wahlen, Jones, & Pagach, 2017).\n  * Recalculation of the amortization amount and reconcile the accumulated amortization account with the expense account.\n\nNon-Current Assets\n\nThe following audit procedures will be performed during the engagement.\n\n  * Physically confirm the existence of non-current assets, including equipment, furniture, and leasehold improvements.\n  * Collection of the summary of the opening values of non-current assets, accumulated depreciation, and net book value (Whittington & Pany, 2012).\n  * Recalculation of the depreciation amount based on the estimated useful life and report any significant changes in the value of non-current assets.\n  * Reconciliation of the non-current assets schedule to the general ledger.\n\nLine of Credit\n\nIt is one of the critical objectives of the audit plan, which will be addressed by performing the following procedures.\n\nExamination of the covenants of the letter of credit and determine the compliance of KCN with them.\n\nChecking documentation and confirming the balance with Western Financial Services (Whittington & Pany, 2012).\n\nMiscellaneous Expenses\n\nThe audit will carry out the following procedures.\n\n  * Review of individual accounts such as marketing and travel expenses included in miscellaneous and check for accuracy, completeness, and legitimacy by selecting an adequate sample of transactions.\n  * Reconciliation of individual accounts to the general ledger (Kumar & Sharma, 2015).\n  * Overstatement of marketing expenses will be checked and reconcile with the client\u2019s correspondence, invoices, and payments to marketing agencies.\n  * Checking for overstatement of traveling expenses and negotiating with the general ledger.\n\nUtilities\n\nThe audit will perform the following tests.\n\n  * Review of individual accounts, e.g., electricity, telephone, and gas included in utilities and check for accuracy, completeness, and legitimacy by selecting an adequate sample of transactions.\n  * Reconciliation of individual accounts to the general ledger and bank account (Kumar & Sharma, 2015).\n\nRent, Insurance, and Legal and Accounting\n\nThe following audit procedures will be carried out for these expenses recorded in the income statement.\n\n  * Review the rental agreement and its terms and reconcile the rent account with the bank account.\n  * Review the insurance agreement and its terms and reconcile the insurance account with the bank account.\n  * Review the legal and accounting agreement and its terms and reconcile the legal and accounting account with the bank account.\n\nSignificant Risks\n\nThe following audit procedures will be performed in light of the identified risks.\n\n  * Analysis of the information about the client\u2019s relationships and environment to verify the information provided by the management of KCN.\n  * Since KCN has recently adopted the strategy of selling computers to high-credit risk-bearing customers, therefore the audit team cannot rely on the previous year\u2019s audit and need to devise new substantive tests to collect evidence related to such accounts. For this purpose, the auditor will check stores of high-credit risk customers and determine the terms of sales and analyze any delinquency or default on payments.\n  * The audit team will review the policy of KCN to pay bonuses to executives on a quarterly basis and test its procedures for determining dividends paid to its executives in the last four quarters. The auditor will check the client\u2019s sales and net income and adjustments made at the end of each quarter to predict variations. The audit will monitor performance against key financial targets.\n\nSignificant Accounting and Auditing Matters\n\nThe focus of audit procedures will be on the following issue.\n\nA fundamental accounting issue is the capitalization of networking software products developed by KCN. It needs to be assessed under FASB ASC 985-20-25 Costs of Software to Be Sold, Leased, or Marketed, which requires determining whether the software is sold as a separate product or as a part of other products or services (Wahlen et al., 2017). The technical feasibility of the software developed by the company will be ascertained by examining the detailed program design and its working model. After this step, the cost of coding, testing, and other related activities will be determined for capitalization.\n\nA significant audit issue involved in the determination of the software development cost is to determine whether KCN has categorized them properly as research and development is necessary for achieving technical feasibility. Therefore, the timing of these costs is also crucial.\n\nConclusion\n\nThe audit plan prescribed in this report covers different accounts, transactions, and policies of Keystone Computers & Network, Inc. The substantive procedures included in this plan are aimed at collecting sufficient evidence to present a complete audit report, which means that the financial statements of KCN meet various assertions, including appropriateness, accuracy, fairness, and completeness.\n\nIt is also indicated that multiple other procedures can be performed to enhance the quality of the audit to be completed. More importantly, the use of computer-assisted audit techniques can be used to verify various accounts cover in this plan. Furthermore, the findings of the internal audit are crucial to determine the areas of weakness and also the extent of substantive tests to be performed.\n\nReferences\n\nKumar, R., & Sharma, V. (2015). Auditing: Principles and practice (3rd ed.). New Delhi, India: PHI Learning Pvt. Ltd.\n\nWahlen, \u200eJ. M., Jones, J\u200e. P., & Pagach, D. (2017). Intermediate Accounting: Reporting and analysis (2nd ed.). Boston, MA: Cengage Learning.\n\nWhittington, O., & Pany, K. (2012). Principles of auditing and other assurance services (18th ed.). Boston, MA: McGraw-Hill.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics and Cyber Crime Essay\n\nIt is generally accepted that the cyber environment provides people with both positive opportunities such as self-realization or creativity and negative ones involving specific problems. Unfortunately, the digital space can also become an atmosphere of involving children and adolescents in criminal acts, thus bringing to psychological issues by means of mobbing, cyber-fraud, embezzlement of money from an electronic parental purse, pornography, pedophiles\u2019 acquaintance with children, free access to extremist websites, sale of drugs through the Internet, etc. Due to age characteristics, the insufficient educational activity of parents, the provision of unlimited opportunities in the online environment, and also due to the low media literacy of the population, people can become victims of cybercrime and sometimes even turn into criminals. In this connection, it is of great importance to identify potential problems that are encountered by law enforcement when computer crime organizations cannot ensure the safety and security of users.\n\nNowadays, people live in the era of the information society when computers and telecommunication systems cover all the spheres of human and state life. Focusing on telecommunications and global computer networks, it was impossible to foresee the opportunities for abuse these technologies could create. Today, among victims of criminals who operate in virtual space, there are not only adults but also children. Even though computer crime laboratories continuously update their products to ensure the safety and security of users, forensic computing faces plenty of problems. Online games are one of the most popular activities on the Internet, along with searching web pages and socializing (Britz, 2013). The main threats faced by users involve the following points:\n\n  * Phishing. The criminal web-environment generates many fake gaming sites. Some of them use URLs that are very similar to the addresses of real websites. An unsuspecting player is threatened not only by malicious programs but also by traps that can steal personal data and cause real financial problems (Hu, Chen, & Bose, 2013).\n  * Social networks. There are a lot of scammers with fake accounts that pretend to be producers of games, requiring to open personal information.\n  * Infected games and fraudulent programs. The user can easily infect his or her computer through hacked or fake computer games. Their creators abuse the hobbies of users, offering such games for downloading on the Internet. It is necessary to be especially careful when using email attachments and removable media, such as USB flash drives.\n  * Malicious programs. One of the most common malicious programs to date is Win32 / PSW.OnLineGames. This is a family of Trojans used in phishing attacks aimed specifically at users of computer games: malicious programs of this type are able to spy on typing text from the keyboard and sometimes include rootkits that can collect information related to online games and user credentials (Pfleeger, Pfleeger, & Margulies, 2015). It is characteristic that the information is transferred to the computer of the remote attacker. The members of the massively multiplayer online role-playing games (MMORPG), such as Lineage, World of Warcraft, or Second Life, should be aware of these numerous threats.\n\nAs an example, one may note the case when about 18 thousand people lost hundreds of millions because of the bankruptcy of the virtual bank Ginko in Second Life. According to Scarle et al. (2012), prior to the bankruptcy, the bank had savings of more than 700 thousand dollars (or 190 million Linden dollars, the rate of which is relative to dollar as 270 to one). The official cause of bankruptcy was the massive withdrawal of money from accounts in Ginko as a result of the ban on gambling in Second Life. For a week, the virtual bank went bankrupt, and those who did not manage to withdraw their investments lost their savings. Some experts suspect that this was a planned action.\n\nAnother problem is pedophiles who increasingly go to the World Wide Web to satisfy their unhealthy lust through the psychological deception of children. According to Tikhonov and Bogoslovskii (2012), they actively use various computer programs so that no one can identify them by calculating their IP address, creating the identikit by video link, or determining the place of residence. Knowing the basics of child psychology, they get acquainted with minors in social networks, acquire confidence, and then, under the pretext of fun games, make them undress and commit disgusting things (Hu et al., 2013). Due to the anonymity of the media environment, this type of crime has become easier to implement. The pedophiles build trustful relationships with adolescents in chat rooms, forums, and social networks. They get acquainted with children, positioning themselves as a future good friend, a senior companion, and a sincere assistant. After establishing contact with minors, criminals can offer personal meetings, pictures of erotic nature, and intimate communication through a webcam (Reddy & Minnaar, 2015). Despite the ban and the abundance of regulations aimed at combating such crimes, it is rather difficult to control the correspondence of the online environment to the established norms and values as cybercrime strategies are ever-changing and quite resourceful.\n\nReferences\n\nBritz, M. (2013). Computer forensics and cyber crime: An introduction (3rd ed.). Boston, MA: Pearson.\n\nHu, Y., Chen, X., & Bose, I. (2013). Cybercrime enforcement around the globe. Journal of Information Privacy and Security , 9 (3), 34-52.\n\nPfleeger, C. P., Pfleeger, S. L., & Margulies, J. (2015). Security in computing (5th ed.). Upper Saddle River, NJ: Prentice Hall.\n\nReddy, E., & Minnaar, A. (2015). Safeguarding children from becoming victims of online sexual abuse facilitated by virtual worlds. Child Abuse Research in South Africa , 16 (1), 23-39.\n\nScarle, S., Arnab, S., Dunwell, I., Petridis, P., Protopsaltis, A., & de Freitas, S. (2012). E-commerce transactions in a virtual environment: virtual transactions. Electronic Commerce Research , 12 (3), 379-407.\n\nTikhonov, M. N., & Bogoslovskii, M. M. (2012). Pitfalls of new information and communication technologies. Scientific and Technical Information Processing , 39 (2), 67-73.\n",
        "label": "human"
    },
    {
        "input": "Computer Crime Investigation Processes and Analyses Essay\n\nThe wide usage of technologies preconditioned the appearance of a great number of threats. The increased vulnerability of data combined with the exploration of digital storage resulted in the need for efficient computer crime investigations. With this in mind, the given paper delves into the peculiarity of this very process and the most important aspects specialists who work in the sphere should give their attention to. Besides, computer-related forensic procedures that are needed at the first stages of the investigation are suggested in the paper. Moreover, the process of securing digital evidence is also outlined. The document revolves around the necessity of following all these procedures to achieve success and guarantee the preservation of crucial data and increased security of companies and organizations. Finally, the probability of oversight and its hypothetical impact on the image of a crime scene are also discussed. At the end of the paper, the conclusion is given.\n\nThe mass use of innovations altered the character of modern society, which at the moment could be described as a networked one. However, the digitalization of the majority of activities performed by different organizations is a two-edged weapon as it also undermines the privacy and secrecy of data. The appearance of new vulnerabilities demands new approaches to investigate computer crime scenes. The efficiency of these approaches depends on the observance of specific procedures and methods that might guarantee positive final results and the elimination of similar threats that might appear in the future. For this reason, the functioning of these crime scene investigators becomes an important element of the coherent society.\n\nBesides, having arrived at a computer crime scene, an investigator should ensure that the three most important computer-related forensics procedures are performed to process the evidence of the crime and a high level.\n\nFirst of all, he/she should obtain a warrant (U.S. Department of Justice, n.d.). Any piece of evidence that is collected in the course of the investigation should be obtained legally and set down to avoid any misunderstandings or problems. An investigator could be blamed for a prejudiced attitude. That is why a warrant is needed, as it helps to prove the fact that all pieces of evidence are collected in an appropriate way, and there are no points of concern.\n\nSecond, the crime scene should be documented, including the tiniest details that could be found there. It is crucial to ensure that evidence is preserved in the way close to its original state (Britz, 2013). It will help to increase the probability of positive outcomes and guarantee that the following analysis will provide the credible evidence needed for the investigation. In case some alterations are crucial, they should be justified and noted as even the slightest change could distort the final results of the examination.\n\nThird, it is extremely important to preserve all pieces of evidence in the same way they were collected during the investigation of a crime scene (U.S. Department of Justice, 2011). It is the key to the credibility of findings and successful results. Moreover, if the evidence is altered, it should also be noted not to trigger the appearance of numerous misunderstandings.\n\nOnly in case these procedures are observed, an investigator will be able to perform his/her duties at a high level.\n\nTherefore, the increased threat of digital crimes also results in the great necessity of special tools and procedures that might help to secure digital evidence and find offenders. Besides, a financial investigator should, first of all, have a secure storage location that will be used to save all information obtained in the course of the investigation and protect it (U.S. Department of Justice, 2011). One should realize the fact that criminals will obviously want to eliminate any fact that could help to prove their guilt. For this reason, it is also important to have sufficient back-up copies that could be used in case the data that is preserved in special storage becomes corrupted (Casey, 2011). The usage of these copies will obviously help to guarantee the prosecution of offenders and elimination any threat of that sort in the future. For this reason, these approaches become crucial for a financial investigator.\n\nNevertheless, in case one of the above-mentioned procedures is omitted, the final results of the investigation will not be credible and suffer from the lack of real pieces of evidence. For instance, if the warrant is not obtained, all facts collected by a financial investigator will be considered not relevant because of the possibility of fraud or juggling with facts. The absence of credible facts and evidence will deteriorate the results of the investigation significantly, and a team will not be able to find an offender. Moreover, if digital evidence is not secured, offenders might use the same loophole and corrupt them. It will also have a great pernicious impact on the process of the investigation and deprive members of a team of any opportunity to find real offenders and provide them with punishment. In this regard, the observance of these procedures is crucial as it preconditions the positive outcome of an investigation.\n\nThe complexity of data collection and the nature of this sort of crime also precondition the increased probability of oversight that might occur during the process of identification of digital evidence. For instance, in case the scope of computer forensics is too narrow, it will be difficult to determine what systems have evidence or not (\u201cCommon mistakes during a computer forensic analysis,\u201d n.d.). At the same time, the identification of the wrong system might result in a waste of time and an inability to collect the latest facts. For this reason, to avoid such oversights, the investigator should, first of all, avoid using internal IT staff as they might be subjective and provide false data (\u201cUnderstanding digital evidence,\u201d n.d.). Furthermore, he/she should perform computer forensics at the crime scene immediately because it is crucial to be able to collect only credible and relevant evidence. Finally, an investigator should ensure that all-important procedures are observed, and only approved methods are used.\n\nAltogether, considering the fact that digital crimes have become one of the most topical problems of modern society, the functioning of investigators who arrive at a crime scene and collect digital evidence becomes especially important. They should perform a set of complex actions and follow outlined procedures to be able to collect the most credible and relevant evidence that could be used to find an offender. Thus, in case some of the above-mentioned procedures are disregarded, the results of the investigation will be corrupted, and a team will not be able to provide prosecution to offenders. In this regard, the observance of the basic rules of digital evidence collection becomes a crucial aspect of the functioning of any specialist of this sort.\n\nReferences\n\nBritz, M. (2013). Computer forensics and cyber crime: An introduction . New York, NY: Pearson.\n\nCasey, E. (2011). Digital evidence and computer crime: Forensic science, computers and the Internet . Cambridge, MA: Academic Press.\n\nCommon mistakes during a computer forensic analysis. (n.d.). Web.\n\nUnderstanding digital evidence . (n.d.). Web.\n\nU.S. Department of Justice. (n.d.). Forensic examination of digital evidence: A guide for law enforcement. Web.\n\nU.S. Department of Justice. (2011). FBI law enforcement bulletin. Digital evidence . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics: Identity Theft Essay\n\nThis paper is focused on the peculiarities of the forensic process. In particular, it discusses the investigation of the digital evidence connected with identity theft. The authentication examples that can be used to prevent related crimes and investigate them are provided. The ways to maintain a chain of custody are described. The influences of the First and Fourth Amendments on the investigation of identity theft are pointed out.\n\nThe forensics process that is maintained in the framework of computer-related technologies provides professionals with the opportunity to gather, analyze, and report on the information. The point is to make this digital data legally admissible so that it can be used to prevent or identify a crime and to resolve disputes (Watson & Jones, 2013). The very forensic process usually consists of five steps:\n\n  * Plan. First of all, professionals define the way they are going to act. They develop a kind of a guideline to be followed in order to improve understanding of a case, be aware of all sources of evidence, and use appropriate procedures.\n  * Acquire. Digital information should be gathered from all sources. Forensic disk imagination should be maintained.\n  * Extract. Data should be extracted from the device so that it can be used in other environments.\n  * Analyze. The information taken from various files is to be analyzed using forensic techniques and tools.\n  * Report. Results of the analysis should be presented and explained in the report so that other professionals involved in the investigation can resort to them (Global digital forensics, 2017).\n\nIdentity theft and computer crime are among the most critical issues discussed in forensics because they have an enormous influence on people\u2019s lives. In order to deal with cases related to them, particular steps should be taken by an examiner:\n\n  * A case is reviewed to become familiar with all facts.\n  * A victim is re-interviewed.\n  * Obtained information is authenticated.\n  * The analysis is maintained.\n  * Initial leads of a crime are identified.\n  * Evidence is gathered from institutions and governmental agencies.\n  * The timeline is developed.\n  * All involved actors, locations, and businesses are identified.\n  * Link analysis is conducted.\n  * Evidence book is created.\n  * Common crimes are sought.\n  * Information is shared with other professionals (\u201cInvestigating identity crime,\u201d n.d).\n\nIdentity theft is a rather common issue today. It is observed when a criminal obtains and uses personal information that belongs to another individual. In the majority of cases, this kind of theft is associated with financial issues. As a rule, it is maintained with the purpose of economic gain. Rather often, criminals simply try to get money from people\u2019s bank accounts. However, in some cases, they also use personal information in order to get a loan or commit a credit card, etc. The consequences of this crime may be crucial for a victim because one\u2019s reputation and creditworthiness can be damaged. To avoid costly and time-consuming repairs, various organizations provide their customers with an opportunity to resort to electronic authentication methods and services. They are also considered in the investigative process of identity theft.\n\nIn addition to traditional personal usernames and passwords that are unique for every person, some organizations offer their clients special cards, such as Circle Bank. During authentication, people provide a secret number that can be obtained only when using this card. As a result, the second form of authentication is maintained. As no one except for the owner is supposed to have access to this information, fraudulent access can be prevented, and criminals can be identified.\n\nMany institutions start using security tokens these days. They provide one-time passwords that change every minute/hour/day. Users are to copy the correct number to enter their accounts. Other people can hardly obtain an up-to-date password because it changes too often (Bosworth, Kabay, & Whyne, 2014).\n\nIn the framework of forensic analysis, professionals have an opportunity to find out whether other people hacked victims\u2019 accounts and stole their personal information or not. They can also compare images and fingerprints that are needed for authentication to check if they are real or not.\n\nWhen professionals reach a source of information or a particular device, they are supposed to copy information from it in order to analyze it with no changes made to the original version. A chain of custody is used to prove that everything was done properly and this target was reached. The account of the location of every file is to be thoroughly maintained. In particular, it is conducted through chronological documentation of everything that was done to the information. A paper trail can also be used to point out how the information was gathered, stored, controlled, shared, analyzed, and used.\n\nChain of custody for digital evidence can be maintained in the form of a report. Here, it is critical to state several main factors. First of all, it should be mentioned who worked with the evidence because this person is to bear responsibility for everything done to it. Then, the very procedure that was maintained is to be described for professionals to think of possible alterations that could have been done to the evidence. The timeline is also vital, so it should be stated when collection or transfer took place. The location should be mentioned as well. Finally, a chain of custody should include the information of how and why the evidence was stored/collected/shared.\n\nIt is also possible to maintain documentation with the digital evidence management framework. Here, the focus is on the identification, collection, and examination of evidence. Mainly, the same information is to be included because the purpose of this element remains does not change. However, the report is focused on the life cycle of digital evidence. Thus, it is mentioned who, how, and when, etc., identified the evidence, stored it, transported, and examined it. A special form can be used to note this information and obtain an appropriate certificate (Cosic & Cosic, 2012).\n\nWhen conducting an investigation, professionals are to resort to the Constitution so that no laws and freedoms are violated. Dealing with identity theft, they need to consider the First and Fourth Amendments because they define what is believed to be legal and illegal. In other words, they determine what information can be used in the court.\n\nIt is believed that the most controversial issues related to computer crimes and identity theft are connected with the First Amendment. It guarantees major freedoms to the population and limits the information that can be obtained by the professionals and methods used to reach it. For example, those materials that are published online cannot be violated, all communications are generally considered to be secret, and community standards are to be followed as well as legislative acts. The First Amendment allows professionals to define the information the storage and use of, which is considered illegal. It defines obscenity and prohibits the representatives of the general public from using sexual portrayals and situations with children, for instance. The First Amendment protection to children and family is also ensured in this way because it applies standards according to which inappropriate materials are not allowed to be published online and be in free access. Internet gambling is prohibited as well, so if professionals find the information that advertises and broadcasts it, they can treat it as a criminal case. However, the range of restrictions applied to the freedoms of the First Amendment is argued rather often, which makes investigations more complex.\n\nA lot of limitations to the forensic process are made under the Fourth Amendment. In general, it ensures that no interferences with people\u2019s privacy are maintained only when there are enough well-grounded reasons for them. In some cases, it may lead to the delay of an investigation, which gives criminals more time to hide illegal data and escape the punishment. The Fourth Amendment protects personal information that is stored in different devices, including computers, phones, etc. In fact, this kind of information is not easy to reach because it is often protected by passwords. Because of the Amendment, professionals are not allowed to hack the device to get the data. They need to resort to law enforcement and obtain permission to get the information themselves or make a person provide it. What is more critical, in some cases, even if a search is allowed, it may not be possible to maintain it because of sidestep password protections.\n\nHaving permission only for visual inspection, professionals may not be able to identify the information that is encrypted and put in misnamed files. Thus, because of the Fourth Amendment, investigators have limited opportunity to identify illegal activity and prevent or solve a crime. In order to have an opportunity to search for all significant information, professionals are to prove the presence of the evidence on the device, including very specific information. While working with networked computers and those that belong to the family members, investigators can face difficulties because they may be immune from searches. However, regardless of the prohibitions and limitations, both Amendments still give space for electronic surveillance. Even though it is often challenged, surveillance protects Americans and gives an opportunity to resolve many computer crimes (Britz, 2013).\n\nThus, with the development of the Internet, criminals obtained more opportunities to affect the lives of the general population. Maintaining computer crimes, they become able to commit identity theft and reach people\u2019s financial information. As a result, they can spend money on other people and take loans, etc. A proper forensic process focused on digital information can be used to find evidence to resolve the crime and, in some cases, even to prevent it. Authentication techniques that are used by various organizations and are made to ensure the privacy and security of individual information give the general public a chance to reduce the risk of such issues or at least give professionals an opportunity to find a criminal. For the case to be successfully resolved, they maintain a chain of custody of digital evidence, which proves that the information used as evidence was not altered with the help of thorough notes of everything that was done to the data involved in the investigation. Unfortunately, the First and Fourth Amendments provide a range of limitations to the investigation, especially the search procedure. However, they still provide an opportunity to prevent a lot of crimes and protect the population.\n\nReferences\n\nBosworth, S., Kabay, M., & Whyne, E. (Eds.). (2014). Computer security handbook . Hoboken, NJ: John Wiley & Sons.\n\nBritz, M. (2013). Computer forensics and cyber crime: An introduction . New York, NY: Pearson.\n\nCosic, J., & Cosic, Z. (2012). Chain of custody and life cycle of digital evidence. Computer technology and application, 3 , 126-129.\n\nGlobal digital forensics. (2017). The computer forensic process: An overview . Web.\n\nInvestigating identity crime . (n.d.). Web.\n\nWatson, D., & Jones, A. (2013). Digital forensics processing and procedures . Waltham, MA: Elsevier.\n",
        "label": "human"
    },
    {
        "input": "Computer Crimes: Viewing the Future Essay\n\nThis paper is focused on crimes associated with developing technologies, in particular, high-tech crimes. It describes all the main characteristics of such crimes in detail. Main up-to-date technologies used to commit and prevent them are outlined. Finally, the way they are influenced by technology, culture, and politics is revealed. As a result, the university can get to know information critical for the design of a course that focuses on high-tech crimes.\n\nNowadays, technology develops extremely fast, which provides the representatives of the general public with numerous advantages. They obtain an opportunity to communicate with their friends and relatives as well as to get to know people from distant places. In addition to that, advanced technology allows them to maintain a lot of financial operations online without contacting professionals or going somewhere. People\u2019s lives seem to be stored in the form of digital data. Even their homes and vehicles can be controlled with the help of computers. Of course, such innovations appeal to the individuals as they provide them with new opportunities and give a chance to save time for other activities. However, the possibilities to commit computer, cyber, and Internet crimes, which are also known as high-tech crimes, increase enormously.\n\nHigh-tech crimes are observed when technology is used to maintain criminal activities. In the majority of cases, this process is connected with the utilization of computers. These devices can be involved in two ways. First of all, they can be the target of criminals. Rather often, attacks are committed in order to obtain information that is stored on a computer or to alter the data, adding something new and changing original files. In other cases, computers are used only as a facilitating factor. For instance, they can provide an opportunity to prevent the organization from working so that it is easier to maintain another crime and steal some objects.\n\nThere are different types of high-tech crimes, but, as a rule, they are focused on people\u2019s personal lives. The violation of privacy is observed when one\u2019s individual information is revealed to the public. For example, it can deal with photos and videos. The theft of personal and financial data is another crime that happens regularly. Offenders tend to maintain it because they are willing to get access to people\u2019s bank accounts or to take loans, etc. As a rule, these crimes include:\n\n  * Hacking. It deals with exploiting vulnerabilities in the network.\n  * cracking of passwords. Criminals get to know the personal passwords of users.\n  * Copyright infringement. The rights of the owner are violated, as one\u2019s work is used without permission.\n  * Phishing and pharming. These are attempts to manipulate users.\n  * Spreading malicious codes. They affect a computer and allow criminals to send, alter, and still information.\n  * Spreading child abuse images. This content is prohibited under the First Amendment as it hurts children and their families.\n\nThese crimes are facilitated by the Internet that is used by many people today. That is why it would be rather advantageous for the university to offer a new class focused on high-tech crimes (Fairtlough, 2015).\n\nHigh-tech crimes are committed by different individuals and even groups. These may be people who organized in order to get engaged in a particular criminal activity. They can be hired by criminal gangs occasionally or offer their services on a regular basis. Finally, they can be one of those scattered cells that exist almost in all countries and manage to hide from the authorities. These criminals resort to the Internet because it gives them more opportunities to commit crimes and escape any punishment. The difficulty in tracing them is connected with the lack of knowledge about the Internet itself, issues with the reporting systems, and spread of information, etc.\n\nVarious technologies are used to commit and prevent cybercrimes. Criminals, for example, find it advantageous to use broadband connectivity. It gives them a chance to reach millions of subscribers and do them harm. Being cheaper than its alternatives and providing easy access to the Internet, it encourages people to be online 24/7. As a result, hackers have more opportunities to reach digital data that is stored on the computer. Wi-Fi networking is also widely used because it allows people to be connected to the Internet from different places. Even though all up-to-date wireless devices use encryption, they cannot be secure enough and easy to hack. Removable media, including CDs and flashcards, is also often approached.\n\nWith its help, hackers can steal digital data when they get physical access to the computer. In addition to that, it is a simple way to spread malicious codes. Innovative capabilities of the Web browsers provide criminals with an opportunity to implement malicious programs on users\u2019 computers. JavaScript and Active-X controls should be disabled to prevent attacks. E-mails and instant messages can also be used to commit computer crimes. Their origin can be difficult to discover, and content may include links that provide access to users\u2019 computers, etc. Unified communications make it easier to reach all important data without the additional necessity to hack every device and to crack different accounts. Peer-to-peer programs allow distributing information that violates copyright laws. E-commerce and online banking give hackers an opportunity to reach people\u2019s finances (Goodman, 2012). In this way, the influence of developing technologies on high-tech crimes is undeniable.\n\nDifferent security platforms and programs are created to prevent such crimes. Mainly, they are focused on improved encryption opportunities. Among the most recent technologies is mapping. These applications can be of different types. They can focus on geographic information and remote sensing to reveal where crimes are happening. Smartphone apps make it easier to record or take photos of criminal activities and to reach the police. Web reporting allows the authorities to know that crimes are happening or are going to happen. They streamline the identification of criminals and simplify prevention (Gruber, n.d.).\n\nOf course, it is also advantageous to update passwords, secure computers with firewall settings, be social media savvy, use built-in security features of mobile devices, and limit the usage of Wi-Fi hotspots (Linington, 2014). However, the way people treat and use these technologies depends greatly on their cultures. For instance, the majority of Americans use the Internet on an everyday basis and maintain a lot of financial operations with its help. Society makes them be available online, as education and job are almost not possible without technologies regardless of people\u2019s specialization. Similar tendencies are seen in other countries as well. In South Korea, for example, the population can easily use free Wi-Fi and mobile apps so that they can be approached anytime. Such tendencies make it easier for hackers to commit crimes and affect people\u2019s lives (Schuurman, 2013).\n\nPoliticians also deal with high-tech crimes. They develop various initiatives and policies that are expected to prevent and overcome computer-related criminal activities. The most demonstrating example here is the First Amendment. With its adoption, major freedoms were guaranteed to people. In its framework, copyright violations became treated as crimes, the secrecy of communications was ensured, and the necessity to align with community standards and legislative acts was underlined. Thus, politicians defined the information, which is considered illegal even today. For example, sexual portrayals and situations with kids are not allowed to be in free access to the Internet. They\u2019re also the limitation in their storage. Laws protect children and families through the application of standards related to those materials are cannot be shared. Politicians and policymakers also made Internet gambling a crime. Still, different aspects of the First Amendment are argued today, which emphasizes the necessity to address computer crimes additionally (Britz, 2013).\n\nThus, it can be concluded that developing technologies make it easier for hackers to commit high-tech crimes. Even though people seem to benefit from the advanced features of their devices and the Internet, they become more vulnerable. They can be manipulated by criminals, their personal and financial information can be stolen with the help of cracked passwords. Malicious codes, sexual and child abuse content can be spread, and copyrighting rights can be violated. As the representatives of the general public tend to be online almost 24/7, criminals obtain more opportunities to affect them. Being aware of this fact, professionals and policymakers develop special technologies and initiatives that improve people\u2019s privacy and prevent attacks. However, improvement is still needed in this sphere. If the university offers a high-tech crime class, it will contribute to this sphere because students will learn how to prevent computer crimes and avoid them in the future.\n\nReferences\n\nBritz, M. (2013). Computer forensics and cyber crime: An introduction . New York, NY: Pearson.\n\nFairtlough, J. (2015). Cyber crime investigation . San Clemente, CA: LawTech Publishing Group.\n\nGoodman, M. (2012). How technology makes us vulnerable . Web.\n\nGruber, C. (n.d.). 3 ways to prevent crime with technology . Web.\n\nLinington, D. (2014). Top 8 tips to prevent cybercrime . Web.\n\nSchuurman, D. (2013). Shaping a digital world: Faith, culture and computer technology . Dowers Grove, IL: IVP Academic.\n",
        "label": "human"
    },
    {
        "input": "Dam Computers Company\u2019s Strategic Business Planning Essay\n\nIntroduction\n\nThe contemporary business world is highly competitive. In order for an organization to achieve an intended share of the market, it has to plan to strategize its activities. This will be demonstrated in the business plan shown below.\n\nSample Business Plan for Expanding the Computer Business\n\nDAM Computers Company is an innovation of the partnership between two brothers who came up with an idea of starting up a business dealing with the sale of computers and its accessories and has been in operation for two years.\n\nObjectives of the Business Plan\n\nDam Computers has been in existence for 2 years. The business has been flourishing well but without any definite future forecast. Therefore, in order to define clearly the future of the business, the management during the end of the business year meeting decided to formulate the business plan for the company so as to ensure the company strives towards its vision of becoming an international firm. Thus the objective of this plan is to provide a guideline on the business\u2019s mission towards its vision.\n\nHow the Plan was developed (stages)\n\nIn conjunction with the management board, the expert selected for the task set out for the work. It was agreed that the draft is ready within a week\u2019s time such that the business plan developed to be adopted and become effective from the beginning of February 2012. Since the plan is the first one of its kind of business, the strategic plan adopted was a vision-based type. Therefore, the plan set has assumed the following stages as represented by the diagram below:\n\nDam Computers Company\u2019s Values, Vision, and Mission\n\nVision\n\nThe company will aim to be a leader in both the local and international market of computers and their accessories.\n\nMission\n\nThe mission of DAM Computers is to maximize all the opportunities available both nationally and internationally for the sale of computers and their accessories through proper investment, quality care of human resources, and proper handling of all customers.\n\nValues or Operating Principals\n\nThe operating principles of the company are honesty, integrity, customer care, and high technical adoption. The company shall live to ensure information technology is \u2018a must\u2019 achieve knowledge for everyone irrespective of race, sex, location, and status.\n\nInternal Environmental Scan\n\nHistory of DAM Computers Company\n\nDAM Computers Company is an innovation of the partnership between two brothers who came up with the idea of starting up a business dealing with the sale of computers and their accessories. The business has been in operation for two years and has been expanding well. The latest quest has been for it to become an international company.\n\nThe Structure of the Company\n\nThe company is a private limited liability company organized as follows:\n\nThe Structure of the Company.\n\nFrom the structure of the company, it means that the company has all the necessary manpower in place and has the ability to raise capital from the shareholders.\n\nBusiness Internal Situation and Trends\n\nAn assessment of the company\u2019s strengths and weaknesses has been reviewed and will be reviewed periodically. According to the review done, it has been identified that the business has the potential to support the expansion due to the following reasons: the current manpower has the ability to handle export and large scale business due to their prior experience from their previous employers though some additional manpower will have to be done. The directors and shareholders are willing to raise more equity close to 500,000 British pounds although more will be raised through profit plow back. Finally, the business is valued at \u00a32 billion meaning that it has a very good asset base to support it for a long time.\n\nThe External Environment of the business\n\nNational Situation and Trends\n\nThe business country of location, which is England, has a good climate for the business of computers. Due to its policy of maintaining a good postcolonial relationship with its former colonies, there is hope for securing good market grounds from some of these colonies. Since the relationship is likely to remain, the business plan adopted will be a long-term one although this will depend on the success of the first-year plan that shall be reviewed for success and failures.\n\nRecently, the national trend in the economic arena has been to stabilize the pound through export promotions. The legal formalities involved in the exportation process have been simplified. This means that the business will not encounter so many hurdles in its course of export documentation. Another trend in the nation has been an acceleration in the technological climate whereby the government is encouraging everyone to be computer literate beginning at an early stage. This spells good fortune for the computer business. Other external factors such as political and community values will be assessed by the team in the plan.\n\nSummary of opportunities and threats for computer business\n\nOpportunities\n\n  * Potential make both nationally and internationally\n  * Potential capital\n  * Potential manpower\n\nThreats\n\n  * Competition\n  * Unpredictable markets\n\nStrategy formulation process\n\nSince the business plan is vision-based, the organization has formulated its strategies based on the vision of the organization.\n\nThe Vision and the Plans\n\nThe vision of the company is to expand the local market and venture into the export business\n\nPlans\n\nThe company has planned to:\n\n  * Raise more capital from shareholders and directors within a year and in later years also. This will be facilitated by the managing director. The director has already pledged to play their part by increasing the investments. And thus the funds will be availed to the company\u2019s account by October this year.\n  * Advertise/market itself globally through DSTV and the internet, the process will commence on the 1 st of February spearheaded by the marketing manager. The advertisement activities will be directed at creating awareness of the existence of the company. The adverts will be created by the company\u2019s marketing staff.\n  * Identify companies from Africa dealing with the sale of computers and link up with them for possible supply business. The sales department has been allocated this task and will commence as soon as the capital has been raised from the shareholders.\n  * Identify new suppliers of computers at a good factory price. The potential supplier has already been found and the negotiations are going on spearheaded by the purchase manager.\n  * Increase its human resource to meet the demand for expansion. The human resource manager will be responsible for hiring extra sales and technical personnel that will be needed when the company expands its activities. Advertisements for the positions available will be made in June.\n  * Expand its local market. Two shops will be set up in Liverpool and leister cities. The strategic planning committee selected will start negotiating for the premises in August and the findings will be communicated at the end of that month.\n\nThe detailed plan is as shown in the table below:\n\nThe Annual Plan\n\nActivities                               Person In Charge     Start Date                                                          MidPoint Review            Finishing Date  Target                                                                                 projected expenses\nRaising capital                          Managing director    01/02/2012                                                          1/05/2012                  1/08/2012       Would have raised \u00a3500,000                                                             Nil               \nmarketing                                Head of marketing    01/02/2012                                                          1/03/2012 and every month  continuous      Would have identified and linked with potential customers locally and internationally  \u00a310,000           \nLooking for supplier                     Head of procurement  Already in place                                                    1/05/2012                  1/08/2012       To have linked with the already identified supplier                                    Nil               \nLinking with serious overseas customers  Sales manager        1/04/2012 or depending on the report from the marketing department  01/05/2012                 continuous      To have got a network of potential and reliable customers from Tanzania and Egypt      \u00a35000             \nDocumenting of export business           Sales manager        1/08/2012                                                           15/08/2012                 01/09/2012      All documents for export procedures to be ready                                        \u00a31000             \nAddition of labor                        Human resource       When needed                                                         Every month                continuous      To acquire all required manpower in place                                              nil               \n\n\nAccomplishments\n\n  * The company has already identified a local computer manufacturer called Intercom, which is offering a relatively low price than the former supplier Bidcom.\n  * The shareholders have agreed to raise a capital worth \u00a3500000 before the end of the year.\n\nDAM Computer Company after five years\n\nAfter five years, the company would have achieved the following targets using resources such as capital and human resources.\n\n  * Established computer godowns in Arusha Tanzania, Addis Ababa in Ethiopia, and Cairo Egypt.\n  * Employed more than 200 workers\n  * Established three more branches in the city\n  * Become a long term partner of supersport in the advertisement\n  * Been known in more than 50 states.\n\nGoals and Priorities\n\nIn order to achieve the five-year vision, the company will prioritize its activities in the order of importance for each year.\n\nThe important goal is to expand the market so priority will be on marketing and capital acquisition.\n\nStrategies\n\nThe strategies adopted to achieve the above goal are:\n\n  * Marketing through global advertising media, which in this case is the DSTV channel network provider from South Africa and the company\u2019s website. Others will be reviewed by the company\u2019s marketing department.\n  * Capital acquisition from shareholders and profit plow back\n\nMonitoring and Reviewing\n\nMonitoring the Progress\n\nThe company has selected a committee for the project consisting of 10 members. Every department has been represented in this committee and their names are on the company\u2019s website. The committee will be monitoring the progress of each department based on the assignment given and will be responsible for setting targets for every party involved with penalties given for the group that will fail. The major progress is expected at the end of every year, while periodic progress is expected depending on the task as shown in the plan (Mitchell n.d., p.23).\n\nPlans for Reviewing and Refining the Plan\n\nDue to the fact that the business world is dynamic, the plans will be reviewed considering each item separately for the period stated in the plan and then refined accordingly. The monitoring committee will be responsible for reviewing the plan. The whole activity will be headed by the managing director and the monitoring committee.\n\nConclusion\n\nThe strategic plan put in place for the Dam computers has been set to determine its destiny that is unknown. It has been prepared with faith that some factors will remain constant. This is the main undoing of the whole idea since the future is always not known. Nevertheless, there can be no success without planning.\n\nAlternative Strategy for the Company\n\nAlthough a strategy has been drawn for the company in question three, this is not the only option available. In fact, it may not be the best one. In order to get the best option for the company, various options will have to be evaluated.\n\nThe vision-based strategic plan adopted by the DAM Computer Company is not the only strategy that would have been taken by the company. There are other available strategies for the company to explore. They include substantive growth, limited growth, and retrenchment, among many. The choice of a strategic decision will depend on some external and internal factors of the business (Ansoff 1965, p.76).\n\nThis alternative is dependable on corporate business and the functioning strategies. In the case of corporate strategies, the emphasis is on resource allocation. Such strategies involve retrenchment, growth; concentrations, or diversification (Whelan & Hunger 1995, p.106). The corporate strategies available for the DAM computers company are retrenchment and expansion and diversification (Wheelen & Hunger 1995, p.78).\n\nFunctional Strategies\n\nIn the case of functional strategy, the functional units of the business assemble all management aspects of the business for proper functioning. Since the company is geared towards functional strategy, cooperative strategies are not applicable to it at this moment. Therefore, the company is forced to evaluate options available for a functional strategy to be in place.\n\nIn order to evaluate these options, the company should use one of the marketing tools to evaluate its growth options since it is still in its initial growth stages. One such marketing tool that is recommendable is the one called Ansoff\u2019s Matrix. This matrix presents the organization with four categories of options in order for the company to meet its objectives. This matrix can be represented diagrammatically as shown below.\n\nThe Market   Existing            New                \nThe Product  products            Products           \nPresent      Market              Product Development\nMarket       Penetration                            \nNew          Market Development  Diversification    \nMarket                                              \n\n\nDiagram 2: the Ansoff\u2019s marketing matrix (Ansoff 2010, p.1).\n\nIf the company had to adopt the above diagram of Ansoff\u2019s marketing and product matrix, then it means that it will have four options that can be analyzed as follows (Mind Tools 2008, p.2):\n\nPenetration of the Market\n\nFrom this option, the company that has the present product in the market may choose to market the existing product by penetrating through the market and have a bigger market share consequently increasing its revenue. In the case of DAM computers, this option means that since it already has some considerable market coverage, it may concentrate on the marketing of the current computer products.\n\nBy employing this strategy, the business will achieve a considerable increase in revenue and will not incur any risk. In spite of its benefits, market penetration has its disadvantages. If the company adopts this strategy, it will imply that no new customers will be sort after and that the company will not have an opportunity for bringing new products to the market or develop the existing ones. In addition, market penetration has its saturation point or limit to the consumers when it reaches that point, the company will be forced to develop an alternative strategy (Ansoff 1965, p.88).\n\nMarket Development\n\nFor this option, the company is in a position to market its current products to formally unexploited places. Such activities may include exportation to newfound markets in overseas countries or selling to new regions formally not covered by the company. For the case of DAM computers, this option means not altering their computer products and selling them to overseas countries and other parts of the country.\n\nThe benefits of this strategy are that the product is not altered and is relatively cheap compared to the product development technique. Also, the wider area is covered and more customers are found (Proctor 1997, p.16). However, the product is not developed and is prone to becoming less competitive with the improvement in technology. In addition, the exploitation of markets does not guarantee an all-time gain to the company.\n\nProduct Development Strategy\n\nIn this strategy, the company adopting it is the one that has a new product that ought to be marketed to its existing customers. In most cases, the company in question is compelled to be more innovative and thus invents an improved product from the existing one or completely come up with a new one to replace the existing one but with the same ability. These instances are prompted by the improvement in technology that is being witnessed in the market. The trend is to come up with the most efficient and cost-effective product. In the case of DAM computers, this option means that they come up with new models of computers that suit the market at every particular stage. This is of paramount importance since computer technology and models keep on improving every day.\n\nThis strategy is good since it improves the company\u2019s image of innovation. Also, the specific needs of the customer are met as new technology is exploited. The strategy also helps to maintain the uniqueness of the company in the market. However, the process highly risky as the new technology may fail to impress the customer. In addition, the cost involved in the process is too high. This method is only suitable for manufacturers.\n\nDiversification Strategy\n\nThe company seeks to acquire new customers in this strategy. The process usually takes two forms; related and unrelated diversification. In the case of related diversification, the company remains in the range that it is familiar with. For instance, if the industry is a manufacturing business, it may jump from one line of business to another such as from bread manufacturing to soap manufacturing. In the case of interrelated manufacturing, the business may decide to completely change the range of business. For instance, it may change from the manufacturing business to the service industry.\n\nThis option is not viable for the DAM computers since its vision is pegged in the computer business. Diversification strategy is the riskiest strategy and also the most inefficient one. Since the customers have already been established, it becomes hard and inefficient to shift. The only hope lies in the fact that this strategy can be more profitable to an organization if the area of the shift was calculated well (Proctor 1997, p. 34).\n\nThe Most Suitable Strategy\n\nFrom the analysis of the four strategies presented above, it is evident that each one of them has its own outstanding path from another one. Furthermore, each one of them has got its disadvantages which are not desirable. In planning for the best strategy, an organization may want to do away with the majority of these disadvantages and maximize the advantages. Although this is desirable for many, it is impossible for it to be achieved by using only one strategy. Thus, the best alternative is to make use of multiple of these strategies so as to reap their benefits.\n\nIn the case of DAM Computers Company, the alternative strategy will be a combination of three strategies; product development, market penetration, and market development. The reasons for the chosen combination are because the company\u2019s vision is to maintain the current market while at the same time exploring new markets both locally and internationally. Also, the company\u2019s vision is pegged on computer business thus there is no way it is going to change its line thus diversification strategy is not in their picture (Pearce & Robinson 2004, p.106).\n\nTherefore, the company will opt for market penetration since it will ensure the company maintains its current customers, market development since it is in the process of venturing into export business, and product development since computer technology is changing rapidly; they will have to keep themselves abreast with the trend by purchasing from the manufacturer who is keeping the trend. For this reason, the combination of the three mentioned strategies will be taken for the best result\n\nConclusion\n\nThere exist many strategies for any business organization or company to explore in order to reap maximum operational profits and also to sustain itself on the market. They include substantive growth, limited growth, and retrenchment, among many. The choice of a strategic decision will depend on some external and internal factors of the business (Ansoff 1965, p.76). This alternative is dependable on corporate business and the functioning strategies. In the case of corporate strategies, the emphasis is on resource allocation. Such strategies involve retrenchment, growth; concentrations, or diversification. It is upon the company to explore all the available strategies in order to come up with the best.\n",
        "label": "human"
    },
    {
        "input": "Supercomputer Design, Hardware and Software Report\n\nTable of Contents\n 1. Introduction\n 2. Types\n 3. Hardware\n 4. Software\n 5. Performance Measurement\n 6. Conclusion\n 7. References\n\nIntroduction\n\nSupercomputers are computers that can make calculations much faster than those computers we use every day. Also, supercomputers can store and process much more data. They are needed to solve complex mathematical and practical problems, such as physical problems, modeling of complicated systems, such as structures of materials and chemical substances, and simulations in which a lot of factors need to be considered.\n\nThe practical benefits of supercomputers\u2019 work include forecasting the weather, analyzing economic situations, and testing things that people will use, such as airplanes: to see how safe they are, complicated simulations are performed by supercomputers. Although most people do not encounter supercomputers in their everyday life, it is important to understand that supercomputers are crucial not only for science but also for making possible things that all people use, such as weather forecasts or the production of fuel. To understand the design of supercomputers, their types should be described, hardware and software should be explored, and the way their performance is measured should be considered.\n\nTypes\n\nOne of the main criteria by which supercomputers are divided into groups is the way memory is stored in them. Two major ways of storing memory are shared and distributed. Distributed memory means that each processor has its memory; shared memory means that there is one memory space used by all the processors. When distributed memory is used, there is usually a processor and a memory unit, and there is the need to interconnect them so that programs on different processors could interact.\n\nOne of the methods to establish this interconnection is a point-to-point circuit that functions as a communications channel with two permanent endpoints similar to a tin can telephone. Another method of establishing interconnections is using hardware acting as a network switch (Hwang, Dongarra, & Fox, 2013). There is also the combined distributed shared memory type, in which each node (i.e. each element of a network) can access and retrieve data from a shared memory but also possesses its own limited memory that is not shared with other nodes.\n\nThe advantage of shared memory is that it implies the existence of a unified space from which all the necessary data can be retrieved; however, different processors usually need access to certain kinds of data that they regularly retrieve while rarely accessing other parts of the memory space or never accessing them at all. Therefore, the advantage of distributed memory is higher efficiency because the initial design considers which processors will need which memories, and redundant interconnections are excluded. Also, from the point of view of hardware, machines that will use distributed memory are easier to design and build.\n\nOn the other hand, the advantage of shared memory supercomputers is that they allow parallel computing, i.e. a method of solving complicated problems that are based on the assumption that complicated problems can be broken into simpler ones, each of which can be solved concurrently instead of consecutively, i.e. at the same time instead of one by one. Parallel computing is enabled when processors that solve different subproblems have access to one memory in which the original complicated problem\u2019s data is contained.\n\nBased on the above, another way of dividing supercomputers into groups is by their approach to problem-solving: some supercomputers solve problems by doing calculations for each one serially, one at a time, and other supercomputers run a larger number of operations solving subproblems in a parallel manner. Which approach is used depends on the memory type of a supercomputer.\n\nHardware\n\nWhat is referred to as hardware in computer science is computers\u2019 physical elements, i.e. machines and their parts, and one of the main considerations in the history of creating supercomputers was how to develop different components of hardware and how to arrange them (Patterson & Hennessy, 2017). The creators of supercomputers focused on developing central processing units (CPUs) that process data much faster than CPUs in personal computers\u2014the speed of processing is one of the main things that differentiate supercomputers from regular ones.\n\nHowever, it was understood at some point that even if CPUs are fast, the same amount of time would be needed for supplying data to CPUs (or even more time would be needed because there would be more data to process), so the parts of hardware responsible for this supply needed to be improved, too. For this, additional simple computers were connected into one network with the only purpose to receive data from the memory and send it back there so that the CPU focuses on processing only.\n\nFor a long time, the general direction in the development of supercomputers was combining them into larger structures capable of solving problems and performing calculations faster and more effectively than each of them separately. However, the direction eventually changed to improving the quality of design. Explaining this, the so-called father of supercomputing Seymour Cray, the inventor of the fastest computers of his time, once famously said, \u201cIf you were plowing a field, which would you rather use? Two strong oxen or 1024 chickens?\u201d (Machanick, 2015, p. 113). Indeed, the massively parallel computers of the new generation are networks of connected but parallel processing units whose calculation capacities are combined for more efficient processing of data.\n\nOne of the main hardware-related issues for supercomputers is heat management. Supercomputers and facilities that maintain them consume large amounts of electricity that are many times more than the amounts consumed by households. The use of energy inevitably produces heat, and it is a threat to the hardware. Moreover, heat is produced unevenly, i.e. some parts of supercomputers become hotter during their functioning than other parts, and they can damage their own inner structures or pieces of hardware near them. Therefore, complicated cooling systems need to be designed.\n\nThe first consideration is that components of supercomputers should be made, if possible, of materials that are not likely to become extremely hot because of the energy that supercomputers consume, i.e. the initial heat management begins at the stage of designing hardware and selecting materials for it. Further, a cooling system needs to be applied. In early supercomputers, liquid-based cooling was used; however, modern supercomputers rely more on air conditioning and combined systems.\n\nSoftware\n\nThose aspects of supercomputers that are associated with the software, i.e. computer programs, changed within recent decades. The main consideration in the work of supercomputers was initially their speed; a supercomputer was one that could perform calculations much faster than regular computers, which is why the general trend of the early development of supercomputers was creating operating systems and other software individually for each type of supercomputers so that they match the intentions expressed in the architecture.\n\nHowever, with the technological development, it became evident that more unified approaches should be used, such as adopting ready-made software. Moreover, the advantage of this unification is that software used in supercomputers is open-source, i.e. more flexible and able of being adjusted to particular programming needs (Hwang et al., 2013), unlike software that is used in mass-produced regular computers because of the needs of supercomputers are rather more specific, diverse, and advanced.\n\nA specific software-related characteristic of supercomputers is that different operating systems can be used for different parts. Parallel computing requires a certain type of distribution of functions: while some nodes perform calculations, other nodes provide additional services, such as data supply (see Types). Therefore, since the functions are different, the use of different computer programs on such elements as compute nodes and server nodes is justified.\n\nAlso, different kinds of existing supercomputers have dramatically different architectures, which is why, although ready-made operating systems are used in most of them, extensive adjustment is needed for each kind of supercomputer. According to Machanick (2015), a particular software issue for supercomputers is that, in massively parallel computing, programs should not only process data but also plan the extensive exchange of data between different nodes.\n\nThe parallel architecture also requires open-based software solutions that allow coordinating the data exchange. One of the ways of approaching this coordination is by connecting several regular computers into networks that process connections in a distributed memory supercomputer system. For shared-memory supercomputer systems, application programming interfaces (APIs) can be used that can be run on computer clusters and separate nodes to ensure that data is properly retrieved from the central memory unit and further supplied to appropriate units in which it is processed. The goal is to prevent CPUs from wasting time waiting for data to be supplied and to let them perform calculations at their full capacity.\n\nSince massively parallel supercomputers are complicated systems with many complexly connected and coordinated elements, the risk of faults in them is higher than in regular computers. When something goes wrong in the work of a supercomputer, it may be hard to find what the reason is that there are so many components involved in processing a task. Therefore, debugging, i.e. finding and correcting errors, is challenging for parallel structures, and the software that supercomputers use should be built in a way that allows monitoring every step of a particular process so that, once an error occurs, it can be detected, and the element can be found in which the error occurred. For this, there is a separate area of software testing for supercomputers that can be referred to as high-performance computing testing.\n\nPerformance Measurement\n\nIn comparison with the performance of regular computers, the performance of supercomputers is measured differently. For regular computers, the performance (particularly, the speed of processors\u2019 work) is measured in instructions per second (IPS); for supercomputers, the unit of measurement is floating-point operations per second (FLOPS); such operations refer to solving problems through calculations that involve very large or very small numbers compared to those we usually use or even those regular computers use. Such numbers are used in complicated probability calculations, such as weather forecasting. However, how many FLOPS a supercomputer has does not fully show how powerful it is.\n\nAnother consideration is measuring a supercomputer\u2019s capability. The capability is understood as the ability to solve one complicated problem using all the computing resources of a supercomputer; the time needed for this is considered, too. The capability, in contrast, is how much computing a supercomputer can generally do, i.e. how many problems of a certain complexity it can solve within a given period (Patterson & Hennessy, 2017).\n\nIt means that, despite having high capacity, some supercomputers may lack the capability, i.e. be unable to solve one complicated program despite being able to solve several regular ones (regular for supercomputers, but possibly still too complicated for regular computers). Also, it should not be thought that the performance of a supercomputer can be measured by summing up the performances of all of its parts; when working together, different units, nodes, clusters, and components responsible for memory, processing, and interconnection produce better results than the sum of their results would be if they worked separately.\n\nConclusion\n\nSupercomputers\u2019 types, hardware, software, and performance measurement have been explored. There are different types of supercomputers depending on how they store memory: in each processor or in a centralized memory unit. A major aspect of supercomputers\u2019 work is massively parallel computing, i.e. breaking complicated problems into smaller subproblems and calculating solutions for each subproblem separately and at the same time.\n\nHardware used in supercomputers is designed to make it possible, and the architecture features a network with many nodes responsible for different functions. Software is developed so that the fast processing of data is not hindered by data supply. The performance is measured through the number of complicated problems a supercomputer can solve within a second and through the complexity of problems, too. Although there are many different types of supercomputers, and they are designed very complexly, these general principles apply to all of them.\n\nReferences\n\nHwang, K., Dongarra, J., & Fox, G. C. (2013). Distributed and cloud computing: From parallel processing to the internet of things . Waltham, MA: Elsevier.\n\nMachanick, P. (2015). How general-purpose can a GPU be? South African Computer Journal, 57 (1), 113-117.\n\nPatterson, D. A., & Hennessy, J. L. (2017). Computer organization and design: The Hardware/software interface . Cambridge, MA: Elsevier.\n",
        "label": "human"
    },
    {
        "input": "Computerized National Film and Sound Archive of Australia Case Study\n\nTable of Contents\n 1. Introduction\n 2. Background\n 3. Objective\n 4. Strategy\n 5. Execution\n 6. Results\n 7. Conclusion\n 8. Works Cited\n\nIntroduction\n\nProgress in technology development has a significant impact on nearly every sphere of life. During recent decades, technology came to serve the task of heritage preservation through the process of digital archiving. It is applicable to both digitalized and born-digital content (Van Malsen 71). Digitalization of audiovisual collections for preservation has certain benefits compared to physical storage. Thus, digitalized content is more available, which also expands the group of users (Bressan 12).\n\nAlso, original material can be fragile or heavy, and digitalized copies help to protect the original from possible harm in case of handling. Moreover, digitalization allows sharing of audiovisual resources among institutions and empowers the creation of digital collections (Bressan 12).\n\nDifferent institutions apply a diversity of tools and strategies to preserve audiovisual materials. For example, the National Archives of Australia, which collects and preserves government records, and the National Film and Sound Archive of Australia both integrate Mediaflex as a tool for managing and preserving their assets (\u201cNational Archives of Australia\u201d). This paper analyses the case of the National Film and Sound Archive of Australia (NFSA) and its use of Mediaflex, a system that empowers the management of archived collections, both analog and digital.\n\nBackground\n\nNFSA is a \u2018living archive that possesses a collection of more than 2.3 million audiovisual resources (\u201cNational Film and Sound Archive of Australia\u201d). These resources comprise all kinds of \u201cfilm, television, radio, and recorded sound formats\u201d and the related content such as \u201cphotographs, scripts, lobby cards, costumes and memorabilia\u201d (\u201cNational Film and Sound Archive of Australia\u201d). Although the heritage of audio and video content is just more than one century old, it prevails in the contemporary media environment. Thus, audiovisual archives, in addition to collection and storage, accomplish educational, cultural, and historical functions providing access to a variety of digital resources.\n\nObjective\n\nContemporary audiovisual archives are usually concentrated on the preservation of resources for the future. This process is easier with present-day content, which is already digital. It should be mentioned that archives are significant for the contemporary digital culture because they preserve their products (Amenta 15). However, digitalization is applied to preserve fragile samples created with the use of analog technology.\n\nFor example, tape-recorded audio files are digitalized and thus make available using digital delivery (\u201cNational Film and Sound Archive of Australia\u201d). At present, the objective of NFSA is to change the legacy collection management system that has been functioning for decades in favor of a more contemporary system Mediaflex. It is explained by the fact that the first system is more oriented towards analog content, while the second one has the potential to manage both digital and analog materials effectively. This shift to a new system was expected to simplify the collection and preservation of audiovisual materials as well as their distribution.\n\nMoreover, it allows collecting diverse types of audiovisual materials and make them available to the audience. Therefore, the major goal the NFSA hoped to achieve was to apply a system that would allow managing the growing digital collection as well as meet the increasing demand for broader access to the audiovisual collection through digital tools.\n\nStrategy\n\nWith the collection exceeding two million objects, the NFSA needed an effective system to manage its assets. The materials stored by the NFSA include a diversity of works such as \u201ccommercial release documentaries, feature films, and sound recordings; websites relevant to the audiovisual industry; newsreels and broadcasts; television and radio productions of all genres including advertisements\u201d and many others (\u201cNational Film and Sound Archive of Australia\u201d). All of the mentioned objects of the audiovisual industry context are of high cultural or historical value and interest.\n\nThe shift to a new system that implied digitalization was a continuous process. As the first part of the NSFA plan, the institution started collecting born-digital materials which had no analog copies. Another aspect of this stage was the initiation of the digitalization process of the NFSA\u2019s own assets with the use of specially developed digitalization programs. This part of the plan allowed preserving the objects stored on unstable and fragile media such as tape. In the process of collection of objects in digital formats, the system applied by NFSA proved to be inefficient and unable to cope with the increasing digital collection (\u201cNational Film and Sound Archive of Australia\u201d).\n\nThus, the second stage of the plan included the introduction of alternative systems able to manage the existing digital assets. Therefore, such systems as Digital Asset Management (DAM) and Media Asset Management (MAM) were considered by NFSA, and they proved to have the necessary capabilities for managing digital resources (\u201cNational Film and Sound Archive of Australia\u201d). Still, the plan also included the collection of analog resources simultaneously with their digitalization.\n\nIt allowed creating unique databases and improving the availability of collections to anyone who is interested in audiovisual heritage. On the whole, the plan includes a gradual decrease in collecting analog items, digitalization of both existing and new analog materials, and rapid acquisition of digital audiovisual assets. However, despite the focus on digitalization, the NFSA paid much attention to preserving and taking care of the analog materials to ensure that in case their digital copies are lost, there would be an opportunity to make another copy.\n\nExecution\n\nThe execution of the plan presupposed three major options. Thus, the NFSA intended to create a new system \u201cfrom scratch\u201d which would allow managing both analog and digital resources using the tools of one system. Secondly, the NFSA preserved the functioning system that it had been using for decades to manage analog assets. Simultaneously, the institution introduced a DAM/MAM system, which empowered the management of digital objects with their following integration of both systems to make them equally functional and effective. Finally, the NFSA came to reorganizing a DAM/MAM system to manage analog as well as digital items.\n\nSpeaking in more detail, the institution had a thoughtful approach to resolving the issue of managing both analog and digital assets. The analysis of the problem proved that building an absolutely new system could be too expensive and time-consuming.\n\nTherefore, the NFSA concentrated on the other two points of the implementation plan. The study of systems available in the market led to a conclusion that some DAM/MAM products were affordable and could satisfy the need to manage both analog and digital objects (\u201cNational Film and Sound Archive of Australia\u201d). Thus, the institution made a decision to acquire a DAM/MAM system for managing their archive.\n\nAftermarket testing, the use of MAM system for digital assets, and preservation of the existing system for analog ones with their further integration proved to be too complicated. Therefore, the only possible option for the NFSA was to acquire a new system for both types of audiovisual items. A tender was opened and, after some time, its winner got an opportunity to replace the system at the NFSA. TransMedia Dynamics (TMD), the company that won the tender, suggested applying one of the MAM products, Mediaflex.\n\nRealizing the lack of the resources to digitize all-analog items of the NFSA and being aware of the necessity to preserve analog items and keep them available any time, TMD suggested developing a new design of Mediaflex for the NFSA to make it applicable for both digital and analog assets (\u201cNational Film and Sound Archive of Australia\u201d). The NFSA accepted this approach and received an effective system to collect, preserve, and handle both types of audiovisual items. The new system was suitable for managing digital items and also addressed the necessity to handle analog items, which was frequently ignored in other systems. A peculiarity of Mediaflex was its employment of both physical and intellectual description of an item.\n\nThe system applies a multi-layer approach, which resembled the one used in the NFSA for analog resources. Moreover, this feature resulted in greater flexibility of Mediaflex, which was useful for the institution due to a diversity of items that needed preservation. It was particularly important for audiovisual content that had more than one version and allowed more rational storage.\n\nResults\n\nThe implementation of Mediaflex as a system to manage archive items at the NFSA proved to be a successful solution, which led to the following results. First of all, the major outcome of the case is the acquisition of an effective system to collect, store, and manage archive audiovisual items by the NSFA. The fact that Mediaflex was adjusted to meet the needs of the NFSA made it suitable for work with all the diversity of items that were supposed to be archived (\u201cNational Film and Sound Archive of Australia\u201d).\n\nHowever, the new system was beneficial not only for the NFSA but for the country as a whole. Being a national heritage collecting institution, it acquires diverse information and has to process it. Therefore, a multi-functional system was a great contribution to the process of collecting and preserving all the significant items and allowed saving both material and human resources. Another result of Mediaflex implementation was the simplification of the loan procedure.\n\nThus, it became easier to provide the necessary materials to external clients due to more effective management of the loan process. In addition, the new system empowered the search engine provided by the NFSA. It is called Search the Collection and is similar to Google allowing the public to find the necessary materials (\u201cNational Film and Sound Archive of Australia\u201d). Thus, Mediaflex simplified the import of data about collections to the search system and made the search results more relevant.\n\nThe major positive outcome of Mediaflex implementation was the digitization of workflows. Mediaflex supports DIVArchive, a hierarchical storage management product, which was integrated into the system used by the NFSA before (\u201cNational Film and Sound Archive of Australia\u201d). The benefit of Mediaflex in this aspect is that it is applicable to all phases of the digital preservation process. These phases include extraction of the analog item from its permanent storage location, the creation of the preservation copy, and its placement in the Digital Repository. Finally, Mediaflex was adapted to provide support for \u201cmass migration methodologies\u201d (\u201cNational Film and Sound Archive of Australia\u201d). It allowed the mass transformation of analog format items such as audio or videotapes to digital files.\n\nConclusion\n\nOn the whole, the issue of digital archiving is a complex one and implies well-developed technologies. The variety of tools and instruments suitable for digitalizing audiovisual items allow archiving institutions to choose those that correspond to their primary goals. The major challenge faced by the contemporary achieves the necessity to store a continuously increasing volume of assets as well as organize digitalization of the existing items to preserve them and make them available for broad audiences.\n\nThe case of the National Film and Sound Archive of Australia is an example of a careful approach to the selection of a system able to meet the needs of the archive. Their experience proves that contemporary technology provides opportunities able to manage both analog and digital resources. Moreover, an effective system simplifies digitalizing processes as well as enhances access to the stored materials.\n\nIt is particularly important for such organizations as the NFSA because apart from its initial function to collect and preserve audiovisual materials significant for the country, they fulfill educational and cultural functions. Thus, archives need management systems that support all of those functions. The choice of the NFSA resulted in many benefits for the institution because it empowered the process of digitalization of analog items as well as organized collecting and preservation of digital assets. Therefore, the experience of the NFSA is applicable for other institutions fulfilling similar functions in archiving and preserving audiovisual items.\n\nWorks Cited\n\nAmenta, Laura. Building a Future from the Past: The Sustainability of Digital Archiving Processes in Audio-Visual Cultural Heritage Organizations. Master Thesis, Utrecht University, 2014. UU, 2014.\n\nBressan, Federica. The Preservation of Sound Archives: A Computer Science Based Approach to Quality Control . Dissertation, University of Verona, 2013. UOV, 2013.\n\n\u201cNational Archives of Australia Audiovisual Asset Management and Preservation System.\u201d TransMedia Dynamics, n.d.. Web.\n\n\u201cNational Film and Sound Archive of Australia Case Study.\u201d TransMedia Dynamics, n.d. Web.\n\nVan Malsen, Kara. Planning beyond Digitalization: Digital Preservation of Audiovisual Collections. Web.\n",
        "label": "human"
    },
    {
        "input": "Contract Law: Offer in the Acorn Computers Case Report\n\nThe following is legal advice of the Acorn Computers (A) company\u2019s legal position concerning the contractual problem with B supermarkets.\n\nIt is a general rule that when an offer is made as was done by B supermarkets, the contract becomes binding the moment an acceptance is made by the offeree. In this regard, the acceptance is deemed effective the moment it is received by the offeror. For this to hold, the offeror should specify that acceptance takes effect when it is received in writing (O\u2019Sullivan and Jonathan 8). However, in this case, B supermarkets did not give such a condition. On the contrary, they instructed the acceptance to be mailed within fourteen days. As a result, the postal rule which is an exception to the general rule applies.\n\nAccording to the postal rule, acceptance is deemed to have taken effect the moment a reply is properly posted and stamped. The postal rule supersedes the general rule whenever the reply should be communicated by post. It should be noted that the contract will still be binding even when the letter is delayed. However, it does not hold when the actions of the offeree cause the delay (O\u2019Sullivan and Jonathan 12).\n\nIn Adams v Lindsell (1818), an offer was made to sell wool by the defendant and required that acceptance be communicated through the post. The plaintiff did reply through the post in time, but the letter was delayed. The defendant sold the wool to another person assuming the plaintiff had no interest in the offer. The court held that the contract was biding since the plaintiff replied and the date of acceptance was taken to be the date when the letter was posted.\n\nIn Re London v Northern Bank (1990), it was held that acceptance takes place when a letter is posted. In this regard, the day when a letter of acceptance bearing the correct address is deposited in the post box is taken to be the day of acceptance.\n\nIt should be noted that the offeror has the authority to revoke an offer before the offer is accepted. In addition to a letter, the postal rule also recognizes telex and faxes as modes of communication (O\u2019Sullivan and Jonathan 356). However, in the case of revocation, the communication takes effect when it is received by the offeree, and not when it is posted. Moreover, if the communication arrives outside working hours or when the machine is not on, it will take effect the next working day (O\u2019Sullivan and Jonathan 357).\n\nIn Henthorn v Fraser (1892), the defendant revoked the offer before receiving acceptance from the plaintiff. The Court of Appeal held that since the acceptance had been posted before the revoke was received, the revoke was ineffective and the contract was legally binding.\n\nIt is important to note that the offer by C to B cannot revoke the initial offer of B because that is a different offer from a different entity. For an offer to qualify as a counter-offer, it must be from the offeree. In this case, A had accepted the offer by the time the revoke from B was received. Consequently, the contract is still legally binding and A can sue B for breaching the contract.\n\nWorks Cited\n\nO\u2019Sullivan, Janet and Jonathan Hilliard. The Law of Contract . Oxford: Oxford University Press, 2011. Print.\n",
        "label": "human"
    },
    {
        "input": "Allocating a Personal Computer Essay\n\nIn the fourth week, I was allocated my personal computer desk. This implied that I could personally access the computer system without relying on another machine or individual. Having demonstrated a lot of diligence, integrity and hard work over the previous week, my supervisor was evidently pleased with my work. The latter explained why he eventually trusted me to an extent of allocating a personal computer. I was now able to access the Computer Service Department (CSD) with ease. I indeed appreciate this level of trust after being an intern at the department only after a few weeks.\n\nIt is always important to be honest and trustworthy both at a personal and group level. I can confirm that the supervisor and the rest of the team at the CSD treat me as one of their colleagues and not just a temporary intern. As it stands now, I can directly coordinate my activities with both students and members of the faculty. In particular, I can assign a cal for them whenever they have specific problems that need to be tackled. Owing to the direct contact, it has also become quite easy to attend to the individual needs of students without necessarily consuming other faculty members\u2019 time. Both students and faculty members contact me for a number of problems on daily basis. Some of these problems included:\n\n  * How to format laptops and Ipads,\n  * The due process of installing software for Mac and Dell laptops,\n  * Challenges of executing wireless connections,\n  * How to compose, send and read received e-mails and\n  * Hardware0 problems associated with their laptops and personal computers.\n\nIt is prudent to mention that the above problems are just a fraction of what I handle on a regular basis. Some computing challenges presented to me by the faculty and students may range from simple to complicated problems. While some of the problems can take less than 10 minutes to fix, there are those that demand a lot of time and equally overwhelming. However, I always appreciate a difficult challenge because it creates an excellent opportunity for me to gain additional knowledge and grow as a computer expert.\n\nIn a CSD system, students and members of the faculty contact me with their problems. In order to solve their respective problems, I follow a very brief and clear procedure. To begin with, I enter the details of their systems into my desktop computer. It is then followed by the personal information of the student or faculty member. Thereafter, a page known as a \u201ccall\u201d is printed. The printout is stuck on the machine that needs to be diagnosed for problems.\n\nThe Computer Service Department (CSD) also performs a number of other vital roles to assist students and staff. For example, there is an online helpdesk system that can be used by individuals who do not necessarily need to pay a visit to my physical desk. The online help is designed to provide assistance for the daily\n\nproduct tasks. Individuals who need help can view online help information for their items on the current product page. In addition, the online helpdesk system is also well established with a telephone contact information for technical assistance and customer services. The CA SDM web interface allows the user to navigate and access product functionality. Customers who make a call can do so either as incidental or request calls.\n\nIncidental calls are usually made outside the normal working hours. Although clients should be attended to all the time, incidental calls largely disrupt the normal operational processes of an organisation. If a ticket is created as a copy of another ticket, the status field displays all status values. It is also important to note that depending on your role, you do not have access to all the functionality described in this section.\n\nFor instance, some predefined roles can edit records but cannot create new ones. All the reports made to the helpdesk are referred to as requests. The latter term is also used to describe all the activities or procedures performed on the reports at the helpdesk. I also have a telephone device on my desk that I can use to receive calls from students and faculty and consequently offer the necessary assistance to them. If there is a media problem, I assign a call for them and transfer it to another media service that they can refer to and eventually solve their problems. All of the above details are executed at the first level.\n\nIn the fifth week, I was working with both the first and second levels. From my experience, I noticed that the first level was a lot easier. However, the second level entailed complex activities such as constructing images for the Operating System. It was part of my role to work alongside with them to carry out the activity. I was supposed to change the computer name of a certain laptop, a challenge that was not easy at all. Hence, I was compelled to use another domain in order to change the name. Finally, I can confirm that I like the work environment owing to a very friendly team of supportive colleagues.\n",
        "label": "human"
    },
    {
        "input": "Computer and Internet Security Notions Essay (Literature Review)\n\nChapter 1. Network Security Basics\n\nThis chapter provides an overview of some of the general computer and Internet security notions; it also explains how a security plan can be created for an institution or an enterprise (\u201cChapter 1\u201d 2).\n\nFirst, some basic concepts and terms related to e-security are defined. Then, the problems of security access are discussed; after scrutinizing a number of possible threats and safeguards for information, it is explained that creating a proper physical environment that would allow the data to be safe from physical intrusion is of crucial importance if it is to be protected. Next, the issues related to virtual intrusions are considered; these include the prevention of compromise of data occurring due to carefulness or a mistake, deliberate internal breaches of security, and external intrusions carried out by unauthorized individuals.\n\nAfter that, the chapter supplies an overview of threats to network security and the ways to identify them; in particular, the classification of various kinds of attacks is supplied. Finally, some important advice on creating a thorough security plan is given. It is then summarized that it is useful to be able to understand the motivations of hackers and see the vulnerable surfaces of the organization\u2019s informational system; in addition, the awareness of possible ways of attack is also of crucial importance. It is also stressed that, in order to safeguard an informational system, a high-quality security plan should be created and implemented.\n\nThe chapter provides an easy and effective introduction to network security issues that numerous organizations are forced to face. It is important to point out that some most common mistakes related to data security are discussed. For instance, it is explained that hackers may operate in an undercover way, working for an organization as cleaners, etc., which allows them to access computers when the employees are not at work, and not to raise any suspicions; it is also elaborated that in many cases the compromise of data occurs accidentally, due to carelessness or a mistake, which means that the personnel of a company should be provided with proper instruction regarding the data safety.\n\nSupplying a simple and effective explanation of the basic issues related to network security, this text will be of use to any people who are not professional data security specialists but have a degree of responsibility for their organization\u2019s informational system.\n\nChapter 4. AMI Security Requirements\n\nThe Chapter 4 of Smart Grid Cyber Security Strategy and Requirements provides a detailed account of security requirements that are concerned with Advanced Metering Infrastructures (AMI). It supplies a description of AMI requirements that are modified from the Catalog of Department of Homeland Security so that they can be used for AMI security (\u201cChapter 4\u201d 56).\n\nThe AMI requirements include a number of components. First, the security requirements regarding the cyber protection that are necessary for managing the system and communication protection, which consist of actions done in order to safeguard the components of AMI, as well as communication links between the elements of the system, from a malicious cyber intrusion, are explained. Second, the issues related to the management of data and documents are discussed; it is explained that information stored both on hard-copy and digital data carriers and related to AMI is crucial for the proper protection of the AMI elements. Third, the conditions for system development and maintenance are described; it is emphasized that organizations are required to comply with the requirements of the federal laws, policies, and directives, as well as with various standards existing in the country. Fourth, the chapter provides an explanation of the means that are needed in order to provide a proper response to various incidents; the organizations are obliged to create, spread, review and update documents that regulate both the general policy and the methods utilized in case of such incidents.\n\nFifth, the problems concerned with the integrity of the information stored in AMI are discussed. It is stressed that the maintenance of AMI systems should be carried out in a way that allows to maximize the safety of information stored there, i.e. to protect this information from being deleted or modified by unauthorized and/or undetected users. The controls aimed at such protection are provided. Sixth, the issue of access control is addressed. It is highlighted that making sure that only the authorized persons have access to the resources in the AMI system is of crucial importance.\n\nThe possible means of access control include passwords, biometrics, as well as cryptographic tokens. Seventh, the problems related to the audit and accountability of the AMI elements are examined. It is pointed out that both logging and audit require being carried out in a regular manner so as to ensure that the system is adequately protected, and that all the security mechanisms are working properly. It is essential to discover breaches in security and its weak sides if the AMI system is to be safe from threats.\n\nIt should be stressed that the fulfillment of the requirements provided in the chapter is aimed at allowing for safe and efficient use of the AMI systems. Therefore, it is of crucial importance for organizations to comply with these requirements in order to be able to both achieve the organizational goals and satisfy their customers.\n\nNetwork Security: History, Importance, and Future\n\nThe article by Daya is concerned with the issue of the security of data networks, mainly the Internet and networks with the access to the Internet (1). The author examines the problem of network security by analyzing the history of security in networks, the architecture of the Web and its components that can be harmed easily, the kinds of network attacks and means to prevent them, the methods to provide security for different types of networks that have an access to the Internet, as well as the latest developments (both software and hardware) in the security sphere. In order to perform the study, the author conducted a literature review.\n\nIt is stated that in order to provide the security of networks, it is essential to consider a number of factors, namely, access (only the authorized individuals should be able to access the network), confidentiality (the crucial information should not be available to other users), authentication (the users of the network must be confirmed, so that other users might not act using their name), integrity (messages must reach their destination without modifications), and non-repudiation (the users mustn\u2019t deny using the network). It is also important to take into account the possible aims of an attack, which might include wasting resources, interfering with the functions of systems related to resources, or gaining information that might be used in the future attacks.\n\nThe author states that the interest in the Internet security began with a crime committed by Kevin Mitnick in 1995 that resulted in a major financial loss. Since the 1990s, when the Internet became public, major studies were carried out in the field. Further, the author explains that the vulnerable aspects of the Internet are associated with the Internet Protocol Suite (IPS); to protect different levels of IPS, security mechanisms were introduced, such as a security architecture for Internet protocols (IP).\n\nThe author also examines the peculiarities of two versions of IPs, namely, IPv4 and IPv6, as well as the most common attack methods on these IPs, which include eavesdropping, viruses, worms, Trojans, phishing, denial of service, and IP spoofing. After that, an overview of current mechanisms of network security is conducted; it is stated that the hardware developments are rather limited (they are comprised mainly of biometric systems and smart cards), whereas the software developments are much more significant and numerous, including anti-viruses and firewalls.\n\nIt should be stated that the article provides an overview of some of the most important issues related to the Internet security. On the other hand, the author does not go into much detail in the article. It is also important to point out that some of the sources used in the paper are not scholarly. Summing up, it might be said that the article can be useful for those who need some rather general information on the topic of the Internet security, but might be of little help to professionals in the field.\n\nImpact of Network Infrastructure Parameters to the Effectiveness of Cyber Attacks against Industrial Control Systems\n\nThe article by Genge, Siaterlis, and Hohenadel is concerned with the topic of cyber attacks on information and communication technologies systems (ICT) (in particular, on networked industrial control systems (NICS)) used by modern critical infrastructures (CI) (675-675). The paper scrutinizes the manner in which cyber systems are able to influence the physical world by performing an experimental attack on a CI.\n\nIn their study, the scholars employed the Boiling Water Power Plant (BWPP) model as an example of a physical process occurring in a CI. The researchers supposed that bringing the process to a critical state (critical steam pressure) can possibly cause physical damage to the CI. The researchers limited their attack time to 10 minutes because after the start of the attack the personnel of a real CI might notice the significant deviations caused by the attack, and switch the facility off or disconnect it in order to prevent the attack from continuing.\n\nThe control of the BWPP model, an oil-fired electric power plant, was exercised by using three valves: stream valve, fuel valve, and feed-water valve; the process was monitored with the sensors of water level, steam pressure, and created electricity. The attackers used one of the stations located inside the installation\u2019s internal network as their \u201cbase of operations,\u201d for it is known that such stations can easily be compromised remotely from the Internet.\n\nThe remoteness, on the other hand, means additional complications for the attacker due to packet losses, network delays, etc. The \u201cscholarly hackers\u201d attempted to keep the feed-water and the steam valves closed in order to cause the extra pressure (attacker\u2019s valve position, AVP). The experimental attacks demonstrated that the hacker can affect the normal valve positions for all the three valves if the programmable logic controllers use the task scheduling frequency of 100 ms, i.e. sent a single regular Modbus packet 100 times each second.\n\nExternal factors such as network delays, as well as background traffic, significantly affected the potency of the attacks, but it was still possible to create large deviations from the normal valve position, and thus affect the steam pressure in the facility. However, the researchers found out that two key parameters, namely, the control code task scheduling, and the speed of the control valves, were able to significantly enhance the resilience of the physical process under attack.\n\nAn important achievement of the research is that it examined the influence of cyber systems on the physical world and discovered the ways to increase the resilience of these processes in a CI setting. The scholars performed 540 experimental attacks over the span of nine hours, which allows to conclude that the experimental results should be statistically reliable.\n\nCyber Security Challenges in Heterogeneous ICT Infrastructures of Smart Grids\n\nThe article written by Skopik and Langer is concerned with the subject of security of information technologies used in the management of \u201csmart\u201d power grids (463-464). The authors state that in the future, it will be necessary to employ additional sources of energy, not only on the regional or national level but also on-site. In addition, more environmentally friendly sources of energy (such as biological, wind, or solar power grids) are much less reliable than the traditional sources (e.g., nuclear or fossil fuel power plants) when it comes to the stability of power supply. Therefore, it is important to use modern information technologies to manage the environmentally friendly sources in order to provide stable power supply. This opens new surfaces that could be attacked by malefactors, and creates the need for additional security systems.\n\nThe authors provide an overview of different information and communication technologies (ICT) structures employed in smart grids, examine the main scenarios of cyber attacks on them, and review main security challenges that emerge. The methodology employed in order to carry out the research is not discussed, but it appears justified to state that the authors performed a literature review in order to gather the necessary data.\n\nThe authors explain that the structure of smart grids consists of three main components: grid stakeholders, the physical components of the grid (factories and other facilities), and communication between them (ICT systems; physically, it is realized by power line carriers or fiber optics). The authors concentrate on the physical network between the stakeholders and the grid, for these connections may be vulnerable to an attack.\n\nFurther, the researchers supply information about the possible scenarios that might be employed in order to attack grids; they argue that most attacks will be launched against the metering infrastructure of the grid, because this infrastructure is scattered among numerous users, which makes it an easy target. It is also asserted that today these attacks mainly focus on gathering and maliciously using the metering data, or on initiating a dysfunction and denial of service. Other possible attacks may include attacks on distributed energy resources, e-mobility (i.e., on grid load management), and on transmission grids. Finally, the authors provide a brief overview of the currently existing cyber security standards.\n\nIt is important to point out that the research deals with a problem that is going to be of crucial importance in the future, and the article provides significant grounds that can be employed to better safeguard not only the sources of the energy supply but also the consumers of energy. It is stressed that the future research is required in order to test the discussed scenarios of attack and find the most effective strategies for dealing with them.\n\nWorks Cited\n\n\u201cChapter 1. Network Security Basics.\u201d ISA Server 2006 Migration Guide . Tom Shinder. Burlington, MA: Syngress, 2007. 1-45. Print.\n\n\u201cChapter 4. AMI Security Requirements.\u201d Smart Grid Cyber Security Strategy and Requirements: Draft . Ed. Annabelle Lee and Tanya Brewer. Gaithersburg, MD: National Institute of Standards and Technology, 2009. 56-119. Print.\n\nDaya, Bhavya. Network Security: History, Importance, and Future . n.d. Web.\n\nGenge, B., C. Siaterlis, and M. Hohenadel. \u201cImpact of Network Infrastructure Parameters to the Effectiveness of Cyber Attacks against Industrial Control Systems.\u201d International Journal of Computers Communications & Control 7.4 (2012): 674-687. Print.\n\nSkopik, Florian, and Lucie Langer. \u201cCyber Security Challenges in Heterogeneous ICT Infrastructures of Smart Grids.\u201d Journal of Communications 8.8 (2013): 463-472. Print.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Systems Effective Implementation Essay (Critical Writing)\n\nTable of Contents\n 1. Reasons for Selecting this Reading\n 2. The Main Ideas\n 3. My Opinion and Critique\n 4. The relevance of Reading to Course Content\n 5. The Role of the Business Analyst According to the Reading\n 6. Implications of the Reading\n 7. References\n\nReasons for Selecting this Reading\n\nETHICS is an acronym that stands for Effective Technical and Human Implementation of Computer-based Systems (ETHICS). It is the overall methodology of the design process guiding the development of Information Systems in Organizations. Under this methodology, there is a provision that gives attention to the needs of the people involved in the organization about the demands of the technology. The idea behind the methodology of ETHICS was developed by Enid Mumford. The reading is more about the practical adaptation of the ETHICS methodology as used in the development and design of an information technology (IT) system but limited to an academic setting.\n\nIt gives the main factors that contribute to the effectiveness of the new technology. It emphasizes the compatibility of the system with the social and organizational factors that exist in the organization. It also explains the main areas of success of a system and how they can be measured by the ability of the system to improve the quality of working procedures and enhance job satisfaction of the system users (Adman & Warren, 2000).\n\nThe ETHICS methodology features the idea of participation and a socio-technical approach. This is the involvement of the direct and indirect users of the system while making all involved persons satisfied. It also aims at achieving technological efficiency. It is very important to have these people on board while in the decision-making process concerning the design, functions, and operation of the particular system.\n\nParticipation is a very important factor in many methodologies, and in this reading, it is described as a vital frontier while developing and designing an Information system in ETHICS methodology (Hirschheim, 1985). It also ensures the satisfaction of all players using the sociotechnical approach. This reading is the best choice for future students as it is a guideline for the adaptation of this methodology. It helps students learn the various aims of ETHICS.\n\nThey will understand where the emphasis lies in ETHICS and learn the practical way of applying the participative design and how to balance the social and technical aspects of the system in the design and development process. It is also a very important reading which can equip decision-makers in an organization with the necessary steps in developing an Information system that will be both effective and successful in improving efficiency. Future students will have acquired knowledge on important steps in Best practices of Information Systems Development thus training them to be good developers who will develop practically effective and successful systems that will benefit the organizations and improve efficiency (Adman &Warren, 2000).\n\nThe Main Ideas\n\nThe main ideas presented in the reading can be highlighted briefly and be compared to the main ideas that are presented in class. In the reading, we get to understand the principles of participative sociotechnical methodology in the design of Information Systems about the organization\u2019s structure and requirements. Using the research case study also examines how the ETHICS methodology is important but not used in the current Information Systems structures.\n\nThe reading highlights the two strands of methodologies that are commonly used. We learn that in the sociotechnical approach the main objective of the system is to make work more satisfying for the involved individuals while maintaining a high level of technical efficiency. In participation, sharing information is the basis backed by discussions of alternatives choices and methods and making choices from a broad perspective when all players have contributed to the input. It also describes the logical steps and framework of ETHICS and how it is implemented (Adman & Warren, 2000).\n\nIt compares ETHICS to QUICK ETHICS to determine the suitability of both methodologies. The reading takes us through the stages of the research through the final stage. It looks at designing, structure development, process, and implementation, and evaluation. All these lead us to the purpose of the study which is to test the principles, tools, and techniques applied in the participative, sociotechnical design in a real problem situation through ETHICS methodology.\n\nThe reading then looks at the weaknesses of both theories and analyses how they are both important in being used in the development of Information Systems. It finally gives credit to the ETHICS methodology. This is a prototype of what is covered in class. There is no much difference except for the basis on which the information is presented. The research was based in the University whereas it would also be based on an organization\u2019s point of view.\n\nMy Opinion and Critique\n\nThe reading covers the ideas that are important in the topic ETHICS, its importance in Systems development, and the framework of its application. However, the basis of challenges is based only on the research at the University. This setting does not present solid challenges for organizations. A good set of research would have exploited many possibilities of challenges and harder challenges which would have given a clear perspective of an organization. The reading is also centered on the methodology of research and does not expound more on the methodology of Ethics. It is research-oriented other than information-oriented thus findings are likely to leave out many more challenges.\n\nThe relevance of Reading to Course Content\n\nThe reading is relevant to the course content and it covers topics such as ETHICS and QUICKETHICS, the importance of ethics, and the importance of information systems and philosophy of Ethics. This reading helps a student know the requirements when developing an Information System, the good practices of Systems Development, and the challenges. This reading supports the course content and gives more insight into factors that students should consider to excel in the systems development field. They may be able to design systems that are efficient and successful when implemented.\n\nThe Role of the Business Analyst According to the Reading\n\nBusiness analysts are the backbone of the organization\u2019s operations. They should strive to ensure the best practices are observed in the development of Information Systems. This will ensure the final product embraces the functionalities required and is compatible with the staff procedures. The role of a business analyst is therefore mandated to study the organization\u2019s system requirements and map out strategies of ensuring the process involves all the concerned parties.\n\nThe process of designing and development should also include the views of all the people since they will be the users of the system. This will contribute to efficiency and success in the implementation of the system to the benefit of the organization. The reading gives a plan of how the business analyst can embark on the task that will ensure success and efficiency in the organization.\n\nImplications of the Reading\n\nBusiness analysts need to read and analyze the findings in the paper. They must learn the approach that should be followed and what organizations require. They must align both of these features. They must develop a plan and policy for an organization\u2019s development of Information Systems and implementation. The policy should guide the process so that it can have all the recommendations in the reading that will lead to the development of an efficient system.\n\nFuture students can learn the important aspects of Information System Development and apply the learned knowledge on the theories of ETHICS and its benefits when applied to system development. This will enable them to develop efficient systems and also be able to become good business analysts or system developers. This will ensure that they positively influence decision-making for the better of the organization\u2019s performance.\n\nReferences\n\nAdman, P., & Warren, L. (2000). Participatory socio-technical design of organizations and information systems \u2013 an adaptation of ETHICS Methodology. Journal of Information Technology 4(15), 39\u201351.\n\nHirschheim, R. (1985). Information systems epistemology: a historical perspective. Research Methods in Information Systems 3(3), pp. 13\u201336.\n",
        "label": "human"
    },
    {
        "input": "Graphical Communication and Computer Modeling Essay (Critical Writing)\n\nTable of Contents\n 1. Aims of the module\n 2. The learning outcomes\n 3. Course content\n 4. Modes of assessment\n 5. Teaching styles\n 6. Available resources\n 7. Number of students in each class\n 8. Coverage of the course content\n\nAims of the module\n\nThe course aims to equip the students with the knowledge of computer applications in Graphic Communication and Design.\n\nThe learning outcomes\n\n  * At the end of the course, the student should be able to produce orthographic drawings through geometric construction techniques and relate this to architectural settings.\n  * The student should have developed the ability to create 3D Computer-Aided surface and solid models and produce Computer-Aided drawings and images from such models.\n  * The student should have understood the design principles and their relevance in professional graphic presentations.\n  * The student should have learned the functions of desktop publishing in Graphics.\n\nCourse content\n\nThe course contains the following areas of study; Technical Graphics, Computer-Aided 3D Modeling, Visualization and Presentation, and Computer-Aided Graphic Presentation\n\nModes of assessment\n\nAssessments consisted of Continuous Assessment Tests (10%), Practicals/Projects (40%), and the end of Module exam (50%). The project required a group presentation after completion.\n\nTeaching styles\n\nThe lecturer applied a practical approach to tackling the course.\n\nAvailable resources\n\nThere were enough resources for use in this course. The materials that were available in the college\u2019s physical library were adequate and proportional to the student population. The lecturer also provided materials based on recent developments in computer modeling. The students also used online libraries and academic databases that were relevant.\n\nNumber of students in each class\n\nThe population of students in this class was quite manageable. The class had 80 students that were registered. This class was managed by one lecturer.\n\nCoverage of the course content\n\nThe course content was covered as per the schedule. The continuous assessment tests and practical assessments were carried out as per the schedule and the students had enough time to prepare for the final exam.\n\nTable 1.1: Module Planning for Personal Development.\n\nFactor                     Observation                                                                                                                                                                   Importance  Comments on the factors                                                                                                                                              \ndescription                and reflection                                                                                                                                                                                                                                                                                                                                                 \nNature of the subject      The subject is quite practical and needs the hands-on participation of the students. Students were required to think creatively in developing some designs.                   7           This is a practical course and the concepts may vary depending on a particular context. The students need to be creative on their own to solve some arising problems.\nLayout and constraints     The class was organized into groups for the purposes of discussion and group assignments.                                                                                     6           The classroom should have little constraints and the students should be free to consult each other                                                                   \nin the room                                                                                                                                                                                                                                                                                                                                                                               \nThe student number         The student population was quite big but could be managed by a single lecturer.                                                                                               7           The student population should be reduced to a more manageable.                                                                                                       \nLecturer\u2019s teaching style  The lecturer gave a practical hands-on approach. Individual students were given attention and trained on eye and hand movements in the graphic drawing by a computer medium.  9           The course is more practical and the use of demonstrations was necessary. The students are expected to develop individual skills.                                    \nLearning materials         The learning materials were adequate and relevant. The student population did not exert much pressure on learning materials.                                                  8           Materials should be available to facilitate learning.                                                                                                                \nTime management            Time management was good.                                                                                                                                                     6           Both the instructor and the students should observe time management.                                                                                                 \n\n\nTable 1.2: Assignment/Project Planning for Personal Development.\n\nFactor                                Observation                                                                                                                                             Importance  Comments on the factors                                                                                                                                                          \ndescription                           and reflection                                                                                                                                                                                                                                                                                                                                       \nExam scheduling                       The exams and projects had been scheduled at the beginning of the module. The final exam was timed and took three hours as it involved detailed         8           There needs to be proper scheduling of all the assessments at the beginning of a module. The lecturers should follow the schedule.                                               \nand timing                            tasks.                                                                                                                                                                                                                                                                                                                                               \nExam environment                      The CAT and projects were performed in small groups. The final exam was carried out in a supervised environment and the students worked independently.  6           The students should be closely supervised during the final exams to evaluate them effectively.                                                                                   \nInstructions and guidelines provided  Sufficient guidelines were provided for the projects and there were proper instructions on the written exams.                                           7           There is a need to provide guidelines and instructions to be used in the assessments. Wrong or unclear guidelines will mislead the students and do not provide a good evaluation.\nExam standard                         The CAT and final exams reflected the course contents. Projects required the students\u2019 own reflection of the concepts.                                  7           The exam should be relevant to the requirement of the course and should be proportional to the allocated time.                                                                   \nAssessment materials                  There were sufficient materials for the projects/practical and during the final exam.                                                                   7           The materials for the exam should be adequate to enhance the evaluation of all the students.                                                                                     \nLecturer\u2019s evaluation                 The lecturer was not biased in grading the students.                                                                                                    8           The lecturer should be objective in awarding marks to the student,                                                                                                               \n",
        "label": "human"
    },
    {
        "input": "Internship in the Computer Service Department Report\n\nTable of Contents\n 1. Work plan status\n 2. Key achievements\n 3. Problems and solutions\n 4. Potential problems and solutions\n 5. Lessons learned\n 6. Organization and culture\n 7. Miscellaneous\n\nWork plan status\n\nI think that I am on track in relation to what I have done in the last two weeks. I have learned quite a lot with regard to my internship in the Computer Service Department, which is headed by very competent supervisors. In fact, I know that I am on track because I have been assessed by the leaders in the facility with the aim of establishing whether I have gained the required skills and knowledge. In addition, at this point, I can compare my achievements with my goals that were set to be accomplished within the fortnight.\n\nKey achievements\n\nIn the last two weeks, I have achieved a lot vis-a-vis gaining practical skills and knowledge. At the start of the level 2 internship training, I began creating an image using Windows. Although it was my first time to build an image using a computer application, I built it with a lot of ease, implying that I rarely needed my supervisor\u2019s assistance. However, whenever I encountered problems in making sense of the steps outlined in the CDs and DVDs provided, I could request my supervisor to guide me to understand them.\n\nIn fact, the supervisor and other personnel in the department were very willing to assist students, implying that they were committed to effective training. Due to the fact that the level 2 of the department was involved in formatting and preparing laptops for new students, I was able to learn the essential aspects in relation to formatting laptops typified by relatively high levels of new technologies. In addition, I gained critical knowledge vis-a-vis fixing problems associated with videos.\n\nToward the end of the first week, I was taught how to prepare a sheet that was important in formatting a Dell laptop. At the start of the second week, I started working at the level 3 in the department, which was offering computer-networking services. I achieved a lot when I interacted with the university\u2019s network architect, who provided a presentation that contained networking information.\n\nAfter the presentation, I was exposed to practical sessions that involved types of network, types of cables, wireless connections, components of data centers, firewalls, and servers. Personnel could be called to solve problems and I could accompany them, which enabled me to learn a lot. Some of the problems that I learned to fix were related to access points, wireless connections, office telephone connectivity, and printer-computer networks. In fact, I could be given a chance to assess the source of a problem and apply the best solution that could result in improvement of the performances of different devices.\n\nProblems and solutions\n\nWithin the fortnight, it is important to state that I encountered some issues that negatively impacted my learning. In the first week, I could hardly follow instructions in the learning resources provided, which were CDs and DVDs. They contained critical documentation that was aimed at helping learners to understand concepts involved in creating images using applications based on Windows platforms. I solved the issue by asking my supervisor to guide me.\n\nSecond, it took me some time to know all the workers in the department, which affected me due to the fact that I could hardly interact with them at the beginning. I found that it was important to interact with all the staff in order to solve the problem. Finally, it took me a relatively long period to adapt to the working environment in the level 3 because of changing from the level 2 after one week. I solved this by increasing the level of interaction with my supervisor and other workers.\n\nPotential problems and solutions\n\nIn the future, I would anticipate two problems. First, I think that I would experience a problem associated with implementing software documentation. A perfect solution to the issue would be to request my supervisor to explain to me the content in learning resources. Second, I would face the challenge of limited time within a training level of my internship. This would be solved by suggesting that I spend more time within a section of a department.\n\nLessons learned\n\nI learned a lot in the fortnight of my internship within the department. From a general point of view, everything went well, but for the few aforementioned problems. I learned important aspects, such as formatting laptops, creating computer images, implementing software documentation, fixing network problems, among others. In the future, I think that I can improve my speed of fixing computer network issues.\n\nOrganization and culture\n\nPersonnel who aimed at producing excellent performance outcomes typified the work environment. Effective communication was used from the top to the bottom and vice-versa. I also learned that the department had supervisors who considered views of their juniors.\n\nMiscellaneous\n\nIn conclusion, the fortnight was one of my best learning periods. I established professional networks that would go a long way in helping me in my career in the future. Given another chance, I would go to the department to continue learning various concepts.\n",
        "label": "human"
    },
    {
        "input": "Technical Requirements for Director Computer Work Case Study\n\nTable of Contents\n 1. Recommended Computer\n 2. Hardware Devices\n 3. System Unit Components\n 4. Productivity Software\n 5. System Software\n 6. Internet Connectivity & Web Services\n 7. Reference List\n\nAfter a thorough examination of the director\u2019s requirements, a list of recommended articles has been prepared. I suggest a laptop that is already equipped with a large memory card and a camera so that there will be no need for extra expenses. The laptop has Windows 10 office that fits most of the requirements connected with documents\u2019 converting and operating. It is, nevertheless, recommended to install several programs, such as McAfee, Foxit Phantom PDF, and a video editor to meet the requirements of security, files converting and video editing respectively. It is advised to purchase several hardware items, such as a printer and a Bluetooth mouse to enable one to scan, transfer and print documents.\n\nThe extra security matter is suggested to be solved with the help of a surge suppressor that will prevent the system from unwanted breakdowns. GTT as a broadcast provider has been selected on the basis of the price and quality reviews\u2019 monitoring. The articles proposed are represented in the tables below.\n\nRecommended Computer\n\nItem #  Group  Device                                                                                                                                                                        Requirement                                                                                                                                 \n               laptop Latitude 14 5000 Series (E5450)                                                                                                                                                                                                                                                                                    \n               operating System: Windows 10 Pro (it includes Word Outlook that is particularly efficient for schedule handling)                                                                                                                                                                                                          \n               memory: 4GB                                                                                                                                                                   create documents, spreadsheets, presentations, and send and receive email; take high-quality digital photos and video;                      \n1       \u2013      hard drive: 500GB                                                                                                                                                             participate in online video conferences, web courses, and forums; manage the Director\u2019s schedule by using an online calendar and day planner\n               display:14.0\u201d HD (1366\u00d7768)                                                                                                                                                                                                                                                                                               \n               battery life: up to 14.5 hours                                                                                                                                                                                                                                                                                            \n               extra: the laptop has a webcam; it is said to be worked out particularly for business managing and designed to fit all the professional needs ( Smart. Sleek. Secure , 2015)                                                                                                                                              \n\n\nHardware Devices\n\nItem #  Group   Device                                                                                                                           Requirement                                                                                                                                         \n1       output  8 GB USB                                                                                                                         create small databases to manage all audio, video, and photo data                                                                                   \n2       output  Bluetooth Mouse-WM615                                                                                                            transfer information (audio, video, and photos) between PC machines                                                                                 \n                extra: both facilitates the work of those who prefers a mouse to a touchpad and provides an easy access to the Bluetooth option                                                                                                                                                      \n                Colour Laser Printer (Dell)                                                                                                                                                                                                                                                          \n3       output  function type: fax, scan and copy                                                                                                print documents to include photo quality color printing                                                                                             \n                extra: Wi-fi, mobile print, connectivity for USB                                                                                                                                                                                                                                     \n4       output  surge suppressor                                                                                                                 protect the PC and all components from dirty electrical power including under voltage (brownout or blackout) and over voltage (power surge or spike)\n\n\nSystem Unit Components\n\nItem #  Group  Device                Requirement                                                                  \n1       input  adapter card (modem)  connect all required peripheral devices you deem necessary to the system unit\n\n\nProductivity Software\n\nItem #  Group     Device                    Requirement                                            \n1       integral  OneTeam messaging client  instant messaging, and in chat rooms via mobile devices\n\n\nSystem Software\n\nItem #  Group     Device                                               Requirement                                                                                                                                                                                                                                                                                    \n1       integral  McAfee (McAfee Live Safe and McAfee Security Scan)   manage and protect the system, data, and information while working on the Internet including scanning all incoming emails, email attachments, and files downloaded from Web-based sources; firewall, virus and spyware protection; and checking websites for phishing and fraudulent activities\n2       integral  Foxit Phantom PDF                                    manipulate pdf files, including editing pdfs, conversion from pdf to Word and vice versa;                                                                                                                                                                                                      \n3       integral  VSDC video editor (also applicable for audio files)  create and edit audio and video files and share video and audio files via emails                                                                                                                                                                                                               \n\n\nInternet Connectivity & Web Services\n\nItem #  Group     Device                                                                                                                                                         Requirement                                                                                                                                               \n1       integral  Broadband Remote Access Server (GTT provider) \u2013 its installation and employment imply simple handling so that no external support is required ( iFast , 2015)  utilize the internet to make online purchases, conduct banking services, and research new audio, video and photo editing methods using a broadband service\n\n\nReference List\n\nSmart. Sleek. Secure. (2015). Web.\n\niFast . (2015). Web.\n",
        "label": "human"
    },
    {
        "input": "The Popularity of Esports Among Computer Gamers Essay\n\nIntroduction\n\nPreviously, passionate video gamers were criticized, while their hobby was regarded as a mere waste of time. However, the growing recognition and popularity of cybersports prove that the situation has changed.\n\nE-sports or cybersports\n\nE-sports or cybersports are the new terms that can sound odd to the men in the street but are well-known in the environment of video gamers. These terms are used to describe the competitions in playing video games.\n\nLike in any other kind of sport, the E-sports competitions are held on amateur, semi-professional and professional levels. The tournaments and competitions are broadcasted on television and via the Internet, and the winners are awarded substantial money prizes. The Video Games League fosters further development of cybersports and encourages everyone fond of playing games to take part in the competitions. Such examples as GSL E-Sports League in Korea awarding the winner with $ 500, 000 provides gamers with strong motivation for developing new strategies, improving their skills, and participating in the tournaments.\n\nSOGC LAN \u2013 Iron Gamer is the gaming event that will take place on April 30 th in Sydney. It is a two-day tournament that will start at 10.00 am on April 30 th and will finish at 10.00 pm on May 1 st. The Iron Gamer Challenge tournament is a significant event in e-sports of New South Wales. The organizers of the tournament invite the best gamers all over the region to form small squads of no more than 3 members to test their strengths against other warriors or just grieve for the success of the favorite team attending the tournament as a spectator.\n\nApart from winning the title of Iron Gamer which is undoubtedly honorable and desirable, the winning team will receive a cash prize of $ 600 and a lot of other goodies. Moreover, this victory will allow the squad to participate in the National Iron Gamer competition which is planned for November.\n\nWhile Australian e-sport is only at its starting point, the Korean gaming league has remarkable achievements. Korean attitude to cybersport is similar to that of Japanese to baseball. They have an e-sports channel on television, and the largest companies, such as Samsung, SK Telecom, and even the air force have their teams for these tournaments. Ken, a Japanese pro-gamer noted: \u201cVideo games are a part of our future. If you are embarrassed by being a gamer because your friends or family assure you that video games are a mere waste of time, you should bear in mind that till you have enough moral strength to do what you like, the things which you are passionate about are not a waste of time. Moreover, gamers can make a lot of money by participating in tournaments. Currently, I make up to 300, 000 Japanese yen from e-sports competitions. I recognize that my career as a professional gamer has its expiry date, but now I spend 3-5 hours a day playing games and I am not going to give up it.\u201d\n\nYou may join the local event the Iron Gamer and see how far it can go. Maybe, it will allow you to win the title, participate in national competitions, and become the beginning of your professional career in e-sports.\n",
        "label": "human"
    },
    {
        "input": "Computer Games and Instruction Essay\n\nPlaying computer games has always been the thing for all the people who were born in the XX-XXI century; and even though there might be someone who knows nothing about the true delight of playing a PC/video game, most people within that 50-year sphere surrounding video/computer games industry know what I am talking about. And, even though I have never been the one for spending nights to win a virtual prize, I was pretty surprised to find myself actually being captured into the virtual reality of one of the most famous computer games in entre world \u2013 the famous The Sims saga. Even though it is only now that I can interpret the effects of playing the game for hours, The Sims 3 has definitely proven its value as a great way to train some important skills.\n\nI must admit that in my own sandbox, the world seemed incredibly colorful and attractive; filled with adventures and a real exercise for the brain, it had the same effect as a good movie has on the spectators \u2013 it is truly impossible to leave the movie theater until you finally see the climax. Even though the designs of the people in the game left much to be desired and most of the characters looked like the Pixar movie rejects, it was still a lot of fun to create my own world and playing God.\n\nHowever, I soon realized that my new \u201cpets\u201d needed a lot of attention. I was spending hours trying to create a character and help it build its career so hard that I had completely forgotten about mine.\n\nTo fight the given problem, I started spending more time in the \u201csandbox,\u201d which was actually the last thing to do \u2013 as a result, I desperately wanted to sleep, but I finally got the Maid\u2019s character right. Moreover, I started looking for online cheats, which took all the fun out of the game.\n\nNevertheless, it would be unfair to claim that in my own sandbox, there were only unpleasant things to dig up. To my great surprise, I soon realized that The Sims taught me quite a bit about strategic thinking. In a couple of weeks, I understood that I could plan my actions and predict their consequences better. In addition, I felt more self-assured; indeed, being a god of one\u2019s own universe does add some self-esteem.\n\nHowever, just when I started to feel bored with all those characters that I was in charge of, I found out that the game was finished. There was nothing else to do with The Sims . Indeed, according to what Forbus says, the functions of the self-proclaimed \u201cgod\u201d in The Sims are quite restricted:\n\nEach Sim contains behavior calls for each of the possible social interactions (flirt, kiss, etc). When SimSam decides to run the kiss behavior (which is in SimMary) an invisible social interaction object is created. The execution of SimSam\u2019s thread is then passed into this object (as is SimMary\u2019s if she\u2019s not busy). (Access to the parameters of the chosen Sim is provided through a pointer in the Social Interaction object.). (2)\n\nWith a toy that simple and that time-consuming, it was rather weird that this game took over me completely. Anyway, it was a narrow escape for me \u2013 even though the threats of game addiction are considered to be greatly exaggerated, as Vorderer and Bryant explained (Vorderer and Bryant, 332), I would not want to test how addictive they are.\n\nThus, like all good things, my adventure with The Sims finally came to an end. And, taking account of this travel, I must admit that it did have its pros and cons. On the one hand, is it has been mentioned previously, the game did teach me a lot of important skills, especially the ones concerning motor function and memorizing essential things, as well as keeping in mind a lot of crucial information.\n\nIn addition, I have acquired strategic thinking and the capability of thinking fast and efficiently. To add up, I got pretty interested in the way all these characters work, which probable served as the final argument for learning a couple of things about compute design. According to Tobias and Fletcher, computer games can help even surgeons, developing their skills and offering a decent simulation for real surgery (Tobias and Fletcher, 54), which makes the chances of everyone playing simulation games to train the corresponding skills. However, it is also worth mentioning that the game did take a great lot of time which I could have spent in a more useful way.\n\nMoreover, constant playing the game could have made me dependable on playing computer games; luckily enough, I do not have any desire to start another computer adventure (unless Maxis offers the next part of the Saga instead of the despised spin-offs, which very unlikely). Thus, I am quite even with the Maxis and have escaped the sandbox without suffering great losses.\n\nWorks Cited\n\nForbus, Kenneth D. 2001, Some Notes on Programming Objects in The Sims . Web.\n\nTobias, Sigmund, and J. D. Fletcher. Computer Games and Instruction , Charlotte, NC: IAP, 2011. Print.\n\nVorderer, Peter, and J. Bryant. Playing Videogames: Motives, Responses and Consequences , New York, NY: Routledge, 2006. Print.\n",
        "label": "human"
    },
    {
        "input": "Computers and Web 2.0 Essay\n\nWeb 2.0\n\nIn the past ten years Web 2.0 was described from diverse viewpoints and by diverse scholars (Berger, 2010; Chai, 2011). Even the descriptions of Web 2.0 components are exceedingly arguable; however, such definitions do include each other since Web 2.0 is defined as the social application of the website which enables users to team up, to get dynamically concerned in generating material, with a view of promoting understanding and distributing information electronically.\n\nA Web 2.0 platform is perceived to have a rising task of transforming learning and instructing (Buerkett, 2011). Particular components and products contributing to the success of Web 2.0 in schooling comprise blogs, face book, wikis, social bookmarking and media-distributing.\n\nThe growth of Web 2.0 over the years has changed the means in which the online content is applied by most people. Now the online content is no longer a one-side teaching platform where the students download information and related materials generated by a comparatively small number of Web 2.0 users. Instead, the Web 2.0 is therefore perceived to be organized along considerably diverse attributes than the cyberspace-age internet of the late 1990s.\n\nThis sense of website utilization now being a contributory and joint action is replicated in the language utilized in describing social media platforms. This application utilization is frequently defined based on alliance, innovation and friendliness. Social media sites are perceived to be transparent rather than closed. A social platform user goes online with a view of sharing and rating information. The perspectives in which the online platform is expected in 2012 is undoubtedly different to that of 5 years before-thus the emerging of the brand Web 2.0 (Chai, 2011).\n\nAmidst such pedagogical improvements, various learning facilities (and teachers) now find themselves anticipated to remain relevant as far as Web 2.0 components and Web 2.0 users are concerned. Certainly, allegations of a digital divide between learning and the rest of community can be linked to the emergence of radio and video during the early years of the 20 th century. In fact, as with majority of such early trends of emerging technology, Web 2.0 remains a field of substantial anticipation, amplification and exaggeration. It is critical, therefore, that the instructors are capable of approaching Web 2.0 in an objective and measured way (Buerkett, 2011).\n\nThere is need to view Web 2.0 applications from an educational viewpoint, in order that learners can turn out to be electronically confident and prepared for the setbacks of the digital culture. Therefore, we as teachers are at first required to pursue neither a challenging syllabus, thick with theoretical approaches, nor one which enhances logical suppleness, but a course which involves those capabilities which are significant to the future graduate in securing an employment.\n\nSecondly, a teacher has to request his or her learners, when he or she uses Web 2.0 applications, with a view of proving initiative and accountability, imagination and interest, the capability of exploring, creativity, to function jointly and profitably, to teach and link clearly to each other, to be transparent towards determining and providing solutions. Most significantly, teachers need to request students to conduct a successful discussion, not only on pedagogical but also on social concerns.\n\nOnly in this way can a teacher make learners face knowledge challenges, through relinquishing theoretical instructing and integrating a kind of action oriented both on individual and team assignment. It is in addition critical to bear in mind that adjusting the instructing methodology is directly associated with the elements of Web 2.0 applications, which enable learners to work together, to participate fully in generating material, and to exchange electronic content (Berger, 2010).\n\nIn contrast, the collaboration between the Web 2.0 situation and instructing-learning procedure presents a cycle of challenges. Thus, the emerging applications have to be integrated in the syllabus correctly and not erratically (for example the teachers have to verify that the technology works prior to utilizing it with the learners). Then, as teachers, we have the responsibility of uncovering the content of technology, with a view of making a choice appropriate to our pedagogical needs since, the more materials we could give, the higher the requirement of making our learners accountable so as to turn out to be successful and reliable collaborators in the education process.\n\nTeachers need not to forget that misusing Web 2.0 can hinder or destroy content production, and can compromise the quality of education. Based on this background, all learning partners have to be integrated by way of a unique program or a special-subject teaching session. This section has explained the term Web 2.0, how it has changed over time and the benefits and demerits of its key components.\n\nPresentation object\n\nA presentation object refers to a resource developed with an aim of transmitting a body of topic content or result in realization of an explicit education purpose. Presentation objects attempt to transfer skills to students through showing messages illustrating chunks of topic content. Such messages can be assisted through modalities and normally, a definite principle is in place with a view of ensuring that pupils are encouraged.\n\nContent of such presentation object is normally divided into screens and subsections, with learners undertaking one part at a time. Other kinds of presentation objects can be slide objects with or without a talking head, video or sound-recorded lessons, illustrations, teaching video sections and object-oriented teachings. Figure 1 indicates a case of a presentation object generated with an application device that enables direct recording and documentation of a virtual knee replacement surgery procedure suitable for primary education.\n\nFigure 1: A case of a presentation object.\n\nEven though a presentation object is generally generated with a view of supporting traditional educational concepts, they might in addition support more current approaches and actions like problem solving. Chai (2011) proposes that any object might be utilized with a view of mediating learning program if that object is given a significant task in the action. The selected object could be useful for teaching co-curriculum activities in middle primary level. This is because a learner does not learn just from reading and being subjected to teaching notes from inputs, but he or she may efficiently utilize that data with a view of informing his or her choices and acts in a science class.\n\nPractice object\n\nA practice object enables a learner, to exercise some processes (e.g. assembling an electric pump), complete crosswords and perform specific assignments (e.g. utilizing a ruler in measuring a given length), link to a pedagogical game or respond to class assignments. Practice objects are designed to facilitate expansion of a learner\u2019s existing level of awareness and also to allow a learner to create a model of his or her own act and errors while implementing an instruction.\n\nFigure 2 indicates an illustration from the \u201cVolume of a Pyramid\u201d practice object. The query in the illustration needs learners to estimate the volume of the object provided in the case. Such object is a basic 3D illustration that can be turned around and diagrammatically assessed by learners. Learners rotate the object and utilize the given tools to determine its measurements. The selected object could be useful for teaching mathematics in upper primary level. This is because the exercise helps the learner to understand how to calculate volume of pyramids and how to utilize measurement instruments.\n\nFigure 2: A pyramid practice object.\n\nSimulation object\n\nA simulation object represents certain actual model or procedure. It allows learners to discover, normally through trial and error, working elements of a model, do an assignment that the model supports, and create an intellect system of that model\u2019s operations. Even though fidelity is frequently high in a simulation, establishment of knowledge is rarely finished and a learner has to normally move to an actual model to finish his or her work. However, by the time a learner changes to the actual model, he or she would already have developed an intellect system of the model\u2019s operational activities.\n\nFigure 3 indicates a screen of an \u201cElectronic Multimeter\u201d simulation object. The selected object could be useful for teaching science in upper primary level. Such pedagogical entity enables learners to explore applications of an electronic multi-meter device through collection of various dimensions for Resistance and Voltage. Learners in addition explore precise positioning of investigations in the electric circuit.\n\nAs well as the core aim of this model (exploring how to utilize the device), learners might as well carry on various parameters of Resistance and Voltage and explore associations which exist between such aspects with a view of deriving implication of an association called Ohm\u2019s Law (Bush, 2002).\n\nFigure 3: Electronic Multimeter.\n\nConceptual framework\n\nConceptual models are kind of pedagogical objects that represent one or more linked approaches or systems, normally in a visual and dynamic manner. It might be suitable to consider conceptual models as representations of cognitive resources existing in the mind of an expert, as significant cognitive skills that aid problem solving. Psychologists utilize an array of phrases like schemas, mental images and approaches to more or less show the same concept that there are aspects in the human mind that support higher emotional working (Bush, 2002).\n\nA case of an ideological framework is illustrated in figure 4. The selected object could be useful for teaching mathematics in upper primary level. This is because such pedagogical object illustrates an important idea from trigonometry. A learner can input various figures for the angle and explore shifts in value of cosine and sine as he or she conducts an analysis.\n\nFigure 4: A conceptual framework.\n\nInformation object\n\nInformation objects use information representation capacities of latest technology to offer educationally practical data. Such form of educational object might be simply a single visualization (a picture) or a multidimensional illustration and an image system offering data actively based on interface. Data can be displayed in mind maps, animations, 3D objects and by the way of other techniques (Anderson, 2003). For Anderson, a representation can be developed with a view of presenting complexity via image clarity. He proposes that traditional representations are deeply extended with an emerging technology which allows active, animated and 3-dimensional diagrams.\n\nFigure 5 indicates a case of an information interface. Such simple illustration of an information interface involves multidimensional data regarding local and exotic creatures of Australia. Information regarding an animal is accessed by scrolling a mouse over the text including the name of a creature and via choices that comprise the pushing of the creature\u2019s name into an equivalent region showing its origin.\n\nThe basic set of website pages containing data regarding Australian local creatures was transformed via content assessment into an information interface that enables a learner to investigate such information interface. The essence of the data from website pages was retained in the information space; though, extended lines of text have been shortened, educationally adequate phrases that are provided to learners arbitrarily based on information.\n\nThe selected object could be useful for teaching science in upper primary level. This is because various students might explore various aspects regarding an animal, and this can result in events like debates and shared mind-mapping.\n\nFigure 5: An information object.\n\nLearning objects vs. traditional learning\n\nLearning objects \u2013 like all other instructing methods \u2013 should be utilized in the learning phase. Learning objects can be utilized with a view of motivating discussions or introducing complex ideas. However, there are some critical aspects for teachers before they incorporate learning objects or request their trainees to utilize or establish learning objects in their sessions. The wide acceptance of learning objects generates fresh chances for engaging learners compared to traditional learning. Learning objects can engage learners and create more significant and deep education experiences through utilization of films and documentaries from sources like you Tube.\n\nReferences\n\nAnderson, T. (2003). I object! Moving beyond learning object to learning components. Educational Technology , 43(4), 24-19.\n\nBerger, P. (2010). Student inquiry and web 2.0. School Library Monthly , 26(5), 14\u201017.\n\nBuerkett, R. (2011). Inquiry and assessment using web 2.0 tools. School Library Monthly , 28(1), 21\u201024.\n\nBush, D. M. (2002). Connecting instructional design to international standards for content reusability. Educational Technology, 42(6), 5-13.\n\nChai, C. (2011). The Internet and teacher education: Traversing between the digitized world and schools. Internet and Higher Education , 14(1), 3\u20109.\n",
        "label": "human"
    },
    {
        "input": "IBM Website and Human-Computer Interaction Report\n\nTable of Contents\n 1. Introduction\n 2. Main Features\n 3. Positive and Negative Aspects of the Website\n 4. The Views of Other People\n 5. Recommendations on Necessary Improvements of the Websites\n 6. Conclusion\n 7. List of References\n\nIntroduction\n\nGlobally, IBM is among the leading firms in technology. It operates in more than 170 countries and deals with the invention, development, and integration of computer software and hardware provisions. Additionally, its services seek to improve the efficiency of various organizations for enhanced competitiveness and growth. IBM\u2019s products have transformed many organizations in Australia for nearly 80 years.\n\nIn the Australian market, it promotes digital connectivity, the development of sustainable cities, and the use of novel resources. It also invests in the Australian community and offers diverse employment opportunities to many Australians (IBM 2012). This paper provides an overview and description of IBM\u2019s website. Concurrently, it considers features of the website, discerns both positive and negative aspects of the website, captures opinions of other parties regarding the usability of the website, and provides appropriate recommendations relevant in this context. It gives an in-depth evaluation of IBM\u2019s website to determine its suitability for public use.\n\nAdditionally, it examines the features of the website about computer-human interaction. IBM uses its website to communicate its activities to the general public. Due to the diverse nature of its activities, it is recommendable for the mentioned website to exhibit all its activities and also show its technological advancements (Lopuck 2012, p. 77).\n\nMain Features\n\nThe main features of the website incorporate the provision of key words (for navigation) and appropriate links to other areas. This enables clients to access important information with ease. The words that have been made bold and clear on the homepage include IBM, Solutions, Services, Product & Support, and Download. These make it easier for IBM and its website users to interact considerably. The other key feature is the use of large pictures on the website. This has the effect of creating curiosity. Pictures and appropriate color codes can attract the attention of users. Appealing visualizations are also informative and easy to understand (Cards, Mackinlay & Schneiderman 1999, p.562).\n\nAnother vital feature is the presence and use of links at the bottom of the website. These links allow viewers to access more information regarding IBM. They link users to information about the firm, business executives, key news, and how clients may shop. These provisions are user friendly. Additionally, the website contains slide shows containing different information. Finally, the website has incorporated links to social sites such as Facebook and Twitter.\n\nThis enables clients to connect with the firm and access its products and services online (Hansen, Schneiderman & Smith 2010, p.28). Precisely, the use of appropriate colors, links, texts, and animations has revitalized IBM\u2019s website and user interface.\n\nPositive and Negative Aspects of the Website\n\nPositively, the ability of IBM to display vital information through a well-orchestrated website is crucial. The firm has numerous products and services. Besides, it is involved in many other activities, a fact that renders its website viable. Its user interface is comprehensive and viably created. The use of bold and clear texts also eases the usage of the website. It makes it easy for users to access IBM\u2019s products and services online.\n\nAdditionally, users can seek assistance from IBM representatives. The website\u2019s linkage to social sites increases the interaction between IBM and its clients. This is mutually beneficial (Lopuck 2012, p. 77). The links to social sites increase the public knowledge of IBM and increase traffic to IBM\u2019s website. Similarly, social sites increase the favorable perception of the firm and its products, enable easy marketing, aid monitoring of conversations about IBM, and improve insights in the targeted markets (Hansen, Schneiderman & Smith 2010, p.28). Additionally, incorporated slide shows make the usage of the website easy as slides are faster and comprehensible. The mentioned slides make it easy for users to navigate the website. Consequently, they can easily find the required information.\n\nThe pages of the website\u2019s interface are similar. There is an easy reversal of actions. This makes the use of the website easier compared to others. Even though the display of various texts on the website is a positive aspect, it has the potential of confusing viewers. The display of these texts on the website necessitates the use of small fonts. This is strenuous to some users of the website thus a negative aspect. Additionally, the other negative aspect of the website is that the multiple paths in the interface confuse some of the users of the site (Sears & Jacko 2009, p.193). The other negative aspect of the website is the use of dark colors, mainly black and dark blue.\n\nThe Views of Other People\n\nIt is important to know how other people perceive IBM\u2019s website in the realms of its usability and viability. The two respondents engaged (interviewed) to extract this information were my uncle who is a banker and my brother who is a student. My uncle\u2019s views about the website mainly focused on the content of the webpage. He said that the website was appropriate for IBM. It contained detailed information about IBM as a company, its products and services, and other relevant content. He noted that the boldness of key texts such as the \u2018Products and Services\u2019 made it easy to use the website (Smith-Atakan 2006, p. 31).\n\nAn interesting observation that he made regarding the interface is the choice of dark colors. His views were that the black and dark-blue colors were appropriate since IBM is a globalized organization. The combination of these two colors indicated the productivity and prospected prosperity of IBM. He specifically said the two have been used to influence the perceptions of individuals who visit the site.\n\nConcurrently, the views of my brother mainly focused on the usability of the site and the linkages of the site to social sites. He felt that the use of slide shows made it easy to find information quickly. Additionally, the multiple links applied in this context were modern. He also said that the inclusion of social site links enables the firm to popularize its products and services since the sites have become more popular in recent years.\n\nHe added that the website has visualizations that are easy to understand and are highly informative (Chen 2001 p.135). The views of the two were similar to mine with minor disparities. I held the same views that my uncle had. The boldness of the key texts is important and much information is also necessary to be included. I also held similar views of my brother especially the inclusion of the links to social sites and the use of slide shows. It is crucial to agree that various opinions regarding the structure and use of this website can discern the aspects of its usability and other considerable provisions.\n\nRecommendations on Necessary Improvements of the Websites\n\nGenerally, IBM Australia\u2019s website has good features and enables easy interaction and usage. However, it is crucial to front viable recommendations on improvements that may be made to advance the usability and interaction of its interface. The first recommendable change is the increase in the sizes of fonts used on the site. This would make it less strenuous for users who are not able to see small fonts. Secondly, an improvement that the IBM team may make on the website to improve its interaction with humans and its usability is the incorporation of audio assistance to the users (Shneiderman 1998, p. 84).\n\nThese may include directions to the users on how to log in or on how to shop and so on. Lastly, I recommend that more links to social sites be added since social behaviors are a fundamental part of human operations and have many positive benefits to the business. The inclusion of the links will improve the interactions of humans with computers and will make the experience pleasurable. Colors used on the website should be enhanced to augment visual appeals. It is crucial to consider these provisions to enhance the usability of this website. Its interaction with the public can determine the fates of IBM in the realms of international business.\n\nConclusion\n\nIt is important to consider the viability and usability of any given website. Contextually, IBM is a large organization handling numerous technological commodities. It is involved in many other activities such as research, innovation, and invention. Therefore, the company needs a website that can display as much information as possible to the public. In this context, IBM\u2019s website needs to be interactive and easy to use.\n\nThis paper took an in-depth analysis of the website of IBM in Australia with the main focus being on its interactive features and usability. One of the main features of the website is the use of bold and large fonts in the display of key texts. They enhance the usability of the concerned interface. The other feature is the use of large pictures that arouse the curiosity of users and also pass a lot of information. Additionally, the inclusion of links to social network sites such as Twitter and Facebook in IBM\u2019s website increases the human-computer interaction. Additionally, the slideshows and the list of links at the bottom of the webpage increase the usability of the website.\n\nThe website has various positive and negative aspects. These relate to its ability to interact with humans. The display of so much information on the website and the use of bold and conspicuous texts are positive aspects of the website. They make it easy for people to access considerable information. The other positive aspect of the website is the inclusion of links to social sites and the use of slide shows to display different information.\n\nA negative aspect of the site is that numerous texts used in this have the potential of confusing users. In addition to this, there are multiple paths in the interface. This makes the use of the interface difficult for some people. Generally, the website has good features and enables easy interaction and usability. It is recommendable that the inclusion of audio assistance and an increase in the fonts used can make the website more interactive and easier to use.\n\nList of References\n\nCard, S., Mackinlay, J & Shneiderman, B 1999, Readings in information visualization: using vision to think , Morgan Kaufmann, San Francisco, CA.\n\nChen, Q 2001, Human-computer interaction: issues and challenges , Idea Group Publishers, Hershey, PA.\n\nHansen, D., Schneiderman, B & Smith, M 2010, Analyzing social media networks with NodeXL insights from a connected world, Morgan Kaufmann, San Francisco, CA.\n\nIBM 2012, IBM website . Web.\n\nLopuck, L 2012, Web design for dummies , John Wiley & Sons, Hoboken, NJ.\n\nSears, A & Jacko, J 2009, Human-computer interaction. Designing for diverse users and domains , CRC Press, Boca Raton.\n\nShneiderman, B 1998, Designing the user interface: strategies for effective human-computer-interaction, Addison Wesley Longman, Reading, MA.\n\nSmith-Atakan, S 2006, Human-computer interaction , Thomson, London.\n",
        "label": "human"
    },
    {
        "input": "IBM.com Website and Human-Computer Interaction Report\n\nTable of Contents\n 1. Overview of IBM.com\n 2. Discussion\n 3. Recommendation\n 4. Conclusion\n 5. References\n\nOverview of IBM.com\n\nBy reviewing the website, the visitors meet only the desired results. The strategy, intent, and purpose are very clear because they contain ordinary and advanced search engines, especially when one would like more and in depth information from the website. For instance, IBM has incorporated the use of cloud computing in its system, this is supported by the fact that: \u201c whether you work remotely, manage remote teams, or need one place to bring colleagues, partners, and vendors together, our offerings help you transform your business into a social business \u201d ( IBM: Why IBM SmartCloud for Social Business 2012, p.1)\n\nDiscussion\n\nOn strategy, intent, and purpose, the primary action on each website is clear. For example, when examining a worksheet, the client wants to achieve specific tasks and goals when visiting and using the websites as the search engines. The website does not display the icon for identifying the well written and adequate graphical representations that a client may need to find out. However, the website allows the client to use keywords in finding such as software products.\n\nThis means that there is no direct way of finding specific software products (Raskin 2000, p.63). Secondly, the website allows online shopping and buying of software products. The website even indicates software products, which are on offer and the savings in terms of percentage that the buyer would be entitled to when he/he buys the products online. Therefore, the websites address the client\u2019s dilemma on whether he/she wants to buy the products online, request it from the library, or go a buy it in the bricks and mortar store because such options are available on the sites.\n\nOn design and functionality, there is the innovative use of text, graphics, and web-based tools making them more of a brochure than normal websites. For example, a normal website does have the required information on the home page. However, for the website given, the home page does not have all the information but contains a set of interactive web-based tools to make the person find the desired information (Sears & Jacko 2007, p. 49). Therefore, one might not get the required products on the home page.\n\nThe navigation of the website is clear. This is because the interface gives the visitor the chance to navigate the page and make necessary corrections before proceeding with the search. It is possible for the informed internet users if visitors arrive at an internal page via search engine results, they can immediately understand where they are in the site structure and how to navigate to a top-level page. For example, on amason.ca, once the user has typed the keyword and pressed \u201csearch\u201d, the search engine gives various options related to the information that the person wanted. From here, the person can use the web-based tools to navigate the page so that he/she could get the right information. Also, the person can navigate the page backward or move forward when looking for a specific item (Raskin 2000, p.52).\n\nNavigation is not persistent or consistent. Notably, it changes without reason depending on what the user wants to find out. For example, when merely looking for information related to particular products, navigation is consistent. However, when doing different functions such as online buying, navigation changes, and sometimes, visitors who are relatively new to using the website can become lost.\n\nVisitors are not easily, or not able, to complete all three tasks because they differ depending on whatever the user is looking for. This hinders the persistence in the navigation of the website, meaning that each item would shape the nature of navigation that the user should apply. Links and buttons are clickable on the website. There are links with related information to that of the user. Those links are displayed on the current page and the person can click and find such information that may assist him in his search (Sears & Jacko 2007, p. 26).\n\nIn the website, audience-centric keyword phrases are used during navigation. This means that there is no particular taxonomy that is applied, neither are there internal labels and language choice. Notably, each search has specific keywords that the user has to insert so that he/she realizes the intended results. For instance, if one is doing online research on social issues, it would be non-practical for the person to use legal, medical, and religious jargon in the search.\n\nHe/she has to stick to the social aspects, especially those related to the particular search. Also, a word choice that is not related to the primary search information would give undesirable results, thus could lead to confusion during the search (Sharp, Rogers & Preece 2007, p. 87). In such cases, the person cannot get useful information and might discredit the website for lacking information about the website.\n\nOn issues relating to readability, the website pages are simple to read because there is a good use of headings and subheadings, bulleted lists, and bolded text that could be used as key phrases in finding more and right information. The highlights are properly arranged in columns and rows and a list of categories, in which the software products are classified. However, it would important to recognize and state categorically that the website IBM.com has more graphics.\n\nThe contents of both websites are up-to-date. The new content is displayed and dated. Also, some of the software products and their latest prices, discounts, and mode of payment are displayed for prospective buyers. Moreover, the buyers are allowed to make online inquiries about software products and even make orders (Shneiderman & Plaisant 2010, p.38).\n\nLooking at the home pages of the website, visual cues are showing that the sites are regularly maintained or up-to-date. For example , there are pope visual adverts, which keep on changing. This indicates that the website is being maintained regularly. Furthermore, the latest price updates are shown and keep changing, thus a clear indication that its maintenance is done on daily basis. In IBM.com , there are no visual pop-ups, but the appearance of the software products in terms of the color of the products. The other feature such as offers, software products of the season among other latest attributes is shown.\n\nWhen the design of the website is examined carefully, they reflect the company culture and profession. For instance, IBM.com gives the various categories of items that they offer online such as software products, e-Reading, lifestyles, paper shops, gifts, toys among others. Alternatively, IBM.com gives options for the user to search for software products, software, electronics, and others. Besides, the items are arranged in an orderly manner and other related links are provided. This makes the websites have a professional look. Likewise, privacy policies, copyright and legal notices, terms of use give the websites a company and professional outfit (Shneiderman & Plaisant 2010, p.76).\n\nAbout the visuals, the user notes that the layout, colors, typeface reflect the purpose of the site, company culture, and visitor\u2019s expectations. This is because; the tools are somewhat user friendly. Sometimes, even the new user might find the websites easier to use.\n\nNotably, visuals portrayed to give the proper ingression of the items that the client wanted to see in his worksheet. Additionally, most of the things including the query for searching information are perfectly aligned either in the user\u2019s expectations or purpose of the text. Even though the visual presentations and contents of the website are more user-friendly, some interactive components should be incorporated to give the users (Sharp, Rogers & Preece 2007, p. 28). Some of the visual contents are represented as shown below.\n\nVisual contents.\n\nTypes of consumer experiences that brands focus on to open and participate in the social website include engaging, networked experience, on-demand of the users (Grudin 2012, p.57). The networked experience is about self-expression, ego gratification, portability, community, and meaningful change. On the website, this includes things like ratings and reviews, crowd-sourcing, and consumer-generated content, which are expressed very well in IBM.com . On the other hand, IBM.com creates the networking experience through the blog posts, where the readers could express their opinion in line with the client\u2019s interest in seeing some ratings and comments about the products.\n\nThe on-demand experience is about efficiency, ease, control, accessibility, and instantaneousness. In essence, this includes things like on-site search, store locator features and RSS feeds. As given on the website, they have a well-developed search engine that can find a lot of information related to the keyword(s) inserted.\n\nConsidering the client\u2019s worksheet, this might satisfy his/her expectation on matters relating to searching the software products using the keywords, getting the prices of different software products to compare them, and make decisions based on the findings. It might also help the client to get the software products quickly, know whether the products are on soft or hard copy and another variable of different items offered on the website.\n\nAnother important aspect of the OPEN brand metric System is the users\u2019 experience with acknowledgment, dialogue, customization, privilege, and popularity. Therefore, considering the website, IBM.com has helpful features, which include the \u201cContact Us\u201d page, individualized recommendations, and surveys. This seems to answer the client\u2019s concern on areas such as readers\u2019 recommendations on different software products, the popular ones, and the visual representation as shown in the websites (Dix, Finlay, Abowd & Beale 2003, p. 27). IBM also has user-friendly features that enhance the client\u2019s experience.\n\nRecommendation\n\nTo improve on the IBM website content, it is important to incorporate more specific key words to facilitate faster retrieval of the required information. As a result, this will save time for searching the required contents. Even though the company has an online support system, it should also incorporate the use of modern features such as \u201cSkype\u201d, which are more user-friendly.\n\nConclusion\n\nIn summary, the IBM website has various important features that are necessary for the user/client interaction, for instance, the OPEN brand metric system components that make it possible for the users to engage in an interactive dialogue. This has been made possible through the \u201cContact Us\u201d page, where communication between the user and the company can be made possible. The company has also online support system features, which facilitate interactive communication.\n\nReferences\n\nDix, A, Finlay, J, Abowd, G & Beale, R 2003, Human-Computer Interaction , Prentice-Hall, New York.\n\nIBM: Why IBM SmartCloud for Social Business 2012. Web.\n\nGrudin, J 2012, A Moving Target: The Evolution of Human-Computer Interaction, Taylor & Francis Taylor, New York.\n\nRaskin, J 2000, The Humane Interface: New directions for designing interactive systems , Addison-Wesley, Boston.\n\nSears, A & Jacko, J 2007, Human-Computer Interaction Hand products , CRC Press, Boston.\n\nSharp, H, Rogers, Y & Preece, J 2007, Interaction Design: Beyond Human-Computer Interaction, John Wiley & Sons Ltd, Boston.\n\nShneiderman, B & Plaisant, C 2010, Designing the User Interface: Strategies for Effective Human-Computer Interaction, Pearson Addison-Wesley, New York.\n",
        "label": "human"
    },
    {
        "input": "Memex and Dynabook as Early Portable Computers Essay\n\nNowadays, the computer has become an integral part of modern life. Being used in business, in science, at home, etc., it is virtually ubiquitous. But it is also interesting to study the history of this device. In this paper, we will consider two models of the computer that have influenced the creation of this piece of technology, and make a comparison between them.\n\nThe hypothetical future piece of equipment that Bush (n.d.) describes in his article originally published in 1967, the \u201cmemex,\u201d was to serve as a personal device used in order to store vast amounts of information, process it quickly, and retrieve it in the shortest terms. It was to work as an \u201cenlarged intimate supplement to his [a man\u2019s, i.e. a person\u2019s] memory\u201d (Bush, n.d., p. 86). The device was also to be able to tie \u201citems together to form trails in the heart of the matter\u201d (Bush, n.d., p. 87). Interestingly, it appears that, according to the author, the user of the \u201cmemex\u201d was to insert all the information into the device on their own, or perhaps much of the data could be inserted into in the process of production. In other words, no idea of \u201cpre-Internet\u201d is apparent in the engineer\u2019s description of the \u201cmemex.\u201d It is also noteworthy that Bush believes that a different type of machine than the tape-based \u201cmemex\u201d must be used for some mathematical operations, such as \u201cstatistical analysis, evaluating correlation coefficients and the like, and for solving integral equations\u201d (Bush, n.d., p. 89).\n\nOn the other hand, Kay and Goldberg (1977) describe a hypothetical device they call \u201cDynabook.\u201d It was to be used as an instrument for programming and solving problems; for storing and processing information while allowing interactive access to it; for use as a text editor; and for painting, drawing, and writing music (Kay & Goldberg, 1977, p. 393). The digital device was to be maximally small and portable, and was to be able to receive information, as well as to give it to the user in \u201cquantities approaching that of human sensory systems\u201d (Kay & Goldberg, 1977, p. 394). It was also supposed (almost) instantly, responding to the user\u2019s request at once. It was also to be able to be used by children, who could write programs, music, paint pictures, and gain access to a vast amount of various information.\n\nAs we were able to see, Bush\u2019s \u201cmemex\u201d was to be a machine that could store large amounts of information and allow the user to retrieve and process it quickly. The \u201cDynabook\u201d was supposed to do roughly the same, but it was to possess a number of additional functions, such as painting, creating music, and, importantly, programming, which is not present explicitly in Bush\u2019s conception of \u201cmemex\u201d. It is interesting that Bush (n.d.) points out that it would be necessary to create a different device than the tape-based \u201cmemex\u201d for performing calculations and mathematical operations (p. 89); on the other hand, Kay and Goldberg (1977) from their point of view (spoken from a later time than Bush\u2019s article was written) state that a digital computer built to do arithmetic computations could be used as any other type of media (p. 393). Also, clearly, the devices described in the articles are different in size and appearance; while Bush\u2019s \u201cmemex\u201d occupies a whole desk, Kay and Goldberg\u2019s \u201cDynabook\u201d was to look like the modern stationary computer at first, while at later stages it was to resemble the modern tablet computer (Bush, n.d., p. 86; Kay & Goldberg, 1977, p. 395, 394).\n\nAs we were able to see, the models of the computer, naturally, were becoming more complicated with the passage of time. Both \u201cmemex\u201d and \u201cDynabook\u201d were to possess the essential ability to store, process and retrieve information, but \u201cDynabook\u201d was to have a number of functions that were considered to be impossible to combine in one device.\n\nReferences\n\nBush, V. (n.d.). Memex revisited . Web.\n\nKay, A., & Goldberg, A. (1977). Personal dynamic media . Web.\n",
        "label": "human"
    },
    {
        "input": "Zayed University\u2019s Computer Security Internship Report\n\nTable of Contents\n 1. Work Plan Status\n 2. Key Achievements\n 3. Problems and Solutions\n 4. Potential Problems and Solutions\n 5. Lessons Learned\n 6. Culture and Organization\n 7. Miscellaneous\n\nWork Plan Status\n\nI have been working as an intern at the College of Technological Innovation at Zayed University for the last three weeks. Based on the activities that I have carried out within this period, I believe that I am on track towards achieving the internship objectives. Within this period, I was directly working with students in the IT faculty and the staff members. I had set clear objectives that I had to meet within this period, and I believe that I have successfully achieved them. These objectives will be discussed in the section below that describes key achievements within the first three weeks. Another factor that strongly suggested that I was on track was the comments received from the stakeholders that I interacted with at various levels. I met several students who wanted my help in formatting their laptops and iPads or installing various programs. They were pleased with my work, and others even though I have been doing the work for several years. Another confirmation came from my supervisor at this institution. He informed me that my work had surpassed his expectations and that many members of this college that had interacted with me felt that my work was perfect.\n\nKey Achievements\n\nWhen joining this institution, I was interested in achieving specific objectives within different timeframes. Within the first three weeks, the following are some of the key achievements that I made.\n\n  * I learned how to format Dell laptops and Mac laptops.\n  * I learned how to install Microsoft Office for Mac laptops\n  * I learned how to install Adobe Photoshop for Dell and Mac\n  * I learned how to scan viruses for laptops and flash drives\n  * I learned how to identify and address problems with wireless connections.\n  * I learned how to restore data in laptops and iPads that are lost because of virus\n\nThe above key achievements were part of my objectives set for the first month. It was impressive to achieve them in less than three weeks.\n\nProblems and Solutions\n\nWithin the first three weeks, I experienced several problems. The main problem that I faced was the need to meet the demands of the students. Once the students realized that I was able and willing to help them in addressing various problems with their laptops and iPads, they did not give me a break. They would always come to my office with various problems, and all of them expected me to find a way of addressing their issues. Some even asked me to help them with their assignments, something that was not only outside my scope as an intern, but also an unethical business practice. I had to find a way of addressing this problem because I had to address other areas that were also part of my internship. It took me some time to understand some of the fundamental concepts in a wireless connection, but I was able to overcome this challenge. I also found it challenging to meet some of the demands of the staff members in this department. Some of the staff members would delegate their duties to me because I am an intern. I believe they took advantage of my respect and calmness because they would spend the better part of the day chatting and supervising my work. Meeting their expectations was not easy. However, my supervisor would come to my help in case he realized that I was being overworked.\n\nPotential Problems and Solutions\n\nIn the future, one of the fundamental problems that I may face is in understanding the concepts in modern communication technologies such as videoconferencing and the latest concepts that cybercriminals use to access databases of individuals and institutions. This is so because the institution still uses some of the old technologies that can easily be manipulated using modern technologies. I expect to address this problem by visiting other institutions with advanced technologies during my free time.\n\nLessons Learned\n\nIn the last three weeks, I was able to learn a lot about the practical aspect of my profession. However, I also learned that I need to improve my interpersonal skills. I need to know how to work in a diversified environment with people from different socio-cultural backgrounds.\n\nCulture and Organization\n\nI liked the work environment at this institution. The learners, teaching staff, and the non-teaching staff were from different socio-cultural backgrounds. The organization had created a culture of tolerance to this diversity, and it was difficult to see people aligning themselves based on their religion, social class, gender, or any other demographical factors. People at this institution are friendly and very supportive, a fact which makes the work environment pleasant.\n\nMiscellaneous\n\nIt is a fact that the College of Technological Innovation at Zayed University has made a great effort to modernize its communication infrastructure not only for academic work but also for administrative duties. However, I noted that the institution may need to improve the technologies it uses to protect its data.\n",
        "label": "human"
    },
    {
        "input": "Computer Hardware and Software Policies for Schools Essay\n\nTable of Contents\n 1. Computer Hardware Acquisition and Vendor Policies\n 2. Hardware purchase policy\n 3. Policy for software evaluation and selection\n 4. Technology fa\u00e7ade checklist\n\nComputer Hardware Acquisition and Vendor Policies\n\nJust like any acquisition of expensive equipment in the school, purchasing computer hardware is more or less the same. Computer hardware programs have to generally meet the needs of a particular school, and therefore the vendor has to be a significant consideration before acquiring any of them. It is good to avoid mistakes that come as a result of the wrongful acquisition of special computer equipment.\n\nIn most cases, such mistakes may lead to failure to use the hardware in a manner that substantiates its cost. This would mean that the school will have made an error by paying too much for the hardware in which they end up not using it properly. This section of this paper seeks to argue for the need for considering the manufacturer of the hardware before purchasing it to avoid mistakes.\n\nHardware programs are essential in the running of the entire school computer systems. This includes the finance, registration, students and staff records, examination and also in the teaching and learning process. Acquiring computer hardware without following a vendor policy is similar to putting the fate of the entire school computer system in the hands of a sales person, which a wrong thing to do in any organisation.\n\nBuying computer hardware from a sales person would also mean that the school will have to incur extra costs that are usually hidden. This includes installation cost or configuration cost and support cost. Manufacturers usually ensure that the organisations\u2019 needs are met by customising the computer hardware programs to fit their goals. It is also easier to upgrade the hardware programs when the purchase is made through a manufacturer. This means that the manufacturer will always be updating the school of any available upgraded version of the hardware program, making it possible for the school to be more satisfied.\n\nCompatibility concern is one other issue that should cause a school to have vendor policy for the purchase of its computer hardware. There could be specific hardware programs that are not compatible with a particular operating system or software. Dealing with the particular vendor directly would help the school avoid mistakes such as the ones mentioned above. It creates an environment where the school and the manufacturer will be on the same page when it comes to responsibilities for specific actions.\n\nIt is possible that with the manufacturer, the school can stick with what it wants and has. For instance, once the school has defined its priorities and needs in a functional term, then the vendor can quickly be involved in coming up with the right hardware that will assist the smooth running of the school system. Therefore, the challenge that the school has is to identify a trustable vendor who understands the technicalities within the school environment.\n\nThe partnership with the vendor is not supposed to end the moment when the school purchases the computer hardware. This partnership continues for a longer duration as long as the school and the vendor remain in good terms. The relationship depends on whether the vendor can handle servicing of the hardware programs whenever required by the school. They do not have to send an outsourced company or group to do their responsibility of maintaining the hardware programs, especially during the valid period of the warranty.\n\nThe vendor policy within my school district is such that for any vendor to do business with any of the schools, then there are essential obligations that they have to follow. First of all, they have to handle all communication process with the schools through the assigned school representatives. Through these representatives, the vendors have to communicate issues to do with available updates for any of the computer programs both software and hardware, advice on public services and products that will improve on the already existing ones and give suggestions for an effort that would benefit both the school and the vending company.\n\nHardware purchase policy\n\nThe primary purpose of this policy is to enable the school to have restrictions for the purchase of any new computer hardware. The policy helps a lot to assist the purchaser only in being limited under the confinements of the school IT budget. Equipment of a specific standard is always a recommendation that any school would have. This policy sets down the particular standards for Information technology equipment that each school within a district needs to have.\n\nThe vendor has to be in agreement with the school to accept equipment configurations that are stipulated in the policy. This helps in improving pricing of the supplies and hardware equipment to both the school and the purchaser. It also assists the school administration to avoid more overheads. The school will incur less maintenance cost and better support for the equipment they have purchased if the policy is put in place.\n\nBefore making any purchase, there are specific hurdles that have to be completed for the purchase to be approved. The first hurdle is the standardisation of the hardware. According to the school hardware purchase policy, standardisation is not entirely restricted.\n\nPurchase of nonstandard hardware component can occur but should at all times be minimised. To justify the purchase of such non-standardised hardware equipment, there should be an exceptional circumstance for its immediate requirement. For instance, the school cannot approve any purchase of non-standardised hardware equipment without indication of how it will be supported and who is responsible for supporting and maintaining the equipment.\n\nThe other hurdle is the review of the vendor to be involved in the acquisition of the hardware equipment. The school is responsible for reviewing the performance of the company before engaging in any transaction with the vending company. Such action is aimed at benefiting the in various terms. It will help the school to see the cost of the products that they intend to buy from the manufacturer and also see how reliable the product is and if there is need for any servicing to be done regularly. According to the policy, once the purchase has been made, the school is also entitled to continue with the product review after every six months.\n\nThere are several evaluation categories in which the performance of the vendor will have to be subjected both before purchase and after the purchase. These categories include cost, the time of acquiring the equipment, performance of the equipment, and reliability of the equipment. However, the categories are not limited to only these four mentioned. The third and final hurdle requires paper work and procedural matters. Before any purchase, some forms have to be filled and several people whom the forms have to pass through to append their signatures.\n\nPolicy for software evaluation and selection\n\nWhen it comes to software purchase, several ways can be employed to evaluate the software. Delgano identifies three categories that can be used to assess instructional software programs. These categories usually form the basis of most software evaluation policies for many schools and school districts. The software must be able to cater for input technique, cognitive task and system response.\n\nThe software must be understood exclusively for purposes of instruction, in as much as software programs are suitable for teaching and creating the necessary connection that the student requires relating to real life. However, this can only be achieved if the instructor plays a role in guiding the students as they interact with the software.\n\nThere are challenges and hurdles that arise during the evaluation process of the software. The first challenge concerns defining the educational outcome that the software tends to provide. It is hard to identify the competency level of the software program and how it is useful in giving the exact intended goal. The best way to overcome this challenge is by testing the program with the students and at the same time without the students.\n\nThis will enable educators to be in an excellent position to make a decision to purchase the software program or not. This means that piloting for instructional software is necessary. If the vendor of the software does not provide a piloting version or a trial version for a demonstration, then the chances are high that the program may not meet the objectives and goals of the school.\n\nMeasuring of intended outcomes is the other major challenge that is usually witnessed during the process of evaluation and selection. This challenge is preceded by the challenge of defining the results. But once the outcome has been determined, then measuring it would be an attainable task. This challenge can be overcome by testing the software on the students. Most of the instructional software programs have students test after every tutorial session. If the students are able to get the test questions and other instructional objectives of the tutorial sessions, then the program can be prejudged to be worth purchasing.\n\nOvercoming this challenge will ensure that the program is tested for validity and reliability. The software program has to be able to refine the skills of the students and at the same time, develop a new essential life skill that the students will identify with in real life. It may not be possible to measure everything that is required because educational measures are restricted to observable skills only.\n\nThirdly, a challenge will arise when it comes to accounting for the impact of delivery that the software program will have to both the instructor who will be entitled to use it and the students. The software program will be flawed under one condition; if it is unable to create a clear distinction and separate the impact, it has on the delivery methods and educational methods. It is known that instructional strategies have to make the necessary motivation that the students require in order to enhance teaching and learning. Using the educational software program must meet the objectives of instructional methods and have a positive impact on the delivery of instruction.\n\nIf a comparison is made between the lectures conducted in a class by the tutors and the sessions in the software programs, one thing will come out clearly. This is that the methods of instruction will be the same, but the practices of delivery will differ. In other cases, the software programs will give a difference in both the delivery methods and in the instructional strategies. This difference is mainly due to the nature of both the two methods of instructions.\n\nIn the first method, the lecturer does not have to be patient with the instructor, but in the latter case, the computer program is self-paced. It is argued that computer programs do not entirely improve learning among learners. What media- and computer-based learning does to the student is that it alters the efficiency of cognitive learning. This alteration is usually on the positive side where the student will be in a better position to grasp what the software program intends for learning purposes.\n\nLastly, there is the challenge of coming up with practical problems for designing learning requirements. This is where the teachers play an essential role in identifying the required practical issues for the students. Before coming up with educational software, the process has to involve several stake holders to make a comprehensive thing that will be appreciated by all. It is not enough for the software programmers to code the software without the appropriate content that will be beneficial for the students in the instruction process.\n\nTeachers have to be involved because they are the right people with the right content and instructional method required for the effectiveness of the software programs. During the evaluation and selection process by a school, these five challenges will be the main things that they have to overcome in order to select an appropriate program for their school. These same challenges apply even when it comes to hardware selection and purchase. The evaluation has to be a successful process and to achieve this then the\n\nTechnology fa\u00e7ade checklist\n\nTechnology fa\u00e7ade is an assessment program that is used to identify whether a school has an outstanding computer program or not. It consists of a series of questions that aimed at assessing the status the technology within a particular school. The section of this paper is an analysis of a field experience in which I conducted in the school in which I work. The first section of the assessment was checking on the use of technology within my school.\n\nTechnology use is a common thing in the school that I work in. For instance, it is not only computer teachers who are left with the work of using school computers. Each and every teacher in the school is entitled to be computer literate and also use the help of computer-based instructional programs to deliver the content of the lesson. I noticed that being able to use a computer in this school is an added advantage for the appointment of any new teacher.\n\nThere are several teachers who have lost their opportunity to be part of the esteemed teaching staff of this school because of their inability to simply operate a computer. Almost everything is done with a computer including the marking of some test, delivering assignments, students\u2019 evaluations and reporting.\n\nThe computer facilities of the school are usually made available, especially to the students during periods in which they have free time such as recess. The facilities are only locked up for security reasons at night when everyone has left the school. This means that during regular operational hours, the students can use the facilities, especially internet services for research and other learning purposes. There is a school computer lab that is enough to accommodate 100 students at a time.\n\nAnd this facility is usually opened to the students. There is also wireless internet connectivity in which staff and students with portable computers can use to access the internet from any point of the school. Each classroom has a computer which is typically operated by the class teacher. The computer is connected to the entire school network system, and it has the records of all the students in the class.\n\nIn most cases, the teachers in the school would use technology for grading the students, preparing their lessons, giving the students out of class assignments and for professional development. For instance, there are teachers who never write assignments on the board but refer students to the department page of the school website to look for an uploaded task. There are several computer-based lessons that the students regularly have, such as web quests.\n\nDuring the web quests, the tutor only provides links to specific sites where the students will find necessary information for the particular task in which they are required to accomplish. Such exercises are essential to the student for enhancing research skills and presentation skills because at the end of it all they will have to use PowerPoint presentation to organise their findings.\n\nAnother common thing with the software found in the computers is that they are regularly updated so that they meet the changes that continuously occur with the curriculum. The software vendors are usually up to speed in providing the updated versions of their software. Some of the updates have to be downloaded from the internet, and this even makes it more comfortable since the updates run automatically.\n\nThe second section of the analysis concerns the necessary infrastructure. It is not just enough to provide the technical resources required for teaching. There are several infrastructures that are needed to go handy with these technological devices. For instance, teachers have to be trained in using the different technological tools including the computers in order to save on the cost of having a specialist who serves the purpose of guiding the students through the various programs.\n\nThe different stakeholders are all actively involved in the school development and technology committee. This committee is essential in sensitising the need to use technology for instruction. Technology is a big priority in the school, and funding for it is one of the areas that have to receive budgetary allocation each and every year. There are also programs that the administration uses every semester to recognise and appreciate teachers who embrace and use technology in their classes for teaching and learning. The school has a clear and well-articulated technology plan that clearly states the mission, vision and motto of the school with regards to technology. From the overall rating of the school, it can be concluded that it has a satisfactory technology program.\n",
        "label": "human"
    },
    {
        "input": "Computer Hardware Components and Functions Essay\n\nHardware is the physical components of a computer, while the software is a collection of programs and related data to perform the computers desired function. The software can be edited while the hardware can not. There are four main groups of hardware; Those concerned with input, the central processing unit (CPU), storage hardware components and those concerned with output.\n\nInput hardware devices are things such as keyboards, mice, scanners or an analogue to digital converter and are used to provide data and control signals to an information processing system.\n\nStorage hardware can be either volatile or non-volatile memory. Volatile memory, or Random Access Memory (RAM), stores data while it is being manipulated and is synonymous with the working memory of the human mind.\n\nThis is the information which is lost on computer shutdown. For example, this word document I\u2019m typing at the moment is stored on RAM until I save it. When I save this document (as I have just done) It is now stored on a form of non-volatile memory; the hard drive. Other forms of non-volatile memory include the read-only memory (ROM) where programs integral to the computer\u2019s function are stored and storage devices such as CDs. ROM hardware also includes expansion cards such as graphics cards.\n\nThe Central Processing Unit (CPU) executes a sequence of stored information, or, programs.\n\nOutput Hardware is any hardware which is used to communicate the results of the computer to the outside world. Monitors, printers and speakers are all forms of output hardware.\n",
        "label": "human"
    },
    {
        "input": "Computer Technology in the Student Registration Process Essay\n\nTable of Contents\n 1. Plan of action analysis\n 2. Outcome\n 3. Dimensional Analysis\n 4. Reference\n\nPlan of action analysis\n\nThe plan of action was developed to improve the student registration process at the college. It involved the introduction of computer technology in the registration process and included an online application for admissions. The same process proved to be successful in other colleges as it had made the registration process enjoyable and faster.\n\nOutcome\n\nIntroducing technology in the registration process increased efficiency and improved time management as per the first objective. The technology used in student registration made the registration process to take less time for all the students who sent their applications online. Hence, all their registration details were already stored in the college database. Besides, online registration minimized the use of manual paperwork during the registration process as the shortlisted students were just required to provide proof of identification (Barrett, 2003).\n\nThe registration process became more efficient due to the reduction in the number of registration staff because they are only tasked with the transfer of the students\u2019 information to their respective departments. Therefore, the extra staff from the registration desk were reassigned to other tasks.\n\nThe second aim was to eliminate unnecessary stages of registration and bureaucracy when registering new students. This was achieved since when a student applied online for admission, most of their information was obtained, and there was no requirement to provide similar papers during the actual physical registration. This repetition was what the new students had found unnecessary and not enjoyable but its elimination made the registration process quicker, and they had time to explore the college and adapt to their new environment (Barrett, 2003).\n\nThe last aim was to ensure that the process was done without any form of bias irrespective of race, religion, or other social beliefs. The use of computers during the registration process provided minimal chances for bias since computers do not have any prejudice unless they were pre-programmed. Also, the reduction in the number of registration staff meant a prejudice reduction, as the staff members could have discriminated against the students. The students too were able to get enough time to interact with their course advisors since the registration process was fast, this enabled them to avoid having biased opinions about their course advisors (Barrett, 2003).\n\nDimensional Analysis\n\nThe dimension used was the computer application, and it was the main component in the working proposal. The objectives were met, despite the slight modifications needed, such as the type of database used to store the students\u2019 details, which needed upgrading constantly for more efficiency and to accommodate more information.\n\nThe main strategy was dependent on providing computing facilities at the registration desk and the provision of a web site address that enables the student to access the application forms online. The development of this type of website was expensive but efficient in the long run. The use of computer technology mandated the registry staff to learn how to use computers effectively, and this improved their service delivery to new students (Barrett, 2003).\n\nThe rationale behind this improvement was the use of computers in the management of students had proved to be very helpful in other colleges. The computers allowed this information to be well organized, and this made the same information easy to retrieve in the future. The other advantage that this change brought to the student registration process was the reduction of bias during registration. This was possible because computers are not bias unless they were pre-programmed. Thus, the students were able to register without being discriminated against based on their personal beliefs or appearance.\n\nReference\n\nBarrett, R. (2003). Vocational Business: Training, Developing and Motivating People . New Hampshire, USA: Butterworth-Heinemann publishers.\n",
        "label": "human"
    },
    {
        "input": "Education Goals in Computer Science Studies Essay\n\nThe main reason for choosing better and advanced education in a competitive field of study lies in a personal perspective of life. It is also a challenge and over persistence or needs to advance knowledge, especially in this era of computerization and technologically advancing platforms. Considering the current situation all over the world, everything seems to involve some change in technology and way of life as a measure to aggressively meeting the harsh economic experiences.\n\nMost people will agree that life depends on information technological panache such as the internet, cable or mobile television, and another phenomenal advancement. Technology completely saturates the current lifestyles. To computer scientists, technology is a useful tool for confronting future problems, thus the need to accept and probe deeper.\n\nUnderstanding the technological world is inevitable, and it is the only essential strategy for anyone living in the 21st century. Everybody ought to understand that technology is here to stay. Other than the great opportunity of creativity and innovativeness, computing is a pillar to careers, offering a wide range of lucrative choices. Lastly, there is a need to mention that future opportunities are almost impossible to predict; they have no boundaries, and the need for advancement will constantly cause one to wait in anticipation for a better lifestyle.\n\nInvolvements\n\nAs a student, I have had some rewarding and equally challenging experiences. Regardless of the field, one has to venture into or engage, majoring in a certain field gives one competitive advantage and great chances of gaining extra unique knowledge or skills, problem-solving tactics, and logical thinking formulas.\n\nIntegrating the theoretical knowledge into the practical world requires one to acquire competence in solving tough multi-dimensional problems that necessitate deep imagination and sensitivity to quite a number of apprehensions. Digital advancement is the main catalyst for positive development or innovations in almost every field. Considering some of the general challenges, for one to make a positive difference in learning, there is a need to incorporate technological advancement into the normal or current challenges. Advancement calls for creativity and patience to deliver the required output. Some knowledge of computation assists one to integrate some individual flairs and imaginations in any field to make it better, interesting, and easier.\n\nChallenges\n\nHaving participated in an excellent competition as a team-leader, anyone would have guessed that the main gain from such a forum would be in the advancement of knowledge or strategies. Such forums or educational competitions come with a lot more than knowledge-gain. As a leader, I acquired meaningful existence and interpersonal relationships through a built-up rapport among people of the same career interest but different backgrounds.\n\nBesides the academic skills, the challenge gave the team a social and teamwork development platform. The competition gave students a chance to interact with a diverse group of participants and collaborate in celebrating their accomplishments and passions. Because of the diversity of the contestants, the presentations were technologically up-to-date in comparison to our team.\n\nBesides the interactions, our team had to engage in some critical thinking, analysis and thus change some aspects in the presentation as a way of ensuring our acquired skills had a close link to future development or modernization. It was a learning experience that critical thinking presents the power to engage precarious advances, to meet the situations. Top leadership enables one to gain critical guidance skills to meet situational demands. A good leader has to make smart choices or decisions, such as a change of strategies.\n\nConclusion\n\nLife challenges will increase, but the biggest of them all is the ability to manage technology and accept it as the catalyst for positive change in society. Computation is a channel of information, which is very useful in solving most current and future challenges in all careers, but we must work in close reference to Martin Luther King, saying, \u201cThe true goal of education is intelligence and character.\u201d\n",
        "label": "human"
    },
    {
        "input": "Enhancing Sepsis Collaborative: Computer Documentation Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Nursing Informatics Project\n 3. Stakeholders\n 4. Project Priority\n 5. Success Evaluation\n 6. Reporting the Results: To Whom?\n 7. Conclusion\n 8. References\n\nIntroduction\n\nPreventing and treating the life-threatening infection that is sepsis requires one of the most rapid reactions from healthcare professionals regarding time. Thus, identifying and documenting sepsis cases becomes another critical step in the patient\u2019s healing process, requiring a specific set of skills and ultimate collaboration from doctors and nurses alike. Despite paper-based databases remaining the traditional way of going about protocol, using advanced and modern means of documentation presents newer, undiscovered aspects of utility. Hence, the presented Nursing Informatics (NI) project at Northwell Health will focus on the ways a shift from paper to computer databases can be achieved, as well as the benefits of this within treating sepsis.\n\nNursing Informatics Project\n\nThe need for a computer-based documentation approach within Northwell Health was identified based on multiple factors, concerned not only with modernizing the attitude of healthcare professionals towards the documentation process. A study by Perez (2013) states that the \u201cresults of adopting innovation and the improvements effected by its implementation go beyond the adoption process\u201d (p. 89). While the mentioned study deals mainly with educative nursing systems, their apperceived benefits may transfer well onto the presented documentation procedures. Considering all this, along with the fact that Northwell Health does not have a computer-based sepsis protocol, creates the need for a project that will construct the possibility of such a changeover.\n\nThe project objectives will include:\n\n 1. The creation of a sustainable computer-based sepsis documentation protocol to be used within Northwell Health.\n 2. The identification of those who would benefit from such computerization of the documentation process.\n 3. An establishment of the project\u2019s priority within Northwell Health and, thus, the need for its timely implementation within the organization.\n 4. The codification of specific values of success evaluation by using a devised rubric.\n 5. The determining of those responsible for the project\u2019s implementation.\n\nAn argument should be made about the need for a computer-based protocol, as it is often associated with unnecessary costs rooted in equipment and software prices, as well as the required staff training. However, computerization allows for an enhanced response time, which becomes vital in life-threatening cases, and electronic documentation has been found to surpass hand-written protocol in speed (Schachner et al., 2016). This fact alone may be enough to give computerization an advantage over slower, paper-based methods, as the positive effects on patient\u2019s health far outweigh the negative repercussions related to cost.\n\nStakeholders\n\nThe effect of decreasing the so-called paper trail on sepsis identification has already been identified as having a positive impact on patients\u2019 health. The influences of inopportune sepsis treatment result in \u201clong hospital stays (median = 10 days), discharge to long-term care settings (20%), and death (25%)\u201d (Novosad et al., 2016, p. 867). Expediting sepsis treatment through its rapid documentation, thus, allows for faster patient recovery through not only its identification but also the collaboration of healthcare professionals in its treatment. Therefore, both actors and seekers of aid within the healthcare system may be identified as those benefiting directly and indirectly from sepsis protocol computerization.\n\nProject Priority\n\nThe importance of this project should not be underestimated in comparison to other attempts to improve the healthcare system mechanism. A study by Clynch and Kellett (2015) focused on the impact of human error in medicine has shown that documentation takes up a significant portion of doctors\u2019 time, which may be combatted by developments in computerized protocols. Thus, the issue of incident codification becomes one of the most important and most requiring solving, especially considering that there is no necessity for a complete revamping of the system bus, only its remodeling.\n\nSuccess Evaluation\n\nAttainment of success should not just focus on reaching the goals set systematically in the presented research but should rely on the creation of a rubric to measure that, which is quantitative. Of such calculable values, it is possible to highlight two, with one of them being the percentage of implementation of the devised computer-based sepsis documentation protocol throughout Northwell Health.\n\nThe second measurable quantity would be that of sepsis treatment time through a comparison of the speed of reaction before/after implementation, calculated in time per aided patient. Factors relating to project priority and stakeholders may be effectively extrapolated from doctor-nurse opinion questionnaires about the subject. However, the success of this project should be defined not only through its perceived utility but also through an assessment of the implementation effort against positive results, to be calculated from the suggested above metrics.\n\nReporting the Results: To Whom?\n\nThe institution of a separate group, or multiple ones, with a defined group leader, may seem necessary for the success of such a project, which will help achieve ultimate collaboration between system-developers and healthcare professionals. Equally as important remains the assignment of those responsible for the assessment of the results and thus playing a role in the evaluation of the advantageousness of the project. Thus, the creation of such a system of group collaboration within Northwell Health becomes vital to the success of the project, as without appropriate evaluation, no outcome may be devised. It would seem practical to place this project under the responsibility of the Taming Sepsis Education Program at Northwell Health, due to the perceived goal alignment.\n\nConclusion\n\nThe partnership instituted between healthcare professionals through the development of a computer-based system of documentation can ultimately lead to faster response time to life-threatening cases, such as sepsis. Through the establishment of appropriate software, training, and a decrease of the infamous paper trail, which slows down aid administration, patients\u2019 health may be positively affected. The highlighted in this research goals and suggested rubrics of evaluation could be implemented to appropriately develop and modernize the health care system by creating efficient ways of communication between doctors, nurses, and patients alike.\n\nReferences\n\nClynch, N., & Kellett, J. (2015). Medical documentation: Part of the solution, or part of the problem? A narrative review of the literature on the time spent on and value of medical documentation. International Journal of Medical Informatics , 84 (4), 221-228. Web.\n\nNovosad, S., Sapiano, M., Grigg, C., Lake, J., Robyn, M., Dumyati, G\u2026. Epstein, L. (2016). Vital signs: Epidemiology of sepsis: Prevalence of health care factors and opportunities for prevention. Morbidity and Mortality Weekly Report , 65 (33), 864-869. Web.\n\nPerez, G. (2013). The adoption of information systems innovation: Study of a learning support system and its adoption in the medical education field. International Journal of Auditing Technology , 1 (1), 75-90. Web.\n\nSchachner, M., Gonzalez, Z., Sommer, J., Recondo, F., Gassino, F., Luna, D., & Benitez, S. (2016). Computerization of a nursing chart according to the nursing process. In W. Sermeus, P. Procter & P. Weber (Eds.), Nursing Informatics 2016: EHealth for all: Every level collaboration \u2013 from project to realization (pp. 133-137). Washington, DC: IOS Press.\n",
        "label": "human"
    },
    {
        "input": "Northwell Health Sepsis Protocol Computerizing Project Research Paper\n\nTable of Contents\n 1. Project\u2019s Tasks and Deliverables\n 2. Work-Breakdown Structure\n 3. Gantt Chart\n 4. Conclusion\n 5. References\n\nAppropriate charts such as the Gantt chart and the Work-Breakdown Structure (WBS) help time-manage the project, creating not only an atmosphere of collective responsibility through task assignment but also setting clear due dates. The corresponding appendices A and B deal with the division of the WBS and the Gantt chart per the identified responsibilities. Identifying and visualizing the tasks and outputs of the project aimed at computerizing sepsis protocol at Northwell Health in this way permits the proper project management execution.\n\nProject\u2019s Tasks and Deliverables\n\nDelivering a final product only becomes possible when the created plan deals with and meets smaller goals throughout its implementation. As deliverables may be \u201ctangible or intangible part of the development process,\u201d their apperception within the project went beyond the scope a list of instruments provided to nurses for sepsis computerization (Sipes, 2016, p. 86). The plan was therefore developed with the requirement for provision of services, such as nurse IT training and feedback lines, in mind.\n\nThus, the identified deliverables are the practicum project plan, project scope statement, project charter, hardware, software, training, lines of feedback, as well as qualitative and quantitative research results. Altogether, these deliverables align with the task of computerizing sepsis protocol and creating an electronic procedure of sepsis treatment. While the creation of the plan itself is the primary goal of the project, the second stage is related to the delivery of assigned outputs.\n\nThe tasks of the project closely mirror the outlined goals, resulting in a dependent relationship between them, with changes in one sphere resulting in changes in the other. An essential step in task assignment and deadline identification became their correspondence and alignment with the actual time possibilities within teams (Harris, 2016). Therefore, close communication of team leaders and inter-team communication are necessary for the efficient implementation of the project.\n\nWork-Breakdown Structure\n\nThe WBS makes possible assigning those responsible and directly contacting those answerable for aspects of project development and avoiding unnecessary confusion concerning the assigned goals. Without breaking down bundles into smaller tasks, it is not possible to make teams responsible for result deliverance and thus puts the project at an executive disadvantage (Sipes, 2016). The roles and responsibilities assigned over the scope of the WBS become a crucial step to project execution.\n\nGantt Chart\n\nA Gantt chart helps to envision the moments where the identified project tasks will overlap, creating possible drawbacks and issues. Sowan (2015) outlines it as \u201ca project management bar chart that illustrates phases and activities of the change, resources required (e.g., cost, time), and personnel involved,\u201d and additionally states its project significance (p. 20). Thus, the Gantt chart as a visual representation of the project deadlines becomes another stepping-stone in achieving project success.\n\nConclusion\n\nWhile it is possible to execute a project without time-management charts, it is evident that their creation streamlines the project into achieving at least the appropriate tasks in the proper sequence. Deadlines and responsibilities become the core of the project, without which its application becomes impossible in a timely fashion. Thus, the WBS and Gantt chart become indispensable aides of the sepsis computerization project at Northwell Health.\n\nReferences\n\nHarris, J. (2016). Key foundations of successful project planning and management. In J. L. Harris, L. Roussel, P. L. Thomas, & C. Dearman (Eds.), Project planning and management: A guide for nurses and interprofessional teams (2nd ed.) (pp. 1-30). Burlington, MA: Jones & Bartlett Learning.\n\nSipes, C. (2016). Project management for the advanced practice nurse . New York, NY: Springer Publishing Company.\n\nSowan, A. K. (2015). Applying IT-related business process reengineering in an informatics course for graduate nursing programs. Archives of Nursing Practice and Care, 1 (1), 16-24. Web.\n",
        "label": "human"
    },
    {
        "input": "Computers R Us Company\u2019s Customer Satisfaction Case Study\n\nAbstract\n\nAfter multiple complaints from its call centre, Computers R Us has decided to launch an analysis into customer complaints. The company uses a survey to draw data-driven conclusions about the current customer satisfaction level within the business and the strategies that will most effectively increase their customer satisfaction. The present study suggests recommendations for improving customer satisfaction within the company.\n\nIntroduction\n\nThe management of Computers R Us acknowledges that customer satisfaction contributes greatly to the complaints that its CompleteCare division raises. Improving customer satisfaction should be the priority of each company\u2019s management (Knox et al. 2003). The management proposed the survey to meet the following goals. First, the survey illuminates on the customers\u2019 perspectives regarding the complaints about its distribution problems, availability of parts, and shortage of technical operators in its call centre.\n\nSujansky and Ferri-Reed (2009) support that survey instruments help a company\u2019s management in learning about the customer\u2019s reception of their services. The survey instrument includes customers across all levels to allow for a comparison of responses across several departments. Employing the survey instrument provides Computers R Us with data that they could not gain internally.\n\nResearch Design\n\nThe management of Computer R Us acknowledges that the company needs to improve its relationship with its customers. The survey approach was the most preferable because of its ability to collect large data from multiple participants. The instrument determines the differences in individual responses and the organizational support structures that could best improve customer satisfaction. According to Thomas (2009), the scale model of the survey avoids multiple invalid answers to improve the respondent\u2019s accuracy. Additionally, the survey design has no time limit and costs less for the company to initiate.\n\nData collection instrument\n\nA survey instrument is employed in the study, to find the possible initiatives that are applicable to improving customer satisfaction to a minimum scale of six out of ten. Yu, Wu, Chiao and Tai (2005) identify the survey method as the best instrument to yield a high rate of responses from its customers. The instrument is also the best approach for completing the project in a well-timed manner.\n\nSample\n\nThe researcher targeted a random sample of 500 customers to fill the survey instrument. However, only 420 responded to the survey. The staff furnished the survey with a stratified data sample that met the parameters of its earlier report. The data reflected the varied experiences of dissatisfaction within the company\u2019s services. The remaining 80 who failed to respond to the surveys were excluded from the study.\n\nEthical considerations\n\nThe company was required to maintain customer confidentiality throughout the survey. The researcher sought the approval of the Industry Review Boards to ensure that the privacy of the subjects was protected.\n\nData Analysis\n\nThe company stored the results on a secure web server, where the data was easily portable to a spreadsheet for analysis using Excel. The six hypotheses identified below guide the study.\n\nHypothesis 1\n\n  * H1: The current level of customer satisfaction differs from management\u2019s goal of 6 out of 10.\n  * H01: The current level of customer satisfaction does not differ from management\u2019s goal of 6 out of 10.\n\nTo test this hypothesis, a one-sample t-test is applied to test the average income satisfaction of the people.\n\nt-Test: Paired Two Sample for Means            \n                                                             \n                                     Variable 1  Variable 2  \nMean                                 36.35238    4.488095    \nVariance                             171.9185    5.505825    \nObservations                         420         420         \nPearson Correlation                  0.077865                \nHypothesized Mean Difference         0                       \nDf                                   419                     \nt Stat                               49.70119                \nP(T<=t) one-tail                     4.4E-178                \nt Critical one-tail                  1.648498                \nP(T<=t) two-tail                     8.8E-178                \nt Critical two-tail                  1.965642                \n\n\nThe results of the hypothesis test support the hypothesis that the current level of customer satisfaction differs from management\u2019s goal of 6 out of 10. The mean of the respondents shows that majority (36.352) are satisfied with the level of customer satisfaction. When compared against the lowest mean variable of 4.488, it is evident that the management of goal of 6 out of 10 is met. Therefore, hypothesis H01, the current level of customer satisfaction differs from management\u2019s goal of 6 out of 10, is true\n\nHypothesis 2\n\n  * H2: A difference exists between the overall satisfaction of male and female customers at Computers R Us.\n  * H02: A difference does not exist between the overall satisfaction of male and female customers at Computers R Us.\n\nA paired sample t-test is appropriate for this hypothesis.\n\nt-Test: Paired Two Sample for Means\n                                                       \n                                     Male      Female  \nMean                                 210.5     4.488095\nVariance                             14735     5.505825\nObservations                         420       420     \nPearson Correlation                  -0.03265          \nHypothesized Mean Difference         0                 \nDf                                   419               \nt Stat                               34.75257          \nP(T<=t) one-tail                     8.6E-126          \nt Critical one-tail                  1.648498          \nP(T<=t) two-tail                     1.7E-125          \nt Critical two-tail                  1.965642          \n\n\nSince the value of the t-stat is greater than the -t-critical, H2 is rejected, and H02 accepted. We also reject H2 if the p-value is also less than the confidence level of 5%. Thus, we conclude that hypothesis H02, a difference does not exist between the overall satisfaction of male and female customers at Computers R Us, is true.\n\nHypothesis 3\n\n  * H3: There are differences in the overall customer satisfaction across the following age groups: under 20, 21-30, 31-40, 41-50, 51 and over?\n  * H03: There are no differences in the overall customer satisfaction across the following age groups: under 20, 21-30, 31-40, 41-50, 51 and over?\n\nANOVA (Analysis of Variance) is appropriate for measuring the mean between groups.\n\nAnova: Single Factor                                     \n                                                                             \nSUMMARY                                                            \nGroups                Count     Sum    Average   Variance                    \nColumn 1              420       15268  36.35238  171.9185                    \nColumn 2              420       1885   4.488095  5.505825                    \n                                                                             \n                                                                             \nANOVA                                                                        \nSource of Variation   SS        df     MS        F         P-value   F crit  \nBetween Groups        213219.9  1      213219.9  2403.502  2.2E-248  3.852579\nWithin Groups         74340.79  838    88.71216                              \n                                                                             \nTotal                 287560.7  839                                          \n\n\nFrom the data, since F-calculated > F-Critical and we accept the hypothesis. Additionally, the p-value is within the confidence level of 5%, meaning that differences exist in the overall customer satisfaction across the age groups: under 20, 21-30, 31-40, 41-50, 51 and over. Hypothesis H3, there exist differences in the overall customer satisfaction across the following age groups: under 20, 21-30, 31-40, 41-50, 51 and over, is true.\n\nHypothesis 4\n\n  * H4: There are differences in the gender compositions across the five age groups\n  * H04: There are no differences in the gender compositions across the five age groups\nAnova: Single Factor                                     \n                                                                            \nSUMMARY                                                           \nGroups                Count     Sum    Average   Variance                   \n1                     419       88409  211       14665                      \n46                    419       15222  36.32936  172.1066                   \n                                                                            \n                                                                            \nANOVA                                                                       \nSource of Variation   SS        df     MS        F         P-value  F crit  \nBetween Groups        6391810   1      6391810   861.5979  1E-130   3.852606\nWithin Groups         6201911   836    7418.553                             \n                                                                            \nTotal                 12593721  837                                         \n\n\nSince F-calculated > F-Critical and we accept the hypothesis. Additionally, the p-value is lower than the confidence level of 5%, meaning that differences exist in the gender compositions across the five age groups. The hypothesis H4, there are differences in the gender compositions across the five age groups, is true.\n\nHypothesis 5\n\n  * H5: There exists a difference in customer satisfaction based upon \u2018response times in the CompleteCare division\u2019 and the \u2018loyalty rewards program.\u2019\n  * H05: There exists no difference in customer satisfaction based upon \u2018response times in the CompleteCare division\u2019 and the \u2018loyalty rewards program.\u2019\n\nA paired sample t-test is employed for this hypothesis\n\nt-Test: Paired Two Sample for Means              \n                                                             \n                                     Variable 1    Variable 2\nMean                                 3.242857143   5.645238  \nVariance                             4.222502557   7.842817  \nObservations                         420           420       \nPearson Correlation                  -0.011950135            \nHypothesized Mean Difference         0                       \nDf                                   419                     \nt Stat                               -14.09404771            \nP(T<=t) one-tail                     1.69112E-37             \nt Critical one-tail                  1.648498411             \nP(T<=t) two-tail                     3.38224E-37             \nt Critical two-tail                  1.965641764             \n\n\nSince the value of the t-critical is greater than t-stat, we accept the hypothesis. We also accept H2 if the p-value is less than the confidence level of 5%. Thus, we conclude that the hypothesis H05, there exists a difference in customer satisfaction based upon \u2018response times in the CompleteCare division\u2019 and the \u2018loyalty rewards program\u2019, is true\n\nHypothesis 6\n\n  * H6: The initiatives proposed by management relate to the overall satisfaction of Computers R Us customers\n  * H06: The initiatives proposed by management do not relate to the overall satisfaction of Computers R Us customers\n\nA correlation coefficient is applied to answer this question.\n\n                                                          \n            Question 5  Question 6  Question 7  Question 8\nQuestion 5  1                                             \nQuestion 6  0.92481696  1                                 \nQuestion 7  0.00253092  0.023185    1                     \nQuestion 8  -0.0119501  -0.00591    -0.0014     1         \n\n\nFrom the results, question 6 (I am satisfied with the level of advice CompleteCare staff provide on Computers R Us products and services) indicates a strong relationship with satisfaction, because it is closest to 1. The remaining initiatives do not have a positive correlation because they are further from 1. The hypothesis H6, \u2018the initiatives proposed by management relate to the overall satisfaction of Computers R Us customers\u2019 is not convincing.\n\nConclusions\n\nThe results of the current study show that the current level of satisfaction with the company is above the management\u2019s goals. Although customer satisfaction between male and female customers does not exist, there is a difference in the gender composition of the groups. Most of the customers are satisfied with the level of advice CompleteCare staff provide on Computers R Us products and services. This initiative satisfies most Computers R Us customers. This study has successfully measured customer satisfaction with the overall services of Computers R Us. The empirical findings show that customers are satisfied with the service quality of the company. The findings also show that their overall satisfaction outweighs the company\u2019s minimum of six out of ten.\n\nRecommendations\n\nThe company should hold meetings with its senior leadership team to identify how they are to move forward with its goals of satisfying customers. Customers show a general dissatisfaction with the initiatives that have been instituted. Additional research is required to identify this cause of the problem that could be rooted within the new division. This research will identify the root cause of the survey findings amidst all the satisfaction levels.\n\nIt is also recommended that the company encourages on-going assessments to identify the services and support systems that could aid in finding out the nature of problems that customers are experiencing. Open-ended questionnaires are required to complete these assessments.\n\nReferences\n\nKnox. S. et al. (2003). Customer Relationship Management. London, UK: Butterwirth- Heinemann.\n\nSujansky, J. G., & Ferri-Reed, J. (2009). Keeping the Millennials: Why Companies Are Losing Billions in Turnover to this Generation and what to do about it. Hoboken, NJ: John Wiley & Sons.\n\nThomas, J. (2009). Trust in customer relationship: Addressing the impediments in research\u2019, Proceedings of Asia-Pacific Conference on Advances in Consumer Research, 346\u2013349.\n\nYu, C.J., Wu, L., Chiao, Y., & Tai, H. (2005). Perceived quality, customer satisfaction, and customer loyalty: The case of Lexus in Taiwan. Total Quality Management and Business Excellence, 16 (6), 707\u2013719.\n",
        "label": "human"
    },
    {
        "input": "Dell Computer Corporation: Competitive Advantages Essay (Critical Writing)\n\nTable of Contents\n 1. Introduction\n 2. Competitive Advantages\n 3. Resources and Capabilities\n 4. Competitors\u2019 Effort\n 5. Challenges in Matching Dell\n 6. Reference\n\nIntroduction\n\nDell Computer Corporation has been a leader in the personal computer industry for decades. The success of the corporation has led to rival companies re-engineering their operations to overcome competition. Rivkin et al. (1999) claim that Dell remains a company to beat in the personal computer industry despite the initiatives the rival companies have taken. Numerous factors have contributed to the success of the Dell Computer Corporation. They include the company\u2019s distribution model as well as sales and marketing strategy. This paper will discuss Dell\u2019s competitive advantages, its resources and capabilities, competitors\u2019 effort, and the reasons it has been difficult for rivals to outdo the company.\n\nCompetitive Advantages\n\nOne of Dell\u2019s competitive advantages is its mastery of the \u201cdirect model\u201d strategy. Dell Computer Corporation deals directly with corporate clients. A majority of the rival companies like Compaq, Hewlett-Packard (HP), and International Business Machine (IBM) sell their products through retail stores, resellers, and distributors (Rivkin et al., 1999). The ability to contact customers directly allows Dell to manufacture computers that meet the needs of individual clients. According to Rivkin et al. (1999), Dell\u2019s sales and marketing strategy enable it to identify potential clients. The company has divided its customers into two categories: transaction buyers and relationship buyers. The approach allows the company to have a personal touch with each group of clients.\n\nResources and Capabilities\n\nThe success of Dell Computer Corporation lies in its capacity to manufacture personal computers that meet consumer needs within a short period. The company can process customers\u2019 orders and deliver products within one and a half days. Besides, Dell can handle large orders. The ability to respond to customers\u2019 demands within a short period ensures that the corporation establishes a good rapport with clients. The company has a pool of suppliers who ensure that it receives raw materials on time. Effective communication with suppliers has helped Dell to reduce inventory days. The company can quickly direct supplies to appropriate locations. For instance, if its customers require monitors, Dell can instruct Sony to deliver them to clients directly without passing via its facilities. According to Rivkin et al. (1999), Dell has a customized website that enables it to liaise with suppliers. The company uses the site to share manufacturing and ordering information with vendors.\n\nRivkin et al. (1999) argue that Dell has encouraged its suppliers to establish production and storage facilities near its assembly plant. As a result, Dell can easily access all the raw materials that it requires to manufacture personal computers. The products and services that Dell Computer Corporation offers contribute to its success. The company produces two brands of desktop computers. It targets both individual and corporate clients. Besides selling hardware, the company provides numerous services to customers. It installs proprietary and off-the-shelf software. The company also provides financial and asset management services. Rivkin et al. (1999) cite firm infrastructure as one of the resources that contribute to the success of Dell Computer Corporation. The company has a team of seasoned managers sourced from renowned technological companies. They help to streamline operations within the company and monitor market trends.\n\nCompetitors\u2019 Effort\n\nThe domination of Dell Computer Corporation in the personal computer industry has led to rival companies reviewing their strategies. IBM, Gateway, HP, and Compaq have come up with initiatives aimed at competing with Dell. Rivkin et al. (1999) claim that IBM embarked on a strategy to change its distribution channel. The company came up with an Authorized Assembly Program (AAP) that aimed at cutting down on the level of inventory. Eventually, the company managed to enhance the delivery speed of customized personal computers. Besides, the program eliminated the \u201cneed for resellers to \u201ctear down\u201d new personal computers to meet customer needs\u201d (Rivkin et al., 1999, p.10). Apart from working with partners, IBM came up with ways to sell personal computers directly to customers. The company established an independent division called \u201cAmbra\u201d that manufactured and sold low-end personal computers. Later, the company opened a website that enabled it to sell standardized computers directly to customers.\n\nCompaq established an optimized distribution model (ODM) aimed at enhancing its relationship with distributors (Rivkin et al., 1999). The model helped the company to reduce the duration of price protection from seven to two weeks. Additionally, Compaq was able to minimize inventory. Later, the company launched the DirectPlus program that enabled it to market customized computers directly to companies. Even though the program helped Compaq to sell directly to small and medium enterprises, it affected the company\u2019s relationship with resellers. The company had to entice dealers by promising commissions for referrals.\n\nHP launched the Extended Solutions Partnership Program (ESPP) to counter competition that Dell Computer Corporation waged (Rivkin et al., 1999). The program enabled the company to increase the production and sales of personal computers. Additionally, HP established a website that allowed customers to order products directly. However, the company did not sell directly to customers. Instead, it relied on resellers. ESPP helped HP to cut down on the duration of price protection and defects. In 1998, the company established a website dubbed HP Shopping Village. It aimed at using the site to sell refurbished computers directly to clients. However, it continued to sell new personal computers via resellers; a move that did not augur well with many business customers.\n\nGateway came up with numerous initiatives to overcome competition from Dell and slow growth. The company established Gateway Major Accounts, Inc. that targeted educational, corporate, and government institutions (Rivkin et al., 1999). It also created multiple stores across the United States. The move helped to increase its sales volume. Besides, clients could view the company\u2019s goods and place orders.\n\nChallenges in Matching Dell\n\nDespite the efforts that the rival companies have made, it is hard for them to match Dell Computer Corporation. The companies have managed to overcome the issue of the price differential. However, inventory turnover remains a significant competitive advantage for Dell. One of the reasons why competitors are unable to match Dell is the inability to sell directly to customers. Compaq, IBM, HP, and Gateway have to depend on resellers. Even though companies like Compaq and IBM have tried to reduce their inventories, they have not managed to ensure that products are readily available. In some cases, customers have complained that they could not find particular makes of Compaq and IBM personal computers. Besides, the companies have been forced to reconsider their decisions to remain in good terms with retailers, resellers, and distributors. Some dealers have threatened to terminate relations with the companies if they do not change their strategies. In some instances, the rival companies have ended up popularizing Dell Company indirectly. For example, in 1997, the media headlines regarding the effort by Compaq and Gateway signified the success of Dell Computer Corporation. The indirect popularization of Dell has made it difficult for rivals to match the company.\n\nReference\n\nRivkin, J., Porter, M., Bruin, C., Chappel, M., Galizia, T., & Worrell, L. (1999). Matching Dell . Boston, MA: Harvard Business School Publishing.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Information Systems Essay\n\nTable of Contents\n 1. Introduction\n 2. Cost Leadership\n 3. Differentiation\n 4. Focus\n 5. Conclusion\n 6. Works Cited\n\nIntroduction\n\nPorter\u2019s work on competitive advantage indicates that it can either come from low cost or differentiation (\u201cPorter\u2019s Generic Competitive Strategies\u201d). Depending on the competitive scope of the company, which can be either broad or narrow, the firm can follow one of the three competitive strategies: cost leadership, differentiation, and focus (\u201cPorter\u2019s Generic Competitive Strategies\u201d). Today, computer-based information systems are considered to be a necessity for companies operating in the contemporary business environment.\n\nIndeed, if the company uses computer-based information systems correctly, they can become strategic information systems used by businesses to gain a competitive advantage (Rainer et al. 49). The present essay will seek to discuss computer-based information systems in the context of Porter\u2019s competitive strategies and offer examples of how computer-based information systems can be used by different companies to gain a strategic advantage.\n\nCost Leadership\n\nCompanies that focus on cost leadership as their main competitive strategy provide products that have a low price for their value (Amadeo). In order to achieve cost leadership without losing profits, companies need to find ways of reducing operational and production costs. Some companies produce this effect by lowering wages and using low-quality materials (Amadeo). However, this might hurt customer satisfaction and workforce quality, impairing the firm\u2019s success in the long term.\n\nComputer-based information systems, on the other hand, present a sustainable opportunity for reducing operational costs. For example, computer-based information systems can reduce the costs associated with communication or information storage (Rainer et al. 55). Computer-based information systems also allow reducing the number of workers or their workload by automating manual functions. For instance, various accounting programs available on computers can perform sophisticated analysis and other complex tasks, thus decreasing the need for medium or large companies to employ several highly-skilled professionals.\n\nDifferentiation\n\nThe differentiation strategy requires companies to provide a unique product or service features that differentiate it from the key competitors. Computer-based information systems can be used in the differentiation strategy. For example, computer-based information systems can make communication between clients and the company faster and easier (Davies). Robust online customer support and online shop can distinguish the company from its competitors, thus aiding in the implementation of the differentiation strategy.\n\nFocus\n\nFocus strategy is an example of a narrow competitive strategy that includes two main options: cost focus and differentiation focus (\u201cPorter\u2019s Generic Competitive Strategies\u201d). In general, companies that use this strategy aim to target the needs of their consumers better than their competitors. Computer-based information systems offer a wide range of opportunities that can help companies in fulfilling the needs of their customers.\n\nFor example, logistics companies targeting large corporate customers could use computer-based information systems to develop software that would make placing orders easier or allow them to track multiple deliveries. Insurance companies targeting corporate buyers, on the other hand, could use computer-based information systems to build online tools for tailoring insurance plans to the customer\u2019s needs, thus saving their clients time and effort.\n\nConclusion\n\nOverall, computer-based information systems are an example of strategic information systems and are thus useful for the vast majority of contemporary businesses. Not only do they allow improving processes and reducing costs, but they can also help in facilitating the company\u2019s competitive strategy, whether it is based on cost leadership, differentiation, or focus. Therefore, applying computer-based information systems correctly can help businesses to survive in highly competitive markets, while at the same time achieving greater profits and higher customer satisfaction.\n\nWorks Cited\n\nAmadeo, Kimberley. \u201c What Is Competitive Advantage? Three Strategies That Work. \u201d The Balance . 2017. Web.\n\nDavies, Warren. \u201c Advantages of Computer-Based Information Systems. \u201d Chron Small Business , n.d.. Web.\n\nRainer, R. Kelly, et al. Introduction to Information Systems . 3rd ed., John Wiley & Sons, 2013.\n\n\u201cPorter\u2019s Generic Competitive Strategies.\u201d University of Cambridge , 2016. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Emergency Readiness Team Case Study\n\nTable of Contents\n 1. Introduction\n 2. The Role of the US-CERT in Incident and Recovery Management\n 3. ICS-CERT and the Stuxnet Threat\n 4. Feasibility of Alternate Sites\n 5. Planning Required to Prepare for Cyberattacks\n 6. Conclusion\n 7. References\n\nIntroduction\n\nNowadays, cybersecurity is discussed as one of the priorities in disaster and recovery management because of a range of cyber threats to the U.S. industrial systems. These threats need to be addressed efficiently, and the Department of Homeland Security (DHS) is responsible for protecting the nation from cyberattacks and risks similar to Stuxnet (Ferran, 2012; Radvanovsky & Brodsky, 2016). The purpose of this case study is to discuss the role of the US-CERT in disaster and recovery management, the importance of the ICS-CERT for addressing the Stuxnet problem, the role of alternate sites in overcoming the issue, and the planning required for organizations using ICS technologies to respond to cyberattacks.\n\nThe Role of the US-CERT in Incident and Recovery Management\n\nThe US-CERT (the U.S. Computer Emergency Readiness Team) was organized by the DHS to analyze and address cyber threats and attacks to improve incident and recovery management in the country (Radvanovsky & Brodsky, 2016). The role of the US-CERT is important as it works to protect U.S. industrial systems with reference to coordinating cybersecurity activities at the national level. Thus, the US-CERT provides protection for the web infrastructure in the United States, guaranteeing support for public and private organizations through controlling defense against different types of cyberattacks and providing tips and alerts for companies.\n\nWhile discussing the efforts of the team in guaranteeing the preparedness for cyber threats and realizing effective incident and recovery management, it is necessary to state that the US-CERT has significantly contributed to preventing and addressing cyberattacks since its organization in 2003 because of the effective analysis of cybersecurity information (Wilhoit, 2013).\n\nAccording to Ferran (2012), the number of incidents related to cyberattacks increased significantly during 2009-2011, and the US-CERT is responsible for analyzing all these cases and responding to serious ones to guarantee the cybersecurity of facilities and industrial systems. From this point, the US-CERT is crucial for controlling the security of web systems in U.S. industries and providing alerts, as well as coordinating incident and recovery management activities.\n\nICS-CERT and the Stuxnet Threat\n\nThe ICS-CERT (Industrial Control Systems Cyber Emergency Response Team) is critical for defending industrial systems against cyber threats by analyzing reports on incidents, conducting vulnerability analysis, responding to attacks, providing support for organizations, and coordinating recovery activities (Radvanovsky & Brodsky, 2016). The efforts of this team directed towards overcoming the Stuxnet threat include the following steps: Stuxnet was discovered and described in 2010, the complete analysis of the worm was conducted by the ICS-CERT, the team helped organizations to identify attacked systems and proposed the developed incident response plan that included several stages (Chen, Jarvis, & Macdonald, 2014; Ferran, 2012).\n\nIt is possible to state that the incident response efforts were effective enough to address the threat because a complex process to remove Stuxnet from industrial control systems was proposed for the effective implementation in several stages. The ICS-CERT also developed a plan for detecting and addressing further Stuxnet threats. Still, the risks of developing and modifying Stuxnet and similar threats are high (Ferran, 2012). Nevertheless, the efforts of the ICS-CERT can be viewed as appropriate.\n\nFeasibility of Alternate Sites\n\nIt is possible to state that alternate sites, including hot sites, are suitable to be used in organizations that rely on ICS technologies because hybrid systems can guarantee the most efficient cyber protection for industries. Thus, professionals of the ICS-CERT recommend developing hybrid systems based on using innovative ICS components and alternate sites in order to achieve higher results and prevent cyberattacks (Radvanovsky & Brodsky, 2016).\n\nAlternate processing is viewed as one of the most effective strategies in disaster and recovery management, therefore, the use of alternate sites is reasonable in this case. It is important to relocate the most critical data related to systems\u2019 operations to guarantee their protection. From this perspective, the use of hot sites, for instance, is feasible for organizations utilizing ICS technologies.\n\nPlanning Required to Prepare for Cyberattacks\n\nTo prepare for cyberattacks similar to Stuxnet, organizations that utilize ICS technologies need to focus on the efficient planning of their industrial systems. It is important to determine the following parts of the plan: the identification and development of back-up procedures, the development of recovery procedures, and the realization of testing and monitoring (Chen et al., 2014). Alternate processing is one of the components of the plan to guarantee the protection of data and the continuation of operations in spite of cyber threats.\n\nAt the first stage, operations need to be relocated to the back-up site, and emergency operations should be realized. The recovery of the critical components of the system needs to be realized at the next stage. The restoration of other components is completed at the following stages. This approach guarantees the preparation for and protection from cyber threats.\n\nConclusion\n\nThis case study analysis has presented the discussion of the DHS\u2019s activities oriented to protecting the U.S. industrial systems in the context of the Stuxnet threat. The role of the US-CERT has been discussed in detail. The appropriateness of the ICS-CERT\u2019s steps and alternate sites for U.S. organizations has been analyzed. Furthermore, the aspects of high-level planning for these organizations are described.\n\nReferences\n\nChen, T. M., Jarvis, L., & Macdonald, S. (Eds.). (2014). Cyberterrorism: Understanding, assessment, and response . New York, NY: Springer.\n\nFerran, L. (2012). When Stuxnet hit the homeland: Government response to the rescue. Web.\n\nRadvanovsky, R., & Brodsky, J. (Eds.). (2016). Handbook of SCADA/control systems security (2nd ed.). Boca Raton, FL: CRC Press.\n\nWilhoit, K. (2013). Who\u2019s really attacking your ICS equipment? Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Literacy: Parents and Guardians Role Report (Assessment)\n\nFiltering and Monitoring\n\nThe primary role of parents and guardians is to ensure that children are protected from content that might affect their psychological and cognitive development. It is apparent that psychologists have advised parents and guardians to monitor and filter the content accessed by children on online platforms. This is because some of the information might alter their behavior and perceptions in a manner that hinders the desirable growth and development. This implies that it is ethical for parents to install programs that limit the access to various contents on the internet for the safety of their children.\n\nSexual predators have increased on the internet, and it is apparent that children are likely to fall victim of these individuals if they have contact with them through the internet. There has been a heated debate on the ethical implications of filtering and monitoring the usage of the internet and other media platforms for children. The prevailing issues include the fact that continuous monitoring of children might result in negative behavior. This is influenced by children feeling like their parents do not trust them. Additionally, monitoring children and filtering their internet usage might be a sign of over-protection. It denies children the chance to learn through experience, which might affect their social life in the future.\n\nWhile there are several concerns associated with filtering and monitoring practices on the part of parents, it is clear that the main reason for adopting this approach is to ensure that children have limited access to content that is not helpful to their development. Filtering and monitoring is ethical when the content in question involved violence and sexual content. It is important for parents to be open about the content that they are filtering and monitoring so that their children can also refrain from retrieving such content from other computers away from home. Parents can only filter and monitor content on their children\u2019s smartphones and computers, it is important to engage children in conversations about the content being filtered so that they can avoid it when using other gadgets away from home.\n\nParental responsibility in guiding and protecting children from physical and emotional harm is emphasized by many counselors. Whenever children engage in bad behavior such as watching pornography and subscribing to extremely violent media for entertainment, parents and guardians are blamed for allowing children to access such content. It is, therefore, ethical for parents to actively play a role in prohibiting children from accessing such material through monitoring and filtering. Just like the law enforcers patrol neighborhoods and erect cameras to monitor the streets for law breakers, the use of filters and browser monitoring software to reinforce positive behavior in children is ethical.\n\nMany children have expressed concerns about the internet and the sites that they visit. Being targeted by various business entities for the consumption of various commodities, including sex toys, is a common phenomenon, and it is clear that many children appreciate it when their parents are active in ensuring that they do not fall victim to such entities. Filtering and monitoring content on the internet is one of the most important roles that parents in the contemporary world should play, and it reveals that parents care about their children. However, it is important for software developers to design filters that can be customized based on the age of the children to ensure that parents do not appear to be overprotective and hinder children from learning through experience.\n",
        "label": "human"
    },
    {
        "input": "Introduction to Human-Computer Interaction Report (Assessment)\n\nTable of Contents\n 1. Introduction\n 2. Relationship between HCI had Human-Human Interface\n 3. CASA\n 4. Conclusion\n 5. References\n\nIntroduction\n\nHCI describes Human-Computer Interaction . It is a scope of study that explores how individuals view and ponder about computer related technologies, and also investigates both the human restrictions and the features that advance usability of computer structures. Management information structure considers technology as a device to realize certain objectives and insists on organizational effects of assorted appliances.\n\nConversely, HCI regards the relation of humans with computers as a dialogue. It also insists on the significance of interactive properties. This implies that the computer has properties that resemble those of humans. When investigating the issues regarding communication in a computer, it is crucial to concentrate on its shared and behavioural discipline (Lee and Sundar, 2010).\n\nThere are also structural properties of user interfaces, which include modality and interactivity, which affect human, cognitive, sentimental and behavioural responses. HCI in the scope of communication involves perceiving at it as a resource or an interactive channel. The psychology of man-computer relation is primarily founded on the locus of an individual\u2019s bias towards the computer. When individuals perceive the computer as a resource, they relate socially to the equipment itself. However, if they view it as channel the individuals\u2019 social reactions are not inclined towards the equipment but are impacted by it.\n\nCurrent technological developments have broadened computer boundaries. This has been accomplished with the establishment of varied properties of social prompts. These social prompts entail; natural lingo use, ability to interrelate, individual, social functions and anthropomorphic managers. Computers are frequently viewed as a spring of information, as opposed to small channels of messages developed by another person.\n\nAlthough people are alert that computers produce upshots as programmed, they appear to become unaware to the asocial property of communication. Individuals consider utilizing the similar social heuristics with regard to inorganic equipments as they do in person-person relations. With this standpoint, computers are perceived as social performers if not allocated complete humankind, which induce in users a varied collection of social properties (Nass and Moon, 2000).\n\nRelationship between HCI had Human-Human Interface\n\nMany studies have illustrated the operation of social tenets in HCI. However, direct relationships between HCI and individual-individual interface have disclosed some variations between the pair. Participants articulated enthusiasm, made sociable remarks, and disbursed more attempts to achieve the duty when believed that they relating with another individual rather than machinery. Variations between HCI and interrelations among humans become apparent, when comparatively unrestrained communication actions are in query. For instance, persons are likely to utilize relationship inclined assertion and respond to their partners\u2019 impact attempts when relating with humans than computer partners. There are similarities between face to face interrogates and human-computer ones in the sense that in both individuals exude similar emotional responses. This encompass smiling, laughing among others. These non-verbal properties are what make the pair similar.\n\nAs the scope HCI study advances, scholars are shifting from regarding computer technology entirely. These scholars are instead exploring the function of each property transversely technologies. This is attributed to the constraints associated with generalizations of exploring technologies in entirety. Additionally, one application cannot be appropriate for other technologies. This is a serious quandary in HCI study since computer-related technology has been transforming fast resulting in numerous novel devices and Web-related functions. Therefore, learning technology in a variable-based view is advocated as opposed to an object based perspective. This encompasses exploring the functions of the properties entrenched in these technologies.\n\nCASA\n\nCASA refers to the theory of \u2018Computers Are Social Actors\u2019. It explores the applicability of assorted interpersonal, social standards. There are varied studies that have assessed user\u2019s responses to computers bestowed with man-like properties. Distinctively, individuals rank male articulated processors as more adept in mechanical topics than female articulated ones, while the converse is correct for matters such as affection and affiliations (Nass and Moon, 2000). The CASA hypothesis has established that the extent to which individuals interface is similar to humans is in its structure or role, and it influences user reactions. Individuals are stimulated by social likability prejudice in the speaking face situation since it depicts subjects of an actual human.\n\nThis hypothesis has documented that individuals\u2019 reactions to computers are essentially \u2018social\u2019. This implies that individuals utilize social tenets, standards, and prospects central to interpersonal associations when they interrelate with computers. In the lens, of the CASA hypothesis, identifying the circumstances that promote or weaken belief in the perspective of interpersonal interaction and affiliations might aid comprehend the trust active in the human-computer interrelation.\n\nWith regard to the CASA hypothesis, gender typecasting of the processors is in terms of gender-marked animated characters. In addition, another man-like property endowed upon the computer is individuality. Many studies have revealed that individuals not only assume a computer\u2019s personality from oral or para verbal prompts in the borders, but also react optimistically to computers whose traits go with their own. These prompts not only extract social drafts related with them. They also appear to elicit spontaneously social demeanours.\n\nAccording to a research, two information process structures encompass trial and logical. These structures signify intuitive and investigative styles of information synthesizing respectively. The rational structure is a conscious system that is investigative, deliberate, forceful, innately verbal and persisting. The investigational system is a precognisant system that works in a style that is routine, solid, holistic, inherently nonverbal and closely connected with influence (Nass and Moon, 2000).\n\nBased on the records of how machineries are personalized, there is a presumption that more manlike computers aid social reactions to computers. It also supposes that anthropomorphism intensifies sycophancy effects. Additionally, visage depictions result in optimistic social interrelations, especially in conditions of prejudiced estimations. People\u2019s sensibility has moderating impacts. The confidence of individuals with low reasoning is remaining undamaged despite how a computer gives its upshots. This illustrates that investigative thinkers are inclined to make deliberations founded on inconsequential data. This is explicated by the fact that computers attach no worth to data.\n\nConclusion\n\nIn efforts to tackle when and why individuals make social ascriptions to computers, present studies assessed how human likeness of the borders as well as persistent and provisional mindlessness changes their reactions to processors that flatter. Other upshots have revealed that adding anthropomorphic traits does not essentially precipitate user\u2019s inclination. Individuals who exploit computers perceive the personal processor in varied ways. Inclination to computers is drawn from a host of personal, circumstantial and equipment variables. However, computer manipulators comprehend that it does not deserve human ascription (Lee, 2010).\n\nStraightforward meta-analytic contrasts between individual-individual and computer-human perspectives need to be carried out. This is attributed to the knowledge that the loyalty to well-established techniques has been constricted to an individual\u2019s capacity to create experimental models similarly right for individual-computer and individual-individual testing. Other researches need to be carried out to substantiate human-interface relations with regard to perceptions, consistency and cognitive capacity. Furthermore, scrupulous group affiliations need to be performed to investigate the human-interface state.\n\nReferences\n\nLee, E., & Sundar. S. (2010). Human-Computer Interaction. Communication Research.\n\nLee, E. (2010). What Triggers Social Responses to Flattering Computers? Experimental Tests of Anthropomorphism and Mindlessness Explanations . Communication Research.\n\nNass, C. & Moon, Y. (2000). Machines and Mindlessness: Social Responses to Computers. Journal of Social Sciences. The Society for the Psychology Study for the Psychological Study of Social Issues.\n",
        "label": "human"
    },
    {
        "input": "Computer Viruses, Their Types and Prevention Research Paper\n\nIntroduction\n\nComputer viruses are somewhat similar to their organic counterparts since they function under the same principles of infecting a system they are introduced to and focus on replication. However, computer viruses are not a natural aspect of software programs; rather, they are purposefully created to carry out various functions, some of which are malicious in nature. Actions such as compromising the integrity of a computer\u2019s security system, introducing flaws in the programming architecture to cause errors, or even cause the hardware to malfunction resulting in its destruction. These are only a few of the possible actions that a computer virus could be responsible for and, as such, show why it is necessary to know about the different types of viruses out there, how they can infect systems and what measures can a user take to either prevent infection or get rid of one.\n\nTypes of Virus\n\nMacro Virus\n\nThe infection vector of a macro virus is through programs that utilize macros such as.doc,.xls, or.ppp. While the extensions may not be familiar, they consist of Microsoft Word, Excel, and Powerpoint. A macro virus infects these files and spreads when it is shared across various emails and USB drives.\n\nMemory Resident Virus\n\nA memory-resident virus is one of the most resilient types of viruses out there since it resides in the RAM of the computer and comes out of stasis every time the computer\u2019s OS is activated. As a result, it infects other open files leading to the spread of the virus.\n\nWorms\n\nA worm is a self-replicating virus that focuses on creating adverse effects on your computer. This can consist of deleting critical system files, overwriting program protocols, and taking up valuable CPU processing space. Worm infections are identifiable based on process errors happening all of a sudden as well as a noticeable decline in the performance of your computer.\n\nTrojan\n\nTrojan viruses are aptly named since they stay hidden in a computer\u2019s system subtly gathering information. Unlike works, the impact of trojans is rarely felt since their primary purpose is to collect information and transmit it to a predetermined location. Banking information, passwords, and personal details are what Trojans are usually after since this enables malicious hackers to use the information in identity theft as well as to illegally access online accounts and transfer funds.\n\nDirect Action Viruses\n\nThis type of virus takes action once certain conditions have been met such as when they are executed by the user (i.e. opened or clicked). They are typically found in the system directory and infect the various therein; however, some varieties of direct action viruses tend to change location depending on how they were initially programmed.\n\nWhile these are only a few examples, they do represent the various types of computer viruses out there and show why it is necessary to devise different methods of combating them.\n\nWhy is it Hard to Prevent the Creation of Computer Viruses?\n\nThe problem with computer viruses is that they are often created by people that are looking for exploits in computer systems. Since they are intentionally looking for \u201choles\u201d in the security to use, it is not surprising that it is hard to create a truly impregnable system that can withstand all manner of computer viruses. The problem lies in the fact that computer viruses are not static entities; rather, they continue to evolve based on new programming architecture. This \u201cevolution\u201d is not the viruses itself evolving. Instead, it is the programmers themselves who create new viruses based on additional principles they learn as technology and in turn software development continues to improve. It is due to this that attempts at creating more efficient anti-virus prevention solutions are met with new types of viruses that try to circumvent them. The only way this practice were to stop altogether would be if all virus creation were to stop, an event that is highly unlikely to occur.\n\nStandard Practices to Prevent Infection\n\nInstall an Anti-Virus Software Program\n\nOne of the best ways of stopping infection is to install an anti-virus program (ex: McAffee, Symantec, Avast). They specialize in scanning programs, identifying a virus based on information from a database, isolating the file, and deleting it if possible. Do note that anti-virus programs are not infallible since new viruses are created almost every day and, as such, the battle between anti-virus companies and virus creators is never-ending.\n\nDo Not Visit Suspicious Websites\n\nSuspicious websites fall under the category of sites that have questionable content or lack the necessary SSL certificates or verifications. These sites often try to draw visitors via advertisements that indicate that free games can be downloaded from the site, or it has other types of content that a person would usually need to pay for to obtain. Torrent websites are often the most visited of these sites since they offer a wide variety of free content that has been illegally obtained by hackers. However, while it may be tempting to download different movies and games, some of these torrent files are often mixed with viruses that can compromise your system\u2019s security. This can lead to instances of identity theft which can cost you several thousand dollars more than the original price of a movie or game that you illegally downloaded. Utilizing anti-virus programs and following the various instructions in this paper should result in a relatively low chance of your computer.\n\nBe Wary of Foreign USB Drives\n\nUSB drives are a ubiquitous method for sharing information around campus; however, since people tend to share these drives among their friends, there are instances where an infection in one computer can rapidly spread to others from that single USB. It is due to circumstances such as these that computer owners need to be cautious with any USB drive that they accept. If you know that the drive has been continuously shared, you need to perform an anti-virus scan on it.\n\nComplete System Reformating\n\nIn cases where a computer system has become unavoidably infected and has slowed CPU processes to a crawl due to junk data, it is often necessary to perform a complete system reformat. A system reformats consists of the OS and BIOS of the computer being erased and reinstalled. This helps to get rid of any viruses that remain and enables the computer to work properly again, though this is at the cost of all the files on the computer unless they have backups.\n\nConclusion\n\nAll in all, computer viruses can cause considerable damage if the proper precautions are not taken. Utilizing anti-virus programs and following the various instructions in this paper should result in a relatively low chance of your computer.\n",
        "label": "human"
    },
    {
        "input": "Computers in Security, Education, Business Fields Report\n\nTable of Contents\n 1. Introduction\n 2. The scope of computers in the human life\n 3. Security dependency on Computer\n 4. Education dependence on computers\n 5. Employment\n 6. In business\n 7. Globalization\n 8. Conclusion\n 9. References\n\nIntroduction\n\nIn the 21 st century, humans have seen an increasing development in a number of areas in their lives. These developments have constantly affected their lives, their relationship with each other, how they communicate, and perform their duties. The development of computers is a major breakthrough in the history of humanity which has drastically changed the way humans live nowdays. The entire world has become a global village owing to the change in interaction influenced by computers. In the first few years when computers were developed, they were not widely used in order to perform human functions. In those days, people were still uninformed technologically; most of the human activities were handled manually. In the 21 st century, that situation has changed and computers have become part and parcel of the human nature. As a result, computers influence the way we live, the way we communicate, the way we interact with each other touching almost every aspect of the human nature.\n\nThe scope of computers in the human life\n\nHumans are rapidly changing and becoming inclined to the use of computers more than any other electronic gadget in the world today. The scope of computer usage is expanding every single day ranging from health to education, security to communicate and so on (Kizza, 2010). Computers are vital in hospitals as they are being used to monitor patients, make diagnosis among many other medical functions. Actually, a hospital can hardly deliver its services to patients without the use of computers. It is not, however, the only critical function of computers in a hospital (Kizza, 2010). There are other scopes, for example, the record keeping system, which requires the use of computers to run effectively.\n\nSecurity dependency on Computer\n\nSecurity, on the other hand, is a major concern in the world today. Security agencies face greater challenges in the current world than in the past centuries. This has prompted the agencies to consider other alternatives in managing and curbing crimes other than the manually patrols around towns and areas of great insecurities (Kizza, 2010). Computers have come in handy to solve this issue and currently the results are proving to be good. In major cities in the developed world, the streets and highways are monitored using CCTVs which transmit their images to computers. Security agencies of the 21 st century depend greatly on the use of computers to maintain peace and order more than they do with physical patrols.\n\nEducation dependence on computers\n\nEducation is another area where computers have infiltrated and have become virtually detachable. It is impossible to picture a student without his or her computer being an impossible endeavour. To show how the use of computers has become important at schools, administrators in the majority of schools, especially in the developed countries make it a priority to embrace and have an ICT department for computer studies (Grieco, 2009). It has become a requirement for all students especially in colleges and universities to have basic knowledge on how to work with a computer. Although not necessarily through policies, rules and regulation, this requirement is necessitated by other factors within the education system. For instance, in institutions of higher education, students are required to hand in their homework and assignments in the form of soft copies. That means they have to type in their work and submit it using computers. Even when a hardcopy is required, handwritten material is of less preference. In addition to that, a lot of research work requires the use of the Internet, hence making computers a necessity (Grieco, 2009).\n\nEmployment\n\nEmployment is one of the most important aspects of human life. In most companies today, the use of computer is widely spread creating problems for people who have skills in using computer. Lowposts, such as secretariat positions are today requiring one to have at least the basics of computer. Such requirements have, consequently, kept some people off the job bracket. The need for computer skills as a factor of production in the majority of companies has made the entire world depend on the computer (Grieco, 2009). This has enhanced our computer dependency since it determines our livelihood through employment. Computers today have controlled every business activity, be it service oriented or production of goods. Financial and money markets exist due to the appearence of computers, further enhancing human decency on this gadget. The impact of computers in such important areas of our lives has made us as humans greatly rely and depend on them for our livelihood (Grieco, 2009).\n\nIn business\n\nIn the business world today, there has developed a lot of dynamics that has facilitated and enhanced the use of computers. Financial records and the assignment of salaries have been simplified and made effective by the use of computers. This has made employers resort to the use of computers to minimize the cost of such tedious activities (Wagman, 2003). Such developments have undermined the use of the old procedures hence replacing them. A number of manual activities have been abandoned in favour of new computer technology. Setting up the systems that facilitate the use of computers makes it very difficult for a company to abandon its use in the future. This is another factor that has tied individual, companies and other entities, for intsnace, the government to the use of computers.\n\nGlobalization\n\nThe current mobilization of the world\u2019s countries to unite and form one huge village has helped in enhancing the use of computers. Due to the structure of communication and other infrastructure, the world has become a village whereby a lot of activities are happening with less mobility. A large number of business opportunities are arising through the internet which requires one to have a computer. This means that if an individual lacks the skills on how to use a computer, he or she is locked out from accessing such opportunities. The Global village has come with great demands that can only be met by the use of computers. Governments need to keep in touch with fellow governments as the new demand for information is rising at an alarming speed.\n\nConclusion\n\nComputers have had a direct impact on the human existence, and although we can directly attach its impact to individuals, it has a major role in the activities and roles of our livelihood. The high dependency on computers is influenced by its efficiency and competence capturing most of the functions and roles performed by humans. This is even worsened by the fact that in many ways computers work and perform better than humans, hence creating a demand for them. In other words, every essential area of our lives is now dependent on the use of computers making the entire human race depend on them.\n\nReferences\n\nGrieco, J., M, (2009). Between Dependency and Autonomy: India\u2019s Experience with the International Computer Industry. California, US: University of California Press.\n\nKizza, J., M, (2010). Ethical and Social Issues in the Information Age . New York, NY: Springer.\n\nWagman, M, (2003). Reasoning Processes in Humans and Computers: Theory and Research in Psychology and Artificial Intelligence . New York, NY: Greenwood Publishing Group.\n",
        "label": "human"
    },
    {
        "input": "Graph Theory Application in Computer Science Essay\n\nIn the field of mathematics and other exact sciences, there is a range of theories that are aimed at explaining and studying theoretical concepts that can be implemented into practice. The graph theory can be regarded as one of the brightest examples of such concepts. Even though there are numerous problems that have not been solved yet, and certain hypotheses within the frame of the theory are not confirmed, such structures as graphs are widely used in different fields. Speaking about the field of computer networking, graphs can be used to describe and study data transfer systems; in addition, they remain an important tool helping to manage social networks and generate recommendations for users.\n\nAs it follows from its name, the discussed theory is aimed at studying graphs that present structures that consist of numerous knots that form unities with the help of verges. Importantly, the structure of a graph can be different due to the number of ends and knots that may vary. On that premise, it is widely accepted that there are about ten types of graphs, having different numbers of vertices. Speaking about the classification of graphs and the way to apply them, it needs to be noted that different graphs present structures helping to represent data related to various fields of knowledge. For instance, graphs can be used in linguistics, biology, and chemistry as they help to solve numerous problems related to studying structures of chemical substances, food chains. Apart from that, the use of graphs helps to define and describe processes taking place when new lexical units become popular and languages change. As it follows from these examples, it is obvious that the graph theory can be applied almost in every field of science because scientific knowledge always requires systematization.\n\nWhen it comes to the field of computer science and networking, it is necessary to state that possible applications of graphs of different types are numerous. Speaking about the latter, it is important to pay attention to the task related to representation of data describing systems for data transfer. Thus, specialists in networking use graphs to outline systems of computer networks, study their properties, and implement changes. In fact, graphs used for that purpose help to create a kind of mathematical model that indicates the interconnection between the parts of system. When it comes to the most common examples, illustrating this way to use graphs, it is necessary to remember the Internet. The structure of the Internet can be presented with the help of graphs, and this way to use the discussed theory definitely helps to identify possible weaknesses of systems based on the type and properties of completed graphs (Xu, Wang, & Gu, 2014). This way to apply concepts from the graph theory is extremely significant to the field as it can be used to plan new networks and improve the older ones.\n\nApart from planning and studying networks, specialists in networking can use graph theory in order to provide services in social networks. The so-called social graph is a type of graph that includes a range of user profiles and represents links between them. Apart from that, social graphs can also be implicit. Such graphs also include information on interactions with other users but the types of links are less defined. The use of graphs presenting social links between users of popular social networks can be regarded as a way to fulfil numerous tasks. For instance, the data retrieved with the help of such graphs can be used in order to create proper recommendations for users. The latter may include messages and lists that indicate \u201cpeople you may know\u201d based on the information related to users\u2019 friends and their activity in social networks (Jiang et al., 2013).\n\nApart from that, graphs can be used to generate recommendations related to media content based on activity showing preferences or users and their personal information. At the same time, it is important to note that graphs containing information on the activity of users can help to retrieve information that users may want to hide (such as the presence of fake accounts used for different purposes). In fact, if these accounts are used to operate on the fringes of the law, graphs can be successfully used to study the most recent activity of the user and identify his or her real personal details (Jin, Chen, Wang, Hui, & Vasilakos, 2013). Therefore, there is a number of ways to use graphs in social networks and they can sometimes prevent illegal activity in social networks. Even though some users do not want their personal details and data on social interactions to be processed and studied, there is no doubt that graphs in social networks help to simplify many processes and provide people with relevant information.\n\nHaving analyzed these ways to apply graphs in networking, it is possible to state that graph theory has advanced the knowledge in this field. In general, it is clear that graphs help to visualize any information to make it easier to understand. Importantly, graphs can be listed among the most common tools that are used for the purposes of network analysis by modern specialists. As is clear from the discussed examples, representing data with the help of graphs, it is possible to keep track of users\u2019 interactions and fulfil a range of tasks helping to manage social networks.\n\nAlso, the use of graphs remains an important tool as it can help to control computer devices that form a network and create plans that can be easily understood. Analyzing the role of graph theory in the development of networking, it is possible to state that a range of conclusions made by researchers developing the theory can be applied to problems of computer networking related to connectivity. In other words, a graph often acts as a good tool allowing to track changes in computer networks. Taking into consideration all the ways to use graph theory in computer science and networking in particular, it needs to be stated that their role in advancing knowledge in the field and enhancing effectiveness of networks cannot be overestimated.\n\nIn the end, it is clear that graph theory can be applied by specialists in computer science and networking in numerous ways. Personally, I will apply the theory in order to use various computer network construction schemes and check if they are appropriate for a particular situation. At the same time, the theory can be used when there is a need to solve theoretic problems \u2013 representing the data with the help of graphs, I will make it more illustrative. Therefore, graph theory can be applied to solve both theoretical and practical problems.\n\nReferences\n\nJiang, J., Wilson, C., Wang, X., Sha, W., Huang, P., Dai, Y., & Zhao, B. Y. (2013). Understanding latent interactions in online social networks. ACM Transactions on the Web (TWEB) , 7 (4), 18.\n\nJin, L., Chen, Y., Wang, T., Hui, P., & Vasilakos, A. V. (2013). Understanding user behavior in online social networks: A survey. IEEE Communications Magazine , 51 (9), 144-150.\n\nXu, K., Wang, F., & Gu, L. (2014). Behavior analysis of internet traffic via bipartite graphs and one-mode projections. IEEE/ACM Transactions on Networking (TON) , 22 (3), 931-942.\n",
        "label": "human"
    },
    {
        "input": "Life Without Computers Essay\n\nTable of Contents\n 1. Introduction\n 2. Communication\n 3. Health Care\n 4. Human Skills\n 5. Conclusion\n 6. List of References\n\nIntroduction\n\nWhat do you think a life without a computer would look like? Technology is a fundamental aspect of our society and therefore it is hard to imagine existing without it. Read \u201cA World without Computers\u201d essay below to find out more. Since the development of the first computer, technology experienced an exponential growth that has led to the concept of globalization and the enhancement of the quality of life for people across the world. It is apparent that the computer technology fostered many positive changes in the society, and computers are an indispensable part of the modern society because they have enhanced the survival of humans.\n\nCommunication\n\nOne of the major contributions of the computer technology in the world has been the enhancement of the quality of communication. The computer technology alongside the discovery of the internet has eliminated the slow communication platforms that were used in the past (Boothroyd 2011). For instance, without the computer technology, people would have to communicate with their distance relatives through letters. It would be extremely slow to share information with relatives from different parts of the world, and it would be especially difficult to be updated on different global developments (Olsen 2012).\n\nComputers facilitated instant messaging and access to information from the mass media companies across the world. Additionally, the growth of the social media platforms has made the world a global village where people from different parts of the world can share information easily. Living without the computer technology would be characterized by slow and limited access to information.\n\nHealth Care\n\nLiving without the computer technology would also be characterized by a high mortality rate because the quality of the health care services would be very low compared to the status in the modern world. It is apparent that computers have facilitated the development of technologies that have enhanced preventive health care in the world, as well as the development of medications for various illnesses. Computers are used to analyze different specimens in laboratories to detect illnesses and their potential remedies (Hawkin 2012). Without computers, health care providers would only rely on the knowledge of symptoms to treat patients. This would result in many deaths because illnesses like cancer would not be easily detected without the use of the computer technology.\n\nHuman Skills\n\nThe computer technology has become an indispensable aspect of the modern world. Scientists have overly concentrated on the enhancement of the capabilities of the computers instead of investing in the enhancement of human skills (Baldry 2012).\n\nIt is apparent that computers have taken over different jobs and made human skills obsolete (Ford 2016). The world without computers would compel the authorities to invest in human skills and knowledge, and this would result in a society that is highly specialized in different doctrines. This implies that the rate of unemployment would be relatively low across the world because the corporate sector would require a high number of employees to handle the tasks that computers perform on a daily basis.\n\nConclusion\n\nLife without computers would be difficult for humankind because the modern world has overly depended on the computer technology to enhance the quality of life. Communication would be relatively slow and limited, and the quality of health care services would be very low. However, the world without computers would compel the authorities to invest in educating the human assets in different doctrines to cover the gap that would be created by the lack of computers. It is vivid that computer technology has had a positive impact on the global society.\n\nList of References\n\nBaldry, C 2012, Computers, Jobs, and Skills: The Industrial Relations of Technological Change , Springer Science & Business Media, Berlin.\n\nBoothroyd, J 2011, From Typewriters to Text Messages: How Communication Has Changed , Lerner Classroom, Minneapolis.\n\nFord, M 2016, Rise of the Robots: Technology and the Threat of a Jobless Future , Basic Books, New York City.\n\nHawkin, D 2012, Twenty-first Century Confronts Its Gods, The: Globalization, Technology, and War, SUNY Press, New York.\n\nOlsen, K 2012, How Information Technology Is Conquering the World: Workplace, Private Life, and Society , Scarecrow Press, Lanham.\n",
        "label": "human"
    },
    {
        "input": "The Accounting Cycle: Phases and Computerized System Research Paper\n\nTable of Contents\n 1. Four Phases of the Accounting Cycle\n 2. Benefits of a Computerized System\n 3. References\n 4. Footnotes\n\nFour Phases of the Accounting Cycle\n\nThere are four main phases of accounting that will be reviewed within the framework of this paper. The first stage is recording. This phase delineates all the activities related to the process of documenting all the available financial information and entering it into the database of the company 1 . These accounting records can be used for different activities as they include an array of various variables and values (including the budget of the company in addition to its business plans, taxes, and financial statements).\n\nWithout this particular phase of accounting, the process of determining the monetary outlays would become problematic. The process of recording monetary information can be met in two different environments \u2013 business and personal 2 . It is during the recording stage where the accountant has the possibility to divide the existing transactions into categories. In the future, this information will be used for tax purposes as their correct management may save a lot of resources for the company.\n\nThe second phase is called classification. Throughout this stage, the accountant determines the amount of expenditures and the number of transactions that were completed over a certain period of time 3 . They need it to be able to categorize the existing data correctly. This accounting phase may help the accountant to perform necessary deductions at the end of the fiscal year. The third phase is summarizing the information. Right after the first two stages, the accountant has to pay close attention to summarizing the information in the approved manner 4 . In order to do that, they go through the numerous sheets (either digital or paper) of information that contain the data regarding all the fiscal operations of the company.\n\nDuring this phase, the accountant can find out the overall amount of expenditures, salaries paid, annual/ monthly savings, and much more 5 . This stage is necessary to facilitate the next accounting phase. The fourth stage of the accounting cycle is the interpretation of the data. This stage is crucial because it is the only way to identify any possible or applied changes that either impacted the company or are going to do so in the future. This phase provides the accountant with the possibility to predict the financial future of the company as well. A correct interpretation of the available data is necessary if the company wants to organize its budget better and save money to purchase innovative equipment or evaluate stock information.\n\nBenefits of a Computerized System\n\nThere are several benefits of a computerized system that significantly contribute to its efficiency throughout each of the stages outlined within the framework of the previous subsection of the paper. The very first advantage of computerized systems is the ability to record data at high speeds without the loss of precision 6 . This claim can be rationalized by the idea that they can enter the data into the system much quicker than their conventional analogs.\n\nAnother advantage of the computerized systems is that they are equipped with a built-in error detection mechanism by default. Nonetheless, it has to be mentioned that in some cases, it may be better to perform a manual cross-check 7 . The importance of a computerized system cannot be underestimated when it comes to recording information. Second, there is the stage of summarizing the data. Here, the users of computerized accounting systems can significantly benefit because of the possibility to create reports that pull all the required information from the database entries and create a brief excerpt 8 . At this stage, computerized accounting saves a lot of time for the person that deals with accounting at the company.\n\nDespite the ability to summarize the data programmatically, the accountant may want to go through the records manually so as to gain more insight into the financial operations that were performed during a certain period of time. The stage of summarizing data can be significantly facilitated by means of computerized accounting. Third, there is the stage of classifying the available data.\n\nAt this stage, computerized accounting systems may be rather useful because they allow the end user to manipulate the excerpts created throughout the previous stage 9 . The key advantage of a computerized accounting system when it comes to classifying the data is its ability to compile the information almost at the same time as the request is sent to the program. If we compare a computerized system to its manual counterpart, we will see that the latter may take a lot of time in terms of locating the necessary data and compiling it correctly 10 .\n\nFourth, there is the interpreting stage. Throughout this phase, manual accounting systems cannot compete with their digital rivals because this accounting stage is the most resource-intensive out of all four. Here, the use of manual accounting systems will slow down the process of completing the task and may have a confusing effect on the accountant who is responsible for interpreting the data 11 . If necessary, both computerized and manual accounting systems can be combined but it is not recommended.\n\nReferences\n\nGilbertson, C. B., Lehman, M. W., & Ross, K. E. (2014). Fundamentals of accounting . Mason, OH: South-Western.\n\nNg, P. (2014). 21st century computer solutions: A manual accounting simulation . Bloomington, IN: Authorhouse.\n\nWang, X. (2015). Financial management in the public sector: Tools, applications, and cases . Abingdon, UK: Routledge.\n\nWeygandt, J. J., Kieso, D. E., Kimmel, P. D., Trenholm, B., Warren, V., & Novak, L. (2016). Accounting principles (12th ed.). Toronto, ON: John Wiley & Sons Canada.\n\nFootnotes\n\n 1. Weygandt et al., 2016.\n 2. Ng, 2014.\n 3. Gilbertson, Lehman, & Ross, 2014.\n 4. Wang, 2015.\n 5. Ng, 2014.\n 6. Gilbertson et al., 2014.\n 7. Weygandt et al., 2016.\n 8. Gilbertson et al., 2014.\n 9. Ng, 2014.\n10. Wang, 2015.\n11. Ng, 2014.",
        "label": "human"
    },
    {
        "input": "Epistemic Superiority Over Computer Simulations Essay\n\nTable of Contents\n 1. Introduction\n 2. Background Information\n 3. Experiment vs. Simulation\n 4. Inferential Power\n 5. Practical Consequences\n 6. Conclusion\n 7. References\n\nIntroduction\n\nFollowing advancements in information technology, the scientific practice in the current century has increased the debate on whether experimentation is the same as simulation. In the past, scientists have relied on experimentation when carrying out different studies. This trend has begun to change since scientists are now combining experimental and simulation methods. This change has developed a new way to view and do science.\n\nBesides, the new trend has \u201ccreated opportunities to review the roles that experiment and simulation play in the scientific process\u201d (Parke, 2014, p. 521). In this paper, I seek to support the widespread perception among philosophers of science that experiments have epistemic superiority over computer simulations. However, I also build on this debate by showing that the distinction between experimentation and simulation reveals more information much concerning epistemic value.\n\nTo defend the view that experiments have epistemic superiority, there are two main challenges that have to be addressed. The first challenge is to explore and determine the source of the epistemic privilege of an experiment. The second challenge is to show why this comparison is essential.\n\nBackground Information\n\nAn understanding of experiments as interventions in real-world systems differentiates them from simulations. Field experiments are differentiated from laboratory experiments simply according to their location (Barberousse, Franceschelli, & Imbert, 2008).\n\nUnlike within the confines of a laboratory, where scientists have control over numerous variables, field experiments might entail manipulating a single variable, such as preventing a certain species from a given geographic location. In this light, experiments are viewed as generating more reliable and trustworthy scientific knowledge. Simulations are viewed as only necessary when experiments are too costly or impossible.\n\nEven though they might be less comprehensive or fertile than experiments done for the generation of knowledge, simulations should not be viewed with suspicion and contempt because they have the potential to generate scientific knowledge. Parke (2014) claimed that such opinions on the inferiority of simulations to experiments are mere generalizations. Experiments allow for more direct and comprehensive analysis.\n\nExperiment vs. Simulation\n\nThere is a widespread assumption that experiments are more precise than simulations, in the sense that they use tangible objects of study. This assumption plays a role in the concern among various experimental scientists that simulations are chosen merely because they are cheap. Consequently, this assumption might hinder the discovery of new knowledge about the world (Barberousse et al., 2008). According to Fallis (2007), simulations only focus on revealing the consequences of the knowledge that already exists. This assumption is backed by the idea that a computer-based simulation is simply revealing the consequence of various theoretical assumptions. Therefore, all information in the results of the simulation reflects what was present in the theoretical assumptions.\n\nVarious considerations play major roles in one\u2019s ability to draw conclusions regarding epistemic superiority. First, how much knowledge is available about the object and the target? According to Parke (2014), when the background knowledge is limited, \u201ca physical sample of the target or close approximation is an essential starting point, and in other contexts, such information is available from the world making simulations reliable methodology; for instance, the study of molecular bond angles in chemistry\u201d (p. 325). Enough information is available about chemical bonding to allow questions to be answered through computer simulations concerning how atomic substitutions will affect the bond angle in a certain molecule. When very little is known, experiments tend to have epistemic value.\n\nParke (2014) also observed that simulations are seen to have less impact than experiments on the generation of scientific knowledge. Essentially, the claim is that experiments create better inferences about phenomena or natural systems compared to simulations. Experiments have proven to be reliable guides to a generation of scientific knowledge. Parke concurred that experiments have a certain level of epistemic value because she acknowledged that all scientific inquiry entails engaging with some objects of study\u2014a model, a physical structure in the laboratory, or a combination of both\u2014to gain insights about some target of research.\n\nThis aspect implies that the methodological difference between experiment and simulation is fundamental for compiling conclusions regarding epistemic value. However, Parke refuted this claim by showing that this situation is applicable only in a context-sensitive manner, but not as an abstraction across science. Based on this notion, Parke insisted that differences between experiments and simulations should not be employed as a foundation for generalizations.\n\nIt would be rather irrational for one to conclude that experiments had no epistemic advantages over simulations. If there were no significant epistemic privilege, why would scientists choose to invest large sums of money in experimental setups when they could buy a supercomputer for simulation? Even though some philosophers such as Parke (2014) have concluded that it is difficult to determine the epistemic difference between experiments and simulations, it is essential for one to understand the distinction between things that are not rationally implied in people\u2019s prior knowledge.\n\nIn this case, simulations are not in a position to solve problems, but experiments are reliable. In other words, simulations can only assist in solving issues that exist within the deductive closure of one\u2019s prior knowledge. Thus, Parke declared that computer \u201csimulations do not produce new empirical data\u201d (p. 326).\n\nMoreover, experiments create higher inferential power. Gendler (2010) suggested that experiments are in a good position to generate credible views concerning their objects of study. This scholar also views experiments as powerful sources of meaningful surprises or valid novel knowledge. However, I do not agree with this claim for just because a case of scientific inquiry counts as an experiment or simulation, it is not proof of epistemic value.\n\nUndoubtedly, there is a great deal of critical work that ought to be conducted if the inferences from objects of research to subjects of inquiry in the natural world are to be realized. By arguing that there is an epistemic value between experimentation and simulation, I do not imply that such a difference is impossible. There are ways to conduct meaningful computer simulations, but only because humanity has learned about the world via observation and experiment.\n\nFocusing on the entire issue, empirical data are essential for solving scientific issues regarding the natural world. Nonetheless, there are two limitations to this claim. First, even though the information is present in theoretical assumptions, it is not obvious that it is known. Thus, even if a computer simulation is simply a calculation, it might generate new knowledge.\n\nInferential Power\n\nParke (2014) rejected the supposition that experiments possess epistemic value over simulation, claiming that this view is pervasive among philosophers of science. Parke further claimed that the epistemic superiority of experiments over simulations is founded on the claims of their inferential ability. Parke claimed,\n\nBelief in the epistemic privilege of experiments over simulations is often grounded in ideas about their relative inferential power. In particular, the idea is that experiments lead to better inferences about natural systems or phenomena than simulations do (this is sometimes referred to as the issue of external validity). This difference has to do with the relationship between their respective objects of study and targets of inquiry (p. 75).\n\nGreca, Seoane, and Arriassecq (2014) argued that the tangible objects of study are the distinguishing features of experiments. Parke referred to these widespread views as the \u201cmateriality thesis\u201d (p. 75). On the contrary, \u201cexperiments\u2019 elements or objects reproduce parts of the natural world while simulations\u2019 objects represent sections of the world\u201d (Gendler, 2010, p. 118). Because the form of a subject or thing in the laboratory is similar when outside the laboratory, experiments manifest more inferential power. This assertion holds because the objects under study are similar to what is in the real world.\n\nThis fact makes inferences about the world possible and simple. In this case, the knowledge has epistemological implications. Scientists tend to feel justified when they reveal new insights into the world through experiments because the world and experiments possess similar characteristics of objectivity.\n\nTherefore, there is undeniably some veracity in the materiality thesis. Experimenting on a real specimen or sample in the natural world is the ideal way to get a grip on acknowledging the process when very little is known about the target under study. For instance, it would be more convincing and idealistic for scientists carrying out laboratory experiments to use living organisms like rabbit fro experiments as compared to using computer simulations of living systems. Computer simulations may not provide the objectivity found in physical objects of study.\n\nGendler (2010) posited that the \u201cthe fact that some experiments operate directly on the target system provides epistemic privilege over simulations\u201d (p. 126). For instance, if physicists aim to know whether the white light is made of various colors, they can shine a beam of light over a prism to find out. In this experiment, \u201cthe object, which is the beam of light, is an instance of the target (light) and the fact that experiments can operate on the target system is enough evidence that they have epistemic reach beyond that of simulations\u201d (Parke, 2014, p. 330).\n\nThe fact that simulations do not incur large sums of money is their only advantage, but it also means that the results might not reflect the real world. Parke (2014) does not concur with this perception and insists that similarity, not materiality, is what matters when justifying certain inferences concerning target systems. However, Parke failed to offer a thorough account of how to determine relevant similarities.\n\nExperiments in \u201cvirtue of the nature of their objects can surprise in a way simulations cannot\u201d (Parke, 2014, p. 331). The exponents of the surprise claim share the notion that experiments and simulations vary significantly, either by value or content. Although surprises arise in simulations, what transpires in a simulation is that familiar and new insights are not anticipated. Arguing from a qualitative viewpoint, Greenwald (2004) showed that although simulations may be in a position to surprise, experiments can both surprise and confound.\n\nParke (2014) insisted that various computer simulations undoubtedly share some epistemic functions with experiments by asserting that simulations are run to offer new data systems that are complex or impossible to investigate with the normal instruments like lab equipment. For example, neutron-matter interaction has been widely tested using molecular dynamics simulations.\n\nGrune-Yanoff and Weirich (2010) suggested that to determine the distinction between simulation and experimentation, it is essential to compare the two methods with other constant factors. For instance, a researcher conducting a biology test might dissect a mouse to study the internal organs as well as the skeleton. The same researcher cannot obtain the same level of experience from a simulated dissection online.\n\nSimulations manifest how the programmer thinks the actual world should be rather than how the real world responds when explored by a student. Besides, learning new knowledge in science entails being a skilled observer, and a precise recorder of every detail observed rather than being a participant in a computer game claiming to teach about the natural world. However, simulations and experiments can be viewed as complementary rather than competitive activities.\n\nWhen viewed in this manner, simulations can be helpful when cheap and faster procedures are needed. Simulations offer a high degree of flexibility. Thus, numerical experiments can be accomplished by turning certain terms on or off and by varying input parameters. Both experiments and simulations are dynamic procedures.\n\nPractical Consequences\n\nParke\u2019s (2014) claims are based on an intuition that an experiment has epistemic privilege, not that it is free of pragmatic constraints. However, in this paper, my defense of epistemic value implies that the superiority claim is a principled one because it signifies that in a certain situation, there never exist actual choices between an experiment and an equivalent simulation. This suggests that in any given situation, the experiment overrides simulation in the realm of epistemic utility.\n\nThe difference between experimentation and simulation manifests epistemic value. Parke also claimed that the only difference is existing between the two models matters for pragmatic reasons. Based on this claim, it is a fact that performing an experiment is more costly than performing a simulation. The materials, reagents, and the workforce required to do an experiment tend to cost more than conducting a model on a computer. Under simulation, one can see the progress and results faster as compared to the normal process of carrying out an experiment.\n\nUnder certain circumstances, several logical simulations provide consistent results, but they will need more background information than experiments do. One of the ways to \u201cdetermine the superiority\u2019s practical salience is that relative to a given set of background information, there are cases when simulations will give arbitrary answers where else experiments provide defined answers\u201d (Gendler, 2010, p. 122).\n\nThis observation implies that pragmatism is not the only essential factor considered when selecting between an experiment and a simulation. For instance, in a given test or question, one cannot justify selecting a simulation over an experiment on the grounds that it is less costly.\n\nEven though such choices might not be made consciously, it is necessary to conduct careful research before deciding which model to adopt. In some cases, those who deny epistemic superiority argue that experiments calling for the deliberate interference of human beings are detrimental and unethical. However, even though the epistemic value might not outweigh the moral cost, it does not mean that there is no epistemic privilege.\n\nAccording to Gendler (2010), arguing on a pragmatic basis could mean more barriers in obtaining reliable results to questions scientists choose to investigate. For example, in a case where a scientist conducts an experiment, resources could be saved by probing into questions that might require more than one simulation to attain reliable results. For clarity, take the case of smallpox, which was eliminated in the 1980s.\n\nThe scientific community has raised debates seeking approval to destroy the remaining stockpiles of the variola virus that causes smallpox in an attempt to minimize the chances of future infections. Recently, research by the World Health Organization has highlighted specific, medically essential research that could not be possible if the existing stockpiles of the virus were destroyed (Greca et al., 2014).\n\nThe reason is that there is not enough knowledge of the virus to answer questions solely through simulation. This case provides evidence that there exist some questions that can only be answered with experiments on the virus itself. However, I think it is justifiable to consider disposing of the stockpiles. This assertion implies that studying a \u201cmaterial system as opposed to a computer model manifests better inferences and opportunities to uncover surprises\u201d (Greca et al., 2014, p. 899).\n\nTo this end, it is necessary to consider some of the concerns that Parke (2014) has raised about the materiality thesis and in particular, her view that material correspondence does not necessarily involve higher inferential power and that it is hard to make sense of the difference between material and formal object\u2013target correspondence.\n\nWinsberg (2008) is sympathetic to the conclusion that such differences exist, although they might not mean much in the present when simulations should be used to complement experiments. Parke (2014) showed that there are exceptions to the generalization that experiments have epistemic privilege, but thinking in terms of such generalizations is misleading when judging the epistemic privilege in cases of scientific inquiry.\n\nParke emphasized that researchers should not look to the experiment/simulation distinction to generalize anything in principle about epistemic value. Contrarily, I have shown that multiple cases in experiments are assumed to have the higher epistemological prerogative as compared to simulations. Even though inferential power and capacity to create surprise may not be broadly generalizable across science, they offer important cues to support the epistemic superiority of experiments over simulations. Studying a material system as opposed to a simulation might not necessarily lead to perfect inferences, but at least there is an increase in reliability.\n\nConclusion\n\nIn this paper, I acknowledge that simulation is a critical new tool for scientists. Even though simulation shares many aspects with experiments, experiments possess a dynamic epistemic capacity. Although the foundations of simulations are still shaky, experiments seem to have gained ground. Simulations can allow one to undertake a study within a very short time and with limited resources.\n\nHowever, this practical advantage comes with an epistemological price. This assertion holds that studying a model as opposed to a material system entails sacrificing realism, which undermines epistemic value. Nevertheless, more information is essential before concluding that one model has epistemic value over the other. Therefore, more research is needed on this subject of the superiority of experiments over a simulation.\n\nReferences\n\nBarberousse, A., Franceschelli, S., & Imbert, C. (2008). Computer simulations as experiments. Synthese Journal , 169 (3), 557-574.\n\nFallis, D. (2007). Attitudes toward epistemic risk and the value of experiments. Studia Logica , 86 (2), 215-246.\n\nGendler, T. (2010). Intuition, imagination, and philosophical methodology . Oxford, UK: Oxford University Press.\n\nGreca, I., Seoane, E., & Arriassecq, I. (2014). Epistemological issues concerning computer simulations in science and their implications for science education. Science & Education Journal , 23 (4), 897-921.\n\nGreenwald, M. (2004). Beyond benchmarking\u2014how experiments and simulations can work together in plasma physics. Computer Physics Communications , 164 (3), 1-8.\n\nGrune-Yanoff, T., & Weirich, P. (2010). The philosophy and epistemology of simulation: a review. Simulation & Gaming , 41 (1), 20-50.\n\nParke, E. (2014). Experiments, simulations, and epistemic privilege. Philosophy of Science , 81 (4), 516-536.\n\nWinsberg, E. (2008). A tale of two methods. Synthese Journal , 169 (3), 575-592.\n",
        "label": "human"
    },
    {
        "input": "Computer Crimes and Internet Security Research Paper\n\nToday, as numerous technologies evolve and innovative prototypes are released almost every day, humanity is on the verge of being overwhelmed by their own success. Of course, saying that the progress that people made over the span of the last 20 years is substantial would be an understatement (\u201cInformation Systems Security Association\u201d par. 3). Nonetheless, there will always be both upsides and downsides to this situation. There are individuals who use these modern technologies to steal other individuals\u2019 information, personal data, and even identity. The problem of computer crimes and Internet security should be addressed as soon as possible. It is evident that there are strategies intended to help guarantee the safety of those in need (Day 33). Nonetheless, those criminals who use computers to steal other people\u2019s information always seem to be one step ahead of the officials, police enforcement agencies, and policy-makers.\n\nThe author of this paper is interested in exploring the phenomena of cyber crimes and finding ways to mitigate potential attacks. The researcher concentrates on the significance of cyber security and reviews possible outcomes of computer crimes. In order to answer these complex questions, the author is going to analyze relevant sources in the area. After the synthesis of this information, the researcher will obtain the necessary background to evaluate the current state of affairs in the area of computer crimes and Internet security. The author is going to draw reasonable conclusions supported by evidence and based on objective facts concerning the current situation.\n\nSolutions to Hacking/ Malware\n\nTough Passwords\n\nFirst, the passwords should be different for every account. This is necessary because in case if one of the accounts gets hacked all of the other accounts belonging to that same user also become vulnerable. Nonetheless, it would be rather hard to remember all types of passwords. This is why it is reasonable to use password managers \u2013 third-party software that allows keeping all passwords in the same place. Moreover, there are apps that are able to generate tough passwords so no one would ever need to create their own passwords. In combination, password managers and generators are designed to take away the anxiety associated with the password management (Schell 80).\n\nTwo-Part Authentication\n\nNumerous websites are presently taking full advantage of two-step verification. This means that user has to do more than just entering his or her password into the designated field (Gross 1). There is also a text message that is delivered to the user which contains a special alphanumeric (in most cases, it is simply numeric) code used to verify the user\u2019s identity.\n\nChange Your Behavior\n\nThis point is rather important because nowadays every piece of information can be used against an ordinary individual. That does not necessarily mean that one should minimize the time he or she spends online. It means that one should be really careful about the details that are posted online (disclosing birth date or graduation year may be harmful). Internet crimes become more and more complex, so users have to pay attention to what they post and where.\n\nConsolidation\n\nIf that is possible, one should delete all the accounts that were created but never used or abandoned. A lot of important personal details can be stolen from those accounts. The less private information is presented online, the better chances are that the accounts will be safe.\n\nBack It Up\n\nThe most significant solution to hacking is the creation of backups. One may use online services (OneDrive, Dropbox) or an external storage device. In general, the data that we only store online is vulnerable because of the attacks and lack of replication (Moore 346). For instance, if someone loses their smartphone, it would not be hard to restore the data if it was previously backed up compared to a situation where no backups were made.\n\nComputer Viruses\n\nThere are three major ways to spread viruses \u2013 Internet downloads, email attachments, and external storage devices. Even though the Internet has a bad reputation for being the major source of malicious apps, the probability to catch a virus is rather low and similar to the chance of catching a virus from brand new out-of-the-box software. Regardless, antivirus software should be updated recurrently, and all the downloads should be checked for viruses. The majority of viruses spread via rich email and email attachments. One should always scan the attachments and never open files attached to the emails from unknown individuals. The era of external drive viruses is almost over, and more and more viruses are being caught online.\n\nTechniques to Steal Information\n\nCookie Theft\n\nBrowser cookies are used to save the state user\u2019s presence on the Internet. A cookie is basically a text file which helps the owners of websites to track their visitors\u2019 activity. Stolen cookies usually result in the account theft and grave consequences for the victim (Freidman 78). Every other invented cookie attack helps the developers to transform their safety measures and teach users to evade any issues. One of the ways to certify users\u2019 safety is to use innovative crypto ciphers.\n\nWaterhole Attacks\n\nThis type of attacks is named after the inventive method that is used to steal users\u2019 information. This methodology is based on the fact that the victims of these attacks are usually operating within a certain physical or cybernetic site. After that, the attackers flood that site with malicious code or software in order to achieve their illegal objectives.\n\nBait and Switch\n\nThis technique is one of the most interesting ways to steal users\u2019 private data. The victims of this approach are told that they are, for instance, downloading one thing, but it eventually gets replaced by malicious files shortly after the download has started (Goodman 103). One should be aware of this peculiar method because any unknown link might lead to adverse outcomes without his or her permission.\n\nSecurity Types\n\nThere are three common types of security \u2013 Internet security, standalone computer security, and data loss security. The most renowned Internet security measure is SSL security protocol. It is used to prevent hackers from stealing users\u2019 personal information (Marzilli 44). SSL is utilized to establish a secure and protected connection between the server and the user. For standalone computers, the most used type of security is antivirus software. There are numerous brands and titles out there, so the users are free to choose from an extensive number of options if they want to protect their computer. When it comes to the data loss security, no one is safe because a network failure or a hard disk crash are never predictable. The user has to have a decent backup plan in order to minimize adverse effects of these accidents.\n\nConclusion\n\nThe issue of cyber security should be addressed because nowadays crimes come in many shapes and sizes. Cybercriminals invent novel ways to steal personal data and use it for their own purposes. Today, no one safe so there is a need to try as much as possible to protect devices in both online and offline environments. The paper enumerated the ways to steal information and presented the methods that could be used to guarantee a user\u2019s safety.\n\nWorks Cited\n\n\u201cInformation Systems Security Association.\u201d Information Systems Security Association , www.issa.org/.\n\nDay, Paul. Cyber Attack: The Truth about Digital Crime, Cyber Warfare and Government Snooping . London, Carlton Books, 2014. Print.\n\nFreidman, Lauri. Cyberbullying . Detroit, Greenhaven Press, 2011. Print.\n\nGoodman, Marc. Future Crimes: Everything Is Connected, Everyone Is Vulnerable and What We Can Do about It. New York, Double Day, 2015. Print.\n\nGross, Darren. \u201cMulti-Factor Authentication Can Prevent Unauthorized Users.\u201d Infosecurity Magazine , 2016, www.infosecurity-magazine.com/blogs/guess-whos-accessing-your-network.\n\nMarzilli, Alan. The Internet and Crime . New York, NY, Chelsea House, 2010. Print.\n\nMoore, Keith. \u201cThe Race Against Cyber Crime Is Lost without Artificial Intelligence.\u201d ISSA Journal , vol. 14, no. 11, 2016, pp. 342\u2013350. Print.\n\nSchell, Bernadette H. Online Health and Safety: From Cyberbullying to Internet Addiction . Santa Barbara, CA, Greenwood, 2016. Print.\n",
        "label": "human"
    },
    {
        "input": "The History of Computer Storage Research Paper\n\nTable of Contents\n 1. Punch Cards\n 2. Magnetic Tape\n 3. Hard Drives\n 4. Floppy Drives\n 5. Optical Drives\n 6. Flash Storage\n 7. Conclusion\n 8. References\n\nThe history of computer storage is an exciting topic that shows some of the most significant innovations that have shaped our current life. Steady at first, the development process became more and more rapid in the recent decades. This project aims to follow the development of storage technology by explaining the various stages of technological innovation. Thus, the scope of the project includes the history of crucial inventions for data storage, from the first punch cards to the latest flash memory storage technology.\n\nPunch Cards\n\nPunch cards were originally invented by Herman Hollerith and adopted for wide use in 1890 (Jones, 2008, para. 1). However, similar ideas could be seen in France in early 19 th century, where they were used to control Jacquard looms (Jones, 2008, para. 2). Jacquard\u2019s punch cards have also inspired Charles Babbage, whose ideas later prevented Hollerith\u2019s company IBM from getting patent rights over the punch card data storage technology (Jones, 2008, para. 4).\n\nPunch Cards\n\nThe technology behind punch cards was quite simple, which is probably why the idea became popular among many people. As Jones (2008) explains, \u201cThe original code used for punched card data recording in the 1890 census had 22 columns with 8 punch positions each\u201d (para. 7). The holes punched in the card were used as a code to be read by a machine. The structure of the cards was later improved and expanded to accommodate more data that could be recorded on punch cards (Jones, 2008, para. 8). The original process of card reading did not require computers: until 1950\u2019s, a semi-manual process involving both a card sorter and a tabulating machine was used (Jones, 2008, para. 16).\n\nMagnetic Tape\n\nThe next step of storage technology development was the introduction of magnetic tape storage by Fritz Pfleumer in 1928 (\u201cThe history of (computer) storage\u201d, n.d., sec. 10). When the concept was popularized, BASF became the company responsible for magnetic tape production, whereas AEG started manufacturing the Magnetophon machine, which was first used on November 19th 1936 to record the performance of the London Philharmonic Orchestra (\u201cThe history of (computer) storage\u201d, n.d., sec. 10).\n\nMagnetic Tape\n\nThe magnetic tape included a long rolled string of film with an iron oxide coating, which recorded the data when exposed to a magnetic field. To recover the data, it was necessary to expose it to another electromagnetic field to reproduce the signal sequence recorded. From the 1950s, IBM computers began using a similar technology, which set the industry standard for magnetic tape to be \u201chalf an inch wide and wound on removable reels 10.5 inches in diameter\u201d (\u201cThe history of (computer) storage\u201d, n.d., sec. 10).\n\nHard Drives\n\nThe world\u2019s first hard disk technology was introduced by RAMAC in 1956, starting the era of magnetic disk storage (Computer History Museum, 2016, sec. 7). Unlike the previous methods of storage, hard disk allowed accessing large amounts of data quickly. The first computer with a hard disk technology was the IBM\u2019s 305 computer with IBM 305 Disk File, which had \u201cfifty 24-inch platters with a total capacity of five million characters\u201d (\u201cThe history of (computer) storage\u201d, n.d., sec. 16).\n\nHard Drives\n\nHard disk consists of a central axis, the platters that spin around it at a certain pace, and the read-write heads, one on each platter (\u201cThe history of (computer) storage\u201d, n.d., sec. 16). The functioning of the disk is granted by the use of rotating patterns to write and replay the information from a flat magnetic surface: \u201cInformation is written to the disk by transmitting an electromagnetic flux through an antenna or write head that is very close to a magnetic material, which in turn changes its polarization due to the flux\u201d (\u201cThe history of (computer) storage\u201d, n.d., sec. 16). The reverse process is required to retrieve the information. The initial storage capacity of the IBM 305 hard disk was 5 megabytes, which was a lot more than could be written to a magnetic tape. However, due to the large size of the hard disk, it was impossible to use it as a portable storage device, which later led to the introduction of floppy disks and optical disks.\n\nFloppy Drives\n\nIn 1967, IBM invented the first 8-inch floppy drive with an intent to create \u201ca simple and inexpensive system for loading microcode into their System/370 mainframes\u201d (\u201cThe history of (computer) storage\u201d, n.d., sec. 21), which would be faster, cheaper, and universal in purpose. 8-inch model was followed by a smaller 5.25-inch drive to be used in desktop computers and then by 3.5-inch floppy with increased speed and efficiency.\n\nFloppy Drives\n\nFloppy disks used magnetic technology to read and record data. The first disks had a circular hole in the middle of the diskette, which allowed for the rotation of a magnetic mechanism inside the computer.\n\nOptical Drives\n\nOne of the most recent inventions in the field of computer storage, the optical drive was first introduced in the form of a compact disc (CDs) in late 20th century (\u201cOptical disc\u201d, 2015, sec. 1). The first person to think of using light for recording and replaying data was James T. Russel, who is now considered to be the inventor of the original concept of an optical disc (\u201cThe history of (computer) storage\u201d, n.d., sec. 23). The first audio CD, however, was presented by Philips and Sony in 1983 (\u201cOptical disc\u201d, 2015, sec. 2).\n\nOptical Drives\n\nThe technology used in optical storage includes the writing of bumps onto the disc surface in a single spiral, where the density of bumps on the surface determines the amount of data (\u201cOptical disc\u201d, 2015, sec. 3). Reading of the data is achieved by directing a laser diode on the reflective coating of the disk. The bumps and dents distort the light, delivering the information code (\u201cOptical disc\u201d, 2015, sec. 3).\n\nFlash Storage\n\nThe most compact method of storage, flash storage, began with the invention of flash memory by the Toshiba employee Fujio Masuoka in 1984 (Computer History Museum, 2016, sec. 30). To this day, flash memory is mainly used for portable storage in laptops and digital cameras in the form of USB drive.\n\nFlash Storage\n\nFlash storage is a form of solid-state storage, which means that it stores data using electricity in chips on a printed circuit board. The absence of moving parts reduces power consumption: \u201cA typical SATA flash drive consumes 50 percent or less the power required by mechanical drives\u201d (\u201cFlash storage\u201d, 2014, sec. 3), and quickens the process of data exchange, making it the most efficient portable data storage device to this day.\n\nConclusion\n\nOverall, data storage evolution is one of many examples of how certain technological advances have been changing various aspects of our lives with every passing generation. The technology is still improving: every day, IT companies are working to find new ways to add more capacity to the common data storage devices and to increase their efficiency. There is a definite possibility that in a decade, a whole new method of data storage will be used all over the world.\n\nReferences\n\nFlash Storage . (2014). Web.\n\nJones, D. W. (2008). History of the punch card . Web.\n\nOptical disc. (n.d.). In New World Encyclopedia . Web.\n\nComputer History Museum. (2016). Timeline of computer history . Web.\n\nThe history of (computer) storage . (n.d.). Web.\n",
        "label": "human"
    },
    {
        "input": "Fertil Company\u2019s Computer and Information Security Report (Assessment)\n\nIntroduction\n\nThe rapid growths of technologies and the rise of digital devices that impact the functioning of various establishments and organizations introduce numerous changes to the way the modern world functions. Computerization conditions the improved efficiency of numerous agents and companies. However, at the same time, the given process poses a significant threat to security as the usage of digital data storages and mediated ways of information exchange might result in its loss and even theft.\n\nFor this reason, there are numerous attempts to improve the sphere of security and guarantee the stable and efficient functioning of organizations and companies that integrate technologies into their functioning. For this reason, the given project is devoted to the comprehensive analysis of security measures that are explored by a certain company to preserve its status, competitive advantage, and guarantee its further beneficial existence.\n\nOverview of the chosen company\n\nThe company FERTIL is chosen as the background for the analysis. The choice is not accidental and it is predetermined by several important factors. First, its geographical location is important for research. Being situated in the UAE, the company functions under the conditions of the unique market that is characterized by numerous perspectives and good tendencies for its evolution.\n\nA limitless number of opportunities, the great level of incomes of the local organizations, and the great wellness of the population obviously influence the way the companys functioning is organized and introduce unique demands to the security that should be fulfilled to assure that a company will continue its rise. The analysis will help to indicate the most crucial aspects of FERTILs functioning and improve the comprehending of computer security and its significance.\n\nFERTIL company was founded in 1980 with the idea to become a leader within the fertilizer industry by offering customers the unique products and services that would satisfy their needs and desires (\u201cMessage From CEO\u201d par. 7). The company provides its services worldwide and is also focused on international cooperation with numerous agents and potential partners. Moreover, the company aims at producing and supplying sustainable and value-added products that could be considered environmentally friendly and improve the state of the world significantly.\n\nFERTIL is a member of the ADNOC Group of companies. It also utilizes the lean gas that is supplied from the onshore fields located in the country (\u201cMessage From CEO\u201d par. 5). This fact evidences the great importance of FERTIL and provides numerous opportunities for its further rise.\n\nThe company could be characterized by the tendency to its further growth. The fact is that the existing stable demand on the products of the oil refinery industry conditions the further evolution of the importance of services provided by FERTIL. Yet, the high rates of its development also mean that a number of new workers will be needed to support its functioning and assure that it will become even more efficient. At the moment there are several thousand workers who work in different departments located in various regions (\u201cMessage From CEO\u201d par. 8).\n\nThe company tends to create a certain corporative culture that is characterized by the efficient and well-thought-out collaboration of members who want to condition the company to rise and evolve. FERTIL also tries to create conditions that might contribute to the personal and professional growth of every individual who works there.\n\nBesides, being a great company that is focused on the acquisition of a certain competitive advantage and winning the rivalry in this sphere, FERTIL obviously implements computers and digital devices into its functioning. At the moment, there is an IT department that functions across the companys branches to guarantee the further spread of technologies and preservation of important data that is needed for FERTILs functioning.\n\nAccording to the annual report, the company acknowledges the importance of technologies and creates the conditions beneficial for the further implementation of innovations (\u201cGranular Urea\u201d par. 5). That is why the number of computers is great. Almost all workers have to work with them to perform their daily tasks and improve efficiency. Furthermore, a new employee who is eager to work in the company is expected to demonstrate good skills in working with a computer.\n\nIt is also obvious that there is a data center that guarantees the functioning of the corporative network and preservation of data. It could be taken as the heart of the company as in case computers fail to work its functioning will be paralyzed.\n\nFor this reason, the data center is traditionally placed in a secured room that excludes the opportunity of occasional interventions. There is also a backup system that is needed in case the main one is corrupted or suffers from some fatal error (\u201cCorporate Security Measures and Practices\u201d par. 4). Besides, storing of data is crucial for the efficiency of the majority of projects and tasks performed by the company (FERTIL 6). That is why there is a great need for specific security measures that will assure the absence of any data leaks and protect FERTIL from the appearance of numerous problems in the future.\n\nBesides, the team of IT specialists is responsible for the monitoring of the corporative networks functioning and its ability to satisfy the companys needs. Additionally, the company also has a certain security team whose main task is to work directly with security concerns and protect the company from any interventions that might result in the decrease of the efficiency and data losses. The security team is comprised of qualified specialists who bear responsibility for the creation and adaptation of the most important strategies that will be able to create a certain barrier between FERTIL and some potential offenders or hackers. This very team should also provide newcomers with detailed and understandable instructions that describe the basics of computer security and help to improve it.\n\nYet, the pattern that rests on the usage of a specialist team to protect data also presupposes the wise allocation of IT departments. At the moment, there is one main center that creates the security policy and guarantees its implementation in the FERTILs functioning through other local smaller divisions that exercise this very policy and monitor the functioning of a certain office in the area.\n\nAt the same time, the head office also creates and provides a specific software that is supposed to protect the companys computers and guarantee the absence of serious problems. The importance of the tasks performed by this team could hardly be overestimated as they guarantee the further existence of the FERTIL. There were numerous cases when the weak security system resulted in the collapse of a certain organization as the information needed for its further evolution was stolen (\u201cCorporate Security Measures and Practices\u201d par. 7).\n\nSpecific Policies\n\nISPP document\n\nTitle\n\nFair and Secure usage of the corporate network to share important information and pieces of data that are needed for the FERTILs further evolution and growth.\n\nClassification internal usage only.\n\nStatement of Policy\n\nThis very security policy is focused on the creation of a secure environment for all users of a specific corporate network that is implemented in the FERTILs functioning. It includes hardware, software, firewalls, and other technologies and devices that might guarantee the preservation of crucial data and its further distribution between the agents who have the permission to work with it. For this reason, the policy is intended for authorized users within the enterprise. All workers who guarantee the companys evolution and have been granted approval to access the data system. The above-mentioned category of users is expected to understand and comply with the importance and the main points of this policy and the document that describes it.\n\nAuthorized access and usage of equipment\n\nFERTILs employees are permitted to use the internal wireless net and the corporative net and to link their laptops to them. The company adheres to the pattern according to which the description of tasks that should be accomplished and the essential information needed for their completion are sent to workers emails. For this reason, the usage of laptops and WLAN is inevitable. To ensure the appropriate protection and the level of privacy, the following steps should be made. First, when linking to the corporate net, all users should pass through the procedure of strong mutual authentication and encryption.\n\nProhibited usage of equipment\n\nMoreover, in case a worker also uses his/her laptop to establish a connection to some public net or hotspot, the companys developed and approved firewall should be installed to protect the given device from any potential threat and guarantee the security of data stored there. Additionally, the usage of non-standard or non-approved hardware and software and protocols are prohibited and monitored by the companys security system (\u201cCorporate Security Measures and Practices\u201d par. 3). This approach establishes the efficient protection of data.\n\nSystems Management\n\nBesides, the main responsibilities of the Network Administrator who belongs to the companys security team are the configuration of the most important devices and software and the provision of proper settings that are defined by the given policy. These also include authentication and access to the corporate network. At the same time, an employee should ensure that his/her laptop is configured and protected in accordance with the standards outlined by this policy. He/she should be ready to check the main configurations, go through the authentication procedure, and bear responsibility for the potential data leaks that might happen in case the basic demands are ignored. Altogether, the FERTIL security team is focused on the encryption of the most important data and its protection by all means.\n\nViolations of Policy\n\nIn case the above-mentioned security system is not used in a proper way or its basic assumptions are ignored by the members of the collective, the FERTIL company, and its security team reserve the right to perform any action that might prevent data leak and guarantee restoration of security and protection of significant information. Moreover, a first-time violator should be warned about the great pernicious impact of his/her actions and the potential threat posed by his/her irresponsible attitude to security measures. However, in case the violator remains inattentive, his/her actions should be reported to a manager for him/her to create an appropriate solution and provide the needed punishment for a person who ignores the basic points of the introduced security system.\n\nPolicy Review and Modification\n\nThis security policy should be reviewed by the security department every three months. This pattern guarantees the ability of the policy to respond to new threats that might appear every day. Moreover, the introduction of new patterns to the system helps to confuse potential offenders by the constantly changing approaches used to encrypt the important data and guarantee the security of email boxes. Furthermore, the modification should be accomplished in accordance with the new methods and solutions suggested by the analytical team which is focused on the investigation of the topical and dangerous threats that could destroy the corporate net and result in the collapse of the company.\n\nLimitations of Liability\n\nFERTILE does not guarantee any liability for acts that are aimed at doing harm to its networks or corruption of the important and needed data. In case these actions occur, the company will inform a violator, local authorities and will provide no legal assistance. These actions should be terminated to guarantee the preservation of the companys security.\n\nConclusion\n\nAltogether, the evolution of digital devices and the spread of technologies preconditioned the appearance of numerous threats that might have a negative impact on the functioning of a certain company and even result in its collapse. For this reason, FERTIL has the specialist team whose main task is to introduce the efficient system that protects the important data and provides employees with the information that is needed to work on the local network and prevent data leaks. The efficiency of this system is conditioned by the great importance given to its main aspects and the practice of its constant improvements that are needed to make it topical and efficient.\n\nWorks Cited\n\n\u201c Corporate Security Measures and Practices. \u201d New Era Associates . n.d. Web.\n\nFERTIL. \u201cEmployee Handbook.\u201d FERTIL. n.d. Web.\n\n\u201cGranular Urea.\u201d FERTIL . n.d. Web.\n\n\u201cMessage From CEO.\u201d FERTIL . n.d. Web.\n",
        "label": "human"
    },
    {
        "input": "Personal Computers and Protection of Privacy Research Paper\n\nProtection of privacy plays an important role in the maintenance of freedom of property, speech, and press supported by the First and Fourth Amendments. Since nowadays personal computers contain large volumes of private information, the unwarranted search and seizure of the technology may violate the basic right for privacy without which the government will have an opportunity to violate all other rights.\n\nIn 1949, the judge, Robert Jackson, emphasized the significance of constitutional protection from unreasonable property search and arrests in the case of Brinegar v. the United States . Jackson claimed that uncontrolled arrests and seizures are the effective methods of suppressing the population and terrorizing society (McLean, 1968).\n\nThe First and Fourth Amendments forbid the law enforcement agencies to conduct search and arrest citizens without sufficient reason and evidence that the suspected citizen violated the law. To minimize the risks of the deprivation of rights, it was decided not to provide the police authorities with the opportunity to determine a sufficient ground for a search or seizure. Police must announce reasons for the search or arrest to a judge who issues a warrant (Cornell University Law School, n.d.).\n\nHowever, under certain circumstances, the Fourth Amendment permits carrying out a search or arrests without a warrant. The Amendment protects the rights of the citizens through the limitation of a judge\u2019s power in issuing warrants by demanding a warrant to contain the precise information about the location, persons or things, including computers and other technologies, which are to be seized and searched ( Search warrant requirements , n.d.). Thus, the Amendments do not allow law enforcement agencies to conduct searches wherever they deem it necessary.\n\nThree major problems that arise when interpreting and implementing the First and Fourth Amendments are the identification of cases that allow conducting unwarranted search and seizure, the formulation of a rationale for warrant issuing, and the application of the Amendments\u2019 statements. Usually, at the moment of search, a law enforcement agency already has credible information about the intended or committed crime which it uses as the ground for arrest conduction, but there may be situations when it is impossible to apply for a warrant.\n\nFor example, an immediate arrest when a person uses technology for the demolition of the national security or harms society by disseminating illegal information. Although such cases of unwarranted computer seizure are rare, in these circumstances, the arrest without a court order may become a necessity. Nevertheless, law enforcement agents should later prove that they had reasonable grounds for the arrest and evidence that they have been unable to apply for a warrant.\n\nSimilar to other forms of seizure, the criteria for assessing the scope and reasonableness of computer search or technology seizure is constantly refined by the Supreme Court. For example, in the case of Davis v. Gracey devoted to the distribution of pornographic information, the arrest of computers containing both pornographic and \u201cinnocent\u201d materials was justified because the arrested computers served not merely as information containers but as instruments of the crime (Adams & Blinka, 2012, p. 124). At the same time, during the fraud investigation in the case of United States v. Falon , the seizure of the personal computer was held unreasonable as the proof that \u201cindividual\u2019s entire life was consumed by fraud\u201d was not provided (Adams & Blinka, 2012, p. 125).\n\nLaw enforcement agencies should have sufficient rights to maintain the protection of society from crime. However, power is always associated with the potential to abuse it. Therefore, to control it, the citizens are allowed to file a civil suit when they believe their rights are violated by police officials ( Police misconduct and civil rights , n.d.). Therefore, law enforcement agencies need to consider all ethical and legal aspects of professional performance and integrate them into daily practices, including the computer search and seizure activities.\n\nReferences\n\nAdams, J. A., & Blinka, D. D. (2012). Prosecutor\u2019s manual for arrest, search, and seizure . Charlottesville, VA: LEXIS Law Pub.\n\nCornell University Law School. (n.d.). Search warrant. Legal Information Institute .\n\nMcLean, D. (1968). What to do if arrested. Ebony, 60c , 174-183.\n\nPolice misconduct and civil rights . (n.d.). FindLaw .\n\nSearch warrant requirements . (n.d.). FindLaw .\n",
        "label": "human"
    },
    {
        "input": "Computer-Assisted Language Learning: Barriers Report (Assessment)\n\nAn 18-page electronic document in interactive PDF format. McKenzie March 2001\n\nDescription of Content\n\nDescription of the way teachers might use technologies and the way they improve their functioning\n\nOutcomes & Main issues\n\n  * Understanding of the term \u201cPowerpointlessness.\u201d\n  * Reconsideration of the role of technologies\n  * The best way to implement them into teachers functioning.\n\nBrief\n\nPowerpointlessness The author delves into the advantages and disadvantages of the mass usage of technologies in the modern sphere of education. He admits the fact that along with numerous benefits, specialists and students might also face significant problems resulting from the unique character of technologies impact. Therefore, the author states that nowadays there is some risk that all innovations might condition the slick of student performances. Moreover, there is also a tendency towards the excessive usage of software and different programs. For this reason, some specialists might call this sort of behavior powerpointlessness. McKenzie is sure that technologies should be used in an appropriate way to increase the efficiency and guarantee that certain tasks will be performed faster and with better outcomes.\n\nHowever, when a specialist, either a teacher or a student, uses software to emphasize his/her innovative character and show people that he/she comes along with the modern tendencies, it might have a pernicious impact on the performance. McKenzie is also sure that when networks enter schools and provide great amounts of data for students and teachers, a certain threat appears. The information from the internet could be distorted or corrupted. For this reason, teachers should monitor the usage of technologies and assure that only appropriate software is used to achieve certain goals and increase the efficiency of performance. Only under these conditions technologies could be used.\n\nCALL A 19 page electronic document in interactive PDF format. Egbert, Paulus, and Nakamichi November 2006\n\nDescription of Content\n\nThe authors provide the ways teachers might use CALL and barriers that should be overcome to become an efficient and innovative teacher.\n\nOutcomes & Main issues\n\n  * Identified the main aspects of the CALL application.\n  * Description of the main barriers appearing in the teaching process.\n  * Better comprehending of.CALLs peculiarities\n\nBrief\n\nBBarriers preventing teachers from using CALL Egbert et al. investigate the way teachers understand the basic principles of CALL. They say that despite the obvious positive effect of the usage of CALL, there are still numerous problems that might appear for teachers who try to implement technologies into their functioning. For instance, these might be time pressures, lack of resources and materials, negative attitude to technologies, inefficient guidelines, lack of support, a clash between the old and new technologies, gender, personal qualities, teaching skills, experience, and other personal factors. The authors state that all these factors should be considered to guarantee that a specialist will be able to understand the basics of CALL and implement them into his functioning. In this regard, consideration of the ways and methods that could be used to overcome the above-mentioned barriers is crucial.\n\nIt is also possible to suggest several approaches that could help to eliminate these barriers. For instance, Hubbard in his paper states that to help a specialist become innovative and get rid of barriers, an educator could engage in CALL by himself/herself. It will contribute to a better understanding of the main problems that might arise in the course of the investigation of CALL and prepare efficient solutions. Moreover, an educator might also adhere to the efficient strategies that rest on CALL to demonstrate the advantages provided by this very approach and help teachers to reconsider their perspectives on its usage. Finally, the ability to overcome these barriers should be considered one of the main aspects of the modern approach to teaching and learning. Teachers should be able to implement CALL in their functioning.\n",
        "label": "human"
    },
    {
        "input": "Computer Assisted Language Learning in L2 Education Essay (Critical Writing)\n\nTable of Contents\n 1. Abstract\n 2. Significance\n 3. Application\n 4. Alternative Research Approaches\n 5. Reference List\n\nAbstract\n\nThe technological breakthrough that the world has been witnessing since the mid-20 th century has affected an array of domains and the environment of education is not an exception to this rule. As a result, second language education has come to incorporate both traditional and modern tools, including software that was either designed specifically for the needs of L2 learners or that can be shaped to meet them.\n\nUnderstanding the direction in which IT advances can influence L2 processes is crucial to the design of teaching strategies and helping learners acquire necessary skills. In his paper, Mohsen (2016) addresses the issue of combining visual and narrative elements in a video as one of these teaching tools.\n\nSignificance\n\nThe study conducted by Mohsen (2016) contributes to a better understanding of the needs of L2 students, as well as how the latest technological advances can be used to meet these needs in an appropriate and timely manner. Furthermore, the author explores the opportunities for improving the quality of the teaching and learning processes alike by reconsidering how modern technologies are applied. Therefore, the significance of the article and the issue that it raises is rather high.\n\nAccording to the author of the study, the paper was aimed at detecting the efficacy of using narrated texts, along with the relevant imagery, by utilizing YouTube services as the means of improving students\u2019 knowledge of the L2 vocabulary. Therefore, the research can be considered a crucial addition to the range of studies that explore the opportunities for improving L2 teaching strategies currently promoted among educators to enhance learning processes.\n\nThe use of IT devices as the means of reinforcing the process of L2 skills acquisition, in general, and vocabulary learning, in particular, can become an essential element of promoting the concept of lifelong learning and the consistent acquisition of new skills among students (Rogan & San Miguel, 2013).\n\nThe outcomes of the study indicated quite clearly that there is a direct correlation between the application of the teaching strategy involving the use of both narrations and pictures and the results delivered by the learners when carrying out the corresponding L2 vocabulary tests: \u201cPositive findings of the use of annotations and captions align with previous literature findings, which reported annotations or captioning modes would help improve L2 vocabulary acquisition\u201d (Mohsen, 2016, p. 12). In other words, the research results imply that the combination of visual and narrative elements provides a rather strong foundation for the successful incorporation of the words and collocations into the learner\u2019s vocabulary.\n\nMohsen (2016), however, admits that the outcomes of the study cannot be generalized to the required extent: \u201cWith regards to the tests, the number of the target vocabulary was small, possibly restricting the generalizability of the findings. Therefore, a similar study with a larger vocabulary sample size is recommended to obtain more reliable and valid results (Mohsen, 2016, p. 15). The identified characteristics of the research, however, do not make the paper results any less significant; rather, they point to the evident limitations.\n\nTherefore, the outcomes of the research can be viewed as the implications for building a more comprehensive and successful approach to teaching students L2, in general, and helping them acquire the necessary vocabulary, in particular. Herein lies the significance of the article; Mohsen (2016) sets prerequisites for an extensive update of the approaches that are currently used in teaching L2.\n\nApplication\n\nTo evaluate the effects of IT on the TESOL processes, Mohsen (2016) uses a set of tests designed to determine the students\u2019 ability to spell words correctly and understand the mechanics of the English language. By selecting the \u201cL2 form recall test and L2 spelling tests\u201d (Mohsen, 2016, p. 6), the author made it possible to assess the mounting evidence as precisely as possible.\n\nAs a result of the application of the tests mentioned above, the learners\u2019 ability to apply the newly acquired skills to writing correctly were bound to come to light (Le\u015bniewska & Pichette, 2014). In other words, the tests, as the tools for data collection, can be considered as highly accurate due to the objectivity of the data classification and the assessment of the learners\u2019 results (Balance, 2016).\n\nAs far as the actual experiment is concerned, Mohsen designed a narrated story that was made available to the learners using the YouTube service: \u201cA 542-word story entitled \u2018The Tooth\u2019 was downloaded from the Internet on March 2013 (Mohsen, 2016, p. 7). From this, it is clear the author prefers a quantitative, as opposed to a qualitative, approach, thus, indicating that his focus is more on the statistical correlations between the essential variables rather than the nature of the problem and the implications that it is likely to have on the future progress of L2 methods and techniques (Ghasemi, Kermanshahi, & Moharami, 2015).\n\nSimilarly, there is strong evidence that the application of both imagery and auditory information allows for an increase in learners\u2019 abilities to retrieve, interpret, and use the provided data properly: \u201cOur data showed that words that were presented both through songs and stories were not recalled significantly better than those presented in either mode\u201d (Le\u015bniewska & Pichette, 2014, p. 8). To be more specific, the approach devised by Mohsen (2016) falls into the category of retrieving the necessary data from management information systems and vocabulary tests can be deemed as the essential tool for collecting the necessary information (Venkatesh, Brown, & Bala, 2013).\n\nArguably, the author used a variation of a survey that implied selecting specific words from the texts provided to the participants. As such, the author determined the research design as a quasi-experimental design, since a group of students was chosen to participate in the experiment with no pre-selection processes carried out before the testing. However, one might debate the accuracy of using the term quasi-experimental as the author confesses to subjecting two of the groups to a pre-test selection:\n\nI employed a quasi-experimental design in which only two experimental groups had been examined because the researcher believed that prior studies have already established the fact that using help options were significantly better than no help options in L2 vocabulary development. (Mohsen, 2016, p. 6)\n\nNevertheless, the fact that none of the rest of the groups that took part in the experiment points to the fact that its design has warranted the definition of a quasi-experimental one (Chamberlin-Quinlisk, 2012). The positive implications of the applied framework are quite obvious. By incorporating the elements of a quasi-experimental design, Mohsen creates the research environment that is very close to the actual one. As a result, the model created by the researcher reflects the real-life relationships between the essential variables (i.e., the use of YouTube and the related services and the students\u2019 ability to acquire the necessary L2 vocabulary within the required amount of time) accurately (Linck, Osthus, Koeth, & Bunting, 2014).\n\nOn the one hand, the use of a test as a quantitative analysis device can be considered a powerful tool for revisiting the essential stages of L2 strategy developments. Furthermore, the author introduces the readers to the effects that IT has had on the intended target, as well as the potential problems that L2 educators will have to address (Ahmadian & Tavakoli, 2014). To put it another way, the survey results inform the reader about the correlation between the progress in L2 strategies development and the evolution of IT in general.\n\nOn the other hand, the application of tests triggers an immediate drop in the variability of the outcomes, therefore, reducing the accuracy of the data retrieved in the process (Plonsky & Oswald, 2016). Nevertheless, tests serve as a rather accurate tool for retrieving statistical data, which means that their adoption in the identified environment is justified.\n\nAlternative Research Approaches\n\nIt could be argued that a qualitative approach, particularly the use of a grounded theory or a case study as the means of exploring the effects of IT on L2 strategies, could have been viewed as an option (Lee, 2012). Indeed, a closer look at the data retrieved in the course of the study could reveal that the current framework of information collection and interpretation could be tweaked slightly so that a qualitative approach could be utilized (Creagh, 2014). Specifically, the author of the study could consider YouTube narration as the foundation for creating a larger test with general questions that imply expanded answers.\n\nAs a result, more detailed information could be elicited from the participants in the form of unstructured texts, which could, later on, be processed to determine the participants\u2019 understanding of how the target language works (Pr\u00e9fontaine & Kormos, 2012). It could be argued that the identified approach would not help measure the degree to which the learners will have progressed in their studies. However, the framework described above would have provided prerequisites for a follow-up study that would have shed light on the subject matter in a more detailed way based on the implications of the qualitative study (Plonsky, 2014).\n\nIf the quantitative research design is considered as a compulsory element of the research, one might suggest the use of correlational research. This aims to determine the correlation between the chosen variables in the environment as close to the real-world scenarios as possible (Wang & Vasquez, 2012). The analysis would have benefited from the use of the correlational research design since it would have helped focus on the interactions between the two variables solely without being distracted by other factors (Sandbank & Yoder, 2016).\n\nThe use of the correlational quantitative research as an approach to studying the variables under analysis, however, would still have lacked precision since correlational research does not allow for the acknowledgement of the extraneous variables. Since it focuses closely on the study of the two key variables involved, i.e., the students\u2019 grasp of the L2 vocabulary and the use of modern IT media, such as the YouTube services, it would have delivered the results that implied the absence of the factors that may have affected the learners\u2019 performance. As a result, the outcomes of the study would have been deemed as generic (Zhang & Zhang, 2013).\n\nTherefore, with specific regards to the case in point, the quantitative analysis should be preferred to a qualitative one due to the focus of the study. Seeing that the author\u2019s objective is to identify the extent to which the use of IT tools such as YouTube affects the learners\u2019 performance in acquiring the L2 vocabulary, it is necessary to quantify the results to make them palatable and relevant to the hypothesis of the study. A change in the focus of the research, however, could have made a case for a qualitative design; for example, the author could have focused on the types of outcomes, the nature of the learners\u2019 success, or, which is the most obvious choice, the attitude of the learners toward the application of the YouTube material as the means of enhancing their productivity (Ardasheva, Norton-Meier, & Tretter, 2015).\n\nNevertheless, the overall design of the article analysed above can be viewed as suitable. It helps gather the information necessary to answer the research question, it allows the collection and analysis of the data within a relatively small amount of time, and it provides extensive information for a follow-up study, i.e., an in-depth assessment of the effects that the application of various YouTube tools has on the learners\u2019 ability to acquire specific types of vocabulary, the challenges that learners are likely to have with the tool, and the means to manage these challenges, etc. (Norris, Ross, & Schoonen, 2015). A solid and credible piece of academic analysis, Mohsen\u2019s (2016) article is bound to serve as the foundation for addressing the related issues in both the IT and L2 domains (Barkaoui, 2014).\n\nReference List\n\nAhmadian, A. M., & Tavakoli, M. (2014). Investigating what second language learners do and monitor under careful online planning conditions. The Canadian Modern Language Review, 70 (1), 50-75.\n\nArdasheva, Y., Norton-Meier, L. A., & Tretter, T. R. (2015). Integrating science and literacy for young English learners: A pilot study. NYS TESOL Journal, 2 (1), 3-16.\n\nBallance, O. J. (2016). Analysing concordancing: A simple or multifaceted construct? Computer Assisted Language Learning, 0 (0), 1-15. doi:10.1080/09588221.2016.1209527\n\nBarkaoui, K. (2014). Quantitative approaches for analyzing longitudinal data in second language research. Annual Review of Applied Linguistics, 34 (1): 65-101.\n\nChamberlin-Quinlisk, C. (2012). Critical media analysis in teacher education: exploring language-learners\u2019 identity through mediated images of a non-native speaker of English. Convergence, 18 (2), 215-229.\n\nCreagh, S. (2014). A Foucauldian and quantitative analysis of NAPLaN, the category \u2018Language Background Other Than English\u2019, and English as a second language level. TESOL in Context, 24 (2), 7-9.\n\nGhasemi, A. A., Kermanshahi, F. V., & Moharami, M. (2015). Conceptualizing willingness to communicate: A quantitative investigation of English-language major students. Modern Journal of Language Teaching Methods, 5 (1), 103-110.\n\nLee, L. (2012). Appraising research in second language learning: A practical approach to critical analysis of quantitative research by Porte, Graeme Keith. Modern Language Journal, 96 (2), 333-334.\n\nLinck, J. A., Osthus, P. K., Koeth, J. T., & Bunting, F. M. (2014). Working memory and second language comprehension and production: A meta-analysis. Psychonomic Bulletin & Review, 21 (4), 861-883.\n\nMohsen, M. A. (2016). Effects of help options in a multimedia listening environment on L2 vocabulary acquisition. Computer Assisted Language Learning, 29 (6), 1-18. doi:10.1080/09588221.2016.1210645\n\nNorris, J. M., Ross, S. J., & Schoonen, R. (2015). Improving second language quantitative research. Language Learning, 65 (Suppl. 2015), 1-8. doi:10.1111/lang.12110\n\nPlonsky, L. (2014). Study quality in quantitative L2 research (1990\u20132010): A methodological synthesis and call for reform. Modern Language Journal, 98 (1), 450-470. doi:10.1111/j.1540-4781.2014.12058.x\n\nPlonsky, L., & Oswald, F. L. (2016). How big is \u201cbig\u201d? Interpreting effect sizes in L2 research. Language Learning, 64 (4), 878-912. doi:10.1111/lang.12079\n\nPr\u00e9fontaine, Y., & Kormos, J. (2012). A qualitative analysis of perceptions of fluency in second language French. IRAL: International Review of Applied Linguistics in Language Teaching, 54 (2), 151-169.\n\nRogan, F., & San Miguel, C. (2013). Improving clinical communication of students with English as a second language (ESL) using online technology: A small scale evaluation study. Nurse Education in Practice, 13 (5), 400-406.\n\nSandbank, M., & Yoder, P. (2016). The association between parental mean length of utterance and language outcomes in children with disabilities: A correlational meta-analysis. American Journal of Speech-Language Pathology, 25 (2), 1-12.\n\nVenkatesh, V., Brown, S. A., & Bala, H. (2013). Bridging the qualitative-quantitative divide: Guidelines for conducting mixed methods research in information systems. MIS Quarterly, 37 (1), 21-54.\n\nWang, S., & Vasquez, C. (2012). Web 2.0 and second language learning: What does the research tell us? CALICO Journal, 29 (3), 412-430.\n\nZhang, L. J., & Zhang, D. (2013). Thinking metacognitively about metacognition in second and foreign language learning, teaching, and research: Toward a dynamic metacognitive systems perspective. C ontemporary Foreign Languages Studies, 396 (12), 111-121.\n",
        "label": "human"
    },
    {
        "input": "Computer-Assisted Second Language Learning Tools Term Paper\n\nIntroduction\n\nModern technologies are making it easier for students to achieve their academic goals. Such technologies support learners in different settings. For example, the use of assistive-technologies supports the needs of students with learning disabilities. Modern technologies encourage educationists to teach various subjects such as language, science, and mathematics. The role of technology in different learning institutions will, therefore, increase in the coming days.\n\nA new approach has emerged whereby learners can use different computer technologies. Stockwell (2012) describes \u201ccomputer-assisted language learning (CALL) as an approach to teaching whereby internet technologies can support the needs of many students\u201d (p. 76). Computer technology reinforces, analyzes, and presents various materials to the targeted learners. This technology also increases the level of interaction. Students can use CALL to learn new languages. This essay describes the use of various CALL tools in learning a second language. The targeted tools support the CALL approach.\n\nCALL Tools: Learning a Second Language\n\nOnline Exercises\n\nLearners of different languages can benefit a lot from online exercises. Several websites offer a wide range of online exercises for different individuals. Such websites make it easier for learners to improve their pronunciations and spellings (Jarvis & Achilleos, 2013). Online exercises have the potential to support the educational needs of learners. The websites present many questions and exercises.\n\nThese exercises have three major levels. These levels include \u201c beginner, intermediate, and advanced \u201d (Jarvis & Achilleos, 2013, p. 5). The exercises make it easier for every student to understand different topics such as noun clauses, tenses, verbs, phrasal verbs, articles, and prepositions. These skills are critical whenever a person is learning a new language. Such online websites are advantageous because they improve the level of learning.\n\nAnalysis of a Good Online Exercise Website\n\nLearners should always identify the best online exercise websites. Such websites should have meaningful questions and exercises. Learners should be able to use the best online websites without any guidance. Such online exercise websites should support the educational needs of beginners. The websites must support the needs and expectations of every learner of a second language. A good online website should also offer the right answers and feedback to the learners.\n\nThis approach will ensure every learner acquires new language skills. According to Chapelle and Jamieson (2008), a good website should be flexible, accessible, and easy to use. This fact explains why such online websites should have similar features. Such activities and exercises can support the skills of every learner. The site can also offer numerous listening and writing activities.\n\nAnalysis of a Bad Online Exercise Website\n\nThere are many bad online exercise websites. Learners of second languages should consider the appropriateness of different websites before using them. Such websites might contain inaccurate information or answers. Most of the links might be inactive. The websites also fail to support the growth of the targeted learner. The website might not consider the language needs of the targeted learners. Students who use a bad website might not acquire the required language skills. Bad websites make it impossible for learners to evaluate themselves (Chapelle & Jamieson, 2008).\n\nAdvantages and Disadvantages of Online Exercises\n\nLearners of second languages can benefit a lot from online exercises because they present numerous advantages. To begin with, learners can get immediate feedback from their tutors. Students can undertake numerous tests and activities whenever they want. Learners can access such exercises wherever they are. This is the case because such online websites are internet-based. Such websites are \u201cuseful because they deal with cheating\u201d (Chapelle & Jamieson, 2008, p. 24). Teachers and educators can use these exercises for group discussions (Chapelle & Jamieson, 2008). These advantages encourage more learners to embrace the use of online exercises.\n\nSuch websites are disadvantageous because learners should incur more expenses in order to access such websites. They should have stable internet and electricity. Students in the developing world might not benefit the most from these websites. Internet fluctuations can affect the effectiveness of this approach. Some websites require log in details, thus forcing learners to pay money in order to access them (Bilbatua & Haro, 2014). Such weaknesses discourage more people from using these online exercises.\n\nYouTube\n\nYouTube videos present many skills and ideas to targeted viewers. This fact explains why YouTube can be used to educate learners of second languages. YouTube is an internet-based service that encourages people to share videos across the globe. Teachers can, therefore, use YouTube to educate their students. Learners can also visit different YouTube videos in order to acquire new skills and competencies. Such videos also come with meaningful comments that can fulfill the needs of many learners. Learners of a second language can acquire new pronunciations, spellings, tenses, and articles from these videos. The presence of subtitles and voices makes the learning process more effective (Bilbatua & Haro, 2014). The use of YouTube for learning purposes will increase in the coming years.\n\nAdvantages and Disadvantages of YouTube\n\nThe advantages of YouTube make it useful in different learning settings. Learners of new skills and languages can use YouTube to exchange their views. YouTube can be accessed by individuals in every part of the globe. The service is also easy to use. Users should take accurate notes in order to achieve their learning goals. YouTube presents many videos to targeted individuals. This situation makes it easier for many learners to achieve their goals. The videos also have voices and subtitles, thus making it easier for learners to acquire new skills and concepts (Bilbatua & Haro, 2014).\n\nYouTube has some disadvantages. Learners should have stable internet connections in order to access these videos. They might be time-consuming. Teachers should take long to prepare such videos. The videos might not deliver the right content to the students (Bilbatua & Haro, 2014). These weaknesses should not discourage learners from embracing the use of YouTube videos.\n\nBlogs\n\nStudents can acquire useful ideas from blogs since they contain meaningful discussions. Bloggers present their commentaries and guidelines on specific subjects. Learners can focus on specific blogs that widen their language skills. Visitors of such blogs can leave their comments and discussions. Blogs make it easier for learners to acquire new skills (Bilbatua & Haro, 2014). This fact explains why students of a specific language can improve their competencies from such blogs. Students can also create their personal blogs in order to become professional writers. They will also improve their writing and reading skills. Blogs have, therefore, been supporting the educational needs of many learners.\n\nAdvantages and Disadvantages of Blogs\n\nJarvis and Achilleos (2013) believe that blogs can support the educational needs of many students. Blogs can offer useful guidelines and ideas to many learners of a second language. Individuals should identify the right blogs in order to achieve their objectives. The power of the internet explains why such blogs can be accessed from every part of the world. The internet is making it easier for learners to acquire new competencies wherever they are. These aspects explain why learners should use these blogs.\n\nSome blogs contain complex words and terms that might not fulfill the educational needs of the targeted learners. Many bloggers focus on topical issues and discussions. Many blogs do not teach targeted learners on how to write and read. The targeted students should also have access to the internet (Stockwell, 2012). They should understand the targeted language in order to post their comments.\n\nPodcasts\n\nStudents of second languages can benefit a lot from podcasts. A podcast is \u201ca program that can be downloaded over the internet\u201d (Jarvis & Achilleos, 2013, p. 5). Podcasts can deliver audio messages and videos to targeted individuals. This fact explains why learners should use such podcasts to share their ideas, concepts, and skills. That being the case, teachers can deliver the required content to their students using these podcasts. Learners of a second language will \u201cacquire new language skills such as pronunciation, spelling, grammar, and interpretation of different words\u201d (Rosell-Aguilar, 2013, p. 79). Podcasts can also be downloaded and shared by many learners. Many teachers use podcasts to teach their students in different settings.\n\nAdvantages and Disadvantages of Podcasts\n\nStudents can access different podcasts over the internet. Learners can download such podcasts and use them to improve their educational needs. Podcasts make it easier for learners to pronounce different words properly. Podcasts are also easy to create, thus encouraging more learners to share their ideas without meeting in a classroom. Podcasts can be designed to address the different learning needs of many students. The effectiveness of podcasts makes them popular in various learning institutions (Rosell-Aguilar, 2013).\n\nPodcasts present several weaknesses to the user. To begin with, students with various learning disabilities might not benefit from these podcasts. Teachers might find it impossible to create quality podcasts. For instance, podcasts can be time-consuming (Stockwell, 2012). The producers of such podcasts should understand the educational needs of their learners.\n\nSkype and Google Hangout\n\nStudents can use Skype and Google Hangout to practice different languages. Google Hangout is \u201cone of the best CALL tools\u201d (Jarvis & Achilleos, 2013, p. 14). Skype also makes it easier for learners to interact with different members of the global community. Such learners can interact with one another using these two tools. Learners in different locations can practice together for several hours. Learners of second languages can use Skype to interact with their tutors in every corner of the globe. These tools offer powerful incentives and support systems that can support the needs of many learners (Jarvis & Achilleos, 2013). Students can, therefore, embrace the use of these tools in order to improve their language skills.\n\nAdvantages and Disadvantages\n\nThese tools encourage learners to interact with native speakers of the targeted language. The above tools also increase the speed of interaction. Skype encourages many individuals to interact with one another. These tools have voice options that can support the learning demands of many learners. Many people are today using different social networks, thus attracting more learners. This development explains why more students will use Skype in the future (Jarvis & Achilleos, 2013).\n\nOn the other hand, Google Hangout discourages many learners from focusing on the targeted educational goals. It is also notable that the users require stable internet connections. This requirement discourages many students from using Skype for educational purposes. Some native speakers might demoralize these learners. Every learner should \u201cunderstand how to download and activate these applications\u201d (Stockwell, 2012, p. 74).\n\nOnline Articles and Newspapers\n\nLearners can interact with different articles and newspapers in order to improve their language skills. Many websites offer \u201conline dictionary tools and instant translators\u201d (Stockwell, 2012, p. 76). It is necessary for learners to read their favorite articles and topics. This practice will make it easier for them to acquire the best language skills. Such newspapers can be accessed from different parts of the globe. Learners of a new language can benefit significantly from such resources. The decision to read favorite topics will make it easier for many learners to acquire new phrases and words. Students can follow online news in order to acquire the best skills and concepts.\n\nAdvantages and Disadvantages\n\nOnline articles focus on a wide range of topics and subjects. Learners of secondary languages can select their favorite articles. This practice will make it easier for them to achieve their learning goals. Such articles and newspapers are usually written by professional authors. This fact explains why the targeted students will acquire the best skills and competencies (Stockwell, 2012). The presence of online translators and dictionaries makes such websites useful.\n\nMany online websites are usually haphazardly written. This situation explains why learners should be careful whenever selecting the most appropriate online articles. Learners should also have a basic knowledge of the targeted language. Some translators can be erroneous thus affecting the quality of information acquired by the students. Some online newspapers contain unfavorable phrases and words for different learners.\n\nConcluding Summary and Personal Opinion\n\nStudents should use modern technologies in order to achieve their academic goals. Individuals can also use such technologies to widen their competencies. Modern computer technologies facilitate the continued exchange of information. Students can improve their competencies using different computer-based technologies. Online exercises and YouTube videos encourage learners to focus on their academic goals.\n\nBlogs makes it easier for more learners to interact with one another. This exercise has highlighted the issues associated with computer-assisted language learning (CALL). These computer technologies will ensure more individuals acquire new language competencies. Online newspapers, Skype, and Google Hangout can support the educational needs of many learners. Researchers and technologists should identify new applications that can support the language needs of many students (Jarvis & Achilleos, 2013).\n\nThis discussion explains why technology has become an unstoppable force in many learning environments. Students should use the internet to acquire new language skills. They should also interact with one another in order to achieve their goals. Teachers should also encourage their students to use the above CALL tools. Such tools can support the educational needs of many students in different institutions.\n\nReference List\n\nBilbatua, L., & Haro, A. (2014). Teachers\u2019 Attitudes towards Computer-Assisted Language Learning in Australia and Spain. Research Online, 57 (1), 1-44.\n\nChapelle, C., & Jamieson, J. (2008). Tips for Teaching with CALL: Practical Approaches to Computer-Assisted Language Learning. New York, NY: Pearson.\n\nJarvis, H., & Achilleos, M. (2013). From Computer Assisted Language Learning (CALL) to Mobile Assisted Language Use (MALU). The Electronic Journal for English as a Second Language, 16 (4), 1-18.\n\nRosell-Aguilar, F. (2013). Podcasting for Language Learning Through iTunes U: The Learner\u2019s View. Language Learning and Technology, 17 (3), 74-93.\n\nStockwell, G. (2012). Computer-Assisted Language Learning: Diversity in Research and Practice. New York, NY: Cambridge University Press.\n",
        "label": "human"
    },
    {
        "input": "Computer-Supported Collaborative Learning Essay\n\nComputer-supported collaborative learning (also referred to as CSCL) is a relatively new branch for education. It explores people\u2019s ability to learn together with the assistance of computer technologies. The concept seems simple, but actually, it is rather complex. It includes distance learning, collaborative interactions of the students and the use of computers as mediators. In the contemporary world with its rapid technological progress, computer-supported collaborative learning is an excellent way to improve education with the help of innovative technologies and integrate it into the modern life where the use of computers is an essential activity.\n\nComputer-supported collaborative learning stands out because it relies on the independent interaction of the learners, which enforces active learning and facilitates a constructivist environment. CSCL assumes an alternative role for the teacher who is not expected to monitor the actions of the learners, but to provide guidance when students need it and start asking questions. The importance of CSCL lies in its emphasis on collaboration and group work, where thinking and decision making are done together. This way, the groups of learners acquire collective knowledge through participation in the combined working process.\n\nComputer-supported collaborative learning may be used as the main teaching strategy when distance courses are implemented. Besides, CSCL may be employed as a part of the teaching strategy for specific assignments or homework, for example. CSCL is an excellent tool to deliver a group activity and facilitate active learning without the teacher\u2019s supervision. This way, CSCL can be used several times within a curriculum for specific collaborative projects such as group reports, online investigations, scavenger hunt, and brainstorming of new ideas.\n",
        "label": "human"
    },
    {
        "input": "Computer-Assisted English Language Learning Report (Assessment)\n\nUsing the Internet in EFL\n\nItem\n\nA 7-page PDF document available online.\n\nAuthor and Date\n\nDavid A. Trokeloshvili and Neal H. Jost.Aug. 1997.\n\nSource\n\nCourse pack available online at ( http://iteslj.org/Articles/Trokeloshvili-Internet.html ).\n\nDescription of Content\n\nA detailed description of the experiment (introducing Internet and intranet network in schools) and its influence on student needs and teacher goals.\n\nOutcomes & Main Issues\n\n  * Identified the basic student needs and teacher goals related to the use of the Internet in EFL.\n  * Recognized primary constituents of the introduction to the computer process.\n  * Introduced major requirements for using the Internet in EFL.\n  * Pointed to the influence of the Internet on writing skills.\n\nIssues covered\n\nAchievements in the English language learning process when CALL is used\n\nThe major positive outcome of using the Internet in the classroom is the overall improvement of writing skills because students are provided with access to vast resources of information, which might both make the writing more accurate due to the availability of facts and creativity. Moreover, this system is connected with the introduction of the intranet network, which offers options for communication, i.e. practicing both formal and informal English language, thus helping to develop proper skills.\n\nNative speakers in ELT\n\nNative speakers affect language fluency because they use different forms of contractions, linking, and assimilations. In the case of using the Internet in the classroom, students have access to audio resources, which eliminates the risks mentioned above.\n\nThe importance of authenticity of the learning material\n\nAs the focus is made on writing, the authenticity of learning materials is critical because, in case of ignoring it, the students might choose to neglect the quality of their work.\n\nUsing authentic materials with EFL/ESL students\n\nItem\n\nA 6-page PDF document available online.\n\nAuthor and Date\n\nCharles Kelly, Lawrence Kelly, Mark Offner, and Bruce Vorland. Nov. 2000.\n\nSource\n\nCourse pack available online at ( http://iteslj.org/Techniques/Kelly-Authentic.html ).\n\nDescription of Content\n\nA detailed description of different authentic materials that can be used in work with EFL/ESL students as well as the procedure of choosing appropriate tools and potential challenges, which might appear during the implementation of the new system.\n\nOutcomes & Main Issues\n\n  * Explained ways to turn authentic materials in an effective tool for organizing the work of EFL/ESL students.\n  * Recognized the role of authentic materials in increasing the effectiveness of English classes.\n  * Pointed to a great variety of authentic materials, which can be used in a classroom.\n  * Stated that authentic materials are beneficial for creating a multidimensional atmosphere in the classroom.\n\nIssues covered\n\nAchievements in the English language learning process when CALL is used\n\nThe major advantage of using authentic materials is the fact that they are beneficial for creating a multidimensional atmosphere in the classroom. It means that there are higher chances of increasing student involvement in the learning process and improving their performance due to positive motivation, i.e. interest in tasks. Also, because of a great variety of these tools, they help the students to develop critical thinking skills because authentic materials incorporate real-life issues and tasks in the deployed educational program. Finally, they are useful for making tasks diversified and challenging.\n\nNative speakers in ELT\n\nNative speakers tend to use reductions and ignore the criticality of word stresses. Also, they pause their speech if they hesitate in the correctness of an expressed idea, which might lead a non-native speaker into confusion. Because of these specificities, students might feel uncomfortable when around native speakers because of the lack of confidence in their language skills. Nevertheless, in the case of using authentic materials in the classroom, there are more opportunities for helping the students become more confident and teach them to function and communicate properly when in the English-speaking environment.\n\nThe importance of authenticity of the learning material\n\nIf learning materials lack authenticity, the most likely result is the drop in student involvement in the learning process. It can be associated with a lack of interest in completing tasks. More than that, there is a risk of failing to create a multidimensional environment \u2013 one, which commonly exists in the real world. Finally, there is a threat of choosing inappropriate materials because students\u2019 level of skills and knowledge are ignored if learning material lack authenticity.\n",
        "label": "human"
    },
    {
        "input": "Ransomware in Computerized Medical Systems Essay\n\nOne of the most important technological advances of the 20th century was the creation of the internet, which changed many aspects of our everyday lives for the better. However, the enabling nature of the internet means that people can use the internet to perform malicious activities, one of which is distributing ransomware. With the increasing adoption of digital hospital records and the introduction of computerized order prescription systems, ransomware may pose a serious threat to people\u2019s lives.\n\nRansomware is a type of malware, a piece of software that secretly installs on a computer and disables a user\u2019s access to files stored on its hard drive. While I knew about the existence of ransomware before, I was surprised to learn that hospitals and other public organizations became the victims of cyber-criminals. Indeed, one would think that due to the sensitivity and importance of the information stored in digital medical records, hospitals would implement strict security measures to protect its systems from ransomware. However, the news report by PBS NewsHour suggests otherwise.\n\nThere is no denying that computerized medical records and order prescription systems have numerous benefits. In fact, in the previous video, such technology was described as one of the components of the improved healthcare safety net. However, the implementation of such technology in a hospital setting requires special attention to be paid to security protocols. Personnel should be forbidden to download any attachments from third-party emails, and USB-drives on hospital PCs should be locked for security reasons. Ransomware can only be installed by the user; it is just that the user is sometimes not knowing that the attachment they download is ransomware. Another security measure is disabling administrative rights on hospital PCs, thus eliminating the opportunity for any malware or ransomware install.\n\nI do not believe that hospitals and police departments are specifically targeted by criminals. At 5:50 mark the narrator says that criminals cast a wide net in the hope some inexperienced user will download and install malware (\u201cRansomware attack takes down LA hospital for hours\u201d). Typically, public organizations have strict security policies and do not allow employees to download and install any files. Some organizations even have company-wide firewalls that block any download and installation attempt or require special authorization for the installation to begin. As such, I believe that while UAE hospitals might be at risk, the risk is quite low and can be minimized through comprehensive security measures. As such, the issue at hand is not the ransomware, rather, the irresponsible hospital management did not establish security measures to protect its systems from malware.\n\nIt is also important for personnel to understand the risks. While implementing computerized medical records and order prescription systems managers should educate personnel about the consequences of downloading attachments from third-party emails, and doing any other activity which might compromise system security. Strict policies should be established, and random security checks organized to establish high levels of security.\n\nHospital information systems are an emerging technology and as with any emergent technology, its implementation may come across some roadblocks. One of these roadblocks is ransomware and the issue of security of these systems in general. While there is no doubt that digital medical records are the future of healthcare, their benefits will quickly become obsolete if comprehensive security measures are not followed to protect the patient data.\n\nWorks Cited\n\nRansomware attack takes down LA hospital for hours 2016. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Based Learning in Elementary Schools Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Brief background of elementary school\n 3. Discussion of Current Business Issues\n 4. Discussion of Proposed Solution\n 5. Recommendations for the Executive Committee\n 6. Conclusion\n 7. Reference List\n 8. Appendix 1\n\nIntroduction\n\nComputer technologies became an inevitable part of human life. It is impossible to imagine an industry or a sphere where innovative technologies are not used. Education is one of such spheres. When the computer technologies were integrated in education, only secondary and higher schools were influenced as there was an opinion that students of elementary schools are too young to use computers. Nowadays, this opinion is ruined and there is another point of view.\n\nStudents of elementary schools should be taught by means of the computer technologies for a number of reasons. Different business companies try to implement an opinion that computer technologies are really effective while teaching students in elementary schools and to offer numerous programs and applications which can be used in this age category. The main purpose of the research is to consider the background of the chosen sphere, the elementary education, and discuss current business issues in the computer technologies which may be used in the sphere and may be helpful for business development directed at teaching and computer technologies.\n\nBrief background of elementary school\n\nAbout 36 million children attend elementary schools in the United States of America. This period is considered to be the most critical in students\u2019 development of personality, establishment of learning habits, and formation of other skills and attitudes which may be really important in the future life. It is crucial to remember that the basic skills students should obtain at the elementary level are reading, writing and calculating. Physical needs are important as well as \u201cearly attempts to address the cognitive, physical, social, and emotional development of young students are very important in the child\u2019s healthy development\u201d (Bennett, 2008, p. 1).\n\nPresent system of elementary education involves numerous activities which influence students\u2019 understanding of the modern world. It is proven that democracy is really important for the elementary school. Here are some ideas which prove this statement. Attending elementary schools, students face new circumstances different from the family ones. The necessity to follow some specific regulations and rules may make some students feel inconvenient. This may prevent those from the development of personal traits in communication in the society.\n\nAn elementary school is the first step for students socialization. It is really important for the elementary school students to create personal democratic attitude. Students should be learned to make personal decisions without being pressed by the authority\u2019s opinion. The management in the classroom should be democratic. This can be achieved by means of the following actions, \u201cidentifying rules in classroom management, obtaining students\u2019 ideas, and treating all students equally in the classroom\u201d (Samanci, 2009, p. 31). It is natural that being supported in democratic desires in elementary schools students will become the deserving members of the society in the future.\n\nIt should be noted that much attention has been paid to the computer technologies for the latest period. One of the central ideas for implementation of the democratic norms in the elementary education is the use of the computer technologies. Students have an opportunity to control their achievements personally. Many computer programs have been created especially for the development of learning skills for elementary students. There are programs which assess students\u2019 performance and give some specific recommendations for further development.\n\nAll these practices and programs are directed at students\u2019 self-assessment which is conducted under teachers\u2019 supervision. Doing some tasks on the Internet or writing some specific exercises with the help of the computer programs students learn to control their time and effort personally. It is important to understand that computer based learning in the elementary school is not only a great opportunity for students to become the deserving members of their society with democratic considerations and good knowledge, but it is also a good business which may help numerous companies get the niche and win new markets.\n\nDiscussion of Current Business Issues\n\nBefore getting down to the discussion of the current business issues related to the computer based learning in the elementary school, it is necessary to consider the current situation in the sphere. It should be mentioned that computer based learning should be provided on two levels, at school and at home. These two facilities open new perspectives for business development.\n\nThe implementation of the computer-based technologies in the classroom is really important for a number of reasons. Most of those have been considered above, so it is important to shift to the discussion of the specific issues. According to the research conducted by\u00ad\u00ad\u00ad\u00ad Ming-Chou and Jhen-Yu (2010) the online support of the theme-based learning positively affects students\u2019 learning process. The main conclusions made in the research are as follows, \u201cWeb-based thematic learning actually facilitates a learner\u2019s knowledge integration\u201d and \u201cWeb-based thematic system provides learners with effective scaffolding\u201d (Ming-Chou & Jhen-Yu, 2010, p. 37). These conclusions are really important as this gives an opportunity for the companies to develop the idea.\n\nThere is an idea that the computer-mediated learning and teaching on the basis of the online classroom is one of the best variants for student control and knowledge possession. Dwelling upon the advantages of this method it may be stated that the online classroom may be considered as a learning center with the possibility to renew the sources and to change the form of the learning interaction with the purpose to make the learning more varied (Barabash, Guberman-Glebov & Baruch, 2003, p. 150).\n\nThe creation of the online classrooms may be a good decision for the elementary school students as this will give each student an opportunity to record their success, some through the tasks for several times to understand it fully. Moreover, such opportunity may help students solve the problems connected with class missing for the reason of diseases. Online classrooms may be available from different parts of the city.\n\nMuch attention should be paid to the home computer-based learning as in this case parents are involved in the process. More attention should be paid to mathematics as this is the central subject for many future disciplines. Mathematics for the disables students of the elementary school age is exactly what is considered to be necessary for the modern system of education. It has been proved that if students work successful at home, their learning performance increases. This problem is crucial for students with learning disabilities. The research conducted by Oz (2008) shows that parental interaction and the integration of computers in home studying supports students and makes their progress more effective.\n\nDiscussion of Proposed Solution\n\nDwelling upon the theme-based learning as the issue for the business development, it is important to remember that to implement this strategy in the elementary school, teachers should come across 5 stages,\n\n 1. Find a theme\n 2. Find the focus of interest devoted to the theme\n 3. Find materials\n 4. Integrate the materials\n 5. Publish and share the integrated knowledge (Figure 1).\n\nSuch actions may be rather complicated for the teacher as along with this task, he/she should cope with numerous other tasks. However, this strategy is considered to be the most effective for teaching. So, business companies should create software which would release teachers from coming through those steps. It is more convenient to use the program which has already been created rather than think about the personally.\n\nIt is also crucial for such programs to be related to the elementary school curriculum. The implementation of such software for the elementary school students would be really effective and profitable. On the one hand, students improve their knowledge, and, on the other hand, they improve their skills. The ability of the companies to prove that such software is really critical for elementary school students may be important for the company profit as the more schools order the version, the more money the company gets.\n\nOnline classrooms on the basis of the computer-mediated learning and teaching are great facilities for solving the problem of students\u2019 diseases and lagging behind the whole class. This solution may be effective in a number of reasons, first, a teacher is able to notice the problem at the very beginning and second, the problem of missing school for different reasons may be solved. The online classroom may be a great opportunity for the whole elementary system as being too young students are unable to do some tasks individually, they always require some specific help. The online classrooms may be really helpful. The teachers\u2019 actions will be eliminated to minimum, to check the completed tasks and comment on them.\n\nSpecial attention of the business companies should be paid to the creation of the programs which could help parents teach students with learning disabilities at home. These programs should not substitute the class teaching, they should help those in realization some additional abilities students may have. It is important to remember that students of elementary age have a desire to study if they are interested.\n\nThe solution of the connection of the home programs with the classroom curriculum is a good opportunity to help students cope with tasks more effectively and encourage parents for more collaboration with their children. Turning to the business, this variant is effective as the programs should be created for both schools and homes, so double income may be expected. Thus, it is high time to summarize the information considered in the previous two sections and provide the information in the form of the strategic business planning for the three years frames.\n\nRecommendations for the Executive Committee\n\nBefore getting down to the discussion of the specific actions which should be provided by the executive committee in the relation to the implementation of the computer-based technologies in the elementary schools, the costs and profit of the program should be considered. It is natural that before starting a new program the advantages, disadvantages and risks are discussed. Relating to the information about the costs schools should expect to spend, it is obvious that this affair is not cheap. Million-dollar investments in the technology should be provided by the elementary schools in the whole country. Moreover, one should remember about the support and services which are necessary for the correct functioning of the software and programs (Hurst, 2005).\n\nThus, relating to the considered information, it may be stated that the executive committee should first think about the theme-based learning on the basis of computer teaching. These programs should be aimed at helping students make their learning process more effective and successful. The attention of young learners is unstable and the change of the forms and methods of teaching is crucial in their age. His program may help teachers vary students\u2019 learning. The program should be implemented at the first year teaching.\n\nThe second year should start with the familiarization of students with the online classroom on the basis of the computer-mediated learning. Those children who did not get the main idea of the previous program should continue working there. This may be one of the methods for flexible teaching which is rally effective especially in young students\u2019 age. The programs for home learning should be implemented on this stage. It is important that during the third year all students of the elementary school could use all three programs. The gradual implementation of those is an effective strategy which helps teachers get acquainted with all those programs, understand the strong and the weak sides of each and be able to implement those in teaching process when necessary.\n\nIt is important to remember that technologies do not stand still and in a couple of years the whole world will be controlled by means of the computer-based technologies. Moreover, the whole business world as well as other industries has already been computerized, so the earlier students get to know about computers and their opportunities, the more acknowledge they are going to be in the future. At the same time, the secondary and high school education is based on the computers and online technologies. This is one more reason to involve elementary students in the computer world to make their future collaboration with the technologies easier. Three years is not so much, but this period is enough for creating something new and more innovative in the sphere of education.\n\nConclusion\n\nThus, it may be concluded that elementary school, computer-based technologies and business are absolutely consistent notions and may collaborate together. The computer-based technologies are important for the elementary school as they can solve a number of problems in the future. Moreover, computer-based learning system may help teachers in the educational process and support students\u2019 learning in the situations when they lag behind. The problems and their solutions have been considered and the strategic plan for the nearest three years was created.\n\nIt is important to provide the changes gradually. On the one hand, the business companies can get more profit from the release of different programs, on the other hand, the educational establishments in general and the elementary schools in particular will be given an opportunity to choose the program and software which is the most convenient for them. Thus, it may be concluded that the impact of the computer based learning on elementary schools is great and in the nearest years the sphere of elementary education will be absolutely computerized. At the same time, the teacher will never be substituted with the machines as he/she is a guarantee of students\u2019 success and the means of control of students\u2019 performance that can not only access but also evaluate.\n\nReference List\n\nBarabash, M., Guberman-Glebov, R., & Baruch, R. (2003). Decision-making in Construction of Courses Combining Classroom-based and Internet-based Learning and Teaching Strategies in Mathematics Teachers\u2019 Education. Journal of Educational Media , 28(2/3), 147-163.\n\nBennett, C. (2008). Elementary Education. Elementary Education \u2014 Research Starters Education , 1.\n\nHurst, M. (2005). Schools Eye Future Costs. Education Week , 24(35), 34.\n\nMing-Chou, L., & Jhen-Yu, W. (2010). Investigating Knowledge Integration in Web-based Thematic Learning Using Concept Mapping Assessment. Journal of Educational Technology & Society , 13(2), 25-39.\n\nOz, A. S. (2008). Computer-Supported Collaborative Learning between Children and Parents: A Home-Based Early Intervention Study to Improve the Mathematical Skills of Young Children at Risk for Learning Disabilities. (Doctoral dissertation).\n\nSamanci, O. (2009). Democracy Education in Elementary Schools. Social Studies , 101(1), 30.\n\nAppendix 1\n\n(Ming-Chou & Jhen-Yu, 2010).\n",
        "label": "human"
    },
    {
        "input": "Computer Evolution, Its Future and Societal Impact Research Paper\n\nTable of Contents\n 1. Introduction\n 2. The Evolution of Computers\n 3. The Future of Computers\n 4. Effects of Future of Computers\n 5. Conclusion\n 6. Works Cited\n\nIntroduction\n\nToday, computers have become an integral component of human life. One wonders how life would be without computers. Randell holds, \u201cIn reality, computers, as they are known and used today, are still relatively new\u201d (45). In spite of the computers being in existence since the abacus, it is the contemporary computers that have had a significant impact on the human life. The current computers have progressed through numerous generations to what we have today. The ongoing technological advancement is bound to result in the development of supercomputers in the future (Randell 47). Computer engineers look forward to the development of miniature, powerful computers that will have a significant impact on the society. This paper will discuss the evolution of computers. The article will also discuss the future of computers and its potential repercussions on the society.\n\nThe Evolution of Computers\n\nThe modern-day computers have evolved through four generations. The first generation of computers occurred between 1940 and 1956. The computers manufactured during this period were big and used magnetic drums as memory (Randell 49). Additionally, the computers used vacuum tubes as amplifiers and switches. The use of vacuum tubes led to the computers emitting a lot of heat. The computers did not use advanced programming language. Instead, they relied on a simple programming language known as the machine language.\n\nThe second generation of computers dated between 1956 and 1963. The computers used transistors instead of vacuum tubes. As a result, they did not consume a lot of power. Furthermore, the use of transistors helped to minimize the amount of heat that the computers released (Randell 50). These computers were more efficient than their forerunners. The elimination of vacuum tubes led to a reduction of the size of the computer. The second generation computers comprised a magnetic storage and a core memory.\n\nThe third generation of computers dated between 1964 and 1971. The computers developed during this period were superior in speed. They used integrated circuits. The integrated circuits comprised many tiny transistors embedded on silicon chips. The integrated circuits enhanced the efficiency of the computer. Besides, they contributed to the development of small, cheap computers (Zabrodin and Levin 747). The previous generations of computers used printouts and punch cards. However, the third generation computers used monitors and keyboards.\n\nThe fourth generation computers were developed between 1971 and 2010. The computers were designed at a time when the human had realized tremendous technological growth. Thus, it was easy for computer manufacturers to put millions of transistors on one circuit chip. Besides, the manufacturers developed the first microprocessor known as the Intel 4004 chip (Zabrodin and Levin 748). The development of a microprocessor marked the beginning of production of personal computers. By early 1980s numerous brands of personal computers were already in the market. They included International Business Machine (IBM), Apple II, and Commodore Pet. The computer engineers also came up with graphical user interface (GIU) that enhanced computer usage (Zabrodin and Levin 749). They also improved the storage capability, primary memory and speed of the computer.\n\nThe Future of Computers\n\nThe current computers use semiconductors, electric power, and metals. There are speculations that future computers will use light, DNA or atoms. Moore\u2019s Law hints that the future computers will shift from quartz to quantum. Computer scientists continue to increase the number of transistors that a microprocessor holds. With time, a microprocessor will comprise multiple atomic circuits. That will usher in the era of quantum computers, which will utilize the power of molecules and atoms to execute commands (Ladd et al. 47). The quantum computers will use qubits to run operations. A quantum computer will ease the computation of complicated problems. Unfortunately, such computers will be unstable. People will require ensuring that they do not interfere with the quantum state of the computer. Interfering with the quantum state will affect the computing power of the computer.\n\nLajoie and Derry claim, \u201cPerhaps the future of computers lies inside us\u201d (23). Computer scientists are in the process of developing machines that use DNA to execute commands. The collaboration between biologists and computer scientists could see the creation of the next generation of computers. Scientists argue, \u201cDNA has the potential to perform calculations many times faster than the world\u2019s most powerful human-built computers\u201d (Lajoie and Derry 31). Therefore, in future, scientists may look for ways to develop computers that exploit the computing powers of the DNA. Scientists have already come up with the means to apply DNA molecules to execute complicated mathematical problems (Lajoie and Derry 34). Indeed, it is a matter of time before computer scientists use DNA to develop biochips to enhance the power of computers. DNA computers will have a storage capacity that can hold a lot of data.\n\nEffects of Future of Computers\n\nThe development of sophisticated computers will have a myriad of effects on human life. The future computers will have an intelligent that is akin to or superior to that of humans. Presently, some computers can read multiple books in a second. Besides, some computers have the capacity to respond to questions asked in natural language. Google Company is working on a project to develop an artificial intelligence that can read and comprehend different documents (Russell and Norvig 112). Such an artificial intelligence will serve as a source of information. People will no longer require reading books or going to school. Besides, it will render insignificant the need for human interactions. People will use computers to get answers to all their problems.\n\nDevelopment of sophisticated computers will result in many people losing their jobs. Once computer scientists develop a computer with intelligence akin to that of human, there will be the rise of intelligent robots that will perform most human jobs. Currently, some robots facilitate production of products (Doi 201). In the future, there will be robots that can construct roads, work in supermarkets, and prepare meals in restaurants. There will be no need for human labor any longer. The development of supercomputers will have positive impacts on the provision of quality healthcare. There will be computers that can perform blood tests, measure the level of cholesterol, and diagnose allergies (Doi 203). Besides, the computers will examine people\u2019s DNA to determine potential genetic risks and forecast possible illnesses. Such computers will help to boost the quality of healthcare and minimize deaths that result from erroneous diagnoses.\n\nConclusion\n\nComputer development has evolved over time resulting in the formation of personal computers that are not only small in size but also efficient. Computer scientists continue to develop sophisticated computers. In Future, computers will use DNA, light, and atom to process data. The scientists are in the course of developing quantum computers. Additionally, collaboration between computer scientists and biologists will facilitate the creation of biochips using human DNA. Development of supercomputers will not only enhance the provision of quality healthcare but also eliminate the need for schools and human interactions.\n\nWorks Cited\n\nDoi, Kunio. \u201cComputer-Aided Diagnosis in Medical Imaging: Historical Review, Current Status and Future Potential.\u201d Computerized Medical Imaging and Graphics 31.5 (2007): 198-211. Print.\n\nLadd, Thaddeus, Fedor Jelezko, Raymond Laflamme, Yasunobu Nakamura, Christopher Monroe and Jeremy O\u2019Brien. \u201cQuantum Computers.\u201d Nature 464.1 (2010): 45-53. Print.\n\nLajoie, Susanne, and S. Derry. Computers as Cognitive Tools , New York: Routledge, 2009. Print.\n\nRandell, Brian. The Origins of Digital Computers , New York: Routledge, 2013. Print.\n\nRussell, Stuart and P. Norvig. Artificial Intelligence: A Modern Approach , London: Prentice Hall, 2003. Print.\n\nZabrodin, Aleksey and Vladimir Levin. \u201cSupercomputers: Current State and Development.\u201d Automation and Remote Control 68.5 (2009): 746-749. Print.\n",
        "label": "human"
    },
    {
        "input": "Computer Gaming Influence on the Academic Performance Essay (Critical Writing)\n\nTable of Contents\n 1. Purpose of the Study\n 2. Research Questions, Hypothesis, and Objectives\n 3. Literature Search\n 4. Research Design\n 5. References\n\nPurpose of the Study\n\nThe purpose of this study is to evaluate the influence of the computer and video gaming on the academic success of the school children. The literature review of this study is most directly based on the scholarly journal articles considering the correlation of the gaming activities of students and their academic performance; apart from the journal articles, the literature review includes several news and magazine articles. Namely, the reviewed literature connects the two activities (studying and computer gaming) in a variety of ways and reports that in most cases, playing computer and video games encourages the students to spend less time on their homework and as a result, demonstrate a poorer performance during standardized testing (Dewar, 2013).\n\nAt the same time, some sources point out that the impact produced by computer and video gaming on the students\u2019 academic success is not always negative. For instance, according to Gibbs (2016), the learners who are engaged in gaming regularly show better results in such disciplines as math, science, reading, than the students who do not play computer and video games. In that way, this study uses the results of the previous researches and surveys to elaborate on the topic and find the possible connections between gaming among school children and their academic performance.\n\nThe study is anticipated to either support or oppose the findings of the previous research. Also, this study may find some new correlations between gaming and learning and thus provide the basis for future exploration. In turn, it may facilitate a better understanding of the connection between computer gaming and school performance that will contribute to new ways to combine the two activities and incorporate one into another. This tendency has the potential to benefit the education of the future.\n\nResearch Questions, Hypothesis, and Objectives\n\nThe purpose of this research is stated in the forms of questions and objectives.\n\nThe research questions of this study are the following:\n\n  * In what way do computer and video gaming of the learners during their leisure time correlate with their academic performance?\n  * How does the number of hours the students spend playing computer and video games correlate with their GPA in different subjects?\n  * How does the number of hours students spend playing games correlate with the number of hours they spend doing homework?\n\nThe research objectives are the following:\n\n  * To determine the positive and negative effects of gaming of the students on their academic success\n  * To identify the disciplines where the learners benefit from their gaming hours\n  * To identify the disciplines where the students\u2019 performance is diminished due to the hours of leisure gaming\n\nLiterature Search\n\nTo search for the literature related to the selected research topic, I plan to use versatile databases (such as Ovid and ProQuest) as well as Google Search and Google Scholar. The scholarly journal articles will be prioritized as the sources for the literature review. However, such sources as magazines and news articles will be used for additional information and a wider scope. The sources will be searched using keywords; some of them are \u201ccomputer gaming\u201d, \u201cvideo gaming\u201d, \u201ceffect on academic performance\u201d, \u201cstudents\u201d, and \u201clearners\u201d.\n\nFirst of all, the sources whose titles and topics match the selected research question will be selected. Secondly, all the sources will be sorted based on their dates, and only the most recent ones will be prioritized. To be more precise, the sources published before the 1990s will be considered irrelevant and outdated. Further, ten of the most suitable sources will be selected for the review; and the preference will be given to scholarly research articles (both qualitative and quantitative) so that the majority of works in the review are academic and trustworthy.\n\nThe preliminary sources chosen for the literature review are listed below with the brief descriptions:\n\n 1. \u201cGaming frequency and academic performance\u201d by Ip, Jacobs, and Watkins.\n\nThe article researched the behavioral patterns of studying among college students and found that many sacrifice their studying activities to dedicate more time to games (Ip, Jacobs, & Watkins, 2008).\n\n 1. \u201cGender Differences and Related Factors Affecting Online Gaming Addiction among Taiwanese Adolescents\u201d by Ko, Yen, Chen, Chen, and Yen.\n\nThe authors focused on the gender-related factors of the game playing habits of the adolescent school students and found that gaming was more popular among male participants (Ko, Yen, Chen, Chen, & Yen, 2005).\n\n 1. \u201cVideo-Games Do Not Negatively Impact Adolescent Academic Performance in Science, Mathematics or Reading\u201d by Drummond and Sauer.\n\nThe article researched the effect of video games on the learners\u2019 performance in reading, math, and science and found insignificant correlations (Drummond & Sauer, 2014).\n\n 1. The Impact of Video Games on Student GPA, Study Habits, and Time Management Skills: What\u2019s the Big Deal?\u201d by Weaver, Kim, Metzer, and Szendrey.\n 2. \u201cA Study of the Influence of Gaming Behavior on Academic Performance of IT College Students\u201d by Ku, Kwak, Yurov, and Yuva.\n\nThe authors found that the GPA is affected in a statistically significant manner by computer gaming (Ku, Kwak, Yurov, & Yurova, 2014).\n\n 1. \u201cThe Effect of Videogames on Student Achievement\u201d by Craton.\n\nThe article researched the behavioral patterns of studying among college students and found that many sacrifice their studying activities to dedicate more time to games (Craton, 2011).\n\n 1. \u201cVideo Games and Academic Performance\u201d by Khadra, Hackshaw, and Mccollum.\n\nThe study aims at finding patterns in game-playing and academic success and establishes no particular correlation between the two activities (Khadra, Hackshaw, & Mccollum, 2013).\n\n 1. \u201cThe effects of video games on school achievement\u201d by Dewar.\n\nThe author cites the findings of previous research that connects gaming with the hour\u2019s children spend doing homework, and the overall cognition and thinking behaviors (Dewar, 2013).\n\n 1. \u201cAcademic Effects of Video Game Playing on Children\u201d by Lister.\n\nThe work lists various effects of gaming on academic success and states that multiple factors determine the quality of the final impact (among them there is the length of play, the types of games, the hardware used, and the contents of the games) (Lister, 2016).\n\n 1. \u201cPositive link between video games and academic performance, study suggests\u201d by Gibbs.\n\nThe article cites the findings of a study that compares gaming with solving puzzles and complex tasks that create a positive effect on the learners by improving their problem solving and logical thinking skills (Gibbs, 2016).\n\nResearch Design\n\nThe study design of this study will be causal-comparative. It will focus on the participants\u2019 academic performance comparing the results of 2 groups of students \u2013 the children who play games regularly (every day for several hours) and the students who do not play games at all or spend a significantly shorter time doing it. The data will be collected including the number of hours students spend playing daily for a lengthy period and their GPA in different subjects. The numbers of the two groups will be compared to establish correlations and patterns. The independent variable is represented by the hours of game playing, and the dependent one is academic performance (GPA).\n\nThe threats to validity are inaccurate data about hours of gaming. Confidentiality will be the factor for encouraging the participants to report accurate numbers. Generalizability will be impacted by the income level of the participants as wealthier families have more devices and better games that may create different effects from cheaper and simple ones. A separate set of questions targeting the kids of games played will be added to consider this factor.\n\nReferences\n\nCraton, J. (2011). The Effect of Videogames on Student Achievement . Web.\n\nDewar, G. (2013). The effects of video games on school achievement . Web.\n\nDrummond, A., & Sauer, J. (2014). Video-Games Do Not Negatively Impact Adolescent Academic Performance in Science, Mathematics, or Reading. PLoS One, 9 (4): e87943.\n\nGibbs, S. (2016). Positive link between video games and academic performance, study suggests . Web.\n\nIp, Jacobs, & Watkins. (2008). Gaming frequency and academic performance. Australasian Journal of Educational Technology, 24 (4), 355-373.\n\nKhadra, R., Hackshaw, C., & Mccollum, L. (2013). Video Games and Academic Performance . Web.\n\nKo, C., Yen, J., Chen, C., Chen, S., & Yen, C. (2005). Gender Differences and Related Factors Affecting Online Gaming Addiction among Taiwanese Adolescents. Journal of Nervous & Mental Disease, 193 (4), 273-277.\n\nKu, C., Kwak, M., Yurov, K., & Yurova, Y. (2014). A Study of the Influence of Gaming Behavior on Academic Performance of IT College Students. Gaming Behavior and Academic Performance, 1 (3), 1-11.\n\nLister, J. (2016). Academic Effects of Video Game Playing on Children. Web.\n\nWeaver, J., Kim, P., Metzer, R., & Szendrey, J. (2013). The Impact of Video Games on Student GPA, Study Habits, and Time Management Skills: What\u2019s the Big Deal? Issues in Information Systems, 14 (1), 122-128.\n",
        "label": "human"
    },
    {
        "input": "Human Computer Interaction: Types of Special Needs Essay\n\nTable of Contents\n 1. Introduction\n 2. Types of Special Needs\n 3. Design Considerations for Professor Hawking\u2019s Special Needs\n 4. Types of Solutions Available\n 5. Recommended Solution\n 6. Conclusion\n 7. Reference list\n\nIntroduction\n\nThe rapid growth of the information technology sector worldwide, with the accompanying rise and spread of the internet denotes the intensity of the challenge of meeting the special needs of persons who cannot use the standard versions of tools and equipment available for accessing information technology services. Computer access stands as the standard measure of participation in the information age.\n\nResearchers continue to investigate possible solutions by making unique adaptations to equipment to address the limitations caused by a wide range of impairments that impede access and service delivery, through information technology equipment, to individuals with special needs. The mouse and the key board are the two basic input devices necessary for the operation of a computer. Various adaptations of these two primary input devices are available to serve the needs of persons with special requirements.\n\nTypes of Special Needs\n\nSpecial needs take various forms. Physical impairment, caused by injury or disease, makes it difficult and in some cases, impossible for individuals to interact with information technology equipment, usually by rendering parts of their bodies unresponsive. Mental retardation on the other hand interferes with the process of control such that an individual, who may not have any physical impairment, does not have the necessary degree of mental control to interact sufficiently with information technology infrastructure.\n\nThe third type physical impairment that interferes with interaction with information technology infrastructure is perceptual disorders. Blindness or unreliable vision limits reception of visual information, and the operation of controls requiring visual feedback. Most information technology equipment use a screen to display information and as the interface for receiving commands.\n\nDesign Considerations for Professor Hawking\u2019s Special Needs\n\nProfessor Hawking has limited motor control. The key design considerations to put into focus when providing Professor Hawking with a solution to enable him control his computer include the nature of impairment and the degree of control he has over his body. These considerations will influence the choice of possible input devices. It is also imperative to know his technical ability. This refers to the degree of training he possesses in the use of a computer and the training he may require to operate the computer effectively.\n\nThe third consideration will be the peripherals he will require for his operations in his line of work, such as a printer and a scanner. The other class of consideration is summarized by Seba, Hew and Huang (2004) who noted that, \u201cin many important applications such as computer aided tutoring and learning, it is highly desirable (even mandatory) that response of the computer takes into account the emotional and cognitive state of the human user\u201d.\n\nOne of the main issues relating to the nature of Professor Hawkins impairment required for the design of an effective human computer interface is the nature and degree of his impairment. From the picture provided, it is evident that he has a degree of motor impairment since he sits on what appears to be a wheelchair, which has a head motion restraint. It is important to verify the degree of this impairment to determine whether he has any form of control over his limbs, and the degree of dexterity available with such control. It is crucial to determine whether he can move his head and how well he can move his eyes. We can safely assume that his cognitive abilities are not seriously impaired since this would have impeded his work as a professor.\n\nThe second class of issues to determine is Professor Hawking\u2019s technical ability. This includes the degree of training and experience the professor has in operating a computer. This will inform the design of a training program for him when he starts using the computer. The third category of issues to determine refer to the Professors\u2019 actual needs.\n\nAs a professor, there is a wide range of applications for which he may need a computer. These include teaching, research, and the production of modules and conference papers. Over and above the primary uses, the Professor may have other personal uses for which he requires a computer, such as playing music and storing and viewing photos. Additionally, there is need to consider the peripherals that the professor will require to effectively use this computer. All these needs require documentation in order to design an appropriate solution for him.\n\nTypes of Solutions Available\n\nThere are a number of solutions available for the professor. As Weber, Zimmermann, and Zink (1995) mentioned, \u201climited capabilities can be replaced by soft and hardware tools\u201d. These solutions depend on the degree of control he has over his body. Basic control over a computer requires the use of a keyboard and a mouse. The range of solutions available includes an eye-gaze system, foot-control mouse, head-control mouse, joysticks, and mouse-keys. Other mouse alternatives include switch-adapted mouse, track balls, touch pads, and touch screens. Some of the keyboard options available for persons with special needs like the professor include an expanded keyboard, which is larger than normal for persons with low motor control. Others are an eye-gaze system, one hand keyboards, and onscreen key boards.\n\nRecommended Solution\n\nThe recommendation for Professor Hawking is a solution that features an eye-gaze system. Through this, he will be able to perform input functions using his eyes only. From the picture provided, it is highly likely that he has advanced motor impairment, which rules out the use of any motor-based devices. The two eye-motion based technologies available are eye tracking systems and eye gaze systems. Drewes, Luca and Schmidt (2007) stated that eye trackers \u201care used for computer input in the field of accessibility providing means for input for motor impaired people.\n\nThe technologies have some limitations. Drewes and Schmidt (2007) pointed out that the systems are, \u201ccumbersome to operate and less efficient compared to the classical way of interaction with keyboard and mouse.\u201d This setback must not discourage the seeking of a solution for Professor Hawking. While it will not normalize his interactive experience, it will greatly enhance it. Weber et al (1995) said that through enabling technologies, \u201cthe user gains from having more independence and greater opportunity to create an individual life\u201d\n\nConclusion\n\nIn conclusion, there is adequate opportunity to design a system suitable for professor Hawking to enable him to control his computer. The technology chosen must take into account the degree of control he has over his body and it should take advantage of any control he has over his eyes. This way, there will be an enhancement of Professor Hawking\u2019s participation in the information technology experience, making him more productive as a professor.\n\nReference list\n\nDrewes, H., Luca, A. D., & Schmidt, A., (2007). Eye gaze interaction for mobile phones. Proceeding of the 4 th Conference on Mobile Technology, Applications, and Systems, and the 1st International Symposium on Computer Human Interaction in Mobile Technology, 366-370.\n\nDrewes, H., Schmidt, A., (2007). Interacting with the computer using gaze gestures. The 11 th international conference on Human \u2013Computer Interaction, INTERACT 2007 , 2.\n\nSeba, N., Lew, M. S., &Huang, T. S. (2004).The State of the art in computer human interactions. Computer Vision in Human-Computer Interaction. ECCV 2004 Workshop on HCI Prague, Czech Republic, May 2004 proceedings. Berlin: Springer-Verlag.\n\nWeber, H., Zimmermann, G., & Zink, K., (1995). Computer access for people with special needs. In, Rafetty, J., Steyaert, J., & Colombi D., (Eds). Human Services in the Information Age . New York: The Harworth Press, Inc.\n",
        "label": "human"
    },
    {
        "input": "PowerPoint Computer Program: Principles and Processes Essay (Article)\n\nTable of Contents\n 1. Introduction\n 2. Culture of laziness\n 3. People with special needs in the society\n 4. Conclusion\n 5. References\n\nIntroduction\n\nGeneral Mattis, a joint forces commander in the Marine Corps stated that \u201cPowerPoint makes us stupid\u201d (Bumiller, 2010, p.1). Indeed this is a strong sentiment about PowerPoint, but maybe there sits some form of truth in this sentiment. You have to agree with me that computer technology has taken over the world. In every aspect of life, in every profession, computer skills are undoubtedly one of the basic requirements. Does this have any element of truth?\n\nIf it is true, then you must have read or at least heard about or used PowerPoint or PowerPoint presentation. PowerPoint computer program is a computer software package that helps in summarization of bulk information for easy understanding when presented to an audience. Have you ever attended a class or a conference where PowerPoint was used by a presenter to pass ideas? Did you grasp the details of the subject that was being presented? With increased conferencing, the adoption of this \u201cshortcut\u201d to present ideas has become a common phenomenon (Bastable, 208).\n\nCulture of laziness\n\nHowever, PowerPoint has been communized and abused and is not a convincing way to pass ideas. It is earmarked with a lot of negativities and hinders communication in both education and business institutions (Bastable, 208). PowerPoint form of presenting ideas using slides has given birth to a culture of laziness. I remember an engineering student in our college complaining that their lecturer was using slides to teach engineering mathematics.\n\nIt is not good to use PowerPoint to present complex units in colleges. Lectures however, take advantage of PowerPoint to shorten these units so that they can get more time to attend to their private work. Complex organizational financial and accounting reports need to be presented in an elaborate and explicit manner for the audience to understand them. Complex subjects in academic needs to be presented as they appear. However, summarizing of information more so in real and life sciences as part of embracing \u2018technology\u2019 has become common. It\u2019s common to find lectures in class lecturing using slides (Gayle, 2006).\n\nPeople with special needs in the society\n\nAnother question about PowerPoint concerns people with special needs in the society. If their needs are considered when the use of PowerPoint to communicate is addressed is a subject to be debated. However, it is quite difficult to fulfill the needs of the blind when PowerPoint is used to present visual information. PowerPoint uses visual aids to put across ideas. The ideas are summarized; hence, if a person is visually impaired; he or she is bound to be locked out of the presentation unless the needs of the person are addressed which is very rare. There has developed a culture of adopting in management. This comes with costs to be met. For PowerPoint to be used, several equipment needs to be purchased. They include the projector and a laptop. The equipment is expensive for small institutions (Arvidson, 2008).\n\nConclusion\n\nPowerPoint however, is an enjoyable and simplest way of presenting ideas. It saves time when PowerPoint is used to present long topics. The topic is summarized in such a manner that one is able to tackle very many things in one paper. Visual aids play a big role in passing information, thus PowerPoint is key here. As I noted above, PowerPoint has been misused and it is very important to be very careful when using PowerPoint. To avoid its misuse, a greater consideration on the subject being presented has to be emphasized. PowerPoint needs not to be used as a time \u2013 saving tool of communication.\n\nReferences\n\nArvidson, P. S. (2008). Teaching nonmajors: Advice for liberal arts professors . Albany, NY: State University of New York Press.\n\nBastable, S. B. (2008). Nurse as educator: Principles of teaching and learning for nursing practice . Sudbury, Mass: Jones and Bartlett.\n\nBumiller, E. (2008). We Have Met the Enemy and He is PowerPoint: The New York Times.\n\nGayle, B. M. (2006). Classroom communication and instructional processes: advances through meta-analysis. Mahwah, New Jersey. Lawrence Erlbaum Associates, Inc., Publishers.\n",
        "label": "human"
    },
    {
        "input": "Computer and Digital Forensics and Cybercrimes Research Paper\n\nComputer forensics\n\nWithin the last decade, computer forensics has solved so many cases that if it was not for this technology the situation would have been worse. This has been mainly enhanced by the ability of computer and digital devices to store data (Goode, 2009). Luckily, these are the same gadgets that criminals use to perpetrate their crimes and therefore it has been become easier to trace criminal activities through digital platforms. For these reasons, computer forensics has become one of the most used tools of investigation. However, it faces numerous challenges especially the aspect of privacy and the right to private information.\n\nChallenges and opportunities\n\nComputer forensics can benefit greatly from the current digital developments. This includes the use of GPS devices in vehicles and the use of Smartphones. With the numerous use of electronic gadgets such, computers, cameras, gaming devices and music players that contain storage media is a sign of a good opportunity for computer forensics to develop. However, the major challenge faced by the police and computer forensics department is that information on forensics countermeasures is found easily and freely online. This has greatly affected the success of computer forensics and it is the main drawback in this area.\n\nIn addition to avoiding forensics specifications, forensics counter software also can hide terrorists\u2019 activities form the authorities\u2019 surveillance. Nonetheless, coming up with new techniques in computer forensics has positively and negatively affected the world at large. The world is now safer due to the increasing usage of computer forensics in court cases. In essence, the use of computer forensics has enhanced the justice system by ensuring that culprits and perpetrators of criminal activities are brought to book.\n\nDue to the increasing use of computer forensics, more criminals are being convicted from the evidences gathers from digitals gadgets. Currently, the use of conventional telephones and the use of letters as a mode of communication have become obsolete. Modern criminals are using highly sophisticated communication gadgets and this is a great opportunity for computer forensics. Most of the modern equipment used today in communication have the storage media capability. Most of the communication electronics today have a camera and a recording system and their connectivity is linked through centralized networks.\n\nThis means that forensics teams are capable of accessing such data as evidence before a court of law. However, even with the expansive surveillance resources, there is yet another major drawback for the forensics department. Most of the internet service providers have very limited data retention periods. A data retention period is the timeline that specific data has before it is permanently deleted to create more room for newer information. With the limited data reinvention periods, some vital information can be lost or inaccessible.\n\nThis is a major drawback for the forensic department because it can affective or inhibits evidence. Unfortunately, this cannot be changed since some of the data stored consumes a lot of pace. For example, CCTV in the streets and around the cities collects data all day for months. After some time, the data collected may be deleted from the servers to create more room for recent recordings.\n\nImproving Computer forensics\n\nData storage capabilities\n\nOne of the most effective ways of improving forensics includes enhancing and increasing data storage capabilities (Taylor, Fritsch, & Liederbach, 2014). As noted earlier, forensics is facing a challenge due to the limited time of storage or rather the retention period. As data keeps on being removed from the database, it becomes very difficult for the investigators to follow leads and to prove the occurrence of a crime. Some of the criminal activities are purely arranged and planned using the internet. To stop such crimes, the computer forensics needs to monitor the flow of information from one terrorist to another for some time. This is considering some of these attacks have been planned for some time. With the lost data over such long periods, computer forensic evidence may not be sufficient to prove a felony.\n\nPartnership between forensics and the state\n\nComputer forensics cannot work sufficiently without the cooperation of the state/government. For computer forensics to succeed, there must be a substantive and sustainable program to govern and enhance a strategic alliance between the stakeholders. A good example of a working agency partnership is the Indiana state police\u2019s partnership with the Purdue University Department of Computer and Information a technology (Goode, 2009). This is not the only interagency collaborative program that the Indiana state police have engaged in. The police also have a partnership program with the National White Collar Crime Center (Goode, 2009).\n\nThese partnerships have been developed to pursue one agenda which is to hare unique skills and attributes to enhance computer forensics. Computer forensics is mostly very useful in financial crimes. This is why the NW3C is important in this process. The NW3C is a federally funded organization that is responsible for training the police on matters involving financial crimes (Goode, 2009). The organization has been very useful in training the police on computer forensics relevant to the banking industry and financial markets.\n\nIt has also been very instrumental in training the police on various cybercrime investigations (Goode, 2009). Since this is a partnership based on mutual benefits, the police have to return the favor to the organization for the training received. Therefore, as a way of showing its appreciation, the police provide subject matter experts with the platform to experience the real-world situation in crime (Goode, 2009). This gives the organization the advantage of tasting their developed courses to see their effectiveness (Goode, 2009).\n\nUltimately, the organization gets a platform to practice and utilize their developed forensic skills. Students at the Purdue University IT department are also benefiting from this partnership by getting access to the practitioners in this field. In return, the school offers its best brains in research and the digital forensic field. With the highly intelligent students and researchers from Purdue, the Indiana state police have been able to develop one of the most effective forensic networks in the world.\n\nThe concept of Bring Your Device\n\nIn recent developments, the concept of Bring Your Device has been increasingly utilized in many organizations. BOYD is a special concept that allows employees to use their electronic gadgets to access classified and privileged company information (Sridhar & Govindarasu, 2014). There are several challenges that the adoption of this concept experiences. Supporters of this concept argue that the ability for workers to perform their duties from any location is good for business.\n\nWhile such flexibility may be advantageous, there are also several risks involved. One of the fundamental risks that the BOYD concept exposes companies and organizations is the fact that sensitive company information can easily fall into the wrong hands (Sridhar & Govindarasu, 2014). When employees are allowed to access the company\u2019s database using their gadgets, such information can easily be accessed by an authorized person. Devices such as phones, tablets, laptops among others can be stolen and the information stored in them accessed.\n\nThis can put the company at greater risk. BOYD can result in a massive and dangerous data breach hence compromising data security (Sridhar & Govindarasu, 2014). Another way that the company risks a data security breach is when an employee who was using his or her gadget to access the company data leaves the organization. When they do so, they leave with their gadgets and the company\u2019s data they had stored in their devices. This also can create a very serious data security breach.\n\nDealing with digital threats created by the BOYD concept\n\nDealing with digital threats resulting from BOYD CONCEPT the ICT departments need to be on high alert. In every organization today, ICT has been significantly incorporated in the daily organizational functions. In a situation where a fired or an ex-employee is using a password-cracker to gain access to restricted information in an organization, an appropriate measure must be taken to secure the incident. To secure the scene, one requires software like Log2timelieme (Taylor et al., 2011). This software is used to identify the timelines from system logins.\n\nHowever, for this particular occurrence, an incident response software is the most appropriate to address the issues. Volatility is one of the best software available for such a function. The software is designed to address incidences and malware analysis and it allows the investigator to extract digital artifacts from RAM dumps (Chung, Park, Lee & Kang, 2012). This software allows one to extract information from the current running process and also from the cached registry hive, process IDs among another process (Chung et al., 2012).\n\nSteps in dealing with the situation\n\nThe initial steps to follow when investigating a digital crime scene involve obtaining authorization to search and seize the facilities used by the suspected perpetrator. After gaining authorization from the organization management, the next step to secure the area of or the crime scene would be the most prudent action. This helps to avoid an instance where colluding colleagues can tamper with the evidence to influence a favorable forensic outcome. The entire items that were seized during the investigations must be documented and recorded and if any transportation of the confiscated equipment and evidence is to be made, safety should be a priority. Acquiring the evidence from the equipment should be done using forensically acceptable methods.\n\nAfter the evidence is acquired, the forensic images should be used to analyze the data and come up with interpretations based on the collected facts (Garfinkel, 2010). Presenting the analysis and findings of the investigation must be simplified even though complex methods of analysis have been used. The results must be presented in simple easy to understand language and in a written report. The evidence is thereafter presented in a courtroom under an affidavit.\n\nExtracting evidence with the volatility software\n\nFirst, one needs to identify a folder or folders he or she needs to investigate. After identifying the folders, one only needs to place the Volatility-2.1 standalone.exe and open a command prompt window (Garfinkel, 2010). From this window, one needs to click on the executable file and type the name of the software, the plugging name, and the profile name. The plugging name is the name of the file the investigator wants to extract information from. The software does the rest automatically.\n\nAdmissibility\n\nTo enhance the admissibility of evidence, the original copies of the collected evidence should be copied. The collected evidence must be authenticated through an electronic process to prove that the said crime and presented evidences are genuine (Goode, 2009). The evidence must meet the relevance threshold to be admissible in a court of law. In addition to this, an affidavit is required to hold the investigator directly responsible for the evidence provided. Evidence provided must be extracted from the gathered information and not from outside sources (Goode, 2009). These are some of the highest steps that should be taken to ensure that the investigation is legitimately carried out in all fairness to both the defendant and the plaintiff.\n\nSystem upgrading plan\n\nIn every organization, system evaluation is very important for the success of the company. In the banking industry, security checks a regularly required to ensure the bank has the latest system to enhance its security. The growth of ICT has further increased the necessity of systems evaluation on a regular timeframe to enhance security. This paper seeks to discuss the different evaluation strategies for a bank, appropriate monitoring of the system\u2019s progress and evaluation of success and failure methods.\n\nSystems evaluation strategies\n\nTest and evaluation is vital before an upgrade is carried out in any organization. This enables the company to evaluate the present system and identify the loopholes therein. With this knowledge, appropriate measures can be taken in the upgrading procedures. The system evaluation strategies involve testing the bank\u2019s vulnerability in terms of outside infiltration. This requires the bank\u2019s IT department to try and hack their systems to see whether it is possible to steal data from outside (Ammenwerth, Brender, Nyk\u00e4nen, Prokosch, Rigby & Talmon, 2009). Using the system\u2019s protocols, the IT personnel can try to create overrides to determine the vulnerability of the bank\u2019s system.\n\nWhat evaluation methods could be used?\n\nTo evaluate a system may also require the administration to authorize an operation that aims at bringing out the risks of that are unforeseen in a system. Appropriate strategies include a call by value, partial evolution and applicative order (Ammenwerth et al., 2009). In banking, speed is very important and so is accuracy. Upgrading requires the IT department to identify the issues raised about the current computers. Banks require high-speed computers and enough memory for data storage. If the company\u2019s needs are not effectively met by the current desktops, then the most product auction would be to upgrade to higher performance desktops.\n\nMonitor progress and methods of evaluation of success/failure will you use?\n\nTo monitor the progress of a network system in a bank requires time and patience. One cannot determine the extent of a failure in a system if the system is not put in use for long enough for these issues to begin arising (Ammenwerth et al., 2009). Therefore, monitoring a system has to be a gradual process aimed at identifying the underlying issues and risks that a system may expose the bank to. Monitoring the performance of the computers, the software installed and the servers to see determine whether they match or compete with the recent technologies in the market. The method of evaluation appropriate in determining the failure or success of the systems would be the call-by-value strategy (Ammenwerth et al., 2009).\n\nRecommendations\n\nAs a group we recommend a complete overhaul of the entire analog system to be replaced with new technologies. High-speed desktop no less than a dual-core and new installation of modern servers to support the company\u2019s network data should be reconstructed. For efficient services to clients, the bank must enhance its banking service through other platforms such as social media, mobile banking, and online banking.\n\nSuch improvement requires very sophisticated data control systems to ensure they are secure. ICT is a major component in the banking industry and the sooner banks adopt the trends the better for business. This paper has critically analyzed the process involved in upgrading a system in a bank. System evaluation processes have been outlined in the paper as well as strategies for monitoring progress in the systems.\n\nUnderstanding the risk assessment methodologies and its applications is important in that it makes one able to create a more secure computing environment. However one of the challenges is that professionals in information face difficulty due to the fast rate of change in technology. Various tools are used for risk assessment. A good one is the Operationally Critical Threat Asset and Vulnerability Evaluation (Sridhar & Govindarasu, 2014). This helps organizations protected from information security risks. Although OCTAVE is workshop based and not tool-based.\n\nThreat Modeling is in simpler terms a procedure which is used to optimize network security. It does this by checking for vulnerabilities and counters the mechanisms of the vulnerabilities or threats to the system. In this case, a threat is a malicious act that is directly harmful and can cause damage to your system. The point here is to go through the whole system and find where the most effort should be applied or the riskiest area which should be taken care of first to keep the system safe and secure. The technique it uses is it changes with the change in the development of new factors.\n\nRisk assessment is being able to control and manage the potential risks or dangers and taking the necessary steps to make sure they are managed and well taken care of. In other words, it\u2019s the act of controlling the risks and potential dangers. Risk assessment is important in that it protects various aspects of an organization such as its assets. The most important thing to consider in risk assessment is identifying the potential dangers. Risk assessment entails various processes such as qualitative and quantitative risk assessment (Sokolov, Mesropyan & Chulok, 2014). Octave consists of phases and each phase contains several processes. For example phase, one contains processes such as identifying senior management knowledge and creating threat profiles. Phase 2 consists of identifying key components and evaluating selected components.\n\nReferences\n\nAmmenwerth, E., Brender, J., Nyk\u00e4nen, P., Prokosch, H. U., Rigby, M., & Talmon, J. (2009). Visions and strategies to improve evaluation of health information systems: Reflections and lessons based on the HIS-EVAL workshop in Innsbruck. International journal of medical informatics, 73 (6), 479-491.\n\nChung, H., Park, J., Lee, S., & Kang, C. (2012). Digital forensic investigation of cloud storage services. Digital investigation, 9 (2), 81-95.\n\nGarfinkel, S. L. (2010). Digital forensics research: The next 10 years. Digital Investigation, 7 (1), 64-73.\n\nGoode, S. (2009). Admissibility of Electronic Evidence. Rev. Litig, 29 (1), 134-138.\n\nGordon, L. A., Loeb, M. P., Lucyshyn, W., & Zhou, L. (2014). Externalities and the Magnitude of Cyber Security Underinvestment by Private Sector Firms: A Modification of the Gordon-Loeb Model. Journal of Information Security, 6 (01), 24.\n\nSokolov, A., Mesropyan, V., & Chulok, A. (2014). Supply chain cyber security: A Russian outlook. Technovation, 34 (7), 389-391.\n\nSridhar, S., & Govindarasu, M. (2014). Model-based attack detection and mitigation for automatic generation control. Smart Grid, IEEE Transactions on, 5 (2), 580-591.\n\nTaylor, M., Haggerty, J., Gresty, D., & Lamb, D. (2011). Forensic investigation of cloud computing systems. Network Security, 1 (3), 4-10.\n\nTaylor, R. W., Fritsch, E. J., & Liederbach, J. (2014). Digital crime and digital terrorism. New York, NY: Prentice Hall Press.\n\nUsha, M. (2014). A Study on Forensic Challenges in Cloud Computing Environments. Journal of NanoScience and Nanotechnology, 2 (1), 291-295.\n",
        "label": "human"
    },
    {
        "input": "Computer Reservations System in Hotel Term Paper\n\nTable of Contents\n 1. Introduction\n 2. Background\n 3. Opposite points of view\n 4. Findings\n 5. Conclusion\n 6. Works Cited\n\nIntroduction\n\nPeople have always needed some place to live. At the dawn of time, a human being needed some shelter from the wind, frost, rain and, moreover, it should be able to protect him/her from wild animals. With the development of society main function of house has not changed. It still serves as the place where people can shelter from different phenomena of nature and find comfort and relax there. However, sometimes a person needs to go away from his/her home on business.\n\nHaving developed from simple taverns or some other establishments, hotels nowadays play an important role in the life of every city suggesting their services to travelers, businessmen or simple tourists. Resting on these facts, special system which main aim was to book a room for a person appeared. Reservation system became an integral part of the functioning of every hotel. The following work is devoted to investigation of some peculiarities of its functioning and some other aspects connected with the work of this system.\n\nIt is obvious, that this system did not appear at the same time when the first hotels were created. Nowadays, all establishments of this kind in the world are equipped with complicated electronic system which has a lot of different functions. However, situation was absolutely different may years ago. Very often, people had to travel from one hotel to another hoping to find free room for them to be able to have a rest and find some shelter (Riesselman para. 1).\n\nHowever, the need in such system was obvious and people started thinking about possible solutions to this problem. It was necessary to inform the owner of a hotel about the existing need. That is why, it is obvious that blistering development of systems of this sort became possible due to the evolution of means of communication and discoveries in the field of digital technologies. Appearance of telegraph promoted development of systems of this sort while telephone made them very efficient. However, there is no use denying the fact that computers and digital technologies brought the most significant changes to hotel reservation systems, adding new functions and making them much more efficient. With this in mind, it is possible to say that modern age is characterized by blistering development of these systems.\n\nBackground\n\nThat is why, it seems important and interesting to analyze peculiarities of the functioning of these systems nowadays and understand the way in which they obtained new functions. To do it the most efficiently, different sources connected with hotel business were analyzed. Moreover, great attention was given to the needs of clients and their demands for this kind of service. It becomes obvious that different attempts to align efficient functioning of the system of this sort were made by numerous hotels.\n\nThat is why, there is a great number of different materials available for analysis. Besides, taking into account high level of cybernation of modern society and different systems, the attempt to analyze the influence of digital technologies on the work of this system was made. With this object in mind, different sources connected with this way of usage of digital technologies were analyzed. The Internet was used as the main tool which helped to find the needed information and understand peculiarities of the issue better. Moreover, with the help of the Internet the attempt to book a room using the online reservation system was made. This attempt can serve as the beet evidence of availability and clearness of this service.\n\nOpposite points of view\n\nHowever, it also should be said that being very convenient, reservation service, though, is often criticized. There are several points of view on this question. The first obvious remark of the opponents of this service is the lack of the firsthand experience about the hotel. Using the Internet to book a room, a person does not have much information about the hotel as the only data he/she can obtain can be found at the same site and very often it can turn out to be false.\n\nAnother drawback of the given system is connected with the issue of security. The thing is that booking a room online, a person pays with the help of his/her credit card. It is obvious, that every self-respecting hotel has its own security system which should protect payments of this kind, however, there is no ideal protection in the world and it can be overcame. Under these conditions, a person has a risk to have his credit card number stolen.\n\nFindings\n\nHowever, in spite of all these drawbacks, hotel reservation systems are widely used nowadays in almost all hotels of the world. Modern hotel reservation systems allow a person to book a room from any place in the world where the Internet is available (\u201cFeatures of the Online Booking System and Reservation Software\u201d para. 5). Moreover, potential client is able not just to note that he/she needs a room. Besides, there is a wide range of options for him/her to choose.\n\nHe/she can state what kind of room is needed, its price, the view from the window, the menu and the level of service (Niemann, Mochol and Tolksdorf 88). Additionally, it is not so difficult to do as the interface of systems of this sort is rather clear and understandable for the common user of the Internet be able to book a room. Additionally, all modern systems are connected with the service of online payments. It is a great advantage of this tool.\n\nThere are several main modules in hotel reservation system. Users accounts and ordering system play the most important part of its functioning. Usually, all these modules are created in a very simple way for users to understand it easily. The interface is very convenient and understandable. All registered users are included in the database. It helps system to function more efficiently and take into account preferences of people who use this system not for the first time. Moreover, such kind of a database is secured to protect some private information.\n\nIn general, the whole process of booking a room with the help of this service can be described with the help of the following flowchart.\n\nProcess of booking.\n\nClearness and understandness of the given flowchart can show that the whole process of regestration and reserving a room is very simple and can be easlity understood by a common user. Usually, web sites which suggest servives of this sort are organised according to the same pattern. Person should create an account adn than chose the room and servcie. Payments are made with the help of credit card. The sustme is very simple though efficient. However, it is possible to suggest certain steps which can help to evaluate its efficiency and usability. The given plan can be used to test all modules of the system.\n\n 1. Registration of a great number of users.\n 2. Great number of reservations at the same time.\n 3. Analysis of the users feedback about the functioning of the system.\n 4. Analysis of the system load and its functioning under extreme conditions.\n\nConclusion\n\nHaving analyzed peculiarities of the functioning and development of hotel reservation systems, it is possible to make certain conclusion. The issue of hotel reservation system is very important nowadays. The thing is, that all hotels are equipped with systems of this kind and it provides their efficient functioning. Moreover, there are obvious advantages connected with the usage of this system. People are able to book a room being far from a hotel.\n\nAdditionally, they are able to choose some additional options which are needed for them. However, there are also some disadvantages of this system connected with the impossibility to obtain credible information about a hotel. Nevertheless, there are still some risks of losing money. Besides, in spite of all these disadvantages, hotel reservation systems are widely used nowadays and this fact can serve as the best evidence of their efficiency.\n\nWorks Cited\n\nFeatures of the Online Booking System and Reservation Software . n.d.. Web.\n\nNiemann, Magnus, Malgorzata Mochol and Robert Tolksdorf. \u201c Enhancing Hotel Search with Semantic Web Technologies \u201c. Journal of Theoretical and Applied Electronic Commerce Research. 3.2. (2007): 82-96. Web.\n\nRiesselman, Jenifer. Hotel booking history shapes future planning . 2011. Web.\n",
        "label": "human"
    },
    {
        "input": "Online and Computer-Based Technology Issues Research Paper\n\nIntroduction\n\nTechnology has become a critical component in the present world due to its extensive application in different fields of human operation (Cleaver, 2014). Whereas technology has been applied vastly, the education sector has been influenced greatly (Higgins, Xiao & Katsipataki, 2012). Importantly, online education and computer-based learning have taken a critical position in the sector. As such, it is evident that education stakeholders, including teachers, students, and policymakers, must consider technology-based education (Marinkovic, 2011). The important concern relates to the effectiveness of technology-based education when it comes to imparting skills. Also, the ability of teachers to adopt the new method of teaching is of great concern due to the adoption of technological change. Indeed, the current teaching staffs are not effectively equipped with technological efficiency. As such, the ability to adopt technology and applying it in education is questionable. Also, the ease of students using technology is education evokes critical interest too. This is based on the fact that the application of technology in education might not receive great enthusiasm as opposed to general technological communication (Muirhead, 2000). Lastly, the environment created by technological education must be interrogated deeply to understand its benefits in the education system.\n\nProblem Statement\n\nThe introduction of technology in the education sector has led to a critical development in the education sector. However, some important issues have not been addressed satisfactorily. One of those issues relates to the embracement of technology by the teaching staff. In most cases, the current teachers and lecturers are either of average or old age. As such, these ages are not as conversant with technology as the young generations. This implies that teachers and lecturers have deficient skills when it comes to the use of technology. In addition to the deficiency of skills, the staffs have a low willingness to adopt the technology due to conservatism. Conservatism mentality makes the staff stick to the traditional methods of teaching (Ray & Coulter, 2010). As such, the question of whether the staffs are capable and willing to adopt technology becomes of critical importance. Also, even if the upcoming staffs are not above the average age of technology, the ease of using traditional methods still evokes similar concerns. For example, lecturers find it effective to distribute handouts through the class monitor rather than send emails without the guarantee of delivery.\n\nThe second point of concern is the degree to which the students, although young when it comes to age, are prepared to use technology in education. As such, it is understandable that the use of technology in communication is different from the application of the same methods in learning (Reynolds, 2009). In this case, students embrace technology easily when it comes to communication and other general application. However, it does not imply that the students are willing to use technology readily when it comes to education. Importantly, this becomes of great importance when it comes to the use of mobile technology. Indeed, the extent to which a student wishes to receive, tackle, and send assignments is questionable. The use of mobile technology evokes a critical concern when it comes to students\u2019 attentiveness and uptake. In this case, if students are allowed to use mobile technology to access learning materials during class sessions, their concentration reduces significantly to affect the skills uptake.\n\nPurpose Statement\n\nEducational Technology (online and computer-based technology)\n\nThe purpose of this research from the context of educational technology will focus on online and computer-based technology. In this case, online education uses mobile and computer-based technology to facilitate the different functions of education. These functions include accessing the learning materials and assessing skills\u2019 proficiency. In that regard, therefore, the problems that have been identified in the problem statement will be studied from the perspective of online education and the computer-based view.\n\nAcademic Discipline and the Content Field\n\nWhereas the application of online education and computer-based technology can be applied in many disciplines, this research will be concerned with the education sector. Indeed, teachers and lecturers provide vital services in the economy because they are the facilitators of education. Also, the educational staff is expected to disseminate the ideology of educational technology after qualification. As such, it becomes important to identify some of the benefits of using technology to impart skills to educational specialists. Also, it is critical to identify some of how the use of computer-based education can help educational professionals to create a good environment for learning in the technological context.\n\nTarget Audience\n\nIn the case of the target audience, this research will focus on the students who have already attained the college level. Indeed, the college level students have already determined their areas of specialization. In particular, it will target college-level students who have specialized in education. The education students are the relevant specimen in this research because it seeks to determine how technology-based learning can be promoted by involving its stakeholders. Importantly, teachers and lecturers are some of the most crucial stakeholders in this venture. Secondly, it will target teachers and lecturers whose age is above 50 years and another set of respondents below that age. This will facilitate the study of the age-based attitude difference which exists towards technological education.\n\nSignificance to the Field\n\nThe significance of this study should be viewed from two different and crucial perspectives. First, the significance of the study relates to the use of technology in academic institutions. The world is experiencing a paradigmatic shift from an analog-centered economy to a digital globe. The world has been interconnected by technology because people have adopted it as a means of communication. As such, the education sector cannot be ignored because it has to change by this shift (Walker et al., 2013). It is, therefore, important to determine how technical education can be designed to create an appropriate learning environment. Indeed, this becomes important because the introduction of technology should not reduce the quality of education offered. In any case, it should make it easy to access and improve the quality of the skills.\n\nTheoretical and Conceptual Framework\n\nTheory of Change\n\nIn this research, it is evident that the world has shifted from an analog globe to a digital system. As such, people have become used to technology in their daily communication and transactions. As such, the educational sector will be forced to change how it imparts skills. As such, since this aspect warrants change in the sector, the Theory of Change serves a critical role in this research. In principle, the theory of change focuses on how human behavior changes with time (Whitney, 2009). Also, it focuses on the outcomes of the process of change regardless of whether it has been induced naturally or artificially. Importantly, the theory revolves around showing the clear difference between the prevalent and the intended outcome of change. This is very important in this research because it will help to analyze the intended plan and the actual result of the computer-based technology in the education sector. Secondly, it seeks to address the issue of how stakeholders should set their intended results to determine how they will participate in the process of change. This factor will help to identify the stakeholders that should be involved in the implementation of computer-based education, how they should set their goals and the strategies which should be used to achieve those objectives.\n\nResearch Questions\n\nBy the problem statement, three research questions will be considered in this study.\n\n 1. To what extent are the college students pursuing the education specialty prepared to use technology for educational purposes?\n 2. What are the difficulties incurred by the old-aged instructors when using technology to impart skills and how can those deficiencies be eliminated?\n 3. If any, how can the stakeholders eliminate the possible deterrents and inhibitors of technology-based education?\n\nMethodology\n\nParticipants and Availability\n\nThe research participants will include the stakeholders of the education sector in the country. As such, the participants will be divided into three segments including the lectures, students and the policymakers. The lecturers are expected to provide information concerning the difficulties they incur when using technology to teach. On the other hand, the students pursuing education at the college level will be required to answer the question of whether they are prepared to adopt technology to learn. Lastly, the administrator/policymakers will provide information that relates to the objectives of computer-based education as well as the strategies used to achieve the goals.\n\nResearch Method and Design\n\nFirst, the research will use a deductive approach when it comes to the research design. A deductive approach provides a research question, makes a hypothesis, and then proceeds to collect data to either accept or reject the hypothesis. Also, the research will incorporate both qualitative and quantitative research design because the research questions involve both aspects. In particular, the fist and the third research questions are qualitative because they seek to determine the difficulties incurred by the old-aged instructors and the solutions to the deterrent factors. The second question is quantitative because it seeks to determine the extent to which the students are willing to use technology to learn. Importantly, the research data will be collected by administering questionnaires using an online survey. The online survey is meant to ensure that the data collection is timely. Also, the sampling will be conducted using the random purposeful sampling which included two different sampling techniques. Random sampling is meant to ensure that the results are naturalistic while purposeful sampling ensures that the respondents provide relevant data.\n\nMeasurement of Students\u2019 Outcomes\n\nRegarding the measurement of the students\u2019 outcomes, there will be two different and critical measurement parameters. First, the students will be assessed from the perspective of whether the students are capable of using technology effectively in education. The second parameter will seek to measure the willingness of students to use technology to learn.\n\nAdditional Expected Outcomes\n\nIn addition to the students\u2019 outcomes, the effectiveness of instructors and the policymakers will be evaluated. In this case, the assessment will be based on whether their objectives are achieved within the stipulated period. As such, their assessment will be tied to the performance of students taking online courses and their effectiveness when it comes to the application of the learned skills in the field.\n\nInstruments\n\nOne of the instruments that will be used is a questionnaire that will be administered using an online survey. Also, the research will require the use of sound recorders in case the respondents are required to make an audio presentation through Skype and phone calls. As such, the recorder responses will be meant to ensure that all the information is available in its authentic form. The phones will also be required to call the respondents in case clarification and follow-ups are needed after the questionnaires are received.\n\nProcedures\n\nThe research will start with a pilot study which will be meant to determine the practicability of the research methods and design. The pilot study will incorporate all the aspects of the research including experimental sampling, data collection, and analysis. After the pilot study, the identified flaws will be corrected and the actual research conducted from the sampling to analysis.\n\nLimitations\n\nAlthough the research is essentially significant, it is necessary to state that there are a few limitations. One of the critical limitations revolves around the use of online surveys rather than the physical interviews. Indeed, the online survey will eliminate the opportunity to detect some of the emotional and nonverbal cues which are very important in qualitative analysis.\n\nReferences\n\nCleaver, S. (2014). Technology in the Classroom: Helpful or Harmful? Web.\n\nHiggins, S., Xiao, Z.,& Katsipataki, M. (2012). Interpreting the evidence from meta-analysis for the impact of digital technology on learning. Web.\n\nMarinkovic, D. (2011). Sociology and constructivist perspective: Sociological theory and constructivist meta-theory. Developmental Psychology and Learning, 13(6), 109-124.\n\nMuirhead, B. (2000). Enhancing Social Interaction in Computer-Mediated Distance Education. Educational Technology & Society, 3 (4), 104-115.\n\nRay, B., & Coulter, G. (2010). Perceptions of the Value of Digital Mini-Games: Implications for Middle School Classrooms. Journal of Digital Learning in Teacher Education, 26 (3), 92-100.\n\nReynolds, M. (2009). Wild Frontiers Reflections on Experiential Learning. Management Learning, 9(6), 387-392.\n\nWalker, K., Curren, M., Kiesler, T., Lammers, H., & Goldenson, J. (2013). Scholarly Networking Among Business Students: Structured Discussion Board Activity and Academic Outcomes. Journal of Education for Business, 88 (5), 249-252.\n\nWhitney, L. (2009). Change Theories. Nursing Research, 7(3), 234-229.\n",
        "label": "human"
    },
    {
        "input": "Computerized Accounting System Project Report\n\nIntroduction\n\nMany researchers define a project as a distinctive undertaking that tries to achieve a distinct purpose. A project, normally, has multifaceted but interconnected small projects within it. A project\u2019s limitations include duration, budgets, and scope. Researchers further state that each project is unique, as it is unlikely to be repeated. Project objectives are determined by the parameters of duration, budgets, and scope (referred to as performance) (Gray & Larson 2008). Duration, budgets, and scope have to be balanced for the most favourable outcome. Thus, duration, budgets, and scope are a triangle of objectives, referred to as \u201cthe magic triangle of project management\u201d. If one is affected, the two other objectives will also be affected. However, the quality/performance objective is often considered paramount (Hay 2010).\n\nReview of project management methodologies\n\nProject Cost\n\nThe cost of the project depends on the selected software and the properties of the hardware installed for the new system. The investment for the project will involve the cost of disposing of the current computers used by the accounting department. It will also include purchasing new computers, procuring the accounting software, and installation of the system to substitute for the existing procedures. Another cost driver is the training of the users of the new system for accounting and auditing processes.\n\nProject Scope\n\nThe primary scope of the project is to install an automated computerized accounting system to operate the government\u2019s accounting process. The main objective of the project is to provide an effective system to facilitate the accuracy and efficiency of accounting for transactions as well as financial information. The system needs to be user-friendly and supported by a centralized IT infrastructure. The system will be configured to ensure simplified posting of accounts payable and accounts receivable. The system should also eliminate the need for multiple postings in the books of accounts to maintain accounting records and comply with accounting principles.\n\nThe current project will necessitate the government to acquire new computers with more efficient processors and reliable memory capacities. Also, the project should include the installation of secure and reliable data storage and backup system and devices. The project will also incorporate training of the government\u2019s staff to match their skills with the demands of both the new system and the accounting principles.\n\nTeam Organization\n\nThe team that will be in charge of this project includes key leaders in charge of all the departments that will be affected by the new system. A project manager who will be assisted will head the team by the current head of the IT department as a project assistant manager. A project management committee that will meet every week to review the progress of the project will make major decisions on the project. Overall, the project team will comprise four teams responsible for specific roles. These teams are the system development, the hardware installation, the accounting, and the training teams. A supervisor who will be a member of the project management committee will head each of these teams (Rossberg 2014).\n\nProject Schedule\n\nThe project will be held in a period that will be determined by the government\u2019s management since its implementation will disrupt the operations of the government\u2019s accounting department (Johnson, Whittington & Scholes 2011). Project evaluation will be done weekly, but daily assessments will be carried out to review its progress. There will ensure there is no time gap between project pilot exercise and project rollout to avoid delays in the implementation schedule. The management will decide on the expected completion dates from three dates recommended by the project management committee (Westland 2007).\n\nNetwork Diagram\n\nNetwork Diagram\n\nThe following are the identified project activities represented by each letter in the\n\nnetwork above. Each activity will be implemented independently but in line with the schedule of the project (Schwalbe 2013).\n\nTable 1: Activity Durations\n\nIdentification of all the paths of the project\n\n  * A \u2013 D \u2013 H \u2013 K = 5 + 3 + 5 + 1 = 14\n  * B \u2013 E \u2013 I \u2013 K = 2 + 1 + 6 + 1 = 10\n  * C \u2013 F \u2013 I \u2013 K = 2 + 3 + 6 + 1 = 12\n  * C \u2013 G \u2013 J \u2013 K = 5 + 4 + 2 + 1 = 11\n\nThe project\u2019s critical path is B-E-I -K and will take one year to complete. The shortest time it can take to complete the project is, therefore, one year (Kirkpatrick & Locke 1991).\n\nGantt chart\n\nThe project\u2019s Project Manager may find it quite useful to engage the use of a Gantt chart in managing complexity in cost and time (Vargas 2007). A Gantt chart is an intricate tool used for the management of interrelated tasks with different durations. When using a Gantt chart, the project manager assumes that the tasks are linear, and their durations can be determined beforehand with a high degree of precision. However, management should have duration estimates with the relevant possible contingencies (Ahrens & Chapman 2007).\n\nA Gantt chart has several benefits to the project manager. First, it diagrammatically represents the whole project, which makes it easy for the project manager to identify the activities to complete first and clearly shows the relationships between tasks. Second, it shows the duration of a project. However, in as much as it may show the tasks clearly, it does not indicate dependencies among tasks and the project manager may not know from the Gantt chart how the delay of one task may affect another. For this purpose, the project manager will have to use the network diagrams. The figure below shows the Gantt chart for this project. It indicates the start times and durations for each activity. However, it does indicate the costs. The durations are indicated in days/months (Snyder 2010).\n\nProject cost\n\nMeasuring performance is used to determine the success or failure of a project. The project is successful if it has been completed according to specifications and on eight time. However, for a long-term project such as National IT Project, these criteria cannot be used to assess the entire project while it is still ongoing. However, they can be used to measure the performance of project tasks, which are an indicator of the eventual outcome of the project.\n\nAs the project parameters are time, cost, and performance, the first measurement parameter for National IT Project is whether the subcontracts have been completed on time and within budget. In terms of performance, some aspects of the project can only be assessed when it is complete. Nevertheless, if quality control is done for each segment of the project as and when it is completed, the likelihood of the completed project meeting and /or exceeding performance requirements will be increased.\n\nProject and Budget Control Charts\n\nThe project manager may use the Project and Budget Control Chart below (Johnson, Whittington & Scholes 2011).\n\nTable 2 Example of Project and Budget Chart\n\nProject communication\n\nThe management of the project will adopt several methods of passing information as well as tools of sharing data among the team members and other users involved. The management will print out a Gantt chart for the project to indicate progress and use a timescale to make the Gantt chart fit one page. The chart will then be pasted in a PowerPoint or printed on a slide for easy presentation.\n\nThe management will also print a \u201cTo-do List\u201d information sheet for all the team members to ensure that everyone understands his or her role. Also, the project manager will prepare a \u201cWho Does What\u201d information sheet that will determine all the roles and people responsible for them. Hence, the team will facilitate coordination among the team players. The continuous update will be done to the communication plan as appropriate and will be reviewed during the weekly status meetings. However, the management of the government and the Project Manager will approve all the changes that should be made to the plan during the meetings.\n\nIn the National IT Project, project change is authorized through change orders. The project manager keeps track of them and reports on them to stakeholders at all levels of the project. Changes are often caused by a change in the clients\u2019 requirements, changes in local authority regulations, correcting errors in the specifications, unavailability of specific materials or equipment, and new technology. Minimizing misunderstanding due to change is the responsibility of the project manager. Requests for change should be detailed, including the time and cost estimates for making the change, and the period for responding to the change request.\n\nIn the National IT Project, the change procedure is the responsibility of the project manager. As far as reasonably possible under the contract agreements, subcontractors are supposed to fulfil the terms and conditions of their contracts. If a subcontractor is unable to do so, they are required to communicate with the project manager, who will then decide what level of change (if any) can be allowed from the original requirements without compromising project schedule, budget or quality.\n\nThe following is a model communication matrix (Manzoor, 2012).\n\nCommunications Matrix\n\nRisk Management Plan\n\nRisk assessment of the project will be defined in terms of the probability of occurrence of risky events (Hitt & Hoslisson 2008). Concerning the impact of the risks, the negative cost of each one will eventuality be regarded as an impact of the risky event on the project? Risk analysis will, therefore, involve the increase in the impact of the speculated risk events and their probability to occur as well as their sensitivity to change. Risk response will be determined by the findings of risk analysis of every event (Castells 2011). The response will determine appropriate actions to be taken if the speculated events occur. The response will be coordinated through a risk response table that will include a summary of a contingency plan for all the risks (Holliday 2007). Designing an automated system will encounter several risks. For example, the basic probability of equipment failure to more complex issues, including the threat of the wrong choice of information management strategy. The risks might lead to confusion in the data and the ultimate failure of the venture. However, an efficient risk management plan will ensure coordination (Chase & Aquilano 2006). In this project, the highest risk is a technical risk.\n\nRisks Register\n\nTable: 3 National IT Program Risks Register\n\nSensitivity Analysis\n\nTable 4: Best/Worst Case Scenario\n\nRisk Response Plan\n\nThe risk response plan includes the following recommendations to mitigate risks.\n\nTable 5: Risk Responses\n\nProject review\n\nFor this project, the most appropriate method of reporting the project should be the linear method. In this method, project members will report to the project manager over their respective tasks. The project manager will compile this report to help determine the level of success of the project. The project manager will then prepare a detailed report and give it to the projects coordinator. The project coordinator will verify this report and then send it to the top management. The top management will approve and make a publication of the report at a preferred time.\n\nIn the process of implementing a project, problems would always rise. When such cases arise, the management is forced to come up with a solution that will help in dealing with the problem. One such problem that can arise when implementing this project is when employees start giving fake reports to please the management. The latter is quite dangerous because of the hope and subsequent responsibility and finance that will be committed to the project will be lost. To deal with this problem, the management should conduct regular evaluation of the project and discourage unfaithfulness among employees. Another problem may arise when there is lack of corporation or proper coordination of the employees who form the team implementing the project. Therefore, there will be an ongoing review of work done as it progresses.\n\nSupplier Management\n\nThe project management committee will float tenders to interested suppliers and receive their offers according to the government\u2019s procurement procedures (Barney 2002). After assessing the received tenders, the project management committee will recommend the best suppliers to the procurement department to select the most appropriate one among them. The selected supplier will be contracted to supply the government with the ordered software, computer hardware and system development services. In such a way, the project management team will acquire the services of the best supplier while maintaining the government\u2019s procurement standards.\n\nReference List\n\nAhrens, T & Chapman, C 2007, Management Accounting as Practice, Accounting, Organizations and Society, vol. 32 no. 1, pp 1-27.\n\nBarney, J 2002, Gaining and Sustaining Competitive Advantage, Pearson, Upper Saddle River, NJ.\n\nCastells, M 2011, The Rise of the Network Society: The Information Age: Economy, Society and Culture, John Wiley & Sons, New York, NY.\n\nChase, B & Aquilano, N 2006, Operations Management for Competitive Advantage, McGraw Irwin, New York.\n\nGray, C & Larson, E 2008, Project management: The managerial process, McGraw\u2013Hill Education, Singapore.\n\nHay, I 2010, Qualitative Research Methods in Human Geography, London, Oxford University Press.\n\nHitt, M & Hoslisson, R 2008, Strategic Management Competitiveness and Globalization, Thomson, London.\n\nHolliday, A 2007, \u2018Doing and Writing Qualitative Research\u2019, Journal of Geography, Vol. 65, no. 2, pp. 14-16.\n\nJohnson, G, Whittington C & Scholes, K 2011, Exploring Strategy Text & Cases, FT Prentice Hall, New York.\n\nKirkpatrick, S, & Locke, E, 1991, \u2018Leadership: do traits matter?\u2019, Academy of Management Executive, vol. 5 no. 2, pp 48-60.\n\nManzoor, Q 2012, \u2018Impact of employees motivation on organizational effectiveness\u2019, Business Management and Strategy, vol. 3, no. 1, pp. 1-12.\n\nProject Part One 16 Rossberg, J 2014, Beginning application lifecycle management, Springer, Berkeley, CA.\n\nSchwalbe, K 2006, Introduction to project management, Thomson Course Technology, Boston.\n\nSchwalbe, K 2013, Information technology project management, Cengage Learning, Boston.\n\nSnyder, C 2010, A user\u2019s manual to the PMBOK guide, Wiley, Hoboken, NJ. Stonebumer, G, Goguen, A & Feringa, A 2002, Risk management guide for information technology systems, McGraw-Hill, London.\n\nVargas, R 2007, Practical guide to project planning, CRC Press, New York, NY. Westland, J 2007, The project management lifecycle, Kogan Page, London.\n",
        "label": "human"
    },
    {
        "input": "Human Computer Interaction in Web Based Systems Research Paper\n\nAbstract\n\nThe purpose of the report will be to look at human computer interactions in web based systems. The report will contain a definition of human computer interactions and web based systems as well as the various types of human computer interaction systems. The report will also focus on the background of the concept as well as provide information on the history of human computer interactions focusing on when this concept emerged and when these systems were developed. The report will also look at the various research writings and literature that are available to explain the concept of human computer interactions. Methodology will also be used to determine what techniques have been used to collect the information used in the research. The scope of research in the report will focus on the amount of research findings and discussions that will provide new information on the topic. The report will also focus on recommendations on human computer interaction and web based systems.\n\nIntroduction\n\nThe advancements in technology and technological innovations have made it possible for the gap that exists between the human and user interface to be bridged. A user interface is described as the system that enables users or computer operators to have an interaction with a computer or any computerized machine. The components that make up a user interface include the hardware components which are mostly physical and software programs that are used to run the hardware. User interfaces or UIs provide machine users with the option of inputting information into the computer systems and providing output information for the input request by the user. The current information systems categorize the human-computer interaction to take place through the use of the user interface.\n\nThe purpose of the user interface is to foster an interaction between the user and the machine that will result in effective operations and carrying out of computerized activities. The interaction also ensures that there is feedback and communication in what the user wants to be performed by the machine which will also aide in decision making. User interfaces are also designed to make the use of the machine to be an enjoyable and efficient experience. With the continued improvements on technology taking place everyday, the user interface has faced modifications that have seen the system become more graphical and user friendly in nature.\n\nThe personalization of computer systems with particular focus on the user interfaces has been viewed to provide each individual user with the particular set of information that they need. The personalization of web services has been seen to improve the interaction that the user will have with the information presented by the machine. The growing use of the Internet as a means of sharing and distributing information has initiated the growth of instruction oriented websites or web pages that offer instructions in the form of supplemental materials. The Internet incorporates the use of hypertext or hypermedia programs in the form of link nodes that provide accessibility to various information sites.\n\nThe information contained in the hypertext is usually in a non-linear and an unstructured form, with the main method of data manipulation being scrolling and clicking. The amount of data contained in a web page makes it difficult for the user to view all the information in one sitting when compared to using a textbook. This creates a loss of interaction effectiveness between the user and the computer; such a challenge presents a problem to the designers of software programs who find it difficult bridging the gap of effectiveness in the user interface (Dillon & Zhu, 1997).\n\nHypertext functionality provides approaches for finding information over the Internet. The most common approach is typing the URL link of the information that one is interested in within the browser location after which the page shows the information requested by the user. The user mostly has to describe the information they are looking for by keying in a search word or letter. This however presents a problem when the results from the web page display information that the user was not looking for or the requested information is too short and unclear. Another problem presents itself when the webpage provides interesting information making it difficult for the user to navigate through the page and find the most useful piece of data (De Bra et al, 2004).\n\nBackground and History of Human Computer Interactions Web Based Systems\n\nThe history of HCI technology has been characterized by a lot of research work and technological improvements of the concept. Research has been conducted on how to improve human computer interactions through the improvement of the user interfaces. The continuous development of computers and the technological improvement of computer graphics led to the creation of human computer interaction tools and approaches. The HCI approaches were developed to bridge the gap that existed between the human user and the computer\u2019s interface (Kumar, 2005).\n\nThe concept of human computer interaction has its background in industrial engineering which emerged during the industrialization period in the early 19 th century. Industrial engineering arose out of the need to improve industrial productivity by focusing on techniques that would ensure production has been performed in an effective and efficient way. Industrial engineering saw the development of specialized tools that would be used to increase productivity in industrial factories as well as in work stations. These specialized tools included computers that would be used to reduce the amount of work that was performed in administration offices while at the same time increase the efficiency of the office workers (Kumar, 2005).\n\nThe continued increase of industrialization around the world increased industrial engineering activities which saw the mass production of personal computers that would be used in workstations and also at home. The mass production of personal computers was directly associated to the quality of user interfaces. Such associations saw the formulation of architecture that would be used to create standardized human computer interfaces. These architectures included the mouse, Windows operating systems and user interface management systems (Kumar, 2005).\n\nThe first research work and development of user interfaces began with the direct manipulation of graphical objects in the 1960s. Direct manipulation involved moving, selecting or manipulating the visible objects in the computer screen by using a pointing device. The first pointing device to be developed was the SketchPad which was used to manipulate objects by grabbing, moving and changing their size through the use of light pen. Xerox, Macintosh and Apple Lisa were one of the first companies in the world to make use of direct manipulation in the 1980s. The mouse was the next HCI component to be developed in the Stanford Research Laboratory in 1965. The mouse was meant to be a replacement to the SketchPad or light pens that were in use before. The mouse was made into a practical input device in 1970. Commercial developments of the mouse were made in 1981 with Xerox being the first company to use the mouse (Myers, 1998)\n\nText editing was a HCI concept and development that was introduced in 1962. Text editing was meant to help the user in their text editing activities such as cutting, copying, pasting, moving and deleting. Hypertext was also introduced during the 1960s to improve the level of human computer interactions between users and web programs. Research work into gesture recognition began in 1965 with the full commercialization of this feature of HCI being used in 1977. Virtual reality was meant to incorporate aspects such as sound and virtual objects into the human computer interaction system (Myers, 1998).\n\nThe history of web based systems was more recent than human computer interactions. Web based systems have a background in web design which mostly focuses on providing a communication link between the user and the machine. The development and design of graphical user interfaces derives most of the technical information from web design as well as web systems. Web designs provide the appropriate techniques that will be used to improve the appearance of the computer interface improving the HCI aspect.\n\nThe design of web based systems mostly focuses on the human user\u2019s physical and psychological abilities when computing. These abilities are usually addressed when developing the HCI. The user\u2019s cognitive abilities are usually captured by the animations and the menu included in the computer interface (Macdonald, 2003). Web programs contain features that allow the user to interact with the website but the computer hardware itself lacks the proper interaction features that will be used to meet the user\u2019s needs (Milewski, 2004).\n\nResearch Findings and Discussions\n\nThe study of how people design, implement and use the user interactive component of the computer systems as well as assess the impact of this interaction to the individual user is known as the human computer interaction (HCI). This field of technology is viewed to have a basis from computer science, web design, and behavioral science. The interaction that occurs between users and computers is usually performed in the user interface. The interface is made up of both software and hardware components of a computer (Wickens et al, 2004). The tools that are used in fostering user and computer interactions include mouse, keyboards, joysticks, sensor pads and track pads. These devices allow the user to input information into the machine thereby aiding the user interaction process. The interaction devices that are used to provide feedback to the user include the visual displays, auditory cues, voice prompts and graphical buttons (Jacko & Sears, 2003).\n\nThe study of human computer interactions draws from the supporting knowledge that exists on computers and human cognitive processes. The information that exists on computers includes the techniques and approaches that are used in creating computer graphics, industrial engineering, operating systems and programs. The knowledge that is used in understanding the human aspect in human computer interactions mostly depends on cognitive psychology, social and behavioral science, communication aspects, graphic and web design and other human factors that come into play when using a computer.\n\nThe multidisciplinary nature of the HCI has seen various contributions and research work being included into the theoretical work that exists about the concept. Researchers, computer engineers and program developers have focused on human machine interactions when developing computer programs and computer upgrades. This focus on interaction is important as it ensures that technical problems do not arise during the human computer interaction (Wickens et al, 2004).\n\nThe main aim of studying human computer interactions is to develop technological innovations that will be more user friendly and acceptable. The field of HCI is multidisciplinary in nature since there is no one theory that can be used to explain the relationship between the user and the machine. There are however traditional cognitive psychological studies based on human cognition that view the user as the major dominant player in the human computer interaction process. According to these psychological studies, humans process information based on their sensory abilities and the type of knowledge they posses (Dillon & Zhu, 1997).\n\nThe basic goal of the HCI field is to improve the usability of the computer\u2019s interface to meet the user\u2019s needs. To meet these goals and objectives, the field of human computer interaction has concerned itself with methodologies and processes that will be used in developing user interfaces as well as approaches that will be used in implementing these user interfaces. HCI has also concerned itself with the techniques that will be used in evaluating the developed interfaces and identifying whether any improvements are necessary. The long term goal of HCI is to design user interface systems that will minimize the barrier\u2019s that exist between the user\u2019s cognitive abilities and the computer\u2019s understanding of the human\u2019s cognitive abilities (Bastide et al, 2005).\n\nResearch into the field of human computer interactions has been successful in the recent past and most of the developments that have emerged as a result of the research work have contributed in the improvement of computing. An example is the creation of the graphical user interface by Microsoft which was incorporated in the Windows 95 version. The creation of this interface was based on research conducted by Xerox PARC, Macintosh and early research work conducted by the Massachusetts Institute of Technology and the Stanford Research Laboratory which is now known as the Stanford Research Institute. Generally without any research work, the field of HCI would not have been able to advance to the current state (Myers, 1998).\n\nThe human computer interaction concept differs from human factors because it focuses more on how humans interact with computers. The computer and human knowledge fields that overlap with HCI include personal information management (PIM), computer supported cooperative work (CSCW) and human interaction management (HIM). Personal information management studies the interaction of human users with computers in a larger informational setting by focusing on the many forms of information that are computer based. Computer supported cooperative work involves analyzing the computing systems that are used to support human computer interactions within a network of people in an organizational or community setting. Human interaction management involves extending the scope of CSCW to cover an organizational level (Wickens et al, 2004).\n\nThe two principles that underlie human computer interactions are vision and hearing. Vision involves the information the machine user receives through their eyes. This information is in the form of text and computer images. The hearing principle involves the information that the user receives through their ears. Computers incorporate audio software programs which relay information through, music or speech programs that have been incorporated into the computer\u2019s hardware.\n\nThese principles are important in human computer interactions as they allow for the user to understand the various functions of the computer. The various technological innovations that have taken place over the years have seen the adoption of virtual reality which enables other modalities such as sense and touch to be incorporated into the user interface. Such modalities form the basis for the interaction aspect of HCI. Interaction accompanies the user\u2019s commands as they are being performed by the computer (Jacko & Sears, 2003).\n\nDesigners of web systems seek to ensure that users can be able to interact with the web programs and with technology in general in an efficient and effective way. The design of the web program will therefore be based on the level of competency the user has with web programs. Human computer interaction studies and research are conducted to ensure that the interaction between the human user and the computer has been addressed in the design of the user interface. The analysis is therefore seen as an important factor in developing a system that will meet the user\u2019s needs and capability of handling web based programs. If the designer or HCI professional has information about users who prefer to use positive polarities in their reading, they can be able to design high resolution screens (Dillon & Zhu, 1997).\n\nIn developing human computer interfaces, the main aspect that is considered is the interface usability which is seen to be an important aspect of the HCI process. Interface usability addresses specific issues that influence the users of computers and the computer itself. To add on this, usability deals with the aspect of human performance while the user is interacting with the computer. Usability involves the ease of using the computer system, the satisfaction that the user derives from using the system and the efficiency of using the system which should be free from any errors and mistakes (Staggers & Miller, 2001). Usability engineering strategies provide the basic evaluation criteria that will be used in assessing whether the program design has met the requirements of usability.\n\nUsability engineering involves the use of usability principles such as the efficiency of the program, the ease of learning the program, satisfaction with the application, the amount of errors and the memorability of the web program to the user in developing effective interaction systems (Raymond, 2004). The architects of software programs have mastered techniques that will enable them to deal with the attributes related to quality programs such as performance, reliability, relevance and sustainability. The aspect of usability has however presented a challenge for these designers as it is mostly focused on presenting information which has made it hard to separate usability from the user interface. Usability has been viewed as a problem in the modification of the web based programs (Bastide, et al, 2005).\n\nAnother aspect that is important in developing HCI systems is adaptivity where the interface is designed to adapt itself according the users changing needs. These needs are usually identified by information that has been collected on the user through design research approaches and analysis techniques. Monitoring techniques are usually used to maintain interactive sessions which lead to adaptations while the user is interacting with the computer system. Adaptivity has been viewed as an important tool in providing direct accessibility to the user during the HCI process. It requires for the establishment of the interaction between the user and the computer to already be in existence (Stephanidis et al, 1998).\n\nTypes of Human Computer Interaction Approaches or User Interfaces\n\nThere are three commonly used user interfaces in web based programs and systems which are the graphical user interfaces, voice user interfaces, and the multi modal interfaces. Graphical user interfaces are the most commonly used human computer interaction techniques because they make the computing of information into the computer easier.\n\nGraphical user interfaces (GUI) separate the logical threads that are used by the user in input functions by displaying the information is a visual format through a window system that contains windows, icons and menus. Graphical interfaces are the most preferable HCI approaches because they ensure the user has visibility to the objects of interest, they enable the user to directly manipulate complex command languages that will be used to decipher the object of interest, graphical user interfaces provide the user with rapid feedback once they key in their requests and they allow for the reversibility of actions without any drastic consequences (Dix et al, 2004).\n\nVoice user interfaces (VUI) encompass the use of speech technology that is mostly important for challenged people who want to access web based systems. VUIs were introduced into the user interface market to meet the dissatisfaction that customer\u2019s had with the touchtone telephony concept. The VUI also arose from a need to have advances in speech technology that would see speech systems being more reliable and effective in speech technology delivery than they originally were (Cohen et al, 2004).\n\nA VUI is described as what the user interacts with when using a web program that incorporates an audio application. The auditory interfaces, otherwise known as VUIs, provide a user interface with the human in the form of sound. The input information that is used by the user is speech and the output of the requested information is in the form of sound. The designers of voice user interfaces have to take into consideration the underlying assumptions and a convention of basic human conversation when designing the software for these interfaces (Cohen et al, 2004).\n\nThe components that make up the voice user interface include voice prompts that synthesize sounds, dialogue logic and grammar which is the response to the voice prompts. The VUIs provide the user with an opportunity to use nonverbal audio functions as well as supplying information that will facilitate the interaction of the user with the application program or software. Voice systems are seen to be effective in that they draw on the users language skills by simplifying the input sequences to a more understandable concept (Cohen et al, 2004).\n\nThe multi modal user interface relies on multiple communication channels that are used to during the human computer interaction process. The communication channels include devices such as keyboards, mouse, sensory feedback and the visual display. For multi modal interfaces to function effectively, they have to involve the use of multiple communication channels that have both the input and output functions. The multi modal user interfaces were basically developed to deal with problems that were associated with the graphic user interfaces and the visual interfaces. The interfaces were developed to provide a more immersive environment for the interaction that takes place between the human and the computerized machine (Dix et al, 2004).\n\nMethodology and Scope of the Research\n\nThere are a number of methodologies that have been used in human computer interactions and in developing interaction design. These methodologies have their foundations from early designs that viewed human cognitive processes as predictable and quantifiable. These characteristics saw computer engineers and interface developers focusing on cognitive psychology and behavioral science when designing user interfaces. The current models have focused on constant feedback that exists between computer users and designers (Sharp et al, 2007).\n\nThe design methodologies that are used in HCI include design research, research analysis and concept generation, prototyping and usability testing, and system testing. Design research involves using techniques such as interviews, observations or questionnaires to gather the existing information on HCI. The designers of user interfaces investigate users so as to learn more about their interactions with the computers and how this interaction can be improved (Sharp et al, 2007). For the purposes of this report, the design research that will be used will be secondary data collected from academic books, publications, journals and web articles that contain information on human computer interaction and web based systems.\n\nResearch analysis and concept generation focuses on the design research collected from the observations, interviews or questionnaires. The design research is usually used to create concepts that will be used in software and hardware development. Research analysis of the HCI process involves the refinement of the design research to suit the design needs of the interface. Research design analysis is usually conducted by brainstorming and discussion to come up with appropriate research information that will be used in developing the user interface.\n\nOnce the design research has been analyzed the next stage will involve concept generation where the most suitable analysis is chosen for developing the user interface. Prototyping and usability design methodologies are used to test the aspect of the concepts that have been generated. Prototypes are used to determine whether the user interfaces have met the requirements of the human users as well as the collected design information. Prototype testing is also performed to determine whether the user interface will be usable. System testing is performed once the system has been completed and it is used for identifying any errors that might occur in the interface (Sharp et al, 2007).\n\nThe scope of research in this report has focused on the human computer interactions in web based systems. The theory and fields of study that underlie HCI have been examined as well as the types of HCI that are in use for web based systems. HCI studies and research work that has been conducted to create a better understanding of the cognitive behavior of human users and the impact of this behavior on the computer\u2019s operations. The research findings and discussion have shown that HCI is mostly concerned with the individual user as well as a larger context of computer users. Research findings have also shown that HCI research work has mostly focused on improving the existing versions of user interfaces to a more virtual reality status.\n\nRecommendations\n\nTo improve existing human computer interactions, continuous research work needs to be conducted to develop newer and better user interfaces that are in line with current technological innovations. User modeling concepts should also be used to improve the human machine interaction by offering solutions to the problem of searching for relevant information. User modeling involves gathering information about the user from the search information they have keyed in that will be used to change the content of the webpage and the links that display results of the user\u2019s request. User modeling is meant to capture the thinking process and knowledge of the machine user that will be integrated into the URL links so that the users search request can be determined and processed.\n\nAdaptive hypermedia can also be used to improve the human computer interaction by providing web personalization services for every user of the webpage. Personalization has taken the form of registering user information with particular websites by providing information about the user\u2019s interests and opinions. Once the user registers, they can be able to personalize the webpage and the information contained in the website to suite their personal needs. The website in turn monitors the user\u2019s patterns and behavior which are used to develop a user model that represents the user\u2019s interests.\n\nConclusion\n\nThe field of human computer interaction is no doubt a complex one but vital in bridging the gap between the user and the computer. There is a growing emphasis on designing interfaces that will meet the needs, program abilities and skills of the user as well as provide a personalized experience when using web based programs and applications. There is a need for designers and architects of web based applications to view interactive systems as important in ensuring the success of the program. This can be accomplished by ensuring there is compatibility between the user interface design and the skills, knowledge and characteristics of the user by analyzing their cognitive processes; this will ensure the human computer interaction is effective and efficient.\n\nReferences\n\nBastide, R., Palangue, P. & Roth, J., (2005). Engineering human computer interaction and interactive systems . Berlin, Germany: Springer Verlag.\n\nCohen, M. H., Giangola, J. P. & Balogh, J. (2004). Voice User Interface Design , Boston, US: Addison-Wesley.\n\nDe Bra, P., Aroyo, L. & Chepegin, V. (2004). The next big thing: adaptive web-based systems. Journal of Digital Information , Vol.5, No.1.\n\nDillon, A. & Zhu, E., (1997). Designing web based instruction: a human computer interaction perspective. In: Khan (Ed.) web based instruction. Englewood Cliffs. New Jersey: Educational Technology Publications.\n\nDix, A., Finlay, J., Abowd, G. D. & Beale, R. (2004). Human-Computer Interaction, 3 rd Edition. London: Addison-Wesley Pearson Education.\n\nJacko, J.A, & Sears, A., (2003). The human-computer interaction handbook: fundamentals, evolving technologies and emerging applications . New Jersey: Lawrence Erlbaum Associates Inc.\n\nKumar, R.R., (2005). Human computer interaction . New Delhi: Firewall Media- Laxmi Publications\n\nMacdonald, N., (2003). What is web design? East Susses, UK: Roto Vision Book.\n\nMilewski, A. E. (2004). \u2018Software Engineers and HCI Practitioners Learning to Work Together: A Preliminary Look at Expectations \u2018, 17th Conference on Software Engineering Education and Training, CSEE&T 2004. Norfolk, Virginia . Web.\n\nMyers, B.A., (1998). A brief history of human computer interaction technology. ACM Interactions , Vol.5, No.2, pp 44-54\n\nRaymond, E. S. (2004). The Art of Unix Usability . Web.\n\nSharp, H., Rogers, Y., & Preece, J., (2007). Interaction design-beyond human-computer interaction , 2 nd Edition. New York: John Wiley & Sons.\n\nStaggers, N. & Miller, S. (2001). Using a web-based prototype and human computer interaction concept to develop a vision for a next generation patient care management system. Web.\n\nStephanidis, C., Paramythis, A., Akoumianakis, D. & Sfyrakis, M., (1998). Self adapting web-based systems: towards universal accessibility . Crete, Greece: Institute of Computer Science, Foundation for Research and Technology.\n\nWickens, C.D., Lee, J.D., Liu, Y., Sallie, E., & Becker, G., (2004). An introduction to human factors engineering , 2 nd Edition. Upper Saddle River, New Jersey: Pearson Hall\n",
        "label": "human"
    },
    {
        "input": "VSphere Computer Networking: Planning and Configuring Report\n\nPlanning vSphere Networking\n\nThe process of configuring networks largely employs skills and competences obtained from vSphere Networking. However, it is crucial to mention that the latter mainly offers information and guidelines on how networks can be configured. In this case, planning of the entire process is fundamental. For instance, it is not possible to come up with vSphere standard switches and distributed switches without articulate planning. VMware vSphere requires complex networking that can only be achieved through the initial planning phase (Ferguson, 2012).\n\nDuring planning, networking best practices, managing network resources and monitoring networks should be put into consideration. In other words, planning of vSphere networking should take into consideration the targeted audience and of course the anticipated benefits or gains. This implies that usage is of great importance.\n\nSystem administrators who are well experienced in either Linux systems or Windows should be part of the planning team. In any case, network configuration usually follows immediately after the planning phase. Therefore, administrators should be very familiar with systems at hand. In addition, there are numerous aspects or elements of virtual machine technology that equally count towards planning a vSphere network.\n\nWhen planning this type of network, a number of factors are worth considering. First, the working principle of a vSphere standard switch should be established. For instance, the same switch is used to create a connection among several Virtual Machines (VMs). Several physical and virtual machines using the same host on both vSSs and ESX/ESXi hosts (Mousannif, Khalil & Kotsis, 2013). This type of planning can take place on any location of a physical environment. Moreover, planners should be conversant with the capabilities of various vSS. This is crucial because it reaches a time when they are supposed to be developed, mounted and may be deleted.\n\nConfiguring vSphere Networking\n\nIn order to successfully configure vSphere networking, it is prudent to begin the procedure by establishing the capabilities of a vSS. The latter refers to a standard switch to be used in the configuration process. The external networks are linked to this switch. Hence, traffic can flow between the switch and VMS after the process of configuration is complete (Mousannif, Khalil & Kotsis, 2013). In the ESXi host, the vSS contains two ports. One of the ports plays the role of network management.\n\nThe port is connected to a network interface card. Alternatively, a physical world uplink can still be used to create such a connection. Each port utilizes an uplink adapter during configuration. Nevertheless, vSSs connections and the manner of creating them are the most important aspects when configuring vSphere networking.\n\nThere are two main types of vSS connections that can be created. It is mandatory to make use of both connections even though their distinctions are dramatic. VM ports and VMkernel ports are the two types of distinct connections in this case. Unless the working principles of each connection is vividly understood, the process of configuring vSphere networks can be very complex and challenging (Rong, Tsai, Chen & Huang, 2014).\n\nVMkernel is connected to services VMkernel ports. In this case, the ESXi host contains a single VMkernel even though numerous ports of this type can be used in a configured system. Nevertheless, it is highly recommended that VMkernel service type should utilize an independent port. The VMkernel port offers a number of VMkernel services in a configured system (Henderson & Allen, 2010). IP storage is the first service provided by the above ports. The storage is attached to a given network. vMotion is yet another service offered by the ports. Other services include fault-tolerant logging and management because console parts are not available.\n\nReferences\n\nFerguson, B. (2012). The Official VCP5 Certification Guide (VMware Press Certification). New York: VMware Press. Web.\n\nHenderson, T., & Allen, B. (2010). VSphere rounds into form. Network World, 27 (16), 24. Web.\n\nMousannif, H., Khalil, I., & Kotsis, G. (2013). Collaborative learning in the clouds. Information Systems Frontiers, 15 (2), 159-165. Web.\n\nRong, C., Tsai, H., Chen, C., & Huang, C. (2014). Analysis of virtualized cloud server together with shared storage and estimation of consolidation ratio and TCO/ROI. Engineering Computations, 31 (8), 1746. Web.\n",
        "label": "human"
    },
    {
        "input": "Cybercrime, Digital Evidence, Computer Forensics Essay\n\nTable of Contents\n 1. Website Provides Real-Life Examples of Computer Forensics\n 2. Website Provides an Overview of Computer Forensics Investigation\n 3. Website Provides Examples of Anti-Forensics\n 4. Conclusion\n 5. Works Cited\n\nAt the turn of the 21 st century the world saw the explosion of computer technology, and it has taken many people by surprise. The prediction about computer technology doubling in capacity and complexity every six months came true. As a result computer software and computer hardware are more complicated compared to its predecessors in the past. There are many users who are not even aware of the true capability of their laptops, mobile phones, and tablets. At the same time cybercrime was also on the rise. In the first ten years of the New Millennium, billion-dollar companies were forced into bankruptcies because of fraudulent practices. These fraudulent practices were tied closely to computer systems.\n\nFraudulent accounting records in WorldCom and Enron were brought to the light of day, because of superb detective skills augmented by forensic investigators skilled in retrieving data from computer systems. However, as the success of one crime unit was made known to the public, there were also several cases that are still unsolved, because criminals are becoming more technically proficient in their desire to evade authorities. It is therefore important to update law enforcement\u2019s knowledge regarding Information Technology in order to catch cybercriminals in the act of committing crime. A novice learning the ropes about the cat and mouse game of apprehending criminals may do well by visiting websites that talk about the science of hiding information in computer systems. The website \u201chowstuffworks\u201d carries an article discussing the basics of computer forensics, this is a good example of a website that is useful in explaining or understanding the reality of cybercrime and digital evidence.\n\nThe howstuffworks website is a good source of information regarding cybercrime and digital evidence. A closer examination of the said website revealed three major pieces of evidence to prove that the web creator has above-average knowledge regarding cybercrime due to the following evidences: 1) The website provides real-life examples of computer forensics; 2) The website provides an overview of computer forensic investigation techniques; and 3) The website provides examples of anti-forensics.\n\nWebsite Provides Real-Life Examples of Computer Forensics\n\nEnron used to be a popular company in the United States of America. Investors made money when they invested their hard earned money in this company. It was therefore a major scandal in the financial world when it was discovered that Enron\u2019s corporate leaders were guilty of accounting fraud. According to the author of the article in \u201chowstuffworks\u201d, the United States Congress authorized the investigation of corporate misconduct (Strickland 1).\n\nAccording to the website, the Federal Government sent a specialized detective force that was given the go signal to investigate Enron using computer forensics. The said specialized detective force was responsible in searching, preserving, and analyzing information that were stored in the computer systems. In this particular case, the data stored in the computer systems had a direct or indirect connection to the crime that was allegedly committed by the corporate leaders and their subordinates.\n\nWebsite Provides an Overview of Computer Forensics Investigation\n\nBefore providing an overview of computer forensics investigation, the creator of the webpage cited the name of Judd Robbins who is a known computer scientists and a leading computer forensics expert. Thus, the reader can conduct a background search on Judd Robbins, to verify the claim that the information provided are accurate and relevant to the topic of discussion.\n\nAfter naming the resource person, the site proceeded to highlight major steps undertaken by a computer forensics detective in handling a cybercrime case. For example, the first step is to secure the targeted computer system. Detectives must develop a foolproof protocol to prevent any type of unauthorized access to the computer or to its storage device. Finally, the webpage alerted the forensics detective to the importance of severing the connection to the Internet.\n\nThe discussion about the inaccessible parts of the computer\u2019s hard drive, and the importance of documentation proves the usefulness of the website when it comes to understanding the nature of computer forensics. It gives the idea that cybercriminals are similar to other unscrupulous individuals who are determined to cover their tracks.\n\nWebsite Provides Examples of Anti-Forensics\n\nThe best proof that the webpage creator had an in-depth understanding of computer forensics can be seen in the discussion on how to defeat the specialized detective force that was sent to detect cybercrime. The website talks about how to conceal data by dividing it into smaller sections. The next step was to conceal the smaller sections at the tail end of other files. According to the website, most data files have an unused portion that are named \u201cslack space\u201d (Strickland 4).\n\nIt is an eye-opening revelation for those who are new to the cybercrime game. At the same time, it is a useful piece of information for cybercrime detectives who are trying their best to extract all relevant information within a computer system. Not all the files that are inside a computer system are relevant pieces of evidences needed to build a case. Therefore, it is a challenge for investigators to navigate through a virtual world of computer files. They are reliant on specific computer software to help them understand where files are hidden. However, a skilled cybercriminal knows the weakness of law enforcement tools that were created to track all relevant files needed to convict the guilty party.\n\nThe website also pointed out that aside from using the \u201cslack space\u201d, cybercriminals are also prone to hide files inside executable files. In a normal file retrieval process, the software detection program skips over the executable files, because these are files needed to install software. But clever cybercriminals have found a way to hide files into executable files using programs called packers.\n\nConclusion\n\nThe website \u201chowstuffworks\u201d has created an article that is helpful for those who want to learn more about computer forensics. It is an excellent source of information because of three major reasons: 1) The website provides real-life examples of computer forensics; 2) The website provides an overview of computer forensic investigation techniques; and 3) The website provides examples of anti-forensics. The website contains information that is not commonly found in other sites. It provides basic information on what a detective needs to do in order to increase the probability of success when building a case against a cybercriminal. On the other hand, this website is accessible to the public. Therefore, it is not accurate to say that it serves a public service when it comes to defeating cybercrime. In fact, cybercriminals can use the information in the said website to improve their skills when it comes to hiding critical files.\n\nWorks Cited\n\nStrickland, Jonathan. How Computer Forensics Works . howstuffworks, 2014. Web.\n",
        "label": "human"
    },
    {
        "input": "Human Overdependence on Computers Essay\n\nLike an addict in denial, some 21 st -century folks are trying so hard to prove that they are not dependent on computers. It is a difficult position to defend because computers pervade 21 st -century societies. Computers are no longer limited to desktops and laptops. The basic definition of a computer covers gadgets that use a microchip. This means that a tablet and a smartphone are examples of a computer. With an expanded view of computers, it is easy to say that we are too dependent on them. We are too dependent on computers because it is used for knowledge acquisition, building relationships, and the acquisition of wealth.\n\nThose who disagree with the proposition that we are too dependent on computers utilize a defense similar to an alcoholic in denial. An alcoholic ingests alcoholic drinks for breakfast, lunch, and dinner. But when confronted with the problem, he will deny that he has a problem. He will argue he can stop drinking anytime he wants. The same rhetoric can be heard from someone who is addicted to Facebook or computer games. The same line of reasoning can be expected from a businessman who works during weekends using a laptop and smartphone. The same lie can be heard from a housewife who is addicted to online shopping. Overdependence on computers is perceived as a weakness. Therefore, there is a need for a cover-up. Overdependence on computers carries with it the same stigma as alcoholism and drug addiction. This explains the need to overcome our dependence on computers. This also explains the need to deny that a problem exists. However, there is no amount of denial that can erase the problem.\n\nThere are those who argue that computers are necessary tools, however, they believe that human beings can live without them. Their argument is based on the knowledge that human beings are adaptable. In a time of natural disaster or war, humans have demonstrated their ability to make the necessary changes to survive the challenges that are before them. Victims of natural disasters have demonstrated their ability to survive and thrive even in the absence of electricity. Those who adhere to this mindset attempts to set aside a few days a week where they do not use any type of computer gadget. However, they have not demonstrated total independence from their need to use computers. Those who are working in an environment that requires constant use of computers have imposed some sort of \u201cfasting\u201d and deny themselves the right to use computers. For example, they can develop a system wherein they are able to spend the weekends without touching a single computer gadget. It is through this process that they are able to demonstrate that they can live without the use of computers. It is a defiant stance against the tyranny of computers in their lives. However, the non-usage of computers is limited only on the weekends. Thus, it is not a convincing proof that they kicked the habit goodbye.\n\nThere are three major reasons why 21 st -century folks have become too dependent on computers. First of all, a computer is a primary component of a phenomenon called social media networking. Computers, such as laptops, desktops, tablets, and smartphones, are needed to access the World-Wide-Web. People access the Web for several reasons, but the most frequent excuse for using the Internet is to access social media networking sites. Computers are primary tools in the modern way of establishing and nurturing relationships. People are social beings, and therefore, it is hard to resist the use of a mechanism that enhances their ability to communicate and strengthen their relationship with friends and loved ones.\n\nSocial media networking sites allow for the establishment of long-distance relationships. At the same time, it is a viable tool in the creation of virtual communities in a fast-paced world. Human beings are not only desperate for relationships; they are also desperate for communities. People need communities as a source of guidance and support in times of need. Communities are needed to establish certain identities. Communities provide meaning to individual lives. However, it is difficult to build traditional communities when people are living far apart. It is impossible to build communities when people are always busy. The solution to their problem is the creation of virtual communities using social media platforms.\n\nThe secondary reason for the overdependence on computers is the need for knowledge acquisition. Google and other search engine websites are important tools when it comes to the rapid acquisition of knowledge. Human beings thirst for knowledge in the same way that an African antelope desperately seeks water in a dry land. People are willing to build libraries to enhance the knowledge acquisition process. Hundreds of millions of books had been printed to satisfy this desire for knowledge. Colleges and universities were established because of the desire for more information. The Internet provides the same satisfaction. However, there is a big difference. The Internet delivers information at the speed of light. In other words, there is no need to visit the library. There is no need to peruse several books in order to acquire the needed information. Before the Age of Computers, readers usually require the metacognition of scholars in order to find the answers to their questions. For example, they need to develop a certain skill in order to find a book that will yield the answer regarding the boiling point of water. In the present time, search engines like Google will provide the answer in a click of a button.\n\nFinally, the third reason for overdependence on computers is due to wealth acquisition and the need to buy goods. Computers have become the primary tools in wealth acquisition. 21 st -century bankers are unable to function without computers. Stockbrokers need computers to buy and sell stocks. Businessmen are dependent on computers because of management requirements. They also need computers when they venture into e-commerce. Gone are the days when financial information is recorded on paper. Financial information is now secured using electronic copies. It is easier to create redundant copies of financial information as compared to copying data through the use of ink and paper. But aside from the acquisition of wealth, computers are needed to purchase goods and services. In the past, people are dependent on telephone lines to call storeowners and order the products and services that they needed for their home or their business. In the present time, shoppers can acquire goods and services wherever they are. There is no need to wait until they get home. Smartphones and tablets enable them to access online stores wherever they are. The ability to purchase items using mobile gadgets is a convenient option. They will never revert back to the old way of shopping for the things that they desperately needed.\n\nConclusion\n\nThose who believe that they are not overly dependent on computers have failed to demonstrate that they can go through an entire year without using computers. In reality, they cannot even go through a single week without using computers. This assertion is true, especially after the definition of computers had been expanded to include tablets and smartphones. There are three major reasons why 21 st -century folks are unable to live without computers. First, they need computers to establish and nurture relationships. Second, they need computers for efficient knowledge acquisition. Third, they need computers to acquire wealth and to acquire goods and services. Computers made life easier for businessmen, investors, scholars, and ordinary folks. There is no going back to the time when people are not too dependent on computers.\n",
        "label": "human"
    },
    {
        "input": "Medical Uses of Computer-Mediated Communication Research Paper\n\nIntroduction\n\nComputer-mediated communication (CMC) is a contemporary technological tool creating a network through which professionals and other people can interact easily globally. Computer-mediated communication bridges all physical and social barriers in the society enabling people from different countries to communicate effectively within limited periods. Medical CMC began in the early 20 th century following the inception of technology and this equally happened after the emergence of industrial revolution.\n\nIn the 21 st century, it became possible for people to use radio, phones, television, the internet, and other contemporary techniques of communication in information sharing.in developed countries, CMC is an advanced stage enabling medics and patients to communicate through a 24-hour Medical services. Through this, the health departments can handle emergencies to reduce the number of deaths occasioned by lack of prompt treatment. Computer mediated communication is of great importance to people especially in areas separated by physical distance. Today, medical tourism is a reality because of such techniques of medication that enable people across different cultures to share ideas and experiences about medicine. This paper intends to explain how computer mediated communication is relevant in the field of medicine. Its significance is explicit considering it approaches a population that recognizes computer advancements as a vital tool in awareness creation and treatment.\n\nUses of CMC in the medical field\n\nDimensions of CMC in the medical field are many including:\n\nAwareness creation\n\nWhen technology first emerged, it had the sole responsibility of promoting inventions. Today, technology is a supporter of innovation because it empowers people to reason creatively in generating ideas that will help the entire society in one way or the other. CMC is responsible for creating awareness in the medical field. Initially, medical experts had to organize public campaigns in order to educate people about healthy living. It took a long time for such campaigns to reach marginalized areas characterized by poor terrains. This disabled transport to some regions making it impossible for people to receive vaccines or information about a disease outbreak. Computer mediated communication enables medical practitioners to share such vital information through the internet and other virtual technologies (Mort, May, & Williams, 2003).\n\nInternet networks traverse different terrains enabling people from marginalized communities to access information and to respond promptly to the situations. According to the ObamaCare plan, prevention is the best way to prevent futuristic occurrences of ailments. This makes prevention affordable in comparison to cure explaining the significance of CMC in the field of medicine today and in the future. There are prospects of improvement in this filed including the ability to train the aged population and the poor about CMC (Chang, 2004). They incorporate the most marginalized group when discussing CMC and training them will automatically improve the situation making awareness creation through CMC very easy. Recently, the US government introduced the use of telemedicine in reducing stigma amongst people living with HIV/AIDS. Through virtual means, people easily share their experiences, they are able to meet people with similar encounters, and they promote safe intercourse and protection to reduce the prevalence of the disease.\n\nTelemedicine\n\nTelemedicine is vast because it incorporates communication between professionals in the field of medicine and transfer of such information to the target populations (Chang, 2004). Through advanced computerized mechanisms, surgeons, doctors, and nurses can communicate with different people within different departments. There is an ardent need for the receptionists, the social workers, and medical experts to work together by informing each other of any developments within and outside the medical facility.\n\nThe same should happen between public and private hospitals when there is need for referral services. In such moments, CMC provides the services of hardware and software in order to limit the physical distance that the involved parties would cover to administer treatment or share information. Telemedicine is also explicit when patients visit Google Doctor and other virtual medical administrators in order receive professional medical advice promptly (Dickens & Cook, 2006).\n\nThis mostly happens in cases of emergencies or in scenarios where medical facilities are very far or when a patient needs the doctor-patient privacy. Proponents of telemedicine mention that it is a life savior because of its efficiency. However, others are quick to discourage this technique for lack of credibility considering fraudsters could take advantage of such platforms to mislead patients. Telemedicine remains very vital for many people because there are sites that offer professional assistance because the doctors that provide professional advice enjoy global recognition for the good work they do in medicine.\n\nResearch\n\nResearch can be very expensive especially when it involves investigations of epidemics. Currently, the world is in the process of seeking a cure for HIV/AIDS, cancer, and diabetes. There are different speculations and researchers come up with new ways of managing the ailments daily. This happens because they share such information through virtual means. This explains why researchers can come up with statistics about the prevalence of a disease in one part of the country and compare the same to data from a different nation.\n\nInitially, research was very expensive because people had to rely on secondary material, physical movement, and information sharing through face-to-face communication. Critics of CMC believe that face-to-face communication is credible at has the ability to convince people that research is right. However, CMC overrules this possibility by explaining that people are free to carry out investigations and share the same information through different platforms and not only CMC (Chang, 2004).\n\nCMC makes research easy because investigators do not expose themselves to dangers of communicable diseases whenever outbreaks occur. Advancements in CMC research in the field of medicine will help in the development of vaccines and medicines that will create awareness about healthy living while providing treatment for world\u2019s severest diseases.\n\nE-medicine\n\nE-medicine is still at its infancy making it possible for medical students to access brochures and examinations through online techniques. Advancement in e-medicine is the ability to market hospitals, hospices, and medications through the internet, social media, television, and phones. Home deliveries of the same medications often support e-medication because the rationale of this technique is to limit distance coverage (Mahwah, Turner, & Peterson, 1998).\n\nE-medicine also promotes medical tourism enabling people to visit the countries marketed for provision of excellent medical care for different patients. Currently, India has the greatest rating for medical tourism supported by such technologies. In India, most women become doctors while men become engineers. This explains technological advancements in the country enabling them to device virtual systems of communication in order to improve the quality of life for the overly populated country. Through CMC, America realized that India provides quality and affordable care for different people especially the ones in need of palliative care or the patients suffering from terminal diseases.\n\nAccording to critics, the US targets India because its intention is to take over the industry by employing Indian doctors in their hospitals, offering them high wages, and reducing the overwhelming interest people have in India. E-medical services will still prevail irrespective of the distance involved and this is the only solace for people who depend on Indian medicine.\n\nSignificance of CMC in the field of medicine\n\nEfficiency in service delivery\n\nOne of the evident outcomes of CMC is efficiency in service delivery. Through electronic services, patients are able to receive medication. This mostly involves emergencies and people who deal with weight related problems. Other avenues of ensuring the same include dealing with patients suffering from lifestyle diseases such as diabetes. Such diseases require control and management and without medical intervention, it would be impossible to find the best medical assistant who would offer credible information through CMC. CMC also creates room for the establishment of emergency alarms by collaborating with MediCall Services in London. MediCall Services incorporate the technologies developed for emergency services so that patients can call their preferred medical outlets to attend to patients (Dickens & Cook, 2006).\n\nThrough CMC, the doctors or nurses are able to communicate to the victims to perform first aid through YouTube. This depends on the magnitude of the problem. While carrying out first aid, the medical facility releases an ambulance to attend to the person. In essence, the overall objective is to prevent death by performing services efficiently through technological mediation procedures. Since the introduction of CMC, the number of emergency deaths reduced because people reach the ER after receiving CMC first aid.\n\nUbiquity\n\nThe ubiquity of CMC enables it to achieve high levels of appreciation across the world. Many people use computer mediated communication to share medical information around the world. They prefer CMC because of its affordability and this improves the rating of CMC in the world making the technique ubiquitous. Today even remote villages have internet services, phones, televisions, and other wireless techniques of communication through which they can get medical information. Through television, the people are able to learn about ongoing public campaigns in relation to disease management and outbreaks. The ubiquity of CMC cuts across cultures, genders, age groups, and income levels because the ardent need for education erases the fact that people have to part with little amounts of resources (Cermack, 2006).\n\nInformation is a tool that empowers many people and medical information is a prerequisite for all people because each person wants to enjoy a long healthy life. This explains why people take precautions; they follow expert advice, and extensively engage CMC in research and sharing of medical information.\n\nReliability and quality assurance\n\nIn the field of medicine, hitches in communication, finances, or technology could cause massive destruction. This incorporates loss of life in delayed responses to situations or lack of communication concerning a disease outbreak among other things. CMC promotes reliability in terms of provision of information in a prompt manner enabling people to take precautions and prevent unnecessary deaths.\n\nCMC invests in technology for purposes of research in the field of pathology, maternal care, childcare, treatment, and palliative care among others. Through effective communication amongst the involved parties, there is quality assurance that patients will receive the correct information and medication (Chang, 2004). Multimedia including fax, email services, social networks, and television among others are the most common techniques used between professionals and patients in CMC. Experts consider such techniques reliable because technological failure does not last long in comparison to failed communication in an analog system. There are high chances of rectifying CMC hitches because technical teams are always monitoring such communication structures even though this raises ethical concerns about the possibility of privacy breaching.\n\nFace-to-face versus CMC in the field of medicine\n\nNeed for training and costs of implementation\n\nThe cost of implementing a CMC structure is very high in comparison to the naturally occurring face-to-face communication. The CMC structure should be able to accommodate the executive team, professionals, and patients and a crisis communication plan should be in place to rectify any mistakes. In addition, when introducing such concepts in an existent field of study, it is important to train people. This includes training of people within the facilities and new officials joining the team. The costs of training are equally high, but in face-to-face communication, interpersonal skills are inherent to humanity. On the other hand, patients in CMC plans do not have to part with consultation fees because they get information through multimedia without any costs (Mort, May, & Williams, 2003). Adverts and campaigns through television or phone conversations are cost free making CMC affordable in terms of service delivery, but expensive in implementation of structures.\n\nAnonymity and credibility building\n\nCMC promotes credibility by creating an assurance that various health messages are under the support of credible medical authorities. However, anonymous people are likely to use the same platform to divert the attention of audiences from credible medications to other improper messages. When this happens, it becomes difficult for patients and the community at large to trust medical facilities that use CMC. According to such individuals, they will only use CMC when they have assurance that only credible people use the networks for communication. CMC is efficient even though people prefer face-to-face communication when they need to verify certain facts (Allen & Hayes, 1994). This mostly happens when there are rumors about the ineffectiveness of CMC or past experiences in relation to the same. Medical experts should intervene to stop fraudulent characters from taking advantage of CMC in promoting medical crime.\n\nPromotion of laxity\n\nFace-to-face communication supports movement from one place to another in terms of research, consultation, or treatment seeking. Following the emergence of CMC, many people save the costs of traveling to medical facilities. Some of them do not have to face stigma associated with some ailments including HIV/AIDS, or Urinary Tract Infections. However, all these promote laxity and lack of physical movement might complicate health even for the medical experts who use CMC for similar purposes (Allen & Hayes, 1994).\n\nConclusion\n\nMany people are likely to trust face-to-face communication for the medical field. Though considered credible, it is not costly or time effective because of the delays incurred in research and consultation or delivery of medication. Health is a topic of massive concern for the entire globe and it would be irrelevant to ignore the significance of information sharing through CMC. CMC is better as opposed to face-to-face communication because it has the potential to grow and improve in the future. It also covers extensive regions by bridging socio-cultural barriers.\n\nReferences\n\nAllen, A. & Hayes, J. (1994). Patient satisfaction with telemedicine in a rural clinic. American Journal of Public Health , 6 (5), 18\u201319. Web.\n\nCermack, M. (2006). Monitoring and telemedicine support in remote environments and in human space flight. British Journal of Anesthesia , 97 (1), 107\u2013114. Web.\n\nChang, B.L. (2004). Internet intervention for community elders: Process and feasibility. Western Journal of Nursing Research , 26 (1), 461\u2013466. Web.\n\nDickens, B.M. & Cook, R.J. (2006). Legal and ethical issues in telemedicine and robotics. International Journal of Gynecology & Obstetrics , 94 (1), 73\u201378. Web.\n\nMahwah, N.J. Turner, J.W., & Peterson, C. L. (1998). Organizational tele-competence: Creating the virtual organization . New York: Erlbaum Publishers. Web.\n\nMort, M., May, C.R., & Williams, T. (2003). Remote doctors and absent patients: Acting at a distance in telemedicine? Science, Technology, and Human Values . Oxford: Oxford University Press. Web.\n",
        "label": "human"
    },
    {
        "input": "Pointing Devices of Human-Computer Interaction Essay\n\nTable of Contents\n 1. Mouse\n 2. Foot mouse\n 3. Joystick\n 4. Trackball\n 5. Touch screens\n 6. Light pen\n 7. Touchpad\n 8. Reference\n\nMouse\n\nA mouse is computer hardware used for navigation through files displayed on the computer screen. The standard mouse has two buttons and a scroll ball. The two buttons are used to select actions on the screen by clicking on either of the buttons for different functions. Most computer user interfaces allow users to customise the functions of the two buttons on the mouse. One of the advantages of using a mouse is its ability to simplify navigation through files and folders on a computer by simply pointing and clicking. The main disadvantage and limitation of using a mouse is its space requirement. A mouse cannot be used in an area without ample physical space to drag and scroll (El Kaliouby & Robinson, 2003).\n\nFoot mouse\n\nA foot mouse is a specialised type of a mouse that is operated by the feet. Users with disabilities mainly use the foot mouse. The design of a foot mouse depicts a footpad with several buttons that serve different purposes. The footpad also has a navigation ball that is rolled to the foot to move the cursor on a computer screen. There is also afoot mouse for computer experts who prefer using their feet to free their hands for higher performance in different activities on their computers.\n\nThe advantage of using a foot mouse is that it enables the disabled people to enjoy the easier navigation on their computers. Its disadvantage is that it takes a lot of time for the user to master the manipulation of a foot mouse. Its main limitation is its requirement for larger working spaces because it is larger than a normal mouse ((El Kaliouby & Robinson, 2003).\n\nJoystick\n\nJoysticks are input devices used to control characters in computer games. A joystick comprises of a control column that is attached to a base that allows easy rotation of the stick. The stick normally has other support buttons used for different purposes like executing moves in the game. Joysticks work under the principle of motion whereby the characters on the screen move in a similar manner as the motion executed by the user on the joystick. The advantage of using a joystick is that it acts as a central control centre for gaming users. The buttons are attached to a stick, making it easy for the user to execute moves easily. The disadvantage of using a joystick that becomes tiring because the user has to keep moving his or her hand. Its limit is the availability of working space (El Kaliouby & Robinson, 2003).\n\nTrackball\n\nA trackball operates like a mouse. The standard trackball is made of a socket with a rolling ball and several buttons. The trackball looks like an inverted mouse, and it is very easy to navigate using the device. A trackball user utilises his palm and fingers to get through files on a computer screen. A person using a computer rolls the ball, and the cursor on the screen move to the required place while clicking on the buttons where required. The main advantage of using a trackball over a mouse is its ability to eliminate the large working space requirement. A trackball operates on a static point; hence, it does not require large working spaces. Its disadvantage is it requires more accuracy and expertise in navigation because a slight touch on the ball moves the cursor (El Kaliouby & Robinson, 2003).\n\nTouch screens\n\nTouch screens are modern interactive screens that act both as input and output devices. Touch screens are specialised to allow users to control computers by touching the screen to execute commands. Some touch screens use a special stylus, whereas others respond to the touch of a finger. Technological growth has seen the development of multi-touch touch screens that allow users to use several fingers simultaneously to operate computers. Touch screens operate through screen sensors that manipulate touch signals through special firmware. The main advantage of using touch screen technology is that it eliminates the need for other input hardware devices. There is no need for the user to purchase a keyboard or a mouse if a computer has a touch screen. The disadvantage of using touch screens is that they are quite fragile, and they can easily break and lose their sensitivity (El Kaliouby & Robinson, 2003).\n\nLight pen\n\nA light pen is an interactive computer input hardware that utilises light and the CRT display technology. A wand with light-sensitivity is used to input data through a CRT display screen. The advantage of using a light pen is that it allows users to enter data freely into the computer. It is also easy to use. The limitation is that computers may fail to recognise some patterns when a user is entering data (El Kaliouby & Robinson, 2003).\n\nTouchpad\n\nA touchpad is an alternative to a mouse in laptops and other computer devices. It is an input device that controls a cursor on a computer screen. Computer users operating a touchpad move their fingers across the touch-sensitive pad, and the movement translates onto the output screen. The advantage of using a touchpad is the elimination of the requirement for large spaces to operate a mouse. The main disadvantage is that just like a mouse, users have to keep shifting the position of their fingers to scroll down long files (El Kaliouby & Robinson, 2003).\n\nReference\n\nEl Kaliouby, R., & Robinson, P. (2003). Real Time Gesture recognitions in Affective Interfaces. In M. Rauterberg M., Menozzi & J. Wesso (Eds.). Human-computer Interaction, INTERACT \u201903: IFIP TC13 International Conference on Human-Computer Interaction, 1 st -5 th September 2003, Zurich, Switzerland (pp. 950-957). Amsterdam: IOS Press. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Architecture for a College Student Essay\n\nIntroduction\n\nMost colleges do not necessarily require their students to acquire computer systems, but in the digital age, all scholars are expected to have unhindered access to fully operational and up-to-date computer architecture. Consequently, you need to purchase your student a computer system that can at least compliment the software and hardware standards that can be found in most colleges. It is also important to consider that the student might have to carry the computer to college on a regular basis. These are my recommendations for your child\u2019s computer needs after considering various factors.\n\nHardware\n\nThere are various factors to consider while planning to purchase computer hardware for the student. First, computer hardware is evolving at a first pace and the machines that you purchase during the student\u2019s freshman year may not suffice during all four years of college (Mathews 16). However, the hardware equipment that you decide to purchase should be compatible with both school and home network connections. Computer prices have been dropping rapidly over the last few years and the constantly changing hardware standards should not be a big issue. Furthermore, some hardware components can be purchased or leased at the same time.\n\nThe first piece of hardware that your student requires is a computer. In your student\u2019s case, I would recommend a laptop computer that can easily support the student\u2019s computing needs. The laptop computer should most preferably be a light model and it should be accompanied by a laptop stand for home use. The laptop will cater to the student\u2019s mobility whereby it can be used even for class trips and it can easily connect to college wireless networks. The laptop should come with an operating system of Windows 7 or higher and a multi-core processor.\n\nThese specifications are suitable because most Microsoft Windows systems are supported in almost all colleges and they can also be used in most home environments. The processor choice is viable for both the educational and leisure needs of the student such as video streaming and gaming. Consequently, the laptop\u2019s memory should at least be 4 GB and the hard drive should be at least 250GB. If possible, the memory and hard drive capabilities of the computer should be scalable to give an allowance for hardware upgrades if necessary.\n\nOther important specifications of the laptop computer include highly functional video and sound cards that can support video-lectures, high-definition presentations, and other extracurricular activities. The laptop should also come with a DVD that has to write capabilities. If need be, the student can also acquire a USB mouse and keyboard to make his work easier. The laptop\u2019s wireless and LAN network capabilities should be up to the industry\u2019s standards. Backup capabilities of the computer are also a major consideration when purchasing a laptop although a machine that has the above specifications should be able to perform adequate backups.\n\nThe purchase of a laptop should be accompanied by that of a printer. The printer should at least be an easy-to-operate Laser printer. This hardware equipment will most likely be stationed at home where the student can print course materials and assignments among others. If the printer is part of an open network, it should be switched off to avoid unnecessary usage by outsiders.\n\nFor networking purposes, you should consider acquiring a short-range wireless router from a reputable service provider. Wireless routers are easy to configure and the whole family can use this infrastructure to access the network on other gadgets such as phones, tablets, I-pads, and gaming consoles. Wireless routers are also relatively cheaper compared to other network options.\n\nSoftware Options\n\nFirst, the student will require functional anti-virus software so as to protect his computer against attacks and malicious software. Most new laptops come with free anti-virus software and some institutions provide it to students at no cost. Given that the family is familiar with Microsoft Windows, I recommend an Office Software Suite that includes; Microsoft Office, Word, Excel, and PowerPoint. This software should serve both the needs of an English and History Majors student and at the same time any domestic activities such as inventory or emailing. For browsing needs, I would recommend Firefox, Chrome, or Safari Browsers. All these browsers also support most email platforms including institutional ones. After registering in college, the student will most likely get a discount on essential student-centric software programs.\n\nOther Recommendations\n\nThere are other software and hardware purchases that the student should consider buying. First, the student should consider acquiring an Ethernet network cable for use in college environments. The cable gives the student access to LAN, which is at times faster than wireless networks. It is also prudent to consider purchasing a surge protector that can protect most of the hardware against damage from power surges. The student might also require a backpack for ferrying the laptop to-and-from school. In addition, you should also invest in extra security for locking up the computer equipment thereby offering protection against theft and burglary. A USB stick or an external hard drive is also a viable purchase in this case. Extended warranty especially for the laptop because it will be subjected to various risks during the student\u2019s commute (Mathews 18).\n\nWorks Cited\n\nMathews, Brian. \u201cFlip the model: Strategies for creating and delivering value.\u201d The Journal of Academic Librarianship 40.1 (2014): 16-24. Print.\n\nAppendix\n\nEssentials\n\nFigure 1 HP EliteBook 8440p $300\n\nFigure 2 HP LaserJet Pro Printer $450\n\nFigure 3 Linksys \u2013 Wireless-N Router with 4-Port Ethernet Switch $50\n\nApproximate Total Cost: $800\n\nOthers\n\nFigure 4 14\u2032 Cat-6 Network Cable $ 19.99\n\nFigure 5 7-Outlet Surge Protector $19.99\n\nFigure 6 Centon \u2013 2GB DataStick Pro USB $12.23\n\nNB: All products can be found online at Betbuy.com and they can be reviewed by clicking.\n",
        "label": "human"
    },
    {
        "input": "Human-Computer Interaction in Health Care Settings Essay\n\nThere are various approaches to dealing with changes because of different views, beliefs, and goals people have in their lives. For instance, some people prefer standing on the safe side and waiting until something changes because they are reluctant to introduce changes themselves. This is because they feel comfortable while waiting for certain things to come. However, some people believe that ignorance of changes is explained by fear of the unknown (Johnson, 1998). Though it is impossible to find out whether changes are for the better, using a simple method of trials and errors is often much more beneficial than ignoring the possibility of choosing the new ways of progress and development. In other words, living in the past and impossibility to adapt to the present will not provide benefits for professional and personal growth.\n\nRegarding the above-presented assumptions, health care spheres should also provide a favorable ground for introducing changes because it is the key to ensuring a successful working process. Using different innovative approaches can improve a working atmosphere, increase staff productivity, and provide incentives for sustaining a competitive advantage over other health care organizations (Sharpe, Rogers, and Preece, 2007). Specific attention should be paid to the introduction of technological advances as well as innovative equipment in the sphere of information management. Applying an effective system of information exchange provides more opportunities for effective data flow among the nurses as well as ensures a high level of productivity and cooperation. Engaging with human-computer interaction, therefore, is a step forward to the successful advancement of communication and management. Nurse leaders should be ready to introduce changes and provide consistent implementation plans.\n\nReferences\n\nJohnson, S. (1998). Who moved my cheese? An amazing way to deal with change in your work and in your life . New York: Putnam.\n\nSharpe, H., Rogers, Y., & Preece J. (2007). Interaction Design: Beyond Human-Computer Interaction . US: Wiley.\n",
        "label": "human"
    },
    {
        "input": "Foreign Direct Investment in the South Korean Computer Industry Report\n\nThe assessment of political risks and legal environments is a necessary stage before deciding on the foreign direct investment (FDI) in the concrete country. The reason is that the political situation in a country can have negative effects on the company\u2019s development and the safety of investments. Therefore, international legal environments need to be assessed to propose efficient strategies for the company planning the FDI (Debaere, Lee, & Lee, 2010). The computer and high technology industry are actively developing in South Korea, and it is characterized by a qualified workforce. Therefore, the decisions regarding the FDI in this country are mostly made by IT companies and producers of laptops and computer components. This paper aims to assess political risks and examine specific legal factors that can affect the decision on the FDI in the computer industry in South Korea with the focus on the macro and micro perspectives, and the other purpose is the provision of recommendations to address the situation.\n\nExpropriation and Confiscation Threats\n\nIn South Korea, threats of expropriation and confiscation for laptop production centers are minimal. The reason is that the country refers to the international laws regarding the practices of expropriation of the invested properties ( Foreign direct investment hit record in 2014 , 2015; Lall, 2013, p. 20). Thus, the foreigners\u2019 private property is protected by laws, and this principle is also relevant for producing and selling laptops because they represent the significant niche in the technological market of the country (U.S. Government, 2015, p. 3). Although the confiscation is not prohibited in the country directly, the compensation for the possible expropriation of the property for the public use is guaranteed.\n\nIt is important to state that South Korea has many regulations regarding business development and foreign investment. Therefore, the use of business models that are not authorized or adopted in the South Korean environment has legal consequences (Chung, 2014, p. 223). Moreover, the level of government interference in various industries and businesses is different, and it is correlated with the level of the industry openness to the FDI. The laptop industry of South Korea is open to the FDI, and the level of government control is low (U.S. Government, 2015, p. 2). As a result, many producers of laptops choose to invest in South Korea in order to use the significant technological base and the high-quality workforce.\n\nForeign Goods and Labor Laws in South Korea\n\nThe regulation of business development in South Korea is perceived differently by investors. The opinion shared by the majority of foreign executives operating in South Korea is that it has the \u201coverly burdensome regulatory environment\u201d (U.S. Government, 2015, p. 3). This approach is also reflected in developing campaigns limiting the entrance of foreign goods in the country. However, in spite of the fact that the South Korean market is mainly protected from foreign goods, this practice is not relevant for the laptop production as a part of the computer industry (Vu & Im, 2013, p. 19). Such campaigns do not work for marketing laptop components that are produced abroad (Kotrajaras, 2010, p. 13). Many foreign investors are officially invited to the laptop industry to build partnerships with Korean companies and develop the research and development sector (U.S. Government, 2015). Thus, the FDI in the South Korean computer industry is supported without focusing on threats of foreign goods dominating the market.\n\nPlanning the investment in South Korea, executives rely on attracting a high-quality workforce. Nevertheless, foreign investors in South Korea need to refer to the labor law on unions and associations at the workplace revised in 2011 (Seol, 2012). Employees working for the foreign-invested business should have the opportunity to join the union and receive the associated benefits (Kang, Lee, & Park, 2011). The mandatory labor laws include the provision of paid leave and the recruitment of diverse people and persons with disabilities. These laws should be strictly followed by foreigners producing laptops because the high-quality technicians\u2019 work in this sphere is valued.\n\nPolitical Risks Leading to Economic Changes\n\nTo decide on the FDI in the South Korean laptop industry, it is also necessary to assess the role of such political risks as civil wars and violent conflicts in influencing changes in taxation, inflation, and currency devaluations. The reason is that such risks directly affect the profitability of the developing business and investors\u2019 revenues. Traditionally, South Korea is discussed as a country with a low potential for political risks (Geun-Hye, 2011; South Korea: Country risk report , 2015). However, the problem is in the fact that South Korea is in a state of war with North Korea. As a result, any escalation of the conflict, as it was in August of 2015, has adverse effects on the country\u2019s business environments (Park & Kim, 2015, para. 2). Still, South Korea works to protect the interests of foreign investors in the country, and the production of laptops can be affected only indirectly.\n\nThe dependence of the economy on the political situation is supported by the fact that the escalation of the conflict between North Korea and South Korea in August of 2015 led to increasing the risk of currency devaluations and to changes in the inflation rate. In August of 2015, the inflation rate was 0.70 in contrast to the lowest level in 0.40, recorded in April of 2015 (Trading Economics, 2015). Discussing the taxation in South Korea, it is important to state that taxes typical for the laptop industry in the country are minimal. According to the South Korean Foreign Investment Promotion Act (FIPA), the regular increase in taxation is not related to this industry, and the focus is on providing the tax incentives and decreased rental fees for foreign investors in the laptop sector (Lee & Min, 2011; U.S. Government, 2015). However, such political and economic changes can affect the laptop industry because of the threat of freezing the projects and decreasing the local demand.\n\nDiscussion of Political Risks from Both Macro and Micro Perspectives\n\nPolitical risks are discussed from two perspectives. The macro perceptive analyzes political risks related to all foreign investors operating in the concrete country. The micro perspective explains the risks associated with a certain industry (Kotrajaras, 2010). In the case of South Korea, the macro perspective, political risks, and legal factors are threats of expropriation, necessities to follow the labor laws strictly, and risks of the further conflict escalation between North Korea and South Korea (Poulsen & Hufbauer, 2011, p. 12). Focusing on the micro perspective, it is important to state that the laptop industry is under the threat of intensifying governmental and legal regulations and changes in FDI and taxation laws ( South Korea property investment guide 2013 , 2013). The problem can arise if South Korea limits the foreign investors\u2019 impact and control in the industry after revising the list of industries available for the investment. The problematic political situation regarding the development of the North Korea-South Korea conflict can also result in restricting tax incentives for the FDI in the high technology industry.\n\nConclusions and Recommendations\n\nThe assessment of the political and legal environment for investing in the laptop industry in South Korea demonstrates that the overall environment for the FDI is rather beneficial because South Korea focuses on following the international laws regarding the FDI. The FIPA also contributes to creating positive conditions for foreign investors operating in this industry. However, a year ago, South Korea was discussed as one of the most stable economies to invest in, and the situation is different today. Although the compensation for the possible property confiscation is guaranteed for foreign investors, the development of the military conflict can affect this situation. The other important risk is the strengthening of regulations in the laptop industry that remains open for the FDI and having limited governmental control. The current unstable situation in the country can have adverse effects for producers of laptops investing in South Korea because any political changes will lead to economic losses, and radical political changes will lead to property confiscation without reasonable compensation.\n\nIt is possible to formulate the recommendations for planning the FDI in the laptop industry in South Korea. Currently, the risk for the FDI is rather high, but the implementation of certain recommendations can decrease the risk to a minimum. Thus, the following recommendations need to be used by the company\u2019s management: (1) the choice of the political risk insurance for those situations when compensation is not paid to the investor; (2) the examination of the country\u2019s legal base to determine the secure strategies for the business development; (3) and the formulation of the strict agreement with the South Korean partners on principles of the FDI that is supported by the international and national laws on the FDI. Such an agreement should reflect the laptop industry specifics. In addition, prior to making the final step and investing in business activities in South Korea, it is important to complete the additional monitoring of the political, legal, and economic situation in the country.\n\nReferences\n\nChung, S. (2014). Environmental regulation and foreign direct investment: Evidence from South Korea. Journal of Development Economics , 108 (1), 222-236.\n\nDebaere, P., Lee, H., & Lee, J. (2010). It matters where you go: Outward foreign direct investment and multinational employment growth at home. Journal of Development Economics , 91 (2), 301-309.\n\nForeign direct investment hits record in 2014 . (2015). Web.\n\nGeun-Hye, P. (2011). A new kind of Korea. Foreign Affairs , 90 (5), 13-18.\n\nKang, S. J., Lee, H., & Park, B. (2011). Does Korea follow Japan in foreign aid? Relationships between aid and foreign investment. Japan and the World Economy , 23 (1), 19-27.\n\nKotrajaras, P. (2010). Foreign direct investment and economic growth: A comparative study among East Asian countries. Applied Economics Journal , 17 (2), 12-26.\n\nLall, S. (2013). Direct investment in South East Asia by the NIEs: Trends and prospects. PSL Quarterly Review , 44 (179), 19-28.\n\nLee, B., & Min, B. S. (2011). Exchange rates and FDI strategies of multinational enterprises. Pacific-Basin Finance Journal, 19 (5), 586-603.\n\nPark, J., & Kim, S. (2015). North Korea and South Korea are fighting a war over loudspeakers . Web.\n\nPoulsen, L. N., & Hufbauer, G. C. (2011). Foreign direct investment in times of crisis. Peterson Institute for International Economics , 1 (2), 11-23.\n\nSeol, D. H. (2012). The citizenship of foreign workers in South Korea. Citizenship Studies , 16 (1), 119-133.\n\nSouth Korea property investment guide 2013 . (2013). Web.\n\nSouth Korea: Country risk report . (2015). Web.\n\nTrading Economics. (2015). South Korea inflation rate . Web.\n\nU.S. Government. (2015). Korea investment climate statement . Web.\n\nVu, T. B., & Im, E. I. (2013). Impacts of FDI relations between the US and East Asia on economic growth. Asia-Pacific Journal of Social Sciences, 5 (1), 18-27.\n",
        "label": "human"
    },
    {
        "input": "HP Company\u2019s Computer Networking Business Case Study\n\nHP is a multinational corporation that operates in the computer industry providing different goods and services to the global market. While the compute deals in the manufacture of hardware goods such as computers, it also deals in the software part that involves networking. Under this networking sector that was established in 1985, the firm has been involved in a series of innovation. The firm aims at providing the best innovations that can solve key issues affecting the changing business industry across the globe. In order to attain this, the firm has undertaken measures to incorporate environmental sustainability measures. This paper discusses the effectiveness of HP\u2019s computer networking from a strategic point of view to the structural point of view.\n\nEffectiveness of HP\u2019s Computer Networking Business: Strategic Management\n\nAccording to HP Networking (13), HP is one of the dominant firms in the networking sector because it has capitalized on a networking innovation to create a competitive edge over its competitors. The firm has been able to achieve excellence in networking because it focuses on innovation as the most important strategy. Due to innovation, HP has been able to develop best networking solutions such as the E-series that are manufactured under the ASIC power series. The other is the ProCurve manager that is user friendly and manageable by all users. Strategically, HP uses the game theory to analyze the reaction of its competitors regarding the manufacture of a new networking product. Under this strategy, the firm capitalizes on the first mover advantage to be the first to develop efficient networking solutions and sell in the market.\n\nIn addition to these strategies, HP utilizes other strategic management techniques such as the Lean manufacturing strategies (Lean six sigma) and the marketing strategies. While lean six sigma ensures quality in the manufacturing process, marketing strategies ensure that the market is aware of the firm\u2019s networking products. It has marketed its products online together with allowing online purchases and deliveries.\n\nEffectiveness of HP\u2019s Computer Networking Business: Structural Management\n\nThe effectiveness of HP networking business could also be analyzed using the firm\u2019s structure. HP is characterized by a mechanistic organizational structure that has its management centralized in one location. The mechanistic structure is necessary for HP given its large size. With such a structure, the firm is highly specialized with the networking and other section being allowed to manage internal affairs. There is a clear chain of command that is characterized with a narrow span of control. The centralization of the organization\u2019s management is significant because it increases standardization of manufacturing tasks, and establishment of a good working culture.\n\nTo the good organizational structure, there is speedier decision making process in the firm with the employees such as the sales representative on the ground having a clear chain of command on who to report to regarding the sale of the products or in cases of inquiries and issues. The organizational structure can be attributed to the good performance of the networking section of HP since enhances completion of various activities, setting of goals and ensuring efficiency in the innovation process. As noted by Chama (9), high efficiency and effectiveness go hand in hand and they ensure attainment of set objectives.\n\nThriving of ProCurve under PSG\n\nThe ProCurve networking recently expanded its regional cover to include regions such as Europe, Middle East and Africa with the product offering the ability to meet many specific needs of customer. The ProCurve network and securities channel is designed by the firm to benefit HP, its partners and consumers. The program is expanding and it is vital that it separates form the initial TSG. The autonomy of the networking will come with its management and sales representatives together with experts that develop the system. This will reduce the bureaucracy undertaken to reach the right person and relay information on consumer needs and issues. In addition, autonomy of ProCurve will help the networking section to specialize and therefore increase its efficiency and effectiveness, which is closely related to increased revenue. Given that the level of competition in the industry is high, autonomy of ProCurve will create a competitive edge for the firm. On the contrary, the independence of ProCurve will mean that the section will no longer receive much support form TSG as it has been before, but will have to ensure that it is profitable to sustain itself.\n\nRecommended Strategies\n\nCentralization and manufacturing of products from a single location is not cost effective for HP. The firm needs to minimize costs through outsourcing and off-shoring of its vital services to various countries that can offer services cheaply. U.S. is capital intensive while other countries such as India and China are labor intensive. Outsourcing will reduce labor costs while enabling the firm to continue with its innovativeness in networking. The internet has revolutionized business and there are many system users online. HP should capitalize on electronic marketing of its networking products and allow consumers to purchase online. In cases where the products can be installed online, the firm should minimize its costs by doing it online. The firm could apply selective pricing where prices for the firm\u2019s products are higher in some regions but lower in others, as long as the firm meets its minimum manufacturing costs.\n\nWorks Cited\n\nChama, Kaunda. \u201cCatering for a R24bn Market.\u201d Business and Technology Insight for Solution Providers . 2009. Web.\n\nHP Networking. \u201c Energy Efficient Networking .\u201d 2013. Web.\n",
        "label": "human"
    },
    {
        "input": "Personal Computers in the U.S Market Research Paper\n\nTable of Contents\n 1. Introduction\n 2. Personal Computers in the U.S Market\n 3. Why the Product is Declining\n 4. Recommendations for Action\n 5. The Best Foreign Market for them\n 6. Product Segmentation\n 7. Conclusion\n 8. Reference List\n\nIntroduction\n\nMany products in the market tend to have a life that will see them move from the first to the last stage. This means that they will decline and finally reach the obsolesce level. A product\u2019s life begins at the development stage when an idea of the product is conceived and developed. The second stage will involve the introduction of the product to the market, which will then see the product undergo a third phase known as growth. The fourth stage, which is the most important for the product, is the maturity stage. This stage will see the product perform optimally on the market. Therefore, the stage is the time the product will rake in profits by breaking even. After the maturity level, the product will start losing its appeal in the market due to different factors. Different products have different life cycles. The study narrows down to personal computers, which are in the market today despite their predicted declining demand.\n\nPersonal Computers in the U.S Market\n\nPersonal computers, which are commonly referred to as PCs, are among the most popular products in the American market. Their appeal in the market is slowly declining. Personal computers have been in the American market for as long as the computer was invented. The American market has been one of the largest markets for the product. Most of the top computer companies are found in the America thus making it the top industry in computer manufacturing as well as the top consumer for computers. Personal computers in the United States of America are used as office and household equipment (Lukas, 2013, p. 3). In this case, personal computers are mainly desktops and laptops to a lesser extent. Personal computers are popular due to the different functions they can perform both at home and in the office.\n\nAt home, personal computers can be used for data storage such as music, movies, photographs, and/or accessing the internet. On the other hand, personal computers are used in the office to do an array of functions that have made the present-day office fully dependent on their use. The use of computers has increased in the last two decades with the production of cheap and affordable computers and the development of user-friendly software that allows computers to meet so many different needs of different users (Lukas, 2013, p. 4). The target market for personal computers in the US\u2019 market is distributed for official use, which is the use in offices and other formal places such as schools, and for use at home. Therefore, the target market for personal computers is the whole United States of America\u2019s population. All people of different ethnic origins have almost the same access and use of personal computers. However, people from less privileged minority groups reported a higher number for not having a computer at home, but having access to one at work.\n\nWhy the Product is Declining\n\nA product\u2019s market will decline after it has reached its maturity stage. This case can be attributed to different reasons that have been stated in the previous paragraph. In the case of personal computers, there are many reasons that are leading to its decline in the market, especially in the US\u2019 market (Lukas, 2013, p. 5). To start with, personal computers are slowly outliving their usefulness in terms of data storage, which usually defined the purpose of computers previously. Computers were rated according to how much storage space they had in them and/or how fast they were in terms processing and executing commands. This therefore made them important in places such as offices for the purpose of storing information, as well as for using them to process different kinds of data. This situation is changing so fast with the change in the way data is stored and managed nowadays. The emergence of remote storage services such as clouds has made the need to have a big storage machine in the office obsolete because an individual can store his or her data remotely and safely and access it from anywhere on the globe.\n\nThe emergence of smaller and smaller computers such as tablets has made the need for personal computers unnecessary due to the compact ability and portability of the new types of computers (Lukas, 2013, p. 5). This case has made the need for the traditional personal computer unnecessary. Tablets are small, light in weight, and portable. Besides, they can perform all functions a personal computer can perform. The computer market is steadily shifting towards smart phones, which are mobile phones with computer capabilities. Their entry into the market has therefore reduced the demand for personal computers due to the efficiency that is offered by the smart phones (Dedrick, 2008, p. 5). Most personal computers such as desktops are not easily portable. They require a lot more power to run when compared to smart phones and tablets. Therefore, it suffices to attribute the shift from personal computers to the advanced technological change that is shifting the tide in terms of preference to personal computers.\n\nRecommendations for Action\n\nThe first recommendation for marketers is to change the target market that they have all along been relying on for the sale of their computers. They should now target secondary markets that still need the computers (Dedrick, 2008, p. 6). Such markets can be institutions, which need computers but could not afford them previously. Therefore, computer companies should market them at highly reduced prices that will allow them to enjoy economies of scale thus still making a profit. Although personal computers are becoming obsolete, they are still relevant in terms the services they provide. Nevertheless, they are not as portable as smart phones and tablets. The marketer should exaggerate on the advantages that personal computers such as laptops have over smart phones and tablets.\n\nAs much as new devices are more appealing to the market more than personal computers, they still lack some functions that personal computers have such as reading compact disks, which are usually virus-free. Smart phones depend a lot on the functions downloaded from computers. Consequently, marketers should use such advantages to market personal computers and/or revitalize their declining market or simply maintain their foothold. The third recommendation is that marketers of these products should identify new markets that are yet to embrace the personal computers since they are still developing in that field. Such markets can be found in the developing countries, which are slowly embracing personal computers (Dedrick, 2008, p. 7). Such markets are still on the raising gradient of computer use of the United States of America. Therefore, it will be recommendable for marketers of these products to embrace the emerging markets, which will continue giving them the edge they have had all along.\n\nThe Best Foreign Market for them\n\nThe best foreign market that can be recommended for sale of personal computers is India. India is emerging as the latest technology giant with a huge market potential for many types of goods. The Indian market has been described as an emerging giant in all aspects in terms of consumer goods. This situation has been promoted by a stable economy, which has led to the emergence of a big number of middle class people. The middle class people of the Indian population is what is providing numbers in the emerging markets of India because they are a group of learned people with jobs that have uplifted the buying power of the Indian economy (Lukas, 2013, p. 8).\n\nThis perfectly makes India the best market for the transfer of the declining American personal computer manufacturers. The availability of affordable software and technology has made it easy for India to reach its hands on it. Previously, technology was very expensive to many people. Thus, the majority who make up the middle class could not afford them. India comes with the advantage of a huge population that provides the economies of scale. This makes it cheaper to manufacture computers in India because, as a populous country, it is able to provide cheap but expertise labor that would be needed to manufacture computers.\n\nThe advantage of a huge population makes India ideal because the huge population provides numbers of people who can afford computers. The Indian market is still developing technology wise. It cannot be compared to the American market in terms of saturation in consumption (Lukas, 2013, p. 9). The American market is saturated in terms of consumption in that making sales has become difficult due to so many companies selling in the country and/or due to many people already having computers. A saturated market is difficult for marketers to work in because it requires them to come up with new ideas to sell the same thing, which can be very challenging and expensive to do in the end. Therefore, when one takes this situation to the Indian market, open hands are already waiting. Thus, a marketer may just have to do the basic marketing, which is not expensive as such.\n\nProduct Segmentation\n\nThe product segmentation for personal computers in the new market will be selling new computers to all sectors of the economy that need them. The new computers in this case will be already assembled computers, which will be ready to use so that what all buyers need to do once they buy them is to plug and use them. Computers will be coming with the necessary software. The company will offer warranties that will last for a period as decided by the company. The company will also provide spare parts for sale because such markets do well with spare parts. Many developing countries\u2019 markets prefer buying goods that have accessible spare parts because they prefer goods that have a long life, which can be repaired once they spoil (Dedrick, 2008, p. 13). They do not like goods that have to be disposed once they are damaged. Thus, a spare-part option will be okay for them.\n\nConclusion\n\nProducts\u2019 decline in the market is a common thing that happens to all products as long as there are new products being developed, which are more efficient in relation to the older products. The most important thing for the marketer is to be in touch with the trends of the market as well as with the needs of the emerging markets outside the traditional market. In fact, whatever is considered obsolete in the old market might just be very new in the emerging markets.\n\nReference List\n\nDedrick, J. (2008). Globalization of Innovation: The Personal Computing Industry. Boston: The Alfred Sloan Foundation Annual Conference.\n\nLukas, B. (2013). Why do Customers Get More than they Need? How Organizational Culture shapes Product Capability Decisions. Journal of Marketing, 77 (1), 1-12.\n",
        "label": "human"
    },
    {
        "input": "Starlight Computer Solutions Ltd Business Plan Essay\n\nTable of Contents\n 1. Introduction\n 2. Business Description\n 3. Mission Statement\n 4. Business Goals\n 5. Operational Strategies\n 6. SWOT Analysis\n 7. Conclusion\n 8. References\n\nIntroduction\n\nA business plan refers to a formal presentation of business goals, how the business intends to achieve the goals, and the reasons for choosing those goals (Bangs, 2002). A business plan also includes a description of what the business does, the markets served, as well as the features that make it standout from their competitors. A business plan can apply to a start up business or an existing business that intends to expand its operations, introduce a new product into the market, or rebrand (Bangs, 2002). A business plan focuses on either internal or external elements of a business. External business elements include customers and investors who associate with a business. Internally focused business plans target transitional goals that help in reaching external goals (Bangs, 2002). This essay will provide a business plan for a computer business that I am starting up. The business plan will include a statement of the business vision, goals, operational strategies, as well as an internally and externally focused SWOT analysis. The name of the business I am starting up is Starlight Computer Solutions Ltd.\n\nBusiness Description\n\nStarlight Computer Solutions Ltd will focus on offering cloud computing services and solutions to its various clients. Our client base will include businesses and individuals. The business will operate from the capital city, with a start-up workforce of twenty employees. We will set up an office in the central business district, where we intend to draw most of our clients. The start-up capital will be $1.5 million, which will cater for salaries within the first few months of operation, getting an office, acquiring the necessary resources and assets, obtaining essential documents and licenses, as well as marketing the business (Bangs, 2002). Starlight computer services will offer various types of cloud computing services to various clients in line with their preferences and our corporate values.\n\nThe services we will offer to our clients are social networking, e-mail, online offices, backup services, as well as banking and financial services (Miller, 2008). Provision of these services will require strict adherence to our corporate culture and organizational structure. The corporate values for my business include professionalism, teamwork, integrity, honesty, responsibility, and customer prioritization (Miller, 2008). The organizational structure of Starlight Computer Solutions will have three levels comprising of a business director, business managers, and employees. The business intends to achieve good revenue margins by focusing on our strengths, identifying our weaknesses, exploiting available opportunities, and addressing all the threats to our business.\n\nMission Statement\n\nTo provide the business community and entrepreneurs in my country with quality cloud computing services, technical support, and unequaled customer service in a professional manner that meet their immediate and future needs, while observing the business practice code of ethics.\n\nBusiness Goals\n\nStarlight Computer Solutions developed a set of goals that it hopes to achieve within the next two to three years covered by this plan. The first goal is to build a reputable customer base within the first year of operation (Abrams, 2003). The second goal is to maximize on revenue margins and reduce on operational cost within the first year of operation. The third goal is to nurture a culture of professionalism and ethical behavior in the workforce as part of my business culture within the first six months of operation. The fourth goal is to develop a reliable and professional communication strategy with all clients within the first year of operation (Abrams, 2003). The fifth goal is to earn the trust and support of all employees within the first six months of doing business. The sixth and final goal I want to achieve with this business plan is to retain and attract as many clients as possible within the second and third years of operation.\n\nOperational Strategies\n\nKey to the success of a cloud computing service business is effectively and professionally designed operational strategies (Waters, 2006). The strategies the business will apply in implementing various business activities will determine how well it can meet the needs of its customers and longevity of the business amid numerous competitors. The first strategy is to build and maintain deliberate alliances with various business partners in the field of computer and technology services (Waters, 2006). This is essential when planning to diversify operations and services, as the business will be aware of what their competitors and partners are doing.\n\nThis will help in avoiding duplication of efforts by others as well as market analysis for better performance. The second strategy is designing a marketing strategy that focuses on the needs of the target market, as well as the customer needs (Waters, 2006). For the business to succeed, it has to meet all the needs provided by the market, and at the same time focusing on clients who are core to business success. The third operational strategy for the business is developing a management approach that applies a scorecard in assessing commitment towards business objectives. The scorecard applies in assessing the progress of growth by comparing set goals with their recurrent results (Waters, 2006).\n\nAnother strategy is emphasis on service and support provision (Waters, 2006). The business intends to achieve this through networking, employee training, and encouraging all employees to develop intimate business relationships with as many clients as possible. Value and quality is another strategy for the business, as clients will always look out for products that excite them and meet their needs at the same time. Our sales strategy will be to sell Starlight Computer Solutions as a market leader and friend in cloud computing. This will require a positive attitude, innovation, as well as an orientation towards our clients, activities, and revenue targets (Waters, 2006). Another strategy is professionalism in the manner in which all activities apply. This will include customer relations, employee relations, and commitment to the corporate culture of the business.\n\nSWOT Analysis\n\nThis is a planning approach applied in evaluating the strengths, weaknesses, opportunities, and threats involved in a business (Richter, 2009). The main advantage of SWOT analysis is that it provides information that helps in developing strategic plans for the actualization of an objective (Richter, 2009). Information applied in SWOT analysis falls under internal factors and external factors. Internal factors entail strengths and weaknesses within a business, while external factors entail opportunities and threats external to a business. Starlight Computer Solutions will have numerous strengths and weaknesses from its internal environment, as well as opportunities and threats from the outside environment. The strength of the business lies in the organizational structure and operational strategies. The organizational structure of the business provides an inclusive environment that will allow everyone to participate in all activities of the business (Richter, 2009). Key business decisions will involve everyone, thus keeping motivation and morale to work high.\n\nThe weaknesses of the business lie in the inexperienced workforce and limited resources. Providing computer services to businesses and entrepreneurs requires an experienced workforce that can easily identify customer needs, and the best applicable solutions to the problems. For a start-up business, limited resources are always a challenge that limits the ability to meet all the set goals in an objective and timely manner (Richter, 2009). However, the external environment provides opportunities for the business to counter any challenges. One of the opportunities for the business is a chance to diversify the services they offer their clients to include information system management and information technology training.\n\nApart from the challenge of cloud computing, some clients can have other technology related challenges that my business can diversify into and expand its operations. The possible threats for the business are high employee turn over due to attractive job packages offered by businesses with adequate resources and employee training programs. Employee satisfaction is essential to success in a business, and employees will look for an employer who can meet all their needs (Richter, 2009). Another threat is an over flooded market of cloud computing service providers. Many people have embraced technology, thus a high possibility of numerous companies offering the same service.\n\nConclusion\n\nA business plan is an important part of creating a business. It provides the roadmap for identifying the essential resources, the kind of workforce needed, market needs for the intended business products, as well as the appropriate operational strategies. A business plan also identifies the goals of a business and effective strategies for actualizing the goals. A business without a plan set its self up for failure. Good operational strategies compliment a professionally developed business plan, thus the need to apply effective and economical ones.\n\nReferences\n\nAbrams, R. (2003). The Successful Business Plan: Secrets & Strategies . New York: Cengage Learning.\n\nBangs, D. (2002). Business Planning Guide . London: Oxford Publishers.\n\nMiller, M. (2008). Cloud Computing: Web-Based Applications that Change the Way You Work and Collaborate . New York: Cengage Learning.\n\nRichter, A. (2009). SWOT Analysis- Idea, Methodology and a Practical Approach . New York: John Wiley & Sons.\n\nWaters, D. (2006). Operations Strategy . New York: CENGAGE.\n",
        "label": "human"
    },
    {
        "input": "Computer Mediated Interpersonal and Intercultural Communication Research Paper\n\nMuch like the invention of the telephone, the creation of the Internet has launched a series of arguments regarding its role in everyday life of common people. Some claimed that it was a magic pool of incredible opportunities; others insisted that it was a Pandora\u2019s Box that would make people even lazier. As it always happens, the truth seems to lie somewhere in the middle between these statements.\n\nDespite the fact that social media clearly poses a tangible threat to the culture of live communication and, therefore, will contribute to the shriveling of people\u2019s social skills, computer mediated interpersonal and intercultural communication will allow for creating a stronger bond between the representatives of different cultures and brings the effects of prejudice down even among the members of communities where cultural, national and ethnic diversity is rather low.\n\nThe fact that new media offers better opportunities for multicultural experiences cannot be viewed as either a positive or a negative effect of social networking \u2013 at least, it cannot be positioned as such on its own. Modern media should be viewed as a tool that can be used for educational, professional and personal purposes.\n\nWhile the fact that more people are capable of learning about other nationalities and having unique communication experiences with them is definitely far from negative, its effects should be mentioned to define the value of the given phenomenon.\n\nThe first positive effect to be mentioned, in fact, has a lot to do with education. As it has been stressed above, modern media open a pool of opportunities for multicultural education, which is, in fact, a direct result of diversity and multiculturalism principles having their effect on various aspects of people\u2019s lives. Traditional methods of learning and teaching can, thus, be mixed with the ones that are widely accepted in other cultures, yet are positioned as innovational in the recipient one.\n\nFor example, social networking may help students learn a particular subject, such as the English language. Creating a specific group in Facebook where one can post exercises, rules and tips, or a Twitter group, where one can post short comments regarding the language, its specifics, students\u2019 success and impressions can be of great effect for ESL students. More to the point, the tips on how to connect Facebook posts with \u201ctwits\u201d already exist, which means that, when combined together, several social networks and other computer media may have an even greater effect on students\u2019 progress.\n\nWhen it comes to defining the positive aspects of computer mediated communication, one might want to mention better education opportunities for those engaged into the communication process. In other words, students learn faster and in a much more efficient way in a multicultural environment, which communication via computer programs provides a chance for. For example, in every culture, there is a unique way of approaching students based on whether the local culture is based on visual, aural, or verbal approach.\n\nAs Samovar, Porter and Stephani explain, the culture of Native Americans is based mostly on visual elements of communication, which means that most Native American students are visual learners. Hispanics are classified as aural learners. Hmong, in their turn, have developed impressive aural skills, since they \u201cdo not have written language\u201d (Samovar, Porter & Stephani, 1998, p. 209).\n\nThus, students will be able to develop not only their major skill, whether aural, visual, or verbal, but also train the rest of the aforementioned skills, which will help them in their further learning process, as well as in conversations with the representatives of other cultures, whose vision of the world is based on the principle other than the one of the person in question. In the realm of computer media, where each of the three types of cognition is introduced, combining them in order to create the ultimate environment for studying is shockingly easy.\n\nAccording to Samovar, Porter and Stephani, multiculturalism, which social networking promotes actively, will also contribute to the evolution of learning and motivation styles in education.\n\nWhile the effects of introducing new means to cognize one\u2019s own manner of learning may vary, and \u201cteachers should be aware of what they \u2018bring\u2019 to the classroom\u201d (Samovar, Porter & Stephani, 1998, p. 217), in most cases, students adapt easily towards new learning environment and accept the new rules rather eagerly if the latter are offered to them in the right manner. Providing an opportunity to get in touch with the people living in the most remote corners of the earth, modern media definitely help achieve the given goal.\n\nAnother crucial element that social networking and other computer media provides to the people of the present-day world, a chance to develop emotional intelligence must not be overlooked, either (Bandura, 2005). One of the pivoting stages of a person\u2019s evolution, the development of emotional intelligence is also boosted greatly by modern media.\n\nAccording to what Wood says, emotional intelligence is an \u201cability to recognize feelings, to judge which feelings are appropriate in which situations, and to communicate those feelings effectively\u201d (Wood, 2013, p. 168).\n\nBy introducing the principles of multiculturalism into the classroom with the help of social networking and other types of new media, teachers can help students broaden the horizons of their emotional intelligence by learning about the types of responses accepted in other cultures for similar contexts. Promoting multiculturalism, such social networks as Facebook, Twitter, MySpace, etc. contribute to emotional intelligence development greatly.\n\nAs Lustig and Koester explain, enhanced by the \u201cinteraction sequences that are repeated over and over again\u201d (Lustig & Koester, 2012, p. 275), or social episodes, emotional intelligence also plays a huge role in personal and professional evolution. It is worth noting, though, that the variations of social episodes for a particular situation had not been in existence up until recently, when an opportunity to engage in computer mediated social interactions emerged.\n\nTrue, it is unnatural for a particular culture to have several variants of interaction sequences for a certain situation; as a result, an alternative pattern of interactions represented by the people belonging to another culture could easily spark a conflict between those involved. For instance, the notorious women\u2019s rights issue often happens to be a major source of conflict in a communication between the residents of patriarchal and democratic states (Mookherjee, 2008).\n\nWith the advent of social networking and the ability to cognize the specifics of other cultures, however, the number of social episodes per situation has grown for a number of people, which means that emotional intelligence rates have been increased unbelievably all over the world. With the help of computer mediated communication, emotional intelligence growth can be boosted impressively.\n\nWhen communicating via Twitter, Facebook, Skype, or any other type of modern media, people learn quickly that other patterns of reacting to a certain event exist and that, though they do not have to be followed necessarily, they have the right to exist.\n\nUnfortunately, computer mediated communication also leads to a number of negative outcomes, the unwillingness to participate in live communication and, therefore, losing the basic skills of a live conversation being the key one. When it comes to defining the key specific of computer networking, the lack of nonverbal communication elements always takes the first prize as the most notorious one. True, there is Skype, and, with a webcam, it is possible to see the vis-\u00e0-vis and their reaction towards what the opponent says.\n\nHowever, a webcam offers only a part of nonverbal information that an everyday live communication provides in that a webcam can only capture a single frame, while, in an old-fashioned live conversation, people are capable of choosing their own angle to look at their vis-\u00e0-vis at. Thus, a huge chunk of data is wasted for nothing in the course of a computer mediated communication.\n\nMore to the point, it is questionable whether communication can actually take place without its nonverbal component. Therefore, with all its positive aspects, computer mediated communication still needs improvement, which, in its turn, will be achieved with the invention of new options for social networking.\n\nEven though the lack of nonverbal communication is an obvious flaw of the present-day computer media, it still helps address one of the most notorious issues that exist on par with the rest of the realities of the XXI century. Seeing how some cities and even states suffer from the lack of cultural diversity, a favorable environment for breeding all sorts of national, cultural and ethnic stereotypes is created. Thus, the premises for misunderstandings on interpersonal, intersocietal and intercultural levels emerge.\n\nThe multiculturalism environment created in the course of an interpersonal and intercultural computer mediated conversation, in its turn, provides the means to prove these prejudices wrong and turn over a new leaf in intercultural and international relationships.\n\nThus, it becomes possible for the participants of the communication process to share their unique experience in learning, therefore, creating an entirely new way of knowledge acquisition. With a blend of several approaches based on the features of several different cultures, one is most likely to come up with a highly efficient method of learning. In fact, the given strategy can possibly lead one to shaping the qualities required for lifelong learning.\n\nAlthough computer mediated interpersonal and intercultural communication has rather negative effects on people\u2019s ability to engage in real life communication, it clearly helps promote cultural diversity among every single person that has an access to modern media and partakes in the process of social networking. It would be wrong to deny the obvious problems that social networking and other types of modern media have; however, their advantages are far too big not to notice the, either.\n\nOne of the most important effects of modern media, introduction of cultural and ethnic diversity to a number of people and even communities that lack one clearly allows for not only cultural, but also political, economic, financial and educational progress of a number of states.\n\nBy sharing their experience and introducing each other to a whole new world of different traditions and sociocultural norms, people will be able to solve a number of intercultural and international conflicts, as well as enhance cooperation between countries all over the world.\n\nReference List\n\nBandura, E. (2005). Foreign language teachers and intercultural competence: An international investigation . Tonawanda, NY: Multilingual Matters.\n\nMookherjee, M. (2008). Multiculturalism\u201d. In C. McKinnon (Ed.), Issues in political theory . Oxford, UK: Oxford University Press.\n\nSamovar, L. A., Porter, R. E., & Stephani, L. A. (1998). Cultural influence on context: Educational setting. In L. A. Samovar, R. E. Porter, & L. A. Stephani (Eds.), Communication between cultures (pp. 198\u2013219). Belmont, CA: Wadsworth Publishing Company.\n\nWood, J. (2013). Emotions and communication. In J. Wood, Interpersonal communication everyday encounters (pp. 170\u2013195). Boston, MA: Wadsworth Publishing Company.\n\nLustig, M. W. & Koester, J. (2012). Episodes, contexts and intercultural interactions. In Intercultural competence: Interpersonal communication across cultures: International edition (pp. 274\u2013311). San Francisco, CA: Peachprint Press.\n",
        "label": "human"
    },
    {
        "input": "Computer Apps for Productive Endeavors of Youth Essay\n\nTable of Contents\n 1. Cultural and Social Needs\n 2. Drawing them Into More Productive Endeavors\n 3. Conclusion\n 4. Works Cited\n\nMobile gadgets dominate the lives of teenagers and young adults in industrialized countries all over the world. The existence of cutting-edge technology integrated into mobile gadgets, such as, smartphones and tablets enable teenagers and young adults to enhance communication capabilities. In addition to the reliable communication platforms, enhanced social interactions are made possible through the use of mobile gadgets.\n\nAt the same time, it is also due the presence of apps or computer applications that explains the surge in popularity of mobile devices. Computer applications or apps are helpful in reducing the burden of daily activities. Apps make it easier to buy groceries online, purchase take-out meals, pay utility bills, and transfer funds from one account to the next. Due to the popularity of mobile devices, government officials and parents are worried about the reaction of teenagers and young adults to the life-altering power of mobile gadgets.\n\nIn other words, they are worried about certain obsessive behavior linked to excessive use of mobile gadgets. This abnormal behavior is linked to the need to acquire unnecessary items and the need to spend excessive hours socializing online. One of the best ways to solve this problem is to redirect the passion and obsession of teenagers towards computer applications that encourages them to pursue more productive endeavors.\n\nIn order to curb addictive and obsessive behavior, it is important to acknowledge the problem, and to recognize the impact of mobile gadgets and apps in the present time. It is also important to look into scholarly works that have been published in academic journals. The proponent of this study investigated published scholarly works that talks about the phenomenon of using mobile gadgets and computer applications.\n\nIn addition, the proponent of the study examined scholarly works that focuses on the social aspect of the utilization of mobile apps. Thus, the researcher scoured numerous academic databases in order to search for journal articles that discussed the importance of socialization when people use mobile apps.\n\nCultural and Social Needs\n\nAccording to researchers in the field of mobile apps, addictive and obsessive behaviors require intervention (Kuss et al. 1). This assertion was supported by Bomhold, when she remarked in a study conducted using the facilities of the School of Library and Information Science from the University of Southern Mississippi (427). According to Catharine Bomhold, the world had seen the dramatic increase in the ownership of mobile devices among undergraduate students (427).\n\nBomhold therefore recommends the implementation of strategies that encourages a change of behavior, preferably towards the acquisition of knowledge as opposed to the acquisition of goods. However, it is impossible to initiate any type of intervention strategy if there is no acknowledgement of the problem (Kuss et al. 2).\n\nAccording to David Mosse, there is a failure in the identification of the root cause of the problem (936). He said that policymakers often overlooked the social aspect of the issue (Mosse 937). He presented his results after the completion of a complex study that requires the participation of veteran practitioners from different fields of expertise, the project manager was surprised to discover the power of relationships and social interactions over policies that were created to ensure the operational efficiency of the project (Mosse 936).\n\nDavid Mosse recounted his experience with an organization that was weakened by bureaucracy, and he lamented the fact that top leaders are often blind to the profound impact of human relationships in the context of project implementation (Mosse 936). Veronica Barassi concurs with the conclusion made by Mosse, especially when it comes to insistence of leaders and policymakers to focus only on the technical aspect of the project (50).\n\nBarassi highlighted the fact that it is imperative not to ignore the social component aspect of designing effective intervention strategies. Bomhold\u2019s scholarly study was presented to the Sam Houston State University, and she reiterated that people use online technology to meet their cultural and social needs (Bomhold 47).\n\nIn the same aforementioned study about addictive behaviors, the proponents of the study argued that one of the major setbacks in the creation of intervention strategies is the failure to understand how people use mobile devices to satisfy social and cultural needs (Kuss et al. 2).\n\nThus, according to a study sponsored by the Department of Media and Communications in the University of London, people tend to look at communication networks from an engineering point of view and fail to recognize the sociological aspect of the communication system (Barassi 48).\n\nDrawing them Into More Productive Endeavors\n\nExperts in the field of mobile apps use made an argument that in order to redirect their attention to more productive endeavors, teenagers and young adults must have access to apps that encourage them to perform activities not related to the acquisition of good and spending money online (Kuss et al. 3).\n\nThis conclusion was supported by researchers from the medical field (Conroy, Yang, and Maher 650). Conroy, Yang, and Maher argued that one of the best ways to curb addictive and obsessive behavior related to shopping and other forms of online purchases is to the utilization of apps designed to increase physical activity and app that encourage young people to invest their hard earned money (650).\n\nAccording to Conroy, Yang, and Maher, popular apps that attempt to change human behavior share the following features:\n\n 1. provide instructions on how to perform behavior;\n 2. model/demonstrate the behavior;\n 3. provide feedback on performance;\n 4. presence of social support;\n 5. access to information about other people\u2019s approval;\n 6. the ability to facilitate social comparison (650).\n\nSteinmetz is in agreement with Conroy, Yang, and Maher when it comes to the importance of socialization in an online community (32).\n\nBomhold asserted that once the social component of online interactions has been identified, policymakers, school officials, and social entrepreneurs can help encourage developers to create apps that harness the energy and the passion of the youth, but at the same time, redirect their attention to other more productive activities (424). Bomhold reiterated her stance that successful apps are those that enables users to form online communities.\n\nHowever, this strategy is easier said than done, because there are different factors that must come into play before a thriving online community attracts the attention of teenagers and young adults (428).\n\nMosse stumbled upon the same insight after a lengthy study of human interactions (940). Conroy, Yang, and Maher also pointed out that the developer of next generation apps must understand the value of communication between members and enhance the capability to express approval or disapproval with regards to certain behavior (455).\n\nAn overview of the literature review enables researchers to understand the significance of the problem, especially when it comes to the growing addiction to the use of mobile apps in order to shop online and to interact with people in virtual communities.\n\nThe scholarly researches cited in the review provided a glimpse of the problem faced by policymakers as they attempted to solve the problem without considering the sociological component of the issue. Thus, it is crucial not to prohibit the use of mobile devices and internet-related activities inside the home. It is best to use an indirect approach that allows teenagers and young adults to use mobile devices for more productive endeavors.\n\nConclusion\n\nIt is not a practical approach to limit the use or take away the privilege of using mobile technology. The best solution is to utilize the mobile devices and the Internet to alter the behavior of people. The decision not to take away the privilege of using mobile devices and the Internet was based on the realization that teenagers and young adults were able to satisfy their cultural and social needs using online communities.\n\nTherefore, it is important to provide alternative options that help them experience satisfaction, while interacting with other people online. The end goal is to develop computer applications that help improve certain behavior. A successful intervention strategy encourages the youthful members of the community to focus on more productive endeavors.\n\nGood examples of useful apps are those that enhance physical activity, improve the consumption of healthier food, and develop the propensity to invest hard earned money using investment instruments that are accessible through the Internet. There are so many things to consider when it comes to the creation of intervention strategies to curb the addiction of using mobile devices and the Internet. Policymakers and influential leaders must go beyond the organizational aspect of the online world.\n\nThey must go beyond the engineering component of computer applications and networking systems. They must realize that at the heart of the system are interconnected relationships. Therefore app design must conform to certain standards. An overview of successful apps will reveal the fact that their respective designers understood the value of feedback and seamless communication between different users.\n\nIf policymakers and influential leaders are able to address these issues, then, it is possible to initiate a social movement that encourages people to use mobile devices to improve health and well-being. It must be pointed out that people use mobile devices and the Internet for positive reasons. They believe that it is through these platforms that they are able to find a convenient way to satisfy cultural and social needs.\n\nWorks Cited\n\nBarassi, Veronica. \u201cEthnographic Cartographies: Social Movements,Alternative Media and the Spaces of Networks.\u201d Social Movement Studies 12.1 (2013): 48-62. Print.\n\nBomhold, Catharine Reese. \u201cEducational Use of Smart Phone Technology Survey of Mobile Phone Application Use by Undergraduate University Students.\u201d Program: Electronic Library & Information Systems 47.4 (2013): 424-436. Print.\n\nConroy, David E., Chih-Hsiang Yang, and Jaclyn P. Maher. \u201cBehavior Change Techniques in Top-Ranked Mobile Apps for Physical Activity.\u201d American Journal of Preventive Medicine 46.6 (2014): 649-652. Print.\n\nKuss, Daria J., et al. \u201cInternet Addiction in Adolescents: Prevalence and Risk Factors.\u201d Computers in Human Behavior 29.5 (2013): 1987-1996. Web.\n\nMosse, David. \u201cAnti-Social Anthropology? Objectivity, Objection, and the Ethnography of Public Policy and Professional Communities.\u201d Journal of the Royal Anthropological Institute 12.4 (2006): 935-956. Print.\n\nSteinmetz, Kevin F. \u201cMessage Received: Virtual Ethnography in Online Message Boards.\u201d International Journal of Qualitative Methods 11.1 (2012): 26-39. Print.\n",
        "label": "human"
    },
    {
        "input": "Computer Hardware: Past, Present, and Future Essay\n\nTo a great extent, the development of computer hardware illustrates the technological and scientific progress achieved by people in the course of world history. This paper is aimed at discussing the definition of this term hardware, its history, current development, and prospects.\n\nIt seems that the discussion of these questions can throw light on the evolution of information technologies, and the main factors that influence this process. Overall, one can identify several important trends that profoundly affected the development of hardware, and one of them is the need to improve its design, functionality, and capacity. Secondly, it is important to remember about the increasing commercialization of these technologies.\n\nFirst of all, the term hardware can be defined as physical units of a computer that are supposed to perform the processing, input, storage, transfer, and output of information (Stair & Reynolds, 2011, p. 62). Among these elements, one can distinguish processors, keyboards, hard disk drives, motherboards, graphics adapters, scanners, printers, and so forth (Stair & Reynolds, 2011).\n\nThese units have to do separate tasks, and one cannot say that they are equally important; yet, they are closely dependent on one another (Stair & Reynolds, 2011). Overall, this concept of hardware is used to describe the tangible components of a computing system. In contrast, the word software refers to intangible elements of a computing system, and they are used to manipulate the hardware. This is one of the distinctions that should be taken into consideration. On the whole, this term is critical for the understanding of information technologies and their functioning.\n\nThe importance of hardware can hardly be underestimated because it ensures that every process is completed successfully. Furthermore, the physical units are important for the effective work of software applications and operational systems. Finally, these components are used to connect different computing systems.\n\nScientists have long tried to create a machine that can process information or store it; for example, the famous mathematician Blaise Pascal was able to create a mechanical calculator in 1642 (Singh & Nath, 2007, p. 59). Similar attempts were made by other mathematicians, but it is very difficult to call these inventions computers because they consisted only of mechanical elements (Singh & Nath, 2007).\n\nMore importantly, their functionality was very limited. One can say that the true development of computing systems began in the first half of the twentieth century and at that time, this hardware had to serve military needs (Singh & Nath, 2007, p. 61). In particular, one should mention the model introduced by John Von Neumann who believed that computers had to include such elements as CPU, input and output devices, and memory storage (Singh & Nath, 2007, p. 61). To a great extent, this model contributed to the future development of hardware.\n\nIt should be noted that in the course of history, computer hardware gradually became smaller and more effective. For instance, the Colossus computer occupied an entire floor of the building, while modern computers can be smaller than a notebook (Singh & Nath, 2007). To a great extent, the need to minimize hardware became one of the most important trends, and this requirement is considered by many designers and engineers.\n\nMoreover, the improvement of hardware components led to the greater commercial use of computers or its components. Today, they can be regarded as mass consumer goods that are accessible to people of different incomes. This is one of the issues that should be taken into account.\n\nCurrently, the development of hardware is based on several important principles. First of all, manufacturers attempt to make their devices efficient in terms of energy consumption, size, and compatibility with existing software solutions. These are the most important concerns of engineers and designers who have to work various requirements when developing new products. To some degree, the improvement of CPUs, Random Access Memory (RAM) and other devices reflects the development of existing software solutions that require the excellent performance of technologies. This is one of the most important trends that one can identify.\n\nAdditionally, the physical components of a computer are adjusted to different devices that were not previously related to computing systems. For instance, Random Access Memory can be easily incorporated into mobile phones. Apart from that, the physical components of the computer become are improved in terms of their capacity and connectivity. For example, contemporary hard-disk drives can store several terabytes of data. This capacity was hardly imaginable at least a decade ago when users had fewer opportunities.\n\nMoreover, a hard-disk drive can be connected to a variety of devices such as personal computers, mobile phones, and even TV sets. These are some of the issues that one can identify. Overall, it is possible to argue that the development of computer hardware continuously brings new opportunities to people.\n\nAt this point, it is very difficult to predict the future of hardware development, because a single discovery or invention can significantly contribute to technological progress. Apart from that, it can make previous hardware completely obsolete. Moreover, it is very difficult to appreciate the value of an invention. For example, very few people could predict that the Internet would turn the world into a global village.\n\nSimilarly, the discovery of liquid crystal was not initially connected with computer hardware, but now they are widely used for the production of displays. One can make several conjectures about future trends. First of all, the researchers point out that the speed and efficiency of hardware tend to double every two years (Giaretta, 2011, p. 131). This tendency was observed during the last two decades (Giaretta, 2011, p. 131). This means that in the future, the functionality and performance of hardware will rise dramatically.\n\nMoreover, it is possible to say that the size of different hardware components will significantly diminish (Giaretta, 2011, p. 131). Finally, the hardware, which is now believed to be innovative, may go out-of-date within five years. These are the main arguments that can be put forward. One can say that the development of hardware will offer new opportunities to users.\n\nThese examples suggest that in the course of its history, computer hardware has undergone significant transformation due to technological and scientific discoveries. Although at the very beginning, it was used mostly for military purposes, the situation changed dramatically in the second half of the twentieth century when computers began to serve commercial goals and turned into consumer goods.\n\nMoreover, it is very difficult to predict future changes that information technologies will undergo. Additionally, different elements of computer hardware will be improved in terms of functionality, performance, and design. Finally, different components of a computer can become obsolete very quickly. Overall, by looking at the historic development of computer hardware, one can better understand technological progress and its future trends.\n\nReference List\n\nGiaretta, D. (2011). Advanced Digital Preservation . New York: Springer.\n\nSingh, Y. & Nath, R. (2007). Teaching of Computers . New York: APH Publishing.\n\nStair, R. & Reynolds, G. (2011). Fundamentals of Information Systems . New York: Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Computer-Mediated Communication Aspects and Influences Essay\n\nTable of Contents\n 1. Introduction\n 2. Discussion\n 3. Computer-mediated communication\n 4. Culture and communication\n 5. Conclusion\n 6. References\n\nIntroduction\n\nCommunication plays an important role in influencing human development. It allows people to interact with each other and experience different cultures. Numerous platforms are used to convey information, most of which are influenced by cultural identities (Monaghan, Goodman & Robinson, 2012). Studies have established that cultural patterns have a huge influence on different communication styles used across the world.\n\nOver the years, communication has been characterized by direct human interaction and intercultural exchanges. However, the birth of the internet and technological advancement has changed interpersonal and intercultural communication (Wood, 2011). The development of computers and the ease of internet\u2019s accessibility have played a vital role in improving the efficiency of communication.\n\nThe two factors resulted in the development of Computer-Mediated Communication (CMC). CMC refers to interpersonal and intercultural communication that involves the use of technological devices that support different forms of communication (Wood, 2012). Technological tools used in CMC include computers, internet-enabled phones, and tablets among others. Examples of communication formats supported by these devices include text messaging, instant messaging, chat rooms, and video conferencing. These forms of communication are accessible through social networking sites such as Twitter, Facebook, Skype, Google Circles, and Yahoo (Wood, 2012).\n\nStudies have established that CMC has changed the style of communication because more people are incorporating technology into their daily routines. CMC is different from traditional face-to-face communication in various ways. CMC has pushed interaction between people to a higher level, and it has eliminated the time and space limitations commonly associated with offline communication (Monaghan et al., 2012). Online communication increases the efficiency of human contact, reduces the level of human interaction, and eliminates the therapeutic effect of face-to-face communication.\n\nDiscussion\n\nSince the beginning of the century, there have been numerous technological advancements in the field of communication. Factors such as globalization, population growth, unemployment rates, and advancements in the field of education have increased demand for technology, especially in the field of communication (Barnes, 2002).\n\nPeople possess a desire to interact at a higher level because of limited time and time space for face-to-face communication. The internet has made the world a global village and has created numerous opportunities for people across the world. In the contemporary society, people find most of the information they need on the internet. Over the years, a lot of content has been generated in order to cater for the ever-increasing number of internet users (Barnes, 2002).\n\nStudies have established that the number of people using the internet all over the world has been increasing at a high rate every year. People spend a lot of time online doing various things that include research and communication (Wood, 2012). Experts argue that people are becoming overly dependent on technology and consequently, eliminate the need for human contact.\n\nComputer-mediated communication\n\nOnline communication influences several factors that affect interpersonal and intercultural interaction. Examples of differences between online and offline communication involve the various ways people use to build relationships, create impressions, deceive others, perceive teamwork, and respond to feedback (Monaghan et al., 2012).\n\nCMC experts argue that online communication also differs from offline interaction in terms of the relation that exists when things occur at the same time and remain connected for a long period. A good example is communication through instant messaging and emails. Instant messaging involves communication between two people for a short period (Wood, 2011).\n\nThe information shared in such a conversation gets lost as soon as the dialogue box is closed, thus effecting the efficiency of the interaction. On the other hand, communication through emails is more effective because the messages can be saved and read later after the end of a conversation (Kappas & Kramer, 2011). Communication through email is effective because of the ability to save messages for future reference. The level of interaction through emails is higher compared to that of instant messaging. However, it lacks the depth that is experienced in face-to-face interaction (Barnes, 2002).\n\nAnother characteristic feature of CMC that affects interpersonal and intercultural interaction is lack of a code of conduct. There is no specified code of conduct for people who use online communication (Monaghan et al., 2012). People are free to do anything because of their anonymity, and they can easily get away with anything. CMC experts argue that this element has contributed to the development of negativity with regard to online communication because of factors such as insecurity of internet users caused by the threat of cyber terrorism and bullying.\n\nAccording to research, online communicators ignore people\u2019s age, gender, sexuality, religion, age and race when interacting (Wright & Webb, 2011). Many people have been victimized through online communication due to their identity based on religion, gender, sexuality, and origin. Online communicators are also fond of using vulgar language. Vulgarity has been encouraged by the fact that it is very hard to establish the real identity of users in a virtual world (Flichy, 2004).\n\nIn face-to-face communication, it is hard for people to disregard the code of conduct because of the repercussions associated with vulgarity. Despite the existence of negative attitudes towards online communication, people in the contemporary society prefer it to offline communication. Online communication helps to eliminate limitations such as distance, which compromise the ability to interact effectively through offline communication (Wood, 2012). It allows people to interact and stay connected in various parts of the world.\n\nStudies have also established that CMC encourages people to disclose their personal information easily compared to offline communication. Interpersonal relationships are more open in online communication compared to face-to-face interaction (Wright & Webb, 2011). The main reason for this is the people\u2019s ability to disclose essential information easily.\n\nExperts argue that this element plays a crucial role in building and maintaining relationships. CMC experts further argue that people create better first impressions through online communication in relation to offline interaction. The development of the internet has improved the efficiency of interpersonal and intercultural communication (Kappas & Kramer, 2011). The number of people who have access to the internet and technological devices such as phones and computers is rapidly growing across the world.\n\nStudies have established that people are increasingly using digital media to communicate. The online community continues to expand every year. People have learned effective ways of managing interactions, forming impressions, and building relationships in the virtual world (Wood, 2011). Numerous studies have been conducted to establish the difference in experiences between online and offline communications. One of the main elements of online communication studied is the paralinguistic features.\n\nThe first paralinguistic feature of online communication is emoticons, which refer to a representation of facial expressions such as a smile or frown (Kappas & Kramer, 2011). Emoticons are created by typing a sequence of characters when writing a mail, texting or sending an instant message in chat rooms. The disadvantage of this element is that it is easy for one to interpret an expression wrongly (Wright & Webb, 2011).\n\nFor example, a user can type an emoticon showing they are enjoying a conversation even though they are bored and wish to end it. In online communication, people do not see each other face-to-face. Therefore, it is difficult to understand the other person\u2019s reaction to a conversation (Wood, 2011).\n\nThe second element of online communication examined was the practicability of turn-taking during conversations. Offline communication is characterized by taking turns in talking as well as listening (Monaghan et al., 2012). Turn taking makes offline conversations dynamic, exhilarating, and pleasurable. Responses in offline communication are instantaneous, charming and allow everyone to understand the course of a conversation by observing important communication cues such as body language and posture.\n\nIn online communication, this element applies differently because people do not interact physically (Lustig & Koester, 2005). In addition, it takes longer for someone to get a response if they are not using instant messaging services. For example, communication through email is unreliable in cases where someone needs a quick response.\n\nInteraction through instant messaging and chat rooms can be unreliable at times because some users take time to reply. Certain people are naturally slow and take a lot of time to formulate what to say (Lustig & Koester, 2005). Such responses are often well thought out, and thus lower the probability of getting the truth. In one-on-one conversations, people do not have much time to consider the answers they give. Thus, they have a higher chance of telling the truth (Kappas & Kramer, 2011).\n\nStudies have established that CMC is profoundly influenced by the mode of communication and the immediate environment. In contemporary society, online communication is used in professional, social, educational, political, and economic environments (Wood, 2011). All these situations have their unique style of communication that influences the way people interact. Presentation of political information is different from the presentation of financial information. Different styles of communication require varied technological devices.\n\nTherefore, it is imperative to understand a discipline before choosing a form of communication to use (Flichy, 2004). Some of the common forms of online communication used include e-mails, video clips, audios, blogs, instant messaging, as well as social networking sites such as Facebook, Twitter, and Skype. For example, most television channels and radio stations use Facebook and Twitter for conveying messages to people.\n\nCommunication through social media is different from watching television because in the case of TV, viewers see those who present news and host different programs (Kappas & Kramer, 2011). Media houses and major dailies across the world have websites through which people access news and other content.\n\nCulture and communication\n\nAt the beginning of the 21 st Century, most parts of the world had not been exposed to the internet and technological devices such as computers. Communication was only through the conventional face-to-face interaction (Lustig & Koester, 2005). People had a great chance of experiencing different cultures because they had to interact physically. The traditional forms of communication helped people to develop individual awareness concerning the differences that existed among cultures.\n\nPsychologists argue that such an opportunity had a therapeutic effect on people because they were able to have new experiences and expand their knowledge through interaction (Samovar, Porter & McDaniel, 2012). Cultural beliefs and norms influenced the thinking and communication styles of different communities.\n\nFor example, in Africa, people used various forms of communication to convey specific messages. People in ancient African cultures conveyed information through methods such as singing, smoke, and drumming. If people saw smoke coming from a particular place, they automatically knew that someone was conveying a certain message (Lustig & Koester, 2005). People would gather at the source of the smoke and listen to the news. Such gatherings provided people with an opportunity to interact.\n\nCultural practices dictate the style of communication that people use. Cultural beliefs are concepts that people assume to be true about their community and the world (Wood, 2011). Beliefs of one community could be different from those of others. Cultural beliefs dictate crucial elements in societies such as interpersonal interaction and communication.\n\nFor a long time, people interacted and communicated according to what their culture considered right and respectful. Cultural beliefs are closely connected to standards, which refer to the expectations that people have in regard to demeanour (Lustig & Koester, 2005). Standards vary from one cultural group to another depending on their social practices. For example, the expectations that people have regarding the behaviour of old and young people are different across various cultures around the world.\n\nIn most African and Asian countries, respect for the old is more valuable compared to certain cultures in the United States. Old people in the society play a crucial role of educating the young about their history, how they should do certain things and other essential things about life (Samovar et al., 2012). However, technological advancements have eliminated this important role of older people because this information can now be accessed over the internet.\n\nStudies have established that cultural practices influence communication by dictating the manner and level of human interaction (Lustig & Koester, 2005). Various things such as sports, food, music, agriculture, and dressing define the culture of communities. For example, basketball is a popular sport in the United States compared to other parts of the world. It is considered as one of the main elements of the American culture. This game brings people together and encourages interaction in the United States than everywhere else.\n\nThe basketball history has shaped the communication style of Americans in a way that other people consider controversial. In contemporary society, people follow games over the internet and televisions, thus limiting the chances of physical interaction (Lustig & Koester, 2005). People enjoy many benefits through one-on-one interaction. Therefore, cultural patterns have a tremendous impact on the way communities are likely to interact (Samovar et al., 2012). CMC has shifted the style of intercultural communication to a lower level because people do not interact physically.\n\nCulture also influences the development of interpersonal communication patterns. Communities develop certain systems of communication that align to their culture. The systems, which form the communication patterns, represent the manner in which different groups within a cultural setup should relate and interact with each other (Lustig & Koester, 2005). Some of the groups represented by the codes include parents, siblings, teachers, religious leaders, figures of authority, friends, and neighbors among others.\n\nExperts argue that cultural differences often influence the development of communication patterns. The models often describe the verbal and non-verbal forms of communication used in every culture (Lustig & Koester, 2005). The most notable differences in the way different cultures develop their interpersonal communication patterns relate to non-verbal systems.\n\nDifferent cultures describe non-verbal systems such as the use of space, gestures, facial expressions, touch, walking style and posture differently (Samovar et al., 2012). Definitions for these elements are crucial because they help to maintain a structure through which societies share its practices and values with subsequent generations.\n\nConclusion\n\nInterpersonal and intercultural communication in contemporary society has changed tremendously. Some of the contributing factors to this phenomenon include globalization, advancement of education and high unemployment rates. Globalization has created demand for technology, while unemployment rates have pushed people into being more innovative as the need to tap on the expanding online community increases. Online communication is different from offline communication in various ways.\n\nFor example, online communication is more efficient and eliminates the limitations of distance commonly associated with face-to-face communication. However, the level of interpersonal and intercultural interaction is low in online communication. Online communication has increased the efficiency of human contact and reduced the level of human interaction, which has eliminated the therapeutic effect of face-to-face communication.\n\nPeople enjoy many benefits through one-on-one interaction. Culture also influences the development of interpersonal communication patterns. Experts argue that cultural differences often influence the development of communication patterns. Cultural practices dictate the style of communication that people use.\n\nReferences\n\nBarnes, S. B. (2002). Computer-Mediated Communication: Human-To-Human Communication across the Internet . San Francisco: Allyn & Bacon.\n\nFlichy, P. (2004). Dynamics of Modern Communication: The Shaping and Impact of new Communication Technologies . California: SAGE.\n\nKappas, A., & Kramer, N. C. (2011). Face-To-Face Communication over the Internet: Emotions in a Web of Culture, Language, and Technology . Massachusetts: Cambridge University Press.\n\nLustig, M.W., & Koester, J. (2005). Intercultural Competence: Interpersonal Communication across Cultures . Boston: California State University.\n\nMonaghan, L., Goodman, J. E., & Robinson, J. M. (2012). A cultural Approach to Interpersonal Communication: Essential Readings . New York: John Wiley & Sons.\n\nSamovar, L., Porter, R., & McDaniel, E. (2012). Communication between Cultures . Boston: California University Press.\n\nWood, J. (2011). Communication in Our Lives . New York: Cengage Learning.\n\nWood, J. (2012). Interpersonal Communication: Everyday Encounters . New York: Cengage Learning.\n\nWright, K. B., & Webb, L. M. (2011). Computer-Mediated Communication in Personal Relationships . California: Peter Lang.\n",
        "label": "human"
    },
    {
        "input": "Computer Security and Computer Criminals Essay\n\nTable of Contents\n 1. Introduction\n 2. Computer Security\n 3. Computer Security threats and possible solutions\n 4. Computer Security companies\n 5. Conclusion\n 6. Works cited\n\nIntroduction\n\nNothing is of more concern than the issue of computer insecurity that is cropping up so fast in the day to day life, doing your work knowing that someone somewhere might be accessing all your confidential information or something might just happen, and you lose it all. Some of the main ways through which the computer criminals can access and alter personal information, also referred to as identity theft is through hacking, Phishing, lottery scams, or fraud.\n\nComputer security covers mainly security from breach of information, natural disaster or corruption, and Computer Crime and Intellectual Property Section (CCIPS), offer advice to the people regarding these issues. This is a subject affecting every individual directly or indirectly and thus addressed to the public in general (\u201cComputer Crime and Security Survey\u201d).\n\nThesis statement: Why has insecurity become one of the biggest concerns from around the globe, but when it comes to computer insecurity, most of the people do not know what to do or who to consult for assistance, what is being done to counter computer insecurity and the ways through which people are being enlightened on this matter.\n\nTechnology around the globe has taken a milestone and through firms like Information security and crime prevention people can do so much and can make a lot of money and also improve security, but it is also the main cause of insecurity all round the world (IT security and crime prevention).\n\nThe term computer insecurity revolves around the possible attacks and vulnerability, plus the numerous numbers of people fighting to maintain a high level of security, whereas others try to breach it.\n\nThis poses one of the biggest threats to individuals, organizations, government agencies, medical institutions, educational institutions, financial institutions, and even to the state as a whole. The reason why these people might be trying to breach someone else\u2019s information is that they are jobless thus idle and do not have a more meaningful thing to do: others do it as a means of trying to gain some money.\n\nComputer Security\n\nThe idea of security or insecurity to most people is just about theft, high jacking, carjacking or robbery with violence but in this case, hackers are able to literally steal and get a lot of important data without direct contact to their victim, and in most cases the victim does not have an idea until it is too late.\n\nThe main thing that people need to know is how this breach of security and information occurs and also the possible ways through which they can be able to prevent it, and that\u2019s why institutions like, Computer Crime & Intellectual Property Section feature some of these issues in their article. These strategies and methodologies differ from any other form of computer activity for its main purpose are to hinder unwanted computer activities (\u201cComputer Crime & Intellectual Property Section\u201d).\n\nThis means they get access to some important data across all the security measures put in place and this may result in loss of important data such as usernames, password, banking information or even credit numbers and that is why institutions like Computer Security Institute are coming up with more reliable solutions.\n\nComputer Security threats and possible solutions\n\nVery many companies have been formed to provide computer security and other measures to protect individuals and companies against hackers. An example of these incorporations includes: Symantec corporations, computer security institute, Microsoft safety and security center among others, and they have been trying to fight computer crime and inventing the possible antivirus to ensure that this problem minimizes.\n\nThe main problem that these companies are encountering is the fact that hackers are training other hackers online how to do it and so the trainees want to experiment what they have been taught not knowing the harm they cause to the victims. The reason why the rates of hacking have increased so much despite the formation of such institutions is that people unknowingly give their important information in sites that are dangerous and this provides a lead for the hackers (\u201cComputer Crime\u201d).\n\nComputer Security companies\n\nThe computer security companies are working hard to acquire antivirus against the worms and Trojans that cause the breach of security. Computer Crime Research Center is one among the many other centers revolving around ensuring computer security through sensitizing the public on the dangerous sites and the ways through which they are trying to steal from people, plus how to trace their locations.\n\nThey publicize every site they find fishy and discourage people from using sites they do not know about, because just by clicking the levels of their vulnerability increase (\u201cPossible Vicarious Liability\u201d).\n\nAccording to the \u201cComputer Crime & Intellectual Property Section\u201d, another very important course of effect that people are being encouraged to take is constantly changing one\u2019s passwords and security details, especially that of your computer application, some important sites like the mail providing sites and the pin to the credit card and other important documents. This helps to keep most of your details changing, thus making it hard for any hacker to keep track.\n\nConclusion\n\nThe information about hacking and breach of information should be spread across all populations to ensure that they take the necessary precautions in preventing it.\n\nThe use of computers by children should also be closely monitored because they have also been known to leak information about their parents that have guided the hackers. Computer security information is very crucial as it can prevent a huge loss of either information or money from occurring and also people being cautious about the information or data they share with strangers (Computer Security Institute).\n\nWorks cited\n\nComputer Security Institute: Computer Crime and Security Survey, 2010. Web.\n\nInformation security and crime prevention. IT security and crime prevention methods, 2011. Web.\n\nStandler, Ronald . Computer Crime, 2002. Web.\n\nStandler, Ronald. Possible Vicarious Liability for Computer Users in the USA? 2004. Web.\n\nUnited States Department of Justice . Computer Crime & Intellectual Property Section (CCIPS), 2011. Web.\n\nUnited States Department of Justice. Computer Crime and Security Survey, 2009. Web.\n",
        "label": "human"
    },
    {
        "input": "Dell Computers Company Planning and Organization Essay\n\nTable of Contents\n 1. Introduction\n 2. Organizational Restructuring and the newly hired Executive Team\n 3. New Organizational Culture\n 4. New Strategies to reach customers\n 5. Reference List\n\nIntroduction\n\nMichael Dell, the proprietor of Dell Computers, has been one of the most successful entrepreneurs shipping computers and computer components directly to customers in different locations around the world. However, things have not been rosy of late. Many undertakings in the computer \u2018planet\u2019 have seen their market share shrink.\n\nHowever, Michael Dell has put in place initiatives to reclaim its market share. Pundits have kept asking if Michael Dell will successfully turn around the company and return it to its former glory. This essay seeks to expound on the initiatives and design structures that Michael Dell has put in place to revive Dell Computers.\n\nOrganizational Restructuring and the newly hired Executive Team\n\nDell has hired Ed Boyd to be the Chief Consumer Designer at their Brent Humphreys outfit. Brian Gladden, the General Electric veteran, has come to Dell to take over as the Chief Financial Officer. He has brought in a wealth of experience from General Electronics. Garriques heads the consumer group. Garriques in collaboration with other divisional heads have taken charge of operations. This has ensured that Michael Dell is free to explore opportunities that may be lucrative to the organization (Edwards, 2009).\n\nNew Organizational Culture\n\nOrganization culture is a particular way of behaving. It encompasses the beliefs about the world. It has much to do with experiments and experience. Dell has had a culture where its products are delivered directly to their customers and the company\u2019s Chief Executive Officer still maintains this will not go away any sooner. Dell has strengthened a culture where a direct sale of their products over the telephone and internet has been initiated.\n\nThe customer specifies the features that he wants to be integrated into his personal computer and effects payment using credit card before delivery is made (Gattiker, 1990). After the payment, the machine is assembled and dispatched to the customer. Dell Computers stock a large variety of key components (Golaleh and Merle, 2001).\n\nThis enhances wide choice of customer specifications. Components of Dell computers can be assembled in the least time possible before they are dispatched to the customer. This is called mass customization and helps in boosting consumer confidence (Tantoush and Clegg, 2001).\n\nNew Strategies to reach customers\n\nThe principal beneficiaries of Dell Products have so far been government agencies, home-based consumers, and SME\u2019s. However, these customers have received less attention from Dell because the company focused much on bigger corporations. However, things are changing as salespeople are currently being offered incentives to offer all-rounded solutions to small and medium size companies other than hard wares.\n\nThis has been initiated by the head of the group, Steve Falice. Response to customer\u2019s demands has also improved. Issues that are raised by customers are responded on promptly. Dell is also initiating integration as witnessed in circumstances when GA Communications Mac\u2019s was integrated into Dell\u2019s business initiatives (Edwards, 2009). They have also contemplated purchasing stakes from software providers VMware and Microsoft.\n\nMoreover, their takeover initiatives are also done competently and with appropriate speed. Dell begun selling 0.39-inches notebook, the world thinnest notebook as soon as Microsoft launched its Windows 7 operating system. The notebook boasts of features like heat sensing strip.\n\nThis has sent shockwaves in the market and show people that Dell is up to revolutionizing the market. Apart from Adamo XPS, Dell is also considering introducing its smart phones in the market. Michael Dell still maintains that Dell will still sell computers directly to its customers\n\nReference List\n\nEdwards, C. (2009). Dell\u2019s Extreme Makeover. Business Week , 10, 1-4.\n\nGattiker, U. (1990). Technology Management in organizations , New York: Sage publications.\n\nGolaleh, E. and Merle, J. (2001). Restructuring for agility at Volvo Car Technical\n\nService (VCTS). European Journal of Innovation Management, 4(2), 64-72.\n\nTantoush, T. and Clegg, S . (2001). CADCAM integration and the practical politics of technological change. Journal of Organizational Change Management , 14(1), 9-27.\n",
        "label": "human"
    },
    {
        "input": "Humanities and Computer Science Collaboration Report (Assessment)\n\nIntroduction\n\nConnection among disciplines focuses on the border between Information system (IS) and the humanities. It is evidenced that information technology has often been used as a tool for scholarly humanities research in areas of consulting, technical support, applications development, and networked publishing facilities. Production work of humanities such as writing, reading, and browsing now depends on the propriety Information system platforms and applications.\n\nThe increasing adoption of open source programs of IS has facilitated the true function of IS as a speculative mirror allowing whole new ways of distribution, decentralization, networked and team-worked organizations.\n\nThe research presented here are the products of a unique collaboration between humanities and computer science faculty. It clarifies the relationship two disciplines of information system and the humanities and explains the core issue, namely Humanity computing and Humanities-informed computing which reveals a mutual relationship between the two disciples. Both sides of the mutual relationship are discussed in detail, and suggestions for the way forward suggested.\n\nSince the presentation uses an interpretive approach, references to the existing endeavours of humanities-enriched Information System studies are used to provide evidence ideas and suggestions expressed. Its unique focus will be literature work of; The Dead Men\u2019s Path by Chinua Achebe , two non-literacy works; The Grateful Dead \u201cChina Cat Sunflower film by Jerry Garcia, and The Beatles-Love me Do.\n\nMuch attention has over the years been given to IS as a social science, and much has been published on the application of IS in the humanities, however, the gap in the reflection and theory has often been neglected hence the focus of the rest of the paper.\n\nWe provide a list of theoretical constructs published in the Information Systems Journal and their direct relationship to the humanities, such as narrative thinking and theories of meaning in literature languages, video editing in films and synchronization of sounds in music such as the \u2018The Beatles-Love me Do\u2019 website.\n\nDiscussion and evaluation of studies that implement theoretical constructs borrowed from humanities in various aspects of IS, as well as film industry and music regarding the application of I.T in the Humanities are analysed.\n\nThe paper explores the nature of the relationship between the study field of IS and the humanities using objective and subjective analysis approaches at literature and non-literature levels of humanities. It suggests that, by building on the foundation of existing information system platforms, pre-disciplinary enrichment endeavours, a new paradigm of IS research may be acknowledged and nursed in order to facilitate further growth of the humanities.\n\nIn this regard, IS should be studied as a relevant contributor to the foundation of humanities. In website development for example, artists inputs are necessary to maximise the visual impact to establish the sender\u2019s message. In music presentation for example, the new media art installed in a Website instantiation, dynamic organized scanned images based on the ensemble of their shared and invincible conceptual resemblances such as the \u2018The Beatles-Love me Do\u2019 is a perfect example of IS-humanities relationships.\n\nFinally, a video mural displayed the unexpected, emergent, and changing affinities among the total sets of pictures and instruments in the \u2018The Grateful Dead \u201cChina Cat Sunflower\u2019 film by Jerry Garcia.\n\nFaith is used in groups to reaffirm itself periodically and becomes potential bond when the integration is disrupted. Its evident that The Grateful Dead \u201cChina Cat Sunflower\u201d symbols and totem are tied to one another by beliefs structure.\n\nTherefore social groups formed as a result of religion are very important in any given society as they offer protection against destruction. This relationship implies a commensalistic relationship rather than mutualistic one.\n\nThe study reveals a theoretical constructs borrowed from IS in various aspects of the humanities. This is to conclude that IS looks at ways to facilitate people\u2019s connectedness with each other by drawing attention to essential items by building networks facilitated by computer software like e-mail, social sites, discussion groups, e.t.c.\n\nThis is to say that more recognition should be given to the pre-discipline of Humanistic-enriched IS.\n\nThesis Statement\n\nAs we seek to integrate information technology in the works of the humanities, the best test, and yet difficult to administer, is: does technology while being applied, more likely to educated workers in contemporary information technique such as collection of data, synthesis, attribution of sources, use of media to produce, manipulate and circulate results and in archaic and historical knowledge technique (e.g..music and film), result to fostering a richer, more diverse modern technical identity?\n\nLiterature: The Dead Men\u2019s Path by Chinua Achebe\n\nMichael Obi\u2019s ambition to change the local people\u2019s narrow views and ways is what Chinua, (1953) quotes as \u201cSuperannuated people in the teaching field\u201d by planning to institute modern methods and high standards of teaching. Gives the protagonist an exciting chance to fulfil his dreams (was backward in every sense, turn into a progressive one. Obi displays his ardent desire (p.331).\n\nLiterature contributes to the existing body of knowledge of the domain in terms of theory and practice. The Dead Men\u2019s Path by Chinua Achebe presents its literacy works in form of a dialogue as a process of communicating to the reader. IS have been widely used in Computation Linguistics to enhance the quantity and quality of explorations conducted.\n\nComputer is often used to find themes and patterns that would be difficult for people to find. The Dead Men\u2019s Path by Chinua Achebe provides some examples and guidelines for the use of the electronic texts enrich grammatical and literacy studies.\n\nConclusively, access to old literature also becomes much easier and helps in building repositories of historical knowledge for future generations much more encompassing than ever before.\n\nCoyne (1998) provided that the symbiotic relationships between IS and Linguistics is mutualistic with a remarkable statement: \u201c In fact, it is equally valid to say that information technology is the product of the working of language and texts\u2026\u201d. (p.11).This statement does provide an interesting example insight that IS does really exists in the humanities.\n\nThe use of a linguistic approach to The Dead Men\u2019s Path by Chinua Achebe from simplified texts containing narratives to conceptual analyses of the information systems to be created is magnificent. He combines morphological and syntactical analyses to understand and represent business rules.\n\nSemantic roles, identified in functional grammars, may provide a logical alternative for the more formal syntactic analyses such as uses of pragmatic concepts to sketch a holistic picture of the context of information systems, forming a bridge as informative acts between ICT systems (formative acts) and activity systems (per formative acts).\n\nIS in this regard is a plead for a shared sense of history and collective accomplishments that creates a sense of belonging and also as a way of orientation and seeing the bigger picture. Literature like any other discipline, tried to influence the development of IS by enforcing its approaches on the literature reviews. Pragmatic concepts that clearly define a holistic picture is one of the cognate fields that provided momentum to the growth of the IS discipline and the related area which is used to find source material for literature reviews.\n\nIntegrating literature into web-based readable platforms\n\nThe four elements of artistic composition presented in the paradigmatic identities are informed around differing constellations of beliefs concerning the nature of reality. IS has enhanced the study of disciplines in humanities, such as language and literature, music and the arts. These constitutes valid knowledge of humanity-enriched IS including principles and values that should guide proper academic inquiry such as research methods and ethics.\n\nSong: The Beatles-Love Me Do\n\nThe non-literacy work; The Beatles-Love me Do first performed in October 7, 1962 presents a music presentation with a new media art installed in a Website instantiation in a dynamic organized scanned images based on the ensemble of their shared and invincible conceptual resemblances. This simply implies that without website presentation records of the song could not be easily accessed by viewers whenever they felt like (The Beatles , 1962).\n\nIS assists art in the visual and audio presentation in editing, improving and increasing the speed of their creative work. Use of visualization tools and the visualization of color in art. Murray (2009) refers to the creation of visual worlds to augment the teaching of Art History.\n\nSuch an example as \u2018The Beatles-Love me Do\u2019 is a brilliant view of the creativity of web presentation that drawing attention to essential items works of IS, in which we concluded that, IS is very much still captured in information system inspired by the humanities. Arguments that information systems (IS) may be regarded as an agent of postmodernism is, however, an excellent example of postmodernism. The exact reciprocal nature of these two concepts needs more in-depth study.\n\n\u2018The Beatles-Love me Do\u2019 gives recognition to IS using insights and approaches from humanities. This could indicate a reciprocally beneficial relationship and synergy between these two groups of disciplines. Embracing conceptualizations borrowed from the social and human science could be the only way to embrace IS applications. Inspirations from arts to help IS scholars to come to terms with the pluralism that is inherent in the field is of paramount importance.\n\nFilm: The Grateful Dead \u201cChina Cat Sunflower\u201d by Jerry Garcia\n\nAnother field of humanity where IS is used to make research more efficient is the film industry. IS has been created in abundance to help movie directors and to study and analyse their sources, including texts and libraries of commentaries and reflective work.\n\nThese ideas are some pointers that may be used to direct future studies to uncover the bi-directional relationships in more detail. Example of these works include the film, \u2018The Grateful Dead \u201cChina Cat Sunflower\u201d by Jerry Garcia\u2019, first performed on January 22, 1968.\n\nWith reference to the IS application, themes such as adaptation of totems and taboos in the film and how they can combine to justify a particular approach from borrowed concepts of Humanities-enriched IS illustrate the type of video casting, while creating huge virility and awareness of what is happening around us (Durkheim, 2001). The Grateful Dead \u201cChina Cat Sunflower\u201d song does not only base on illusions but about religion and its influence in shaping society\u2019s every day lives and opinions.\n\nThe Grateful Dead \u201cChina Cat Sunflower\u201d film justifies the theory of cinematic elements of maximising the visual impact to establish the sender\u2019s message by applying basic ethnographic approach to explain how social integration was formed and how religious practices and beliefs of different tribes came into being.\n\nThis art-work is an emblem of what humanities and IS commit to scientific progress by ushering new theatre of technical skills, human possible to perceive, understand an artwork. By self-organizing images, humanities and IS can refresh our organizational imagination by suggesting emergent structures of interdisciplinary collaborations.\n\nThe new media art installed at the \u2018The Grateful Dead\u2019; \u201cChina Cat Sunflower, in a Web instantiation, is a dynamic organized scanned images based on the ensemble of their shared and invincible conceptual resemblances. Finally, a video mural displays the unexpected, emergent, and changing affinities among the total set of story lines images.\n\nThe Grateful Dead \u201cChina Cat Sunflower film is a perfect example of how humanities embrace the poetic power of IT. Tasks such as searching the video clip on the internet, queries sampling, selecting, scanning, filtering, sharpening, blurring, cut pasting, inserting, encoding, mark up, uploading, downloading, attaching, exporting, importing, configuring, installing ,saving, creating and finally the viewer being able to watch, read and write.\n\nThese are some of the verbs on the top-level menu of technical skills that business works, common protocols of knowledge, now need to command. Expressed in literature classrooms; read, write, contextualise, interpret and critique are some of the basic artistic compositions used in this non-literacy work as illustrated below.\n\nLiterature: The Dead Men\u2019s Path by Chinua Achebe. Subjective Analysis\n\nBased on the factors that are currently very subjective and beyond the scope of a reasonable analysis of the humanities-IS relationship, its quite evident from our analysis that humanity informed IS can only give students necessary skills and impart the uniquely humanistic imagination of such skills capable of envisioning a more humane world of global competition. Some borrowed from other science, is a conscious effort to share interest and useful research.\n\nThe paper gives some recognition to the humanities-informed work already done and being done by highlighting some of the research endeavours discussed in available literature. When new theoretical gaps in the field of IS are identified, which may be filed by approaches in the humanities, research should integrate these concepts into existing IS theories and test and refine the combined models.\n\nBy having established that there is a symbiotic relationship between the humanities and IS, we should also ask ourselves of what nature the symbiosis is. Symbiosis describes the reflective relationship that may or may not be beneficial to both disciplines. It\u2019s a mutualism where both parties benefit, or a commensalism where only one party benefits.\n\nWell, in this regard, the relationship between IS and literature should, in general, be mutually beneficial, and IS could especially obtain fresh new insights in connection with fields that bring in notions yet unexplored in information systems theorizing. This is to imply that IS is \u201cpermeable\u201d in its relationship to social and humanistic disciplines.\n\nThis brief overview is not sufficient to appreciate the a mountain of work that has gone into the toil. Proper work of humanities and IS has to be done to attain this goal, and one could only hope that a scholar would like to take on this challenge in the near future. Ideally, such a research will already have both the necessary backgrounds of training in IS and literature.\n\nSong: The Beatles-Love Me Do\n\nThinking about the conscious and purposeful adoption of constructs of the humanities in IS requires an epistemological conversion to help IS researchers and practitioners make sense of the multiple socially constructed word-views they deal with to provide building blocks which they could use to create legitimate, realistic and coherent worlds.\n\nConversion may be a strong concept to use in this context, but one has to admit that it is difficult to change the fundamental assumption on which different software development approaches are founded. The wider trend towards multi-disciplinary between IS and the humanities make this process more acceptable and easier\n\nIn a sub-discipline like human-Computer Interaction, principles from the arts have been used to make systems more user-friendly and submit protocols to interpretive artistic form.\n\nFilm: The Grateful Dead \u201cChina Cat Sunflower\u201d\n\nIn website development, the art inputs used to upload the film for example are necessary to maximise the visual impact to establish the sender\u2019s message and draw attention to essential items. It is argued that e-commerce sites should build \u201chuman universals\u201d in designing their personalized interfaces for diverse audiences. These human universals should not be limited to social sciences, but should include inputs from the arts and other humanities\n\nEditors and reviewers should embrace the concepts of new comers in IS in order to further build the discipline. In this regard, IS should be inclusive in the humanities development-which is a conscious effort to share interest and build useful research to further advance the discipline.\n\nA guide to Humanities-enriched IS research could make a significant contribution to IS if they could act on these suggestions to purposefully investigate and explore new avenues for the enrichment of the discipline. In this case, the discipline should incorporate insights of the humanities that may help to deepen the understanding of IS research problems\n\nRelationship Among the Disciples\n\nThe humanities must begin to teach the technical skills needed to flourish in today\u2019s society, such \u201ccompetence\u201d is most valuable, both to individuals and society, when will it lead to a full technical relationship between contemporary knowledge and learning?\n\nAll of these humanity disciplines use applications of \u201cOffice\u201d to write \u201cfiles\u201d, indicates the sway-subtle yet tidal relationship that uses business protocols. The collaborations features in Microsoft\u2019s Word, for example and XML features tie documents into institutional databases that can be used by users whenever needed.\n\nSuch a community of practice and knowledge with a main focus on the humanities discipline already exists in the humanities (humanities Computing), and one can only hope that a parallel community will grow within IS with its main focus on IS issues because this could provide impetus and direction to humanities-informed research.\n\nThe relationship between humanities and IS also known as Digital humanities has enhanced the study of disciplines in humanities, such as Language and literature, cinematography in films and music. Disappointingly few studies are available that purposefully reflect on the other direction of the synergy. This situations creates the impression that only one partner in the symbiotic relationship (the humanities) receives all the benefits of the symbiosis, using Computing to \u2018refurbish\u2019 the humanities.\n\nWhen we dig deeper into the other side of symbiotic relationship, we are quite surprised to find that IS are very often informed and enriched with humanities too. Although one can not go far by saying that IS is the science of humanities, I try to correct the imbalance in the reflection on the topic by exploring existing research for solid examples of Humanities-enriched IS. In order to be regarded as a discipline, Humanities-enriched IS should refer to a coherent body of topics that are unique and typical of the subject matter.\n\nRecognizing efforts to enrich IS using humanities-based approaches make IS the perfect adhesive within which to coat any profession to make it adhere to the common knowledge-work model. Consider for example, the fusion of information and knowledge in the The Grateful Dead \u201cChina Cat Sunflower, that details an analysis that attempts to uncover the origin true of beliefs and makes us to think more clearly about the society we live in.\n\nThe information and the ability to wield (IS) stick to knowledge that connects; The Grateful Dead \u201cChina Cat Sunflower\u201d film, The Dead Men\u2019s Path by Chinua Achebe and the The Beatles-Love me Do by using interpretive metaphor, fusion of elements, and networking everything together in the runaway fusion explosion called the web. In our specific context this means that the protocol of knowledge work redefined in IS are one of the main vectors by which information system enter the academy of the humanities.\n\nIn summary, IS has a defining role and provides the distinct strength of the humanities that cannot be adequately adapted from the culture of information.\n\nToday we are witnessing the convergence of the professionals in a paradigm of post-industrial \u201cknowledge work\u201d. Ours is the age of the \u201crise of the symbiotic relationships, which is the new millennium to the work knowledge. The new class to the post-industrial program of efficiency-cum-flexibility perfectly fits the paradigm of IS informed humanities.\n\nAs the Kroeze (2008) provides, dominant protocols of knowledge work are those of business. All the humanities sectors, for example the \u2018The Grateful Dead \u201cChina Cat Sunflower\u201d film, The Dead Men\u2019s Path by Chinua Achebe and the The Beatles-Love me Do have been touched by the logic and discourse of post-industrial corporation.\n\nSummary\n\nInformation systems is an academic discipline that covers all aspects of information systems, software products, that integrates knowledge from algorithmic perspectives with applications in business, organizations and societies.\n\nThe characterization of IS as an interdisciplinary science is quoted by Oates (2006) as \u201cis particularly concerned with real-world social and organization context in which information systems are developed and used\u201d (p.2). Where Computing concentrates more on the technical aspects of software products, the themes of computing as Information and communication technology provides qualitative research methods developed within the humanity sciences often used in the study of the Information Systems.\n\nWhile examples of IS-humanity relationships, the fundamental and theoretical discussion in this part is necessary, the development of disciplines cannot be predicted or managed but takes place through a process of learning tension and dialogue in an academic community. This indicate that the humanities are as important as the social sciences in the study of Information Systems and that the time has arrived that proper recognition should be given to the symbiotic relationship as a mutualism.\n\nIn this regard, its evident that IS bundles together some of the nuggets of Humanities-informed IS uncovered from seas of IS information.\n\nSo fully entangled in the humanities as tool, perspective, and a theme, its quite evident that the knowledge of the humanities are valuable in relation to Information System profession. It helps integrate information technology in the work of humanities and proposes theoretical and practical ways to make the field more independent. This aspects fall more within the social sciences than in humanities as they mainly use empirical methods, while the humanities use rational methodologies.\n\nConclusion\n\nAs we seek to integrate information technology in the work s of the humanities, the best test, and yet difficult to administer, is: does technology while being applied, more likely to educated users in contemporary information technique such as collection of data, synthesis, attribution of sources, use of media to produce, manipulate and circulate results and in archaic and historical knowledge technique (e.g..music and film), result to fostering a richer, more diverse modern technical identity?\n\nTo answer this question, the analysis provides a distinct differences by stating that natural science mainly use empirical methods, while the humanities use rational methodologies. These findings motivate an invitation to interdisciplinary scholars to look for more possibilities to grow the IS discipline further.\n\nAs this research reflected on IS-humanities relations, it was founded that the current status quo to be more type of commensalism since most work focuses on the application of humanities and IS in various humanities disciplines.\n\nRecognition is given, however, to some existing endeavours that enrich IS using humanities insights, for example from The Dead Men\u2019s Path by Chinua Achebe, The Beatles-Love me Do and The Grateful Dead \u201cChina Cat Sunflower film pleaded a more mutualistic relationship that benefited both groups of disciplines equally.\n\nIt suggested that, by building the foundations of existing, pre-disciplinary enrichment endeavours, a new paradigm of IS research may be acknowledged and nursed in order to facilitate growth of the discipline. Indeed, IS thinks, therefore IS is and grows.\n\nReferences\n\nChinua, A. (1953). Dead Men\u2019s Path . Nigeria: Questia Online Library.\n\nCoyne, R. (1998). Cyberspace and Heidegger\u2019s pragmatics. Information Technology & People, 11 (4), 1-78\n\nDurkheim, Emile. (2001). Elementary Forms of Religious Life . New York: Oxford University Press\n\nKroeze, J. H. (2008). The mutualistic relationship between Information Systems and the Humanities. A Business Competitive Edge Perspective , 15, 915-927\n\nMurray, S. (2009). Art-history and the new media: Representation and the production of humanistic knowledge,\u2019 Working Together or Apart: Promoting the Next Generation of Digital Scholarship: Report of a Workshop Cosponsored by the Council on Library and Information Resources and the National Endowment for the Humanities, Council on Library and Information Resources (CLIR Publication, no. 145), Washington, DC, 57-61.\n\nOates, B. J. (2006). Researching Information Systems and Computing .\n\nLos Angeles, A: Sage The Beatles (1962). The Beatles-Love me do . Web.\n",
        "label": "human"
    },
    {
        "input": "Individual Computerized Intelligence Tests Research Paper\n\nTable of Contents\n 1. Accuracy Level\n 2. Biasness\n 3. Individual Feelings Towards Individual Computerized Tests\n 4. References\n\nIntelligence is defined as the mental ability of an individual, which differs among people; it portrays the mental power and is sum of different aspects some inborn, developed, shaped and moulded by socialisation. There are several forms of measuring intelligence (Watkins & Vicki, 2000). This paper evaluates individual computerized tests.\n\nAccuracy Level\n\nThe results from the test were accurate at a 95% level of confidence. This is because the questions were structured to meet four major types of intelligence that is Classification skills, spatial skills, Logical reasoning, Pattern Recognition and general knowledge. The questions were simple but tactical to evaluate the level of intelligence. The test had 30 questions of which they covered all the above areas however; some aspects of intelligence were not covered. Areas not covered include short-term memory power and verbal power.\n\nBiasness\n\nThe test was slightly biased since different people in different part of the world have different exposure and socialisation that affects their level of intelligence. The test also locked out those people who could not read or see. Intelligence is a measure of one\u2019s potential; it is thus free of cultural and social bringing, the approach taken by the test was more of an achievement test than an intelligence test (Cohen & Swerdlik, 2010).\n\nIndividual Feelings Towards Individual Computerized Tests\n\nWhen intelligence tests are used to gauge me with my peers, I feel it is not an objective way of comparison. This is so because other than my intelligence, there may be some external or internal factors that might affect my performance in the tests, thus undermining or giving an impression that my intelligence level is high than the way it is.\n\nFor example, a person who has a wide experience in doing aptitude tests may excel in an intelligence test, not because he or she is intelligent but the exposure of questions asked. Finally, I fell a person\u2019s capability should not be measured with simple short questions as the case is in intelligence tests.\n\nReferences\n\nCohen, R., & Swerdlik, E. (2010). Psychological Testing and Assessment. An Introduction to Tests and Measurement. Boston: McGraw-Hill Company.\n\nWatkins, E., & Vicki, C. (2000). Testing and assessment in counseling practice. New Jersey: Routledge\n",
        "label": "human"
    },
    {
        "input": "How to Build a Computer? Essay\n\nIntroduction\n\nDespite having been in the market for more than four decades, brand computers remain as expensive as they were during the last decade. Besides, they sometimes lack the specific specs that a customer is interested in. Thus, it is necessary to acquire explicit knowledge on how to build and customized a personal computer that is relatively affordable. This reflective treatise attempts to prove that it is really not hard to build your own cheaper computer, than taking it somewhere to have it done, by following these simple steps.\n\nBuilding a personal computer\n\nPreparation and Materials\n\nIn order to build a personal computer, it is necessary to choose the performance that you want by considering the aspects such as the desired processor speed, the memory, and storage capacity. This will ensure that you buy materials that will give you the desired performance level. This step is followed by getting the hardwares and tools for the actual construction process.\n\nThe hardwares for a typical computer include the processor, motherboard, RAM (Random Access Memory), hard drive, video card, case, power supply, keyboard, and a mouse. It is important to carry out research on each hardware component before making a purchase. Fortunately, magazines such as Maximum PC, Custom PC, and PC World may give you an insight on the hardwares. The tools you may use include a set of screw drivers and screw holder.\n\nStep by Step Instruction\n\nStep 1: Verification\n\nThe first step involves opening the cases to verify if all the components are in good state. It is necessary to observe personal safety by wearing a protective glove when opening the case.\n\nStep 2: Power supply installation\n\nInstall the power supply by putting the motherboard into the case. Place the data and power lines in the backboard of the case. It is important to ensure that the power supply is properly attached to the motherboard. In addition, ensure that the power supply is stable enough to handle any power need in the computer. After installing the power supply, you should ground yourself to avoid power shock from the electrostatic discharge.\n\nStep 3: Motherboard installation\n\nStart this stage by unpacking the motherboard from its storage container. Place the motherboard on the work station table. However, be considerate of the fact that the motherboard is highly conductive and should not be placed on a surface with electric currents. You may wish to add any special elements in the motherboard to suit your needs.\n\nThe components that may be added include the power regulator, double switch, and extra cables. Upon affixing the additional components on to the motherboard, properly install it in the case of the computer.\n\nProceed to install the processor by mounting it accurately onto the motherboard via the plug spaces. There is always an arrow at the base of the processor to guide the pairing process. To make the process of inserting the processor into the motherboard easier, you may need to carefully undo the socket for the CPU. It is important to note that the pairing process is smooth and should not depend on any force. Should you notice that it cannot slip right in, you should probably try to realign the pairing.\n\nAfter successfully pairing the pins, proceed to tighten the CPU socket through properly securing it on the arm that supports the case. You should then close the CPU socket and secure it on the support arm. Apply the thermal paste gently all over the surface of the mounted processor. Excessive paste might result in very slow heat transfer and may make the CPU cooling process ineffective. Depending on the type of the processor, it might be necessary to affix the heat sink in the stock cooler.\n\nThe next step involves installation of the RAM. Start by unfastening the latches and driving the RAM gently into the case until the holes in the handle matches the holes on the case. Each of the holes is keyed to fit in any case without any difficulty. After this, remove the backplate gently from the computer case by applying a consistent thrush, especially for the case that is not screwed.\n\nYou may remove any unnecessary covering on the plate before returning it in its position. This stage is completed by securing the motherboard. The motherboard is locked by pushing it gently upwards against the backplate. Each back port on the motherboard should be fitted into the ports on the backplate to confirm that is well positioned.\n\nStep 4: Installing extra cards and drives\n\nIn order to install graphic cards, start by removing the covers on the back panel around the PCI slot. The graphic cards may then be inserted through the cover opening. This might require slight bending of the card to fit into position. You may repeat the procedure for other PCI cards as need may require.\n\nIn order to add any relevant drive, you should open the panel cover on the front of the case, on the upper chamber. You may then insert the drive while making sure that the case and the drive are in an upward position. You should secure the drives, when they do not fit perfectly in the panel, by fastening with screws on the left and rights sides. The same procedure can be repeated for the hard disk while making sure that it is inserted in the right opening marked as \u2018hard disk\u2019.\n\nAfter inserting the hard drives and the hard disk, do not forget to attach the SATA cables that connect them to the motherboard. Ensure that each cable is inserted in the right port. Each port is appropriately marked besides having different pairing holes. Interestingly, the SATA cables may be inserted in either direction since both ends have similar connector.\n\nStep 5: The finished product\n\nStart by connecting the power supply directly to the port on the motherboard and the video card. You may proceed by connecting the power supply further to the different drives while adjusting the wire placement to ensure good airflow. Finally, you should install the air fan on to the motherboard and optimize its performance by following the arrow indicating the direction of rotation. Close the case and plug in the computer onto a monitor. You can then power the computer and install your preferred operating system.\n\nConclusion\n\nMaking a person computer involves five steps. The first step is verification followed by installing the power supply. The third step involves motherboard installation after which extra cards and other drives are mounted to complete the assembly. Apparently, it is really not that hard to build your own cheaper computer, than taking it somewhere to have it done, by following the above simple steps.\n",
        "label": "human"
    },
    {
        "input": "Internet Censorship: Blocking and Filtering Essay\n\nIntroduction\n\nAfter the development of the internet, it became quite easy for people to access information through technology. As more users started using computers and the internet, the platform became a viable avenue for people to do business, express their opinion, and upload data for public access. This development has led to the development of some websites containing undesirable content; thus, the necessity for internet censorship.\n\nInternet censorship entails the development of firewalls that hinder access to websites with undesirable content. Internet censorship may also entail prohibiting people from uploading selected information to the internet. Internet censorship is a common phenomenon in the current world, and it is inspired by governments, organizations, and community efforts to ensure that people do not have access to sensitive or explicit information.\n\nInternet censorship may also be applied by individuals for self-regulation to evade some issues inspired by religion and culture. Many governments across the world have implemented internet censorship over the past decade to eliminate the access of specific web content by citizens.\n\nWhile some people show their support for internet censorship, others have condemned it, claiming the denial of their rights to access the targeted information. There are different types of internet censorship, and they are categorized according to the technicality of the process. This paper looks into internet censorship with a close focus on technical censorship, which comprises of blocking and filtering.\n\nBlocking\n\nInternet blocking is one of the technical measures used by the government, organizations, and computer owners to restrict access to specific information. Restricted information is normally against the laws and values of society.\n\nInternet blocking operated based on denying access to certain websites for specific internet protocols (IP). Websites of specific shared hosting servers are blocked by the government and organizations, and the citizens have no access to the websites hosted by the blocked servers (Bee Think IP Blocker: Block Unwanted IP addresses based on IP blocklist, 2014).\n\nThe blocking process entails the use of software that identifies the IP address of the clients trying to view the prohibited websites, and it blocks their access. Internet blocking may also be used to hinder some internet users from uploading data and information to specific websites. This process also uses IP addresses to identify users, and it only allows specific IP addresses to upload data and information. This approach is used by most companies on their websites to enhance security (Frew & Sessano, 2009).\n\nInternet censorship through blocking is subject to over-blocking and under-blocking. Over-blocking occurs when the government or organizations block more websites than intended in their quest to limit access to some websites. Most internet blocking software does not have the option of selecting specific domain names for websites. They just block every domain name from the hosting server, and this may result in blocking some harmless sites.\n\nIt is also possible for some blocking software to block websites with explicit content (How to, 2014). This selective blocking leads to easy access to unwanted information, which should be blocked. Some countries have embraced over-blocking the internet as a measure to ensure the people have no access to obscene content.\n\nInternet censorship through blocking is used in the United States to ensure children do not have access to websites with obscene content. This internet censorship is a provision of the Children\u2019s Internet Protection Act (Children\u2019s Internet Protection Act, 2014). The Act compels the authorities to ensure that internet surfing for children is limited to access to educational content exclusively. It is the obligation of the government to protect the innocence of the children through internet censorship.\n\nFiltering\n\nFiltering is also a technical method of internet censorship. The most common method of filtering is the uniform resource locator (URL) filtering process. The process entails scanning the URLs for specific words and blocking the ones containing specific words. This method is not secure for internet censorship because internet users can easily bypass it through a virtual private network (VPN) (Tyson & Crawford, 2014).\n\nPacket filtering is also a common method of internet censorship, and it entails blocking TCP packets containing a given number of restricted words (PF, 2014). This approach is a realistic and practical internet censorship method because it avoids the restriction of harmless websites.\n\nFiltering is a softer approach to internet blocking. Internet censorship through filtering is the best method of ensuring the public enjoys the freedom of access to information while restricting the nature of the content people can access. The internet is currently filled with explicit content, and it is the obligation of the government to protect people from it (Cho & Feldman, 2001).\n\nIssues with blocking and filtering\n\nWhile many people believe that blocking and filtering websites containing explicit and obscene content is appropriate, critics believe that internet censorship is inappropriate (Qazi, 2014). Critics of internet censorship claim that the process of blocking or filtering websites through hosting servers leads to the blockage and filtering of some helpful websites (Deibert, 2008). It is also apparent that the government may use internet censorship to gag certain news websites as the case is in China.\n\nInternet censorship is a sensitive area that needs to be addressed through laws that do not offend the fundamental right of liberty and access to information (Wagner, 2010). In some nations, the government uses internet blocking and filtering as a method to hide information from the people.\n\nInternet censorship can be used as a weapon to keep the truth hidden from people in a society. China actively censors the internet for civilians to block their access to different websites. Accessing selected websites is illegal in the country, and some people are even compelled to conduct self-censorship.\n\nConclusion\n\nInternet censorship is implemented by the government, organizations, and respective internet users on their personal computers. The process of internet censorship can be conducted through a technical approach based on blocking and filtering. Internet blocking is an effective way of ensuring everyone in a selected society does not have access to websites with undesirable content. Filtering is a technical method of internet censorship, which is based on TCP packets.\n\nThe software used for filtering evaluates the contents of the URLs and decides whether to block the website or to provide access. There are many issues associated with internet censorship, mainly because it can be used to hide relevant information from people. For instance, the situation in China reveals that the government is concerned about the amount of political information available for the people. In such a case, the government uses internet censorship as a weapon to block the society from accessing helpful information.\n\nReferences\n\nBee Think IP Blocker: Block Unwanted IP addresses based on IP blocklist . (2014). Web.\n\nChildren\u2019s Internet Protection Act . (2014). Web.\n\nCho, C., & Feldman, A. (2001). Internet Filters . Massachusets: Marjorie Heins.\n\nDeibert, R. (2008). Access Denied: The Practice and Policy of Global Internet Filtering . Massachussets: MIT Press.\n\nFrew, R. M., & Sessano, N. A. (2009). Survival-Enhanced 6 th Edition: A Sequential Program for College Writing. Michigan: T.H Peek Publisher.\n\nHow to: Restrict Site Access by IP Address or Domain Name . (2014). Web.\n\nPF: Packet Filtering . (2014). Web.\n\nQazi, U. (2014). The Internet Censorship Controversy . Web.\n\nTyson, J., & Crawford, S. (2014). How VPNs Work . Web.\n\nWagner, M. (2010). Internet filtering as a form of soft censorship . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer\u2019s Memory Management Research Paper\n\nAbstract\n\nMemory management is one of the primary responsibilities of the OS, a role that is achieved by the use of the memory management unit (MMU). The MMU is an integral software component of the operating system that resides in the OS\u2019s kernel. The OS manages both types of memory that are categorized into primary and secondary.\n\nThe primary memory\u2019s responsibility is to hold data and programs that are needed for program execution while secondary memory is non-volatile and provides long term data and program storage. However, memory is limited. Due to the limitation of main memory the operating system creates virtual memory to compensate for the need for larger memory, a concept that allows programs to be faked into believing that they are running on a large memory space.\n\nVirtual memory is a concept that provides programs with a large addressable space to support multiprogramming. That implies that the processor operates with the illusion that it is accessing a large addressable storage space. Hence, the OS ensures that the available memory is efficiently and effectively controlled to optimize system efficiency.\n\nTo achieve that entire memory management objective, the OS system assumes a supervisory role through the memory manager as discussed in the paper. The paper starts by discussing primary and secondary memory, the memory management unit (MMU), and culminates with a discussion of memory allocation policies, relocation, paging, and segmentation and other strategies that allow memory to be optimally used.\n\nIntroduction\n\nOne of the most important and critical function of the operating system is to ensure that the computer\u2019s memory is effectively managed and controlled to ensure optimal performance of the computing system. Primary and secondary memories are the two types of memory that are managed by the OS. Primary memory is the random access memory (RAM) whose responsibility is to hold volatile data and programs that are required for programs to run in the CPU.\n\nOn the other hand, secondary memory (hard disk) is a non-volatile memory chiefly meant to provide long term storage of data and programs. Memory management encompasses managing of hardware, operating systems, virtual memory, and application memory. The OS effectively manages memory through a memory management unit (MMU). The MMU maps virtual addresses into physical addresses since running programs can only identify logical addresses only.\n\nVirtual addresses are found in virtual memory. Virtual memory is a concept that provides programs with a large memory address space to support multiprogramming. That implies that the processor operates with the illusion that it is accessing large addressable space which, however, is not real, a concept that is supported by the OS. The CPU is a critical component in generating virtual addresses which are translated by the MMU into physical addresses.\n\nNoteworthy, these addresses have to be similar when a program is loaded for execution. All these responsibilities are brought about by the OS through the memory manager. However, these addresses vary during program execution time. In order to ensure fair allocation and system throughput, the OS exploits the concept of paging, relocation, and segmentation.\n\nAllocating and de-allocating memory creates holes leading to internal and external memory fragmentation. However, internal and external fragmentation can be overcome by using a number of memory allocation strategies to optimize memory usage.\n\nAll these memory management functions are achieved by the OS using the MMU as one of an integral software module in the kernel of the OS. This paper discusses operating systems memory management as one of the responsibilities of the OS and the role of the operating system as a memory management responsibility.\n\nReview of literature\n\nOne of the functions of the operating system is to ensure that the computer\u2019s memory or storage space is efficiently and effectively controlled and managed by the system\u2019s memory management unit (MMU). There are two categories of memory that are recognized by the computer. Primary memory, which is also referred to as the random access memory (RAM) is responsible for holding data and other program elements that are necessary for programs to execute in the CPU.\n\nIt is the type of memory that the CPU deals with. On the other hand, secondary memory (hard disk) is a non-volatile memory chiefly meant to provide long term storage of data and programs. The memory management unit, an integral component of the OS, is responsible for managing the allocation and de-allocation of programs to the available memory (Bhat, 1).\n\nThe rationale for memory management is based on the fact that modern operating systems support multiprogramming environments where a number of executable programs are intended to reside in main memory to be accessed at any time the data structure is required. That implies that several programs that want to execute in the CPU or are executing in the CPU use the main memory to the provide address spaces that are available in main memory (Bhat, 1).\n\nThe MMU provides the functionality of translating virtual addresses which are generated by the CPU into physical addresses. On the other hand, processes only recognize and deal with logical addresses.\n\nBoth addresses have to be the same during both load and compile time but differ during program execution time (Tanenbaum & Woodhull, 4).\n\nThe operating system is typically responsible for assigning memory to a process and ensuring that the assigned memory space is effectively managed. Therefore, the operating system ensures that all running programs or processes are assigned sufficient operating memory to run. Else, if the memory is insufficient, then it takes the responsibility of creating and assigning processes to another type of memory referred to as virtual memory (Tanenbaum & Woodhull, 3).\n\nProcesses can be prepared for execution in the CPU by first allowing them to occupy main memory, before starting to run in the CPU. That can be achieved dynamically. However, it is important note that, the OS ensures that no program that is resident in main memory gets in the way of another program. That functionality is achieved by the operating system MMU (Understanding Operating Systems, 5)\n\nThe MMU ensures that processes which want to run in the CPU are allocated the available primary memory space. The MMU also ensures that processes are effectively moved between primary memory and secondary memory while ensuring optimum use of memory space.\n\nSecondary memory, on the other hand, stores process images that are executable, program scripts, and data files that are important for running programs. In addition to that, secondary memory sometimes stores systems programs and applications (Operating Systems \u2013Memory Management, 3).\n\nThe movement of processes into and out of secondary and primary memory is referred to as swapping. During program execution, processes may need to be swapped to create storage space for other programs to occupy temporarily. Two strategies are used to swap processes. These include the backing store strategy and the roll in and roll out strategy.\n\n\u201cThe backing store strategy is based on the principle that the disk\u2019s storage space should be very large and capable of accommodating every copy of the user\u2019s image\u201d (Bhat, 1). In the views of Silberschatz and Gagne, that strategy efficiently provides the ability to reach out a user\u2019s image, an advanatge of the strategy (4).\n\nOn the other hand, the roll out, roll in strategy uses priority based algorithms to schedule processes in assigning them the CPU. Typically, higher priority processes are loaded into memory to run in the CPU while lower priority processes are swapped out of memory to give higher priority processes time to execute in the CPU. Typically, swap time is the differential time between program movements to occupy or to be removed from memory storage. The following fig. 1 demonstrates process swapping.\n\nFig. 1. Process swapping (Silberschatz and Gagne)\n\nThe above illustration consists of two processes that are swapped into and out of memory by the memory management unit. The operating system provides the platform on which communication commands are carried out when the swapping process occurs.\n\nStorage space may suffer from the creation of holes during the process swapping process. Depending on the number of processes that have been swapped in and out of main memory, a number of holes equivalent to the number of processes that have been swapped are created. These holes can be filled by moving in new programs. However, the swapping of processes in and out of main memory is bound to create many small holes into which new processes cannot fit.\n\nTypically, that means that main memory is fragmented and needs to be defragmented. One of the strategies of defragmenting these holes is through a memory compaction strategy. Here, the OS ensures that available processes are moved and arranged in a contiguous manner to create a large enough memory space that can be allocated to new processes by the OS (Robinson, 4).\n\nNew processes can be allocated memory based on a number of memory allocation algorithms. These include first fit memory allocation policy. In this policy, the hole that is identified to be the first to be large enough to accommodate the incoming process is assigned the process. Typically, that follows the first come first served allocation strategy.\n\nHowever, the first come first served strategy is a typical process management strategy while first come first fit policy is a typical memory management technique. In this allocation scheme, a process is assigned a process index that is specific to its position in a queue. The index is the determining element in which processes get allocated memory space (Maiorano & Marco).\n\nThe data structure is serial. It is also vital to note that memory is always scanned to identify the next in the sequence to accommodate an incoming process. The scanning occurs from first to last. Typically, that implies that a lot of time is spent in scanning the available memory. A typical scanning and allocation mechanism is illustrated in fig. 2 below.\n\nFig. 2 Memory scanning (Bhat, 12)\n\nOne benefit of the above discussed memory allocation policy is its ease of use and optimal performance (Pierre,3).\n\nHowever, the first fit policy has demerits. It has been established that the policy suffers from the problem of creating very many holes during the swapping process. In addition to that, the holes created by this policy are small and not useful for future allocations. That defragments the system and creates very high creates garbage collection demands impairing the performance of the system (Silberschatz & Gagne,5).\n\nResearch shows that a typical approach to solving the problem is by scanning the whole memory to obtain critical information about the positions sizes of the holes. Once the holes and their positions have been identified, then a round robin algorithm is used to allocate processes memory to specific slots.\n\nOn the other hand, it is evident that scanning memory is time consuming and that calls for another memory allocation strategy to optimize system time and efficiency. The use of variable and fixed partitions provides an option for overcoming the above problems (Simsek, 6).\n\nPartitioning\n\nEqually fixed memory storage partitions are created by the OS in a fixed memeory partitioning strategy. The resident programs are then assigned the fixed partitions, a strategy that the OS finds easy to implement. However, a dilemma arises when a program that wants to be assigned a memory space is found to be bigger than the available fixed partition.\n\nHowever, a typical solution to that problem is the use of memory overlays (Silberschatz & Gagne, 3Overlays are a concept that allows program data to be moved in and out of specific program segments while putting into use the main memory areas. Overlays are illustrated in fig. 3 below.\n\nFig. 3 Memory overlays (Silberschatz and Gagne)\n\nHowever, the danger of internal fragmentation is bound to arise again with the use of fixed partitions. That calls for the use of the variable partitioning strategy. In variable partitioning, memory is apportioned to processes based on their memory demands. That ensures that a best fit policy can be achieved effectively. It is possible therefore to organize a specific queue for each program. The problem of internal fragmentation is minimized despite the demerit of the policy being time consuming. That shortcoming is caused by programs that may wait in a queue to be allocated memory space (Pierre, 4).\n\nOn the other hand, both variable and fixed memory allocation strategies are disadvantaged by the occurrence of external fragmentation. External fragmentation is a situation that is characterized by partitions which do not have resident processes in them. A number of suggestions have been fronted that help address the problems of internal and external fragmentation. One such a solution is to use a dynamic memory partitioning strategy.\n\nThat implies that the size of memory required is dynamically determined and allocated. That implies the allocation event happens at run time. However, this memory allocation scheme is difficult to implement. In order to overcome the difficulty associated with the above mentioned scheme, another that has been suggested is the buddy system of memory partitioning.\n\nBuddy Memory Partitioning\n\nThe strategy relies on the concept that memory allocation can be efficiently achieved in sizes with the power of 2. Two methods are used to allocate the available memory space. The first approach is where a hole that is closest to the power of 2 is allocated a process. On the other hand, if a hole is not identified that closely fits into the above description, then another hole that closely fits into the above description is looked for which has a power next to power 2 and split into two halves.\n\nThese two holes are referred to as buddies. The buddy system works by assuming that the number of holes is fixed but with variable sizes. One advantage of the buddy system is that internal fragmentation is minimal and is typical of the system. However, the system has a critical demerit where memory allocation takes much time, hence causing the allocation strategy to be slow.\n\nMemory Partitions and Virtual Memory\n\nIn order for processes to run efficiently, it is important for main memory to be continuously unlimited. However, that in reality is not possible. Therefore, the concept of virtual storage is embraced. The CPU supports programming concerns by generating numerous logical addressable memory spaces. However, the number of addressable memory spaces is limited compared with the logical addresses that are generated by the CPU. It is important to note that the OS supports the concept of virtual memory by copying a large amount of disk memory and writing it into main memory (Simsek,3).\n\nVirtual memory comes with a range of benefits as a memory management strategy. Virtual memory provides a large addressable space and provides residence for a large number of processes. A detailed discussion of the benefit derived by the use of main memory to host several programs will help to crystallize that benefit.\n\nWhen a program runs in the CPU, it operates with a minimum number of set instructions. That is also the case with the data used by the program. Typically, that implies that when a program is running, it optimizes the usage of available memory space and only makes reference to the data being used by the process. This situation is effected due to the locality of reference.\n\nThat means that only the essential portions of a program can be resident in main memory allowing for several programs to occupy the main memory at the same time. In other words a small amount of physical memory can help service several resident programs.\n\nIn addition to that, it implies that a small chunk of memory should be created to address the need for several resident programs to run. This demand makes paging and segmentation very important concepts in memory management. The need to discuss and crystallize the meaning of paging and segmentation is therefore important at this point.\n\nPaging\n\nPaging is where programs get allocated to physical memory when it is made available. In a typical windows operating systems environment, page tables and page directories are created for each of the running programs. In the windows environment, the physical address of a page table is entered into the page directory during the creation of a process (Bhat, 22).\n\nThese page table entries can either be valid or invalid. Depending on their status of validity, each page table may contain the physical address pages that are directly allocated to a running program. At this point, it is important to remember that processes do not identify physical addresses but only know logical addresses.\n\nTherefore, as discussed elsewhere, it is the responsibility of the MMU and the processor to map logical addresses to physical addresses. In windows operating system, the page directory page address is the page directory where a process can be found in the physical memory of the system. It is important to note that the page directory of the page address is found located in the register of the CPU and is referred to as CR3 (Bhat, 22).\n\nTypically, pages are created by dividing the available logical memory data structure into storage blocks while physical storage is partitioned into fixed memory chunks known as frames. Therefore page tables are set to translate logical addresses to physical addresses as discussed above. The operating system ensures that frames are kept track of as they are used during program execution. Therefore, a program acquires a number of pages that are similar to the number of frames to load into.\n\nIdeally, virtual memory is partitioned into page sizes that are equal to page frames, a characteristic that allows the OS to fetch and move pages with flexibility on the disk to the page frames found in the physical memory. That flexibility of fetching and mapping pages to page frames is illustrated in the fig. 4 below.\n\nFig. 4. Mapping of addresses (Silberschatz and Gagne)\n\nIn the above illustration, paging requires the support of the OS and the hardware as illustrated in the figure 4 above. As shown on the diagram, a number of pages have been created and made available in main memory. The pages created using this strategy are referred to as the resident set with a locality of reference being taken care of always.\n\nIt is important to note that the set of pages that are required by a running program are referred to as the working set. Both the resident set and the working set have to be the same despite the fact that the requirement is not always fulfilled. It is the duty of the OS to enforce the latter requirement. When the OS fails to achieve the latter objective, then it is referred to as a page fault (Callitrope, 3). However, when a page fault occurs, the OS\u2019s responsibility is to identify the required page and fetch it and load it into its free page frame.\n\nThen, the OS proceeds to make entries for the specific page into the designate page table. On the other hand, for optimal use of the CPU and main memory, the operating system swaps processes into and out of memory by deleting specific entries of the page tables for the programs that are swapped out of memory. It is important to see the link at this point between the OS and the memory manager. Sometimes the OS forces pages out of memory to allow other pages to be loaded into memory.\n\nThat situation arises when all the page frames in memory are in use, but another process wants to use a page frame. To achieve that objective, the OS uses a page replacement policy. A page replacement policy is typically characterized by the way a process uses page frames. The OS ensures that it has a record of the way pages are used in a read and write operation.\n\nOnce a page has been written to or read from, it subjected to a modified bit. One reason is that the page has already been referenced. Implying that the page can be moved using the correct page replacement policy. That ensures that the system throughput is maintained to achieve good system efficiency (Bhat, 22).\n\nA page replacement policy allows the OS to swap processes within the memory storage area following a specific algorithm. These policies include FIFO, LRU, and NFU policies. The FIFO policy is based on first in first out process swapping. Thus, processes are moved based on their arrival time. On the other hand, LRU is a least recently used policy that identifies and moves pages whose usage was further from the current time. The NFU is based on the frequency of usage of the pages based on program count (Callitrope, 3).\n\nIn a typical windows operating systems environment, the frame size is usually 1024 bytes. However, other page frames vary up to 4 k. It is important to note that paging is a very important concept in multiprogramming. Paging therefore supports an environment where several resident programs execute in the CPU at the same time (Callitrope, 3).\n\nRelocation\n\nResearch has indicated that some programs use dynamically oriented data structures. That implies that the programs uses dynamically allocated and de-allocated memory spaces. Hence, the deleted data structures that are discarded by programs are not collected for use by the operating system immediately. It has been demonstrated that if the OS collects the deleted data structures immediately, that could adversely impair its performance. The space left by the data structures is referred to as garbage.\n\nHowever, the OS allows garbage to accumulate to a certain levels before demanding compaction to be performed so as not to impair the performance of the system (Bhat, 1). Otherwise, id compaction does not occur, the memory may be so defragmented that there could be left no memory to be allocated to programs that want to run in the CPU (Callitrope, 3).\n\nIn a multiprogramming environment, several programs reside in main memory and demand the allocation of the CPU. Therefore, conflicts are likely to occur due to programming errors. In that case, the memory space of other programs can be used by other programs to write data into other programs\u2019 instruction areas.\n\nThat has the potential of corrupting a program. However, it is the duty of the operating system to provide a protection mechanism in which no program is allowed to interfere with another program\u2019s instruction area. One of the methods used to achieve process protection is through process isolation (Callitrope, 3).\n\nProcess isolation is a strategy that ensures that processes are protected from writing into the spaces of other processes and is achieved through virtual address spaces. A virtual address space ensures that a program accesses adequate memory space despite that fact that such addresses may be used by other programs. The OS assigns these addresses to resident user programs which are translated as required into physical addresses by the MMU. The virtual address concept is illustrated in fig. 5 below.\n\nFig. 5 The concept of virtual addresses ( Bhat, 12)\n\nOn the other hand, virtual memory management scheme provides another strategy used by the operating system to manage the memory required by programs that want to run in the CPU. Therefore, virtual memory is a concept where programs are able to see a large contiguous memory without caring whether the actual physical memory is equally large.\n\nTherefore, there is a critical need for virtual addresses to be mapped into physical addresses by the MMU. That enables a running process to access the required data and other requirements to run in the CPU. That typically demands that processes be relocated when required to achieve the objective of protection and efficient memory usage. One of the memory management techniques is relocation (Bhat, 2).\n\nRelocation is a vital concept in memory management. One typical approach to a detailed understanding of the relocation concept is by considering a linear map where known contents of a given address are located and fetched (Bhat, 2).\n\nThen a program that is residing in memory can be loaded into memory with its absolute address that points to the instructions and data where main memory is free (Loepere,5). However, in this strategy, no more than one process can be loaded into main memory if another process is running in the main memory. Another disadvantage with this approach is lack of flexibility. That is, one process must be removed from main memory before another process is loaded into main memory.\n\nHowever, that impairs the efficiency of the system and creates several holes that need to be removed. That is typically accelerated by the fact that when the process is loaded back into the hole it was moved from, it may find the hole no longer available, thus creating a re-location problem. The relocation mechanism uses the single partitioning allocation strategy to protect programs from interfering with each other.\n\n\u201cThe relocation register is characterized by values that describe the smallest physical addresses and a limit register that contains a number of logical addresses\u201d (Bhat, 2). It is worth noting that limit registers are bigger than logical addresses (Bhat, 2). The relocation strategy that is supported by the system hardware is illustrated in the diagram fig. 6 below.\n\nFig. 6: Virtual addresses (Bhat, 12)\n\nIn order to understand the relocation process, it is important to examine in detail the linking and loading mechanism of a typical process as the responsibility of the operating system. Dynamic loading occurs at run time and comes with various benefits. These include unused routines are not loaded therefore enabling efficient utilization of memory, allows efficient use of memory when several processes are running, and minimal use of the OS is required at this time.\n\nOn the other hand, dynamic linking is executed at run time, resident memory library routines are located by stubs which are small pieces of programs, and provides efficient utilization of memory and the CPU. A number of strategies are used to ensure efficient utilization of space and memory as discussed below. One such approach is segmentation (Bhat,4).\n\nSegmentation\n\nAnother memory management concept mentioned is segmentation (Bhat, 22). Segmentation is one of the unique component elements that support the concept of virtual memory. It provides support on what users consider about running programs which are illustrated in the following diagrams in figs 7, 8, and 9 below.\n\nFig. 7. User\u2019s view (Silberschatz & Gagne, 5)\n\nFig.8. Segmentation hardware (Bhat, 12)\n\nFig. 9. User\u2019s view ((Silberschatz & Gagne, 5)\n\nSegmentation may be defined by an object, a method, a stack, and a procedure among other elements. A segment is characterized by logical addresses which consist of uniquely identifying numbers and unique offsets. It is important to note that the characteristics of a segment vary with time. Typically, a segment, like a stack is based on function calls that are active at any given time.\n\nSegmentation is typically implemented like paging except that segment tables are used with look ups to identify and provide information about addresses specific to each page table (Bhat, 22). However, segmentation has the problem of suffering from external fragmentation though the approach is advantaged by the fact that it offers separate program compilation.\n\nTherefore, it is important to combine paging and segmentation to optimize system performance and security. In addition to that, segmentation with paging reduces the search time of a program and the problem of external fragmentation (Bhat, 12).\n\nHowever, the CPU\u2019s performance may be underutilized by a process that may devote more time swapping pages in and out of memory. That concept is referred to as thrashing. Thrashing leads to the underutilization of the CPU due to performance overheads affecting the overall system throughput (Bhat, 22).\n\nPoint of view\n\nIt is important to note, from the foregoing discussion that memory management is one of the critical functions of the operating system. Therefore, memory has to be efficiently managed to ensure efficient utilization. The operating system fulfills the whole objective of memory management by using an integral software component referred to as the memory management unit (MMU).That implies that the OS delegates some of its memory management responsibilities to the MMU while supervising its functionality.\n\nIt is arguable that the OS will always report any error that may arise due to allocation and de-allocation of main memory. The MMU is one of the kernel components of the OS. Typically, the operating system divides memory into two classes. That is primary memory and secondary memory. Primary memory is concerned with the storage of volatile data and programs while secondary memory provides long term data and program storage.\n\nPrograms wanting to execute in the CPU have to be moved into and out of main memory through mechanism known as swapping. Depending on the number of processes that have been swapped in and out of main memory, a number of holes equivalent to the number of processes that have been moved are created. These holes can be filled by swapping programs into and out of memory. However, the movement of processes in and out of main memory is bound to create many small holes into which new processes cannot fit.\n\nThis causes the system efficiency to be compromised. To enhance system throughput, memory can be compacted or a number of placement algorithms can be used to overcome the fragmentation problem. However, to optimize system efficiency, memory should be compacted or defragmented. On the other hand, when programs run system efficiency can also be optimized by use of virtual memory.\n\nVirtual memory is an important concept in memory management as programs that need large memory space in a multiprogramming environment may be limited by physically available memory. To overcome that, the OS creates another type of memory referred to as virtual memory.\n\nVirtual memory is an important concept in memory management as it allows programs to run on limited memory while the OS fakes these programs into believing that they are running on a large contiguous memory. Hence virtual memory provides a large addressable space that accommodates a large number of resident processes. That concept therefore supports multiprogramming.\n\nConclusion\n\nIn conclusion therefore, it is evident that memory management is one of the critical responsibilities of the operating system. The OS divides memory into primary and secondary memory and ensures that policies are put in place to effectively manage and control these memory types. Typically, primary memory is volatile as it holds the data and programs needed for processes to execute in the CPU while secondary memory provides long term data and program storage.\n\nIn order to efficiently manage programs and data in primary memory, the operating system assigns the responsibility of managing memory to the memory management unit (MMU) also referred to as the memory manager. The OS assumes supervisory roles and ensures that programs and data are assigned and moved out of memory during program execution through the MMU, an integral component of the OS.\n\nThe MMU resides in the OS\u2019s kernel. When programs want to run in the CPU, processes have to be swapped in and out of main memory. The swapping process creates holes that have the possibility of impairing the system\u2019s throughput. That is because swapping may cause internal or external fragmentation of the main memory. To enhance system throughput and minimize the effects of fragmentation, several memory placement strategies are used to achieve compaction.\n\nThese include the first fit policy and best fit policy. In first fit policy, the OS allocates a process to a hole that is first available so long as it is able to accommodate the process. Therefore, the allocation mechanism uses a process\u2019s index to allocate the process a position in the queue. However, the best fit policy can easily lead to the creation of many holes, impairing the efficiency of the OS. To overcome that weakness, the OS uses the best fit policy.\n\nThe policy is based on the fact that main memory is first scanned of all the holes that have been created and the hole that can fit the process\u2019s memory requirements is assigned. One of the algorithms that is used to assign processes memory holes is the round robin algorithm. These two allocation methods have been identified to be very efficient. However, for the OS to efficiently allocate memory chunks to running programs, it uses fixed or variable partitioning strategies.\n\nThese partitioning strategies have their merits and demerits which are also overcome by the use of the buddy partitioning strategy. On the other hand, memory management cannot be complete if virtual memory is not considered. Virtual memory supports multiprogramming by allowing several resident programs to run at the same time.\n\nWorks Cited\n\nBhat, P. C. P. Operating Systems/Memory management. Bangalore. Lecture Notes, 2004. Web.\n\nBreecher, J. Operating systems memory management . 2011. Web.\n\nCallitrope, P. T. Withington, Memory Management . 2011. Web.\n\nLoepere, K. Mach 3 Kernel Principles . Open Software Foundation and Carnegie Mellon University, 1992. Web.\n\nMaiorano, A., Paul Di M. , Memory Management. Web.\n\nOperating Systems \u2013 Memory Management. ECE 344 Operating Systems. Web.\n\nPierre, J. An Introduction to Intel Memory Management. Web.\n\nRobinson, T. Memory Management 1. 2002 . Web.\n\nSimsek, B. Memory Management for System Programmers . 2005. Web.\n\nSilberschatz, Galvin & Gagne. Operating System Concepts with Java , Chapter 9, Memory Management 2003. Web.\n\nTanenbaum, Andrew, s. and Albert s. Woodhull. Operating systems design and Implementation . 2006, Prentice Hall. Web.\n\nUnderstanding Operating Systems. Memory Management, Early systems. Web.\n",
        "label": "human"
    },
    {
        "input": "Euro Computer Systems and Order Fulfillment Center Conflict Essay\n\nTable of Contents\n 1. Customers\u2019 Complaints & OFC\u2019s Measure of Efficiency\n 2. Description of Current Process and Problems\n 3. General Changes\n 4. Additional Information for Complete OFC Redesign\n\nCustomers\u2019 Complaints & OFC\u2019s Measure of Efficiency\n\nFrom the case study, it is evident that customers of Euro Computer Systems (ECS) are upset due to an underperforming Order Fulfillment Center (OFC) as demonstrated by the delay in the delivery of computer terminal components to them.\n\nThe customers\u2019 complaints are justified because the order fill rate (completed orders filed on time) is 43 percent, implying that most customers have to contend with the problem of getting the highly specialized components long after the computers are delivered.\n\nAn effectively operating OFC should have the capacity not only to meet specific ordering and distribution needs as demanded by customers, but also to ensure cost savings and substantially reduce customer complaints.\n\nDescription of Current Process and Problems\n\nThe current process in the OFC is largely manual-based, and involves generating the orders on a weekly basis, dating the orders and putting them in an Order Bin located in the order processing area, entering the numbers from the order form into the computer to identify the location of terminals, pulling out the terminals if available, and taking the terminals into a central processing location called the dock for onward shipping (see Figure 1).\n\nThe problems with the current process are nested in (1) the use of different means to locate the terminals and forward them for onward shipping, and (2) the lack of uniformity in the recording of order shortages.\n\nOrder Generation Figure 1: Process Flow Diagram\n\nGeneral Changes\n\nThe terminals requested by customers are highly specialized due to security concerns, hence the decision by management to implement a low stock policy not only to minimize inventory costs but also to lower the costs involved in discarding obsolete terminals. Consequently, a short term change to improve performance could be to streamline the process involved in the recording of shortages with the view to substantially reducing the amount of time taken to replenish the inventory for missing orders.\n\nA long term change could be to fully computerize OFC not only to harmonize the different avenues presently used to locate the terminals from the inventory area but also to ensure that stocks are replenished as soon as possible to meet customer demands. Consequently, the performance in the OFC could be measured in terms of achieving a high order fill rate (above 80 percent), ensuring the stored inventory can successfully meet customer demands without elevating cost overruns, and also maximizing customer satisfaction levels.\n\nAdditional Information for Complete OFC Redesign\n\nA complete redesigning of the OFC process to improve performance needs much more information than is currently available. For example, the information on the availability and nature of inventory needs to be availed to inform efficient replenishment practice. The information filled on the order card should also be availed to shed light on how missing orders are documented and who is responsible for ensuring these components are replenished at the earliest convenience.\n\nAdditionally, information on what has already been computerized, and what is done manually should be availed to assist stakeholders in deciding what more should be computerized to improve efficiency and performance. Lastly, information on all the employees/players working in the OFC and their specific roles should be provided to ensure seamless integration of all processes and activities with the view to substantially reducing customer wait times after making an order.\n",
        "label": "human"
    },
    {
        "input": "Globalization Influence on the Computer Technologies Essay\n\nGlobalization is the process that has completely changed the ways of how the world may operate in different spheres of life. On the one hand, globalization destroys several barriers that have recently deprived people of numerous opportunities like distant communication, fast exchange of information, and software improvements by the users\u2019 needs. On the other hand, it creates several problems like technological, social, or economic inequality as not all countries can meet new standards and correspond to all global expectations.\n\nAre there some problems with globalization? Can it happen that globalization forces may influence the future Information Technologies, Computer Software development, etc.? Current paper aims at answering such questions and defining the connection between globalization and computer information system based on Friedman\u2019s works and personal observations.\n\nIn spite of the fact that globalization may be defined as a serious problem in different spheres of life, it is wrong to believe that the future of IT may undergo negative effects; it seems to be more reasonable to use globalization as a powerful source of inspiration, an engine, or a motivation to improve, create, and promote better living conditions.\n\nFor a long period, people used to think that globalization is something urgent that cannot be neglected. It was necessary to strive for some global improvements, promote innovations, and gain a new portion of knowledge in the possible fastest way. Nowadays, innovations in information technology become affordable for many people, change their sizes, and open a whole new world of opportunities for people at home, work, or school.\n\nStill, there are many groups of people who are not satisfied with the effects globalization brings to society. People cannot understand the essence of the globalization process as they cannot understand what globalization can make. Globalization becomes a problem because people do not want to spend more time and define its pure nature, its purposes, and abilities.\n\nThis is why the experts, for example, the IT representatives and all whose, who get their Computer Information System degrees, should make use of their knowledge and practices and provide society with clear and credible explanations of how useful the effects of globalization can be.\n\nIn fact, globalization is the process that speeds up the interactions between people from different parts of the world, stretches out the relations of different levels and makes it possible that the decisions of one country have certain consequences in another country, and provides each as well as some huge corporations with a kind of cyber freedom.\n\nThere are many reasons to believe that the forces of globalization affect the future of the world in a variety of ways. People want to believe that globalization is something that can change their lives for better. At the same time, people cannot trust this process as they are not always able to gain control over it. Thomas Friedman introduces a powerful statement that \u201cglobalization has gone to a whole new level\u201d (The World Is Flat 9).\n\nHe also underlines the fact that the relations between globalization and the information technologies have gone \u201cfrom connected to hyper-connected\u201d (\u201cA Theory of Everything (Sort Of)\u201d para. 3). In other words, the investigations of this writer show that globalization gains different qualities and new characteristics. And Friedman\u2019s main point is that all these changes were taking place \u201cwhile I was sleeping, and I had missed it.\n\nI wasn\u2019t really sleeping, but I was otherwise engaged\u201d (The World Is Flat 8). It is possible to give many explanations of why people are not able to follow all the changes brought by globalization: weak opportunities, laziness, unpreparedness, etc. However, it is also necessary to remember that globalization presents a number of new possibilities that seek to close technology gaps (Kemeny 2).\n\nThis is why the analysis of the issue concerning the relation between globalization and, for example, information technologies is one of those open-ended questions that cannot be provided with one particular answer.\n\nKey points and evidence offered by different researchers as well as personal investigations and the evaluation of the current situation and the prospects of the future career help to understand that the search for alternatives and the analysis of globalization from different perspectives is one of the most appropriate ways that can be chosen.\n\nThere are many interesting opportunities to prove that globalization may have a positive effect on the future of any career, and the career of an IT expert as well. The only thing that is required is a personal desire and faith that globalization can lead helpful and affordable ideas.\n\nFriedman says that there are all chances to make globalization \u201cworkable, sustainable, and fair for more people\u201d (551). His experience and real-life stories explain that globalization may become a problem in case it is wrongly understood or taken in a bad time. However, it does not take much time and efforts to change the situation, evaluate new possibilities, and make use of globalization to succeed in a future task or action.\n\nGlobalization is rather beneficial due to the possibilities available to different people. Thanks to Skype, children can support real-life communication with their parents from different parts of the world. Thanks to Twitter, absolutely unknown people may share information. And thanks to Google, it becomes possible to find any kind of information about anything. Nowadays, it is hard to imagine our life without such devices like iPads, iPhones, cheap Internet, notebooks, etc.\n\nAll this is possible due to the process of globalization, and people should not forget about it. All they need is to pay attention to the offered alternative and spend more time analyzing this process and its offers. The future careers of the students, who are going to get their Computer Information System degrees, are able to open a charming world of globalization to many people. Such experts should be ready to find a link between an ordinary person, his/her needs, and a technology that may be used to improve life.\n\nIn general, globalization should be regarded as a problem. It should be used as an amazing opportunity to change this life and use the facilities that can help. Of course, much work has to be done to comprehend the essence of globalization and its effects on different spheres of life.\n\nFriedman\u2019s work aims at explaining that globalization is a very insidious thing that has to be treated properly. Personal knowledge and experience show that globalization may be properly understood and explained by good specialists. There is no need to make definite conclusions about globalization and treat it as pure good or bad thing. It has many sides, and people are free to choose their attitude to get a possibility and enjoy the propositions and improvements brought by globalization to the modern world.\n\nWorks Cited\n\nFriedman, Thomas, L. The World Is Flat: A Brief History of the Twenty-First Century , New York, NY: Picador, 2007. Print.\n\nFriedman, Thomas, L. \u201cA Theory of Everything (Sort Of).\u201d The New York Times . 2011. Web.\n\nKemeny, Thomas. \u201cAre International Technology Gaps Growing or Shrinking in the Age of Globalization?\u201d Journal of Economic Geography . 11.1 (2011): 1-35. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer-Aided Cooperative Learning Essay (Critical Writing)\n\nThe effect of computer-assisted cooperative learning methods and group size on the EFL learners\u2019 achievement in communication skills\n\nThe study points at the effect of the computer-aided cooperative learning method and the group size on EFL learning achievement in communication competency. It begins by providing the merits of small groups that accomplish tasks together, including the strength of language use, promoting mastery of language use, increasing participation by individuals, and providing a conducive environment for learning.\n\nHowever, demerits may include group conformity patterns that sometimes do not respond to wishes of the individuals, opposition to group ideals that may be treated with suspicion and rivalry in small groups with vested members\u2019 interests. Loyalty to the group may lead to some members acting modestly to avoid victimization by other group members. Based on this fact, efforts were made to aid learning by eliminating the physical barrier.\n\nThe assumption was that mistrust breeds conflict due to potential arguments; thus, the study on group behavior sought to counter hindrance in group\u2019s behavior by employing techniques that hid identity to compare learning conduct in different settings. This was possible through the introduction of computer-based settings that helped in developing communication skills.\n\nIn this case, the instructor uses hidden electronic secrets, which target students in mutual groups to conceal their identity. The methodology used involved drawing participants from English undergraduate class at All- Bayt University Jordan. The design involved random sampling that targeted two groups, positive interdependence versus the mutual model. Participants were assigned to two groups; one was known, and the other was anonymous.\n\nThey were then assessed to determine their scores in English exams. The study also factored demographic characteristics such as age, nationality, and sex. The results show that individuals who cooperate in learning are likely to have higher scores than those who are independent.\n\nThis is attributed to the group\u2019s identity role that is instilled in mutual groups, ownership of groups success amongst members, level playing ground during groups meetings, equal responsibility allowing individual accountability, and spirit of competency due to individuals identifying with the task delegated, thus allowing responsible action.\n\nThe study is good, as it highlights the significance of computer-aided cooperative learning and groups in acquiring communication skills. The study explains the concept of positive interdependence that accustoms group members to a uniform culture with a view of accounting for behavior. On the contrary, a different group allowing individuals to account responsibility predicts behavior in comparison to the group.\n\nThe study examines mutual learning in computer-based settings and groups structure. The methodology applied suited the study since it allowed for comparisons between two groups in different environments, which help to understand performance with a view of predicting behavior. The findings are good and specific, as they reiterate the importance of groups in learning.\n\nFrom the findings, we cannot underestimate the role that groups play in promoting individuals\u2019 learning. This is probably due to the sense of competency that is reinforced when learners become immersed in groups that identify with vision and mission of their formation, allowing a culture of conformity to group norms that are likely to contribute to the achievement of goals.\n\nThe findings can be inferred to understand group behavior in any typical setting. They share the same features to enhance the group\u2019s success as shown in the Japanese model that initiates a culture of conformity to group norms, thus improving workers\u2019 performance.\n\nBeyond Word Processing: Networked Computers in ESL Writing Classes\n\nThe study sought to establish which ESL writing mode between the computers networked and traditional method enhanced writing well. The findings showed that students\u2019 engagements increased gradually with the assistance of networked computers in writing classes. Students were able to engage freely and participate by commenting on discussion topics, thus eliminating a distraction to students who wanted to be creative.\n\nAlso, teachers had easier access to students\u2019 writing in class that promoted immediate feedback and followed up on students\u2019 progress. Unlike networked computers, traditional classes were found to be a barrier to both teachers and students who had difficulty in comprehending the accents. It is also worth noting that networks allowed students to view each other equally, encouraged students\u2019 participation, and reduced teacher-students physical contact.\n\nThe aim of the study was to establish if word processing in networked computers enhanced writing. Also, the study sought to establish the extent to which networked computers encourage student participation and reduce teachers\u2019 instructions, especially in enhancing grammar. The methodology in the study involved 69 students who were recruited from departments that were meant for ESL students during the winter and spring.\n\nThis involved two categories of students comprising 34 Students in networked classes and 35 students in the traditional classes. Participants were chosen from different faculties majoring in different fields within the university that cut across junior and senior students, although initial targets were students in their first year of study. The study finding showed that several factors contributed to the best quality of writing work in networked classes.\n\nThe results of the study showed that networked computers were likely to promote better student writings, which would further be enhanced via observation, taking risks, and through reinforcements by the teacher and peer students. This highlights the importance of conducive environments that promote learning, which has an impact on improving the quality and quantity of writings.\n\nThe findings further stated that students in networked classes had a more optimistic attitude during the beginning of the semester than their counterparts in the traditional classes, which had a significant impact on the gains of networked computers.\n\nDespite the positive attributes of networked computers in ESL writing classes, it is important to note that they are learner-centered and focus on word process and revision. Moreover, changing from traditional writing to using computers that have been networked is costly since it requires technical expertise.\n\nThe study was good as it sought to establish the role of networked computers in enhancing writing in an ESL class. The importance of this study can be justified by the fact that networked computers actively encourage students\u2019 interactions, unlike the traditional mode of writing. Although the traditional mode of writing encouraged face-to-face discussions through visual aids, it was disadvantageous to students with no mastery of physical cues.\n\nThe methodology used was suitable since it eliminated a bias by relying on a variety of respondents across the various faculties thus it could be stated that the findings were an equal representation of respondent in all the Academic fields.\n\nThese findings are specific and can be applied in different learning settings to show the role that external aids play in influencing the learning outcome. Technology offers an advantage in learning by eliminating physical barriers that are likely to hinder inventiveness due to its practical approach to learning.\n",
        "label": "human"
    },
    {
        "input": "Computer Science Program in Colleges and Universities Research Paper\n\nIn many colleges and universities in the United States and globally, the computer science program is offered to students to prepare them to the identification of computational challenges in all spheres of contemporary life, as well as to designing, implementing, and analyzing algorithmic solutions, and development of software for a multiplicity of applications (University of Kentucky, 2013).\n\nThe program provides graduates with manifold job openings, especially in research establishments, government departments, technology, and software development firms, as well as institutions of higher learning.\n\nHowever, although vocational institutions have contributed immensely to ensuring that qualified individuals enroll into the various programs under the computer science flagship to facilitate skilled manpower and enhance technical productivity, they are yet to be at par with universities in terms of providing quality programs in computer science.\n\nA computer science program at the university level requires students to have cumulative knowledge in the core facets of the program, hence qualified high school students and transfer students with the required number of hours can join the program. With the right level of qualification as per the university regulations, students can join the program as freshmen or as sophomores.\n\nThis is different in colleges which offer a two-year associate degree in computer science, as many of them are forced to enroll students with limited experiences and educational backgrounds arising from poor student preparation (Richmond, 1989). However, such students could still transfer to the universities of their choice to pursue the program if they fulfill the set course and institutional requirements successfully.\n\nWhen discussing the benefits, it is evident that students who pursue the program in university settings receive more quality education than their counterparts in colleges, especially in the context of availability of courses in new and emerging areas of computer science, adequate infrastructure, and cutting-edge equipment, opportunity to be taught by highly-qualified instructors, and adequate financial resources (Goel, n.d.).\n\nThe benefits accruing to colleges entail low educational costs and short time needed to complete the program.\n\nIn terms of noted differences, it is evident that most colleges are constrained by faculty resources and lack of qualified instructors to provide a quality computer science program (Richmond, 1989), not mentioning that some colleges are unable to meet their set objectives in training qualified individuals for lack of specialization and flexibility in admitting students who have not achieved the minimum grade points (Major, 2014).\n\nThe issues that colleges are facing the need to be addressed to enable the colleges to be at par with universities in the provision of quality computer science programs. Because this program is resource-intensive (Richmond, 1989), colleges need to be facilitated by relevant government agencies to access more financial resources to implement adequate infrastructure, purchase cutting-equipment for training, and attract qualified instructors.\n\nThis will guarantee that students in vocational institutions can get quality education in computer science, hence contribute immensely to enhancing technical productivity, filling the gap in skilled manpower, and promoting quality of life (Goel, n.d).\n\nAdditionally, quality in the computer science program cannot be secured if some colleges continue to enroll students with low grades and insufficient competency levels.\n\nBecause this program is quite demanding in terms of education, skills, and competencies, colleges need to develop and implement strategies that will ensure that only those who qualify are enrolled into the program (Goel, n.d). Additionally, issues of lack of specialization, inflexible curriculum, and lack of equivalence for employment purposes need to be addressed by colleges to develop the capacity to offer a quality computer science program.\n\nIn conclusion, it is evident from the discussion that, unlike universities, vocational colleges lack most of the critical components needed to ensure quality in the computer science program. To facilitate skilled manpower and enhance technical productivity not only in the country but also globally, the solutions discussed in this paper need to be implemented by colleges, particularly about the provision of the computer science program.\n\nReferences\n\nUniversity of Kentucky. (2013). Computer Science . Web.\n\nGoel, V.P. (n.d.). Technical and vocational education and training (TVET) system in India for sustainable development .\n\nMajor: Computer Science. (2014). Bigfuture. Web.\n\nRichmond, E.R. (1989). Software engineering education in the associate-degree-level vocational/technical computer science program. CIS Educator Forum, 2 (3), pp. 13-18. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Science: Threats to Internet Privacy Essay\n\nTable of Contents\n 1. Introduction\n 2. Threats to Internet Privacy\n 3. Ease of Access to Personal Information\n 4. Opportunities for Commercial use of Personal Information\n 5. Conclusion\n 6. References\n\nIntroduction\n\nEvery single individual is entitled to his or her privacy. Even though it may be somehow difficult to precisely define what an individual\u2019s right is, it is, nonetheless, a fundamental human right.\n\nOstensibly, privacy has two different sides. On the one hand, it is concerned with information about individuals that should be kept private while on the other hand; it addresses the handling of individuals\u2019 information by third parties. According to Keeler (2006), the Internet provides a channel through which dishonest individuals may use private information for their gain.\n\nGenerally, Internet privacy is regarded as the protection of an individual\u2019s right to non-disclosure of information associated with him or her through an online means.\n\nFor example, Internet privacy that touches on the use of emails in a company may be concerned with the random access of employees\u2019 email messages by the employer. Allegedly, the use of the Internet is considered to be a potential threat to the privacy of individuals and organizations (Imparato, 2000). This paper discusses threats to Internet security.\n\nThreats to Internet Privacy\n\nGenerally, the Internet presents a serious threat to the privacy of individuals. While the huge information generated by the Internet is highly valuable to organizations, it has a major impact on the confidentiality of individuals. Using the Internet, it is possible to gather so much information about users without their knowledge.\n\nAn Internet user accessing different sites may have no clue that information about his or her activities is being captured. Later, the information collected may be used to benefit an organization or an individual. Internet privacy may be threatened by the ease of access to personal information as well as the existence of opportunities for the use of private information for commercial purposes.\n\nEase of Access to Personal Information\n\nWhile access to personal information before the digital era was quite difficult, this has now changed as a result of advances in technology. With the advent of the Internet and the availability of various tools for collecting and storing information, personal information can easily be gathered and used by different people for different reasons.\n\nOne quality of the Internet that threatens privacy is its ability to facilitate the capture and storage of huge volumes of information about individuals. While the use of services such as Google docs and a drop box is beneficial to users, it is potentially risky. While important information stored with the help of these services, may be easily accessible from anywhere as long as one has access to the Internet, the same information may end up in wrong hands.\n\nThe Internet also permits governments to access and easily process personal information. Whereas this may be quite advantageous to the government, inappropriate use of the information certainly interferes with the privacy of individuals.\n\nAlthough the storage of DNA in a digital form has helped law enforcement officers to resolve criminal activities by individuals, it is not fair to keep storing the DNA after a problem is dealt with. Failure to do so creates privacy problems. The stored DNA may, for example, contain information that is very sensitive to individuals.\n\nAccess to personal information has also been simplified as a result of cookies and web bugs. Ordinarily, cookies get registered by a web browser every time a user requests to access a web document on a web server that sends out cookies to web browsers on every access (Mendel et al., 2012). Although users are at liberty to choose how to deal with cookies, rejecting them may deny the user access to some content from the web server that sent the cookies.\n\nWeb bugs are small and exist as part of web pages accessed by individuals through web browsers. It is also possible for web bugs to be attached to email messages. If a user accesses a page or an email that contain web bugs, details of the web browser including its Internet Protocol (IP) address as well as time and date of access are sent to the server.\n\nOpportunities for Commercial use of Personal Information\n\nAlthough the use of the Internet enables businesses to access so much information about individuals for commercial benefits, it hurts the privacy of individuals. Most activities that take place on the Internet rely on the services offered by various middle players including Internet Service Providers and major search engines.\n\nUsually, the approach taken by popular search engine organizations such as Google and Yahoo requires them to use personal information for their marketing activities. Without a doubt, increased access to personal information by intermediaries is to blame for increased disclosure of private information about individuals.\n\nDrawing from a study by Mendel et al. (2012), the presence of go-betweens such as cloud computing capabilities, search engines, and social network sites has worsened the concern about the privacy of individuals. The use of social network sites, for example, permits access to so much information about individuals and thus may be used to violate their privacy.\n\nDue to lack of literacy on media and communication, some Internet users are totally ignorant of the effects of sharing personal information with others. Arguably, many people share personal information with others including those not well known to them without caring about the repercussions of doing so.\n\nThe discovery of cloud computing also presents serious privacy challenges. With cloud computing, individuals and organizations can store data and information without owning hardware equipment, and the information can be accessed anywhere when needed.\n\nAlthough the use of cloud computing eliminates the need to spend on expensive resources to provide for the storage, it exposes private information to a third party who may not be careful to ensure its privacy.\n\nOrdinarily, the task of ensuring the security of software on which data is stored is left with the third party and users have no say. Also, cloud computing is a new concept that is yet to be fully covered under the law. Assurance about the privacy of private information is thus a concern.\n\nOther threats to Internet security include the use of software applications, commonly referred to as sniffer programs. The programs scan information is passing through routers or computers that handle Internet-bound traffic (Schneider, 2014). Backdoor programs may also be used to access and run programs without following the proper authentication procedure.\n\nConclusion\n\nThe advent of the Internet has certainly revolutionized the way information moves from one person to another. This has created serious privacy concerns that users must take note of. Although most Internet services exist for free, efforts must be made to educate users on the effects of free Internet services on their privacy.\n\nReferences\n\nImparato, N. (2000). Public Policy and the Internet: Privacy, Taxes, and Contract . Glenwillow, OH: Hoover Press.\n\nKeeler, M. (2006). Nothing to Hide: Privacy in the 21st Century . Lincoln, NE: iUniverse.\n\nMendel, T., Puddephatt, A., Wagner, B., Hawtin, D & Torres, N. (2012). Global Survey on Internet Privacy and Freedom of Expression . Paris: UNESCO.\n\nSchneider, G. (2014). Electronic Commerce . Stamford, CT: Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Design and Installation of a Computer Network Essay\n\nEstablishing a computer network goes a long way in improving the operations of any company irrespective of its size. The ultimate goal while executing such a project is to achieve results in the least time possible and at the lowest possible cost. Below is a list of a project team that will take part in designing and installing computer network for Igloos Hotels Ltd.\n\nName             Responsibility                    Email              Telephone\nJeff Archer      Team leader                       [email protected]  254777222\nJennifer Hudson  In charge-hardware installations  [email protected]  254999333\nAlex Jamison     In charge-software installations  [email protected]  254888111\nMary Fox         Head of Operations                [email protected]  254000333\n\n\nThe organizational format of the project team will be a pure project organization. The overall team leader supervises three departmental heads. Alex Jamison heads software installations; Jennifer Hudson heads hardware installations, whereas Mary Fox heads the operations department. Jeff Archer is the overall team leader.\n\nFor the project to be successful, all the various teams working together should promote teamwork and mutual support for each other. As well, all team members should understand their individual roles and responsibilities and should strive towards the ultimate success of the entire project.\n\nMost importantly, a dedicated and inspirational leader would help a great deal in guiding and directing the whole project team without which the project would lack direction and crumble. The team leader should liaise between the various departmental heads in order to ensure that all is running smoothly.\n\nOn their part, the departmental leaders should take charge of their departmental roles and responsibilities, and as well liaise with the other departmental heads.\n\nFor instance, the software installations and the hardware installations team leaders should determine their resource requirements in good time and forward such to the operations head. The operations head should see to it that all required materials are availed in good time, and as well oversee the day-to-day operations of the project.\n",
        "label": "human"
    },
    {
        "input": "The Drawbacks of Computers in Human Lives Essay\n\nSince the invention of computers, they have continued to be a blessing in many ways and more specifically changing the lives of many people.\n\nHowever, in as much as computers have changed the way people live and do things, it must also be emphasized that they are associated with negative effects. This paper, therefore, discusses the negative effects or drawbacks of computers in the lives of many people. In trying to explore this issue, the paper will also mention what proponents of computers say before refuting it.\n\nAlthough many people may argue that use of computers help in enhancing education especially through research, when it comes to young people the reality is different. Use of computers reduces quality study time (Lin and Jin 411). That is to say, chatting, gaming and other social related software are highly tempting in the eyes of young people.\n\nAs a result, this significantly eats into their study time. In the end, the effort he or she may devote to education is virtually insignificant. In short, for a nation or its people to reap a sizeable output out of something is to invest heavily. Therefore, spending less time on education-related matters can affect one negatively in the future.\n\nComputers tend making people be over-dependent (Bowers 115). That is, instead of thinking as it was the norm before computers were invented, people today can just find answers without difficulty on the Internet.\n\nAdditionally, spending more time on the computer can easily cause health effects like eye strain, mental disorders, and shortsightedness. What is more, according to psychological studies, being on computers for a long time can as well lead to depression and anti-social behavior, and this is mostly associated with young persons (Kassin, Fein and Markus 601).\n\nUsing computers affects the social lives of people. This is based on the fact that human beings are social animals and they live in a society that is highly interactive (Lin and Jin 411). That is, sharing his opinions, beliefs and ideas with other people play a very important role in his life. However, computers have completely cut him from this kind of life. He or she is completely isolated from the events of the real world.\n\nIn fact, he or she becomes slowly changed to becoming a machine that is lifeless. He or she just like computers will eventually start perceiving things in the context of numerals and numbers. In short, this argument does not underestimate the importance of computers.\n\nIn fact, computers can easily create unimaginable things and support man to reach the intended level of success. However, if not managed well, it can as well be the start of something whose end results may be dangerous.\n\nThe power a computer has over human beings is mesmerizing. Use of computers on a daily basis can easily lead people to commit atrocious crimes that have a negative effect on the natural environment. For instance, people are used to same things like socializing with the same people. Besides, people always want to do things that they are restricted.\n\nThe bes,t example is the tendency to access various sites or carry out certain activities malware, hacking, and even spamming, which are characterized as offenses under the law.\n\nMoreover, the risks involved in talking with unknown people online or strangers have negatively affected the life of many people in the past. With this, the world must come to the realization that what people need is not a potential killer but something that contributes to the general development.\n\nWhilst computers have a lot of benefits such as making clerical and computational work easy. It is undeniable that spending many hours on a computer may easily lead to idleness. As a general fact, laziness reduces the self-respect of the person (Cash and Smolak 446).\n\nLaziness prevents someone from realizing and exploitation his or her innate skills. In fact, the approach that is slothful in nature is known to weaken the body making it unable to function properly. Therefore, in as much as computers have really changed the way people do things, but it also comes at a huge cost.\n\nEven though most users of computer argue that it enhances communication and accessibility of information worldwide. They further argue that computers provide conveniences when it comes to using various programs like accounting programs, Microsoft Office and PowerPoint. The truth of the matter is that dependence on computers reduces one\u2019s outlook.\n\nIndeed, anyone who has been using or who has ever used other machines like typewriters understands the advantages that come with computers when it comes to heavy workloads. However, the argument here is that a prolonged usage or staying in front of a computer for a long time destroys one\u2019s sense of intrinsic knowledge to that he or she can obtain from the Internet.\n\nThis makes him or her less responsive to real life activities whilst he or begin nearly all things as simulated. With this, someone can easily be labeled as an introvert or even contemptuous. His or her life remains tightly held on an imaginable line, that is, hanging somewhere between appearance and reality.\n\nComputers negatively affect one\u2019s creativity. This is because when people want to write assignments these days, they can easily \u2018copy-paste\u2019 someone work although is professionally illegal. The argument is that technology has greatly affected the thoughts to a point where they cannot afford to carry out some additional studies on their own.\n\nEvery human being is born with skills that are inherent and ingenious. Channeling creativity in a proper way can take people far. In any case, intelligence is not just about relying on the works which belong to other people. Intelligence entails making use of one\u2019s abilities to provide new information to the world as well as helping in organizing the proper or solid base to accomplishment.\n\nThe argument here is that, in so doing, one\u2019s insights and thoughts could be of great help to a developing nation when compared with the resources offered by computers. In short, the present day computer era is highly limiting the flow of idea and thoughts.\n\nIn conclusion, in as much as computers are highly acclaimed for what they have done and what they continue doing, the computer is associated with many drawbacks that negatively impact human life. As it has been seen from the discussion, computers reduce people into mere machines making them unable to do any work without them.\n\nNot enough, computers are associated with dangers such as psychological diseases, crime, laziness, lack of creativity, impact on social life and reduction in outlook. The list is not exhaustive as computers are still being used and these continue to affect people in different ways.\n\nWorks Cited\n\nBowers, C. Let them eat data: how computers affect education, cultural diversity, and the prospects of ecological sustainability. Athens: University of Georgia Press, 2000.\n\nCash, Thomas and Linda Smolak. Body image: a handbook of science, practice, and prevention. New York: Guilford Press, 2012.\n\nKassin, Saul, Steven Fein and Hazel Markus. Social psychology. Belmont, CA: Wadsworth, 2014.\n\nLin, Sally and David Jin. Advances in computer science, intelligent system and environment. Berlin: Springer, 2011.\n",
        "label": "human"
    },
    {
        "input": "Computer Science Corporation Service Design Report (Assessment)\n\nIntroduction\n\nComputer Science Corporation (CSC) is a renowned American company that started its operations in the year 1959. The company that has its headquarters in Virginia US was started with a core mission to provide quality information and technology (IT) services to individuals globally. It has been able to achieve its mission through the adoption of effective dimensions of service quality and various performance approaches that are technologically driven.\n\nThe approaches, including quality dimensions, have facilitated superior service delivery by ensuring effective coordination of activities, resource allocation, and advancement in technology. They have also ensured the provision of requisite solutions to various IT complications that affect most institutions.\n\nAs noted, the company has been focusing its potentials in providing a wide range of services that are innovative to its customers. Notably, it is known for providing superior IT and business outsourcing services that include system analysis, applications development, and data center management programs.\n\nIt also focuses its synergies in providing quality network applications or operations, end-user computing, and cybersecurity services. Further, the company provides emerging services that include cloud computing, cyber protection services, software as a service, and other emerging techniques of business operations. The company offers these services under ideal standards of operation to meet the needs of consumers promptly.\n\nThe company\u2019s service design and whether or not it is effective\n\nIndeed, Computer Science Company operates under a well-structured service design that is referred to as matrix service delivery design. The design provides the company with requisite operating incentives that ensure no compromise to quality at various levels of service delivery. For instance, it equips stakeholders, especially employees, with the conventional service delivery systems that are sustainable.\n\nThe design also fosters communication and execution of activities since it reduces bureaucratic procedures that normally affect effective service delivery (Fitzsimmons, 2011). Indeed, the matrix design that the company has been able to adopt enables it to execute reliable operating plans, organize its workforce well, and distribute resources appropriately.\n\nAs noted by Gumzej & Halang (2009), service design is a noble and effective activity that entails planning, organizing people, and infrastructure towards improving performance. It is integral since it ensures that the proper plan of activity is drawn in all business units, including improvement of communication. Similarly, it is imperative in ensuring that services that are provided are of unmatched quality, and they are customer oriented.\n\nThis is evident since effective service design enables institutions to provide user-friendly services. The effective design also enables institutions to provide services that are socially relevant and economically viable. Therefore, institutions like Computer Science Company that aspires to improve their performance continuously and gain competitive advantage should not relent in adopting a viable service design.\n\nNotably, the matrix service design that the CSC has been using over the years has been of great importance and effective to the company. This is apparent since the service design has been a major contributor to the company\u2019s exemplary performance. Firstly, it has provided the company with a flexible communication system that has facilitated the planning and distribution of resources. It has also promoted the process of identification of customer needs, thereby leading to quality and timely satisfaction of the needs (Fitzsimmons, 2011).\n\nConversely, the design has enabled the company to adopt different dimensions of service qualities that are instrumental in advancing service delivery or competitiveness. The dimensions that include reliability, responsiveness, assurance, and others have enabled the company to deliver quality IT solutions to both private and public entities.\n\nFor instance, most of the company\u2019s clients have been able to acquire quality end-user computing services, system analysis programs, and effective data-center management programs due to the use of matrix service design.\n\nDimensions of service quality and the way they are applied by the company\n\nDimensions of service quality are key elements that are used to execute a comprehensive comparison of expectations with performance in diverse settings. The dimensions that include reliability, responsiveness, assurance, empathy, and tangibles are important since various institutions apply them to establish the expectations of consumers.\n\nInstitutions such as CSC should also use the dimensions to facilitate a holistic understanding of performance levels and whether objectives are being met or not (Webber & Wallace, 2011). Indeed, institutions should consider adopting service quality dimensions since they are essential in enhancing performance. This is evident since the dimensions ensure that a company\u2019s services are reliable, responsive in nature, standardized, and tangible.\n\nCSC Company adopted the use of dimensions service quality as a performance measure. The strategy was adopted to enable the company that remains a renowned IT service provider to deliver tailor-made IT solutions to clients. It was keen on ensuring that it delivers reliable services that can enable customers to plan and access IT services, as they need them. This has been significant since the reliability of the provided services has enabled the company to give real-time solutions to consumers IT related complications.\n\nFor instance, most of the company\u2019s customers have been able to get data security management services promptly and effective system integration programs with minimal difficulties. Secondly, the company has been very responsive in responding to various issues of clients. This is evident in the way and manner in which it delivers services to clients whereby it applies total quality ideals and principles (Webber & Wallace, 2011).\n\nThat is the company takes responsibility for the key service delivery processes from the time of ordering to delivery. It takes responsibility by providing relevant support services throughout the process, including after sale services to ensure that customers are always satisfied with the quality of the services and that they receive value for their money. Similarly, the company has been using assurance and empathy principles to ensure that its clients obtain value for their resources.\n\nThe principles enable the company officials to execute quality management and technology consulting activities through comprehensive engagement with other stakeholders. The company has also been using the element of assurance to assure its clients about the effectiveness of the services that are delivered, including their relevance in enhancing performance (Gumzej & Halang, 2009).\n\nFurther, the company has been able to adopt the ideals of tangibility that is a noble dimension of quality. The dimension is important since it emphasizes on delivery of services that presents tangible results to clients. It has ensured that CSC Company provides services that present tangible deliverables that are sustainable.\n\nConclusion\n\nIndeed, the company\u2019s success is due to the absolute adherence to diverse dimensions of quality principles. This is evident since the principles have enabled the company to provide quality, timely and sustainable remedies to IT related complications such as Cybersecurity issues, business process outsourcing difficulties, data center management, and end-user computing issues. They have also enabled the company to be more reliable and responsive in all aspects of its service delivery.\n\nReferences\n\nFitzsimmons, A. (2011). Service Management: Operations, Strategy, Information Technology. New York: McGraw-Hill.\n\nGumzej, R. & Halang, A. (2009). Real-Time Systems\u2019 Quality of Service: Introducing Quality of Service Considerations in the Life Cycle of Real-Time Systems . New York: Springer.\n\nWebber, L. & Wallace, M. (2011). Quality Control for Dummies . Hoboken: John Wiley & Sons, Inc.\n",
        "label": "human"
    },
    {
        "input": "Improving the Security Levels of Software Term Paper\n\nIntroduction\n\nWith the modern uncertainty in information and physical safety, even computer programmers who understand the inner technical operations for software admit that there is no single way to eliminate computer security threats. However, software developers struggle so hard to improve the security levels of existing software.\n\nAs a result, the constant improvement allows more users to enjoy new software functionalities, as well as the security benefits that come with the new deals. According to relevant sources, software risks will always exist as long as various systems within the computer system are working, as well as allow people to access emails, social networks, and e-commerce.\n\nIn addition, programs with malware and hackers are always present and in search for confidential information. The major source of software vulnerability can be attributed to coding errors. While coding errors affect some languages such as C and C more than other languages like Java, coding is a problem that is likely to affect all languages (Foremanm, 2009).\n\nSolutions to Part A: Asset Vulnerability\n\nIn many ways, modern technology seems to offer solutions to almost every single problem. While software vulnerability has been a key problem for the computer programs, technology has come up with solutions to curb this evil. This part of the project presents ways through which computer security has become uncomplicated.\n\nIt has become easy to identify software weaknesses and vulnerabilities identified in phase I as well as give means to fix the vulnerabilities to the original state. With the sophistication of computer security systems, it is continually getting easy to create software that provides protective measures against hackers.\n\nSome of these security systems include windows firewall, antivirus, and antispyware that are designed to prevent the software from several security threats (Wright, 2009).\n\nIt has become clear that in the society today, there is no way to avoid computer security vulnerability; hence, it is worth to understand the ways of preventing the software protection from being compromised. In an effort to the plot and explain the answer to this vulnerability, the study has identified the solutions to preventing this destruction.\n\nIdentifying the area where the computer software is vulnerable is a challenging task, which must be conducted so as to prevent the computer from being vulnerable. Fortunately, technological innovation has enabled the manufacture of honey pot machines, which is considered as the key to stable computer assets (Schalager, 2008).\n\nIn an attempt to identify vulnerabilities posed by hackers and other threats like viruses, honey pot machines have been devised to solve vulnerability as soon as it happens. In addition, the machines give an identified location within the system where there is a key threat to susceptibility.\n\nHoney pot machines have the capacity to catch someone trying to break into the system. These machines play a vital role in computer security and also in guarding the software against various threats by identifying the area where vulnerability lies.\n\nSolutions to Part B: Possible Threats against Asset\n\nPasswords\n\nOn several occasions, passwords have created a large impact in providing security to the computer system. While many people leave their systems without passwords, they underestimate the potential that passwords have in preventing computer software vulnerability. Has a first step, passwords limit the number of people who can access information and destroy or replace it while eliminating the chances of missing and stolen files (Kiountouzis, 2002).\n\nPasswords and other means of authentication block other software that might be detected as malware within the system. Since having a password is a significant step in preventing software vulnerability, computer individuals must be on the watch out for the privacy of their passwords.\n\nThe character combination of password should not be too predictable by other users or individuals who access the same machine. As discussed in phase one, weak and default passwords is increased by use of words found in the dictionary, lack of diversity in the characters used and use of information which is readily crucial to the user (Anna, 2010).\n\nOutdated/in original software\n\nWhile many computer users do not regularly update their system, they ignore the fact that software that is not current is more vulnerable to computer anxiety than software that is constantly updated. In addition, in real software does also pose a greater threat to the system being hacked because they lack the confidence carried by original software.\n\nAccording to relevant sources, computer users must be sensitive and use up-to-date versions of computer software so as to increase the security of their systems. Additionally, this will ensure that their sensitive information is protected against computer malware since new programs are equipped with improved security features. Therefore, it is essential that the average computer user maintains an updated version of their programs, as well as other protection software.\n\nSolutions to Part C: Likelihood of Threat Occurrence\n\nCommon Weakness Enumeration\n\nAs discussed earlier, coding errors have posed the greatest threat to computer vulnerability. In a struggle to eliminate coding errors, various measures have been implemented by software developers.\n\nSoftware such as the Common Weakness Enumeration enables the immediate identification of coding errors that might pose a threat to computer security. The software is providing a networked selection of software weaknesses that are enhancing the development of tools, which can detect vulnerability within operating systems, as well as codes.\n\nCERT Coding Secure Standard\n\nWith modern technology, software engineers have been able to work to develop software that is less vulnerable. In this effort to develop software that is less vulnerable, the CERT coding secure standard has been devised by Carnegie University in collaboration with software engineers.\n\nAs such, the system can detect problems within a program that makes it vulnerable; hence, issue a report on coding standards that will provide security, as well as conduct an educational campaign for software developers. This process is motivated so as to deliver software that will be remarkably secure.\n\nSAMATE\n\nIndividual governments involved in software production have not been left behind in an effort to eliminate coding errors in software construction. In this effort, the US government department of Homeland in its own initiative sponsored tools for evaluating and providing metric security software assessment. The project known as SAMATE is involved in identifying, enhancing and developing tools to improve software security.\n\nSolutions to Part D: Consequences to Mission-Critical Business Processes\n\nFirewall\n\nIn addressing the short term computer security vulnerabilities, many computers use technological innovations such as web-based firewall that acts as a shield of the system. Though firewall is an effective shield that blocks unwanted and threatening viruses or malware, it does not solve the problem permanently. In providing a permanent solution, the computer user must find a solution for the software.\n\nVulnerability scanners\n\nVulnerability scanners have been the primary tools through which computer user provide guidance for their software.\n\nVulnerability scanners in the modern world meet in a variety ranging from database security scanner, port scanner that examines the computer ports, network vulnerability scanners, computer worm and web application security scanner among others. As seen from the names of the vulnerability scanners, the various scanners differ in their function in that they are used to examine different types of computer security vulnerability.\n\nPenetration test\n\nConducting a penetration test is another channel through which the computer security vulnerabilities can be easily detected and repaired. Penetration test in computer software security parameters depending on the amount of knowledge the person conducting the investigation has about the system.\n\nAn individual must be well informed on the systems within their place before conducting the test. They help identify any possible security threat being posed by hackers or other malicious software.\n\nComputer antivirus\n\nComputer antivirus software comes in all prices ranging from a few dollars to thousands of dollars. The computer antivirus plays an enormous role in preventing malware and viruses that have a devastating effect on computer software. Malware and viruses have the potential to slow down computer software and alter information.\n\nWhen left unattended, such software can eventually lead to the breakdown of the entire system; hence, result in loses that would have been prevented. Initiating constant updates for the antivirus ensure that most of the growing inventions in computer viruses are blocked out of the system (McClure, 2009).\n\nAs established within the paper, there is a justified cause for any obligation to adopt the precautionary measures for computer security vulnerability. In situations where the user stays without adequate protection systems, the computer will remain vulnerable to key threats and may breakdown at any moment.\n\nFortunately, several preventative tools are affordable while other offers come free, especially with trial versions. For instance, many antiviruses are free within the computer software market; hence, there is a reasonable accessibility to computer security systems.\n\nMore so, there is no damage that is charged in installing a password into the computer system. On the other hand, while some of the software such as HP security fortified and the IBM application may be a little higher, the additional cost is justified.\n\nSolutions to Part E: Effect to Organization Competitive Advantage\n\nHP fortified software\n\nIn an effort to establish a competitive advantage, software developers have formulated effective software, which is not vulnerable to various attacks. Software provider HP has been in the forefront in devising software security Assurance. HP fortified software can identify the primary purpose of vulnerabilities that are running and those that are dormant.\n\nWhile being aware that some languages are vulnerable to computer security, the Hp fortified software can discern the existing vulnerability in eighteen of the languages. Hp security can eliminate the vulnerabilities and provide protective measures against insecurity (Williams, 2009).\n\nThe internet has become a global village that connects people to almost everybody and everything through online shopping and social networks, among other functions. Though this has changed the world into a global village, it has also posed the largest source of threats to the computer software.\n\nThrough the internet, computer users are able to interact with other users, but at the same time, they become exposed to computer malware and data hackers.\n\nWhile considering threats posed over the internet, IBM has created software that is automatically scanned while on the web and allows the computer user to fix some of the recommendations. The software can identify any vulnerability within the system and support solution to coding errors.\n\nConfiguration setting\n\nOn the other hand, software security threats associated with the infrastructure and network host are easily eliminated by changing the configurations setting for such a system. One question that can be easily solved by a configuration setting is access to insecure website.\n\nThe owner of the computer can receive significant warnings before opening information that is dangerous; hence, the system generates a significant level of protection to the end-user. Viruses and malware are easily blocked and eliminated from the system through a configuration setting.\n\nIn providing a solution to computer security vulnerability, patches can be easily applied in the system in order to repair any weak areas of the system. Patches are often present after the vulnerability in the computer security has been identified.\n\nOn the effect of installing the precautionary measures, there is a 100% assurance that the business will be secure (Wright, 2009). Modern businesses rely on the technological aspect of computers to perform their business; hence, installing proper security measures in such computer systems will provide a significant level of protection to confidential information.\n\nGovernment\u2019s jurisdiction has been exposed to destruction through the leakage of sensitive information, and this is not an exception to existing companies and businesses.\n\nAs a result, sophisticated computer hardware and protection software have undergone continuous improvement so as to counteract such deficiencies. In fact, the future is expected to bring fourth key security features among computer systems that will boost their security.\n\nReferences\n\nAnna, F. (2010). Unknown Vulnerability: The basics . New York, NY: McGraw-Hill.\n\nForemanm, C. (2009). Vulnerability management: Dealing with Computer Security Vulnerability. New Jersey: Morgan Kaufmama publication.\n\nKiountouzis, A. (2002). Information Systems Security: Facing the Information Society of the 21st Centrury. London, England: Chapman and Hall ltd.\n\nMcClure, S. (2009). Hacking Exposed: Network Security Secrets and solutions . New York, NY: McGrawHill.\n\nSchalager, D. (2008). When Technology Fails: Technology Disasters . London, England: Gale Research, INC.\n\nWilliams, A. (2009). Improving Information Technology Security: Vulnerability Management. Ney York, NY: McGrawHill.\n\nWright, G. (2009). Computer and Security: The Vulnerability involved . London, England: Taylor and Francis Group.\n",
        "label": "human"
    },
    {
        "input": "Melissa Virus and Its Effects on Computers Report\n\nMelissa virus affects Microsoft word and spreads as an attachment. If an unsuspecting recipient opens it the virus affects the computer storage. The virus disables the mail servers of a corporate. For example, Microsoft Corporation shut down all its incoming emails because of the Melissa virus. The virus also affected other companies for example Intel.\n\nThe virus spread through email around the US and caused the closure of email servers in government agencies (Mills, 2009). David L Smith, the man behind the virus was arrested. The computer programmer was found guilty of creating and distributing the virus to many computers in the world in 1999. He was detected by the IP address that was proven to be originating from New Jersey.\n\nHe was given a twenty-month jail sentence and a fine of $5000. The virus posed a security threat to government agencies because it interfered with the normal working through forced closure of email servers. The shutting down of the servers compromises the effectiveness of the agencies, and criminals could use such lapses to carry out acts that endanger the lives of the people.\n\nIn conclusion, cybercrime affects security, and governments must put concerted efforts to bring the perpetrators to justice, and ensure citizens of security in the cyberspace. Moreover, people need to be educated on cyber attacks and how to avoid falling prey to cyber criminals and stay safe online. The capture of David is a good sign that the US government is serious about tackling cybercrime.\n\nThe war against cybercrime is a difficult one to win because new viruses keep coming up, but if the government and other stakeholders work together, they will not be caught off guard by new viruses. The government should devote its resources into the study and investigation of how criminals use the click fraud system because genuine advertisers lose their money by making them pay for automated advertisement clicks.\n\nSuch advertisers do not get human traffic to their sites hence cannot make sales. The attack of personal computers without the knowledge of the users is very dangerous because the virus steals personal information. The theft of identity can pose very serious risks to innocent people. Working hand in hand with other stakeholders in the cyberspace will help to apprehend criminals that pose security threats.\n\nMoreover, the government will help to cushion genuine advertisers and online users from financial frauds. More importantly, through vigorous and constant checks on cyber crimes, the government will be able to stop any threats at the early stages and mitigate large damages before they occur.\n\nThe kinds of cyber attacks run by Chinese hackers organizations Unit 61398 are dangerous as they breach the privacy of the US government and corporations. The hackers can steal confidential information that they can use against corporations or governments. The hackers compromised the state security with the confidential information they lay their hands on making it a very worrying trend.\n\nStakeholders ought to work together to curb cyber attacks. However, the Chinese government denies any cyber attacks originating from its country. Hence, it is difficult to win the war over cybercrime because of the mistrust between nations; thus hackers will continue to have a field day hacking systems.\n\nDifferent countries must put away their differences and fight the common enemy of cybercriminals because they make the world a dangerous place for all. The fight against cyber attacks must continue.\n\nReference List\n\nMills, E. (2009). Melissa virus turns 10 . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Security \u2013 Information Assurance Exploratory Essay\n\nIntroduction\n\nThe present epoch of computer technology has come with a number of challenges. One of the challenges is in safeguarding information stored in computer systems.\n\nTo make sure that only authorised persons have access to the stored information, organizations come up with authentication and authorization procedures, which assign unique login details to each person (Bishop, 2002).\n\nWhenever one wants to access information stored in a computer system, he or she has to key-in the login details. Organizations use varied types of authentication depending on the level of required security. This paper will discuss authentication, different types of authentication and authorization.\n\nAuthentication\n\nBirch (2007), defines authentication as, \u201cThe process of determining whether someone or something is, in fact, who or what is declared to be\u201d (p. 86). In many cases, authentication is done by verifying individual\u2019s password or username.\n\nOrganizations use authentication with the sole goal of guaranteeing security of their network systems or data. They require knowing persons prior to allowing them to access crucial information.\n\nHence, authentication helps to make sure that only the authorised persons have access to information. Authentication procedure follows the assumption that only the authorised persons have knowledge about the passwords or usernames that an organization utilises (Birch, 2007).\n\nTwo-factor authentication\n\nTwo-factor authentication is also referred to as strong authentication. The authentication process involves giving two forms of login details.\n\nPersons may be required to give their passwords and swipe their credit cards or, use their biometrics together with a password before gaining authorization. Two-factor authentication is used to secure vulnerable or sensitive information (Jin, Ling & Goh, 2004).\n\nUsername or password only may not be adequate to protect sensitive information. This authentication is mostly used in hospitals to secure health care information.\n\nSome of the health care information stored in shared computers is sensitive, and it calls for health care practitioners to use a two-factor authentication to make sure that other users do not access the information.\n\nMulti-factor authentication\n\nBhargav-Spantzel et al. allege, \u201cMulti-factor authentication occurs when a user is required to provide multiple pieces of information to authenticate them to a system. They require using something they know, something they have, and something they are\u201d (2007, p. 530).\n\nConventionally, multi-factor authentication entails using a combination of password, a token and biometric data. This combination creates a secure authentication procedure that is hard to steal. A cyberstalker is able to steal the login details of individuals that use weak authentications.\n\nThis makes network systems vulnerable to cybercrimes. However, the use of multi-factor authentication makes the systems secure as individuals have to provide a physical gadget before gaining authorization.\n\nMulti-factor authentication makes it hard for cybercriminals to penetrate into computer networks from a distance (Bhargav-Spantzel et al., 2007).\n\nAuthorization\n\nIn computing world, \u201cauthorization is the function of specifying access rights to resources, which is related to information security and computer security in general and to access control in particular\u201d (Ratha & Bolle, 2005, p. 617).\n\nSystem administrator requires limiting the access privileges given to each user in an instance where multiple users have access to a system. In such an instance, the administrator assigns varied access privileges to different users based on their needs.\n\nThis guarantees that individuals do not have access to information that is beyond their areas of operations. It ensures that sensitive and vulnerable information is secure.\n\nAuthorization and authentication software\n\nPresently, organizations use authentication and authorization software to regulate access to information systems. There are different authentication and authorization software that varied organizations use.\n\nConsequently, they chose software depending on its ability to meet the desired level of security and cost. Some of the current authentication and authorization software include AuthAnvil 2FA and Enablon Authorization & Sign Management (ASM) respectively (Scorpion Software Corp., 2013).\n\nAuthAnvil 2AF is authentication software that requires users to produce an identification gadget called a token together with their personal identification number (PIN). The software keeps on changing the authentication password.\n\nFor people to access a secured system, they require providing their PIN together with a one time password. One is not allowed to access the system without providing the two authentication requirements.\n\nThis makes the system secure since once individuals use their passwords; they cannot use the same password in the future. Hence, it is hard for unauthorised persons to steal the password (Scorpion Software Corp., 2013).\n\nThe main weakness of the software is that it is not compatible with all types of hardware devices. Hence, in case one does not have a compatible hardware device, he or she would have to incur extra cost in purchasing compatible hardware.\n\nAuthAnvil 2AF \u201creduces integration and customization costs by providing a single foundation for all core authentication management for your business, and that of your customers\u201d (Scorpion Software Corp., 2013, par. 19).\n\nThe software is compatible with a number of hardware devices and does not involve a lot of licensing overhead. Once individuals purchase the software, they register it with its manufacturing company and are free to use the software.\n\nThe fact that AuthAnvil 2AF issues a different password whenever one wants to log in, makes it appropriate for the Banking industry or Health care facilities.\n\nThe Banking industry requires an Information Technology (IT) system that is very secure. This authentication software would help to secure IT systems in the banking industry.\n\nEnablon ASM is software that helps organizations to manage authorization and allocation of signatures to employees. For an organization to enhance its operations, it requires delegating authorization duties to certain staff.\n\nHowever, if an organization runs numerous companies across the globe, it would be hard to monitor its authorization procedures and to make the necessary changes. Enablon ASM helps to solve this problem.\n\nThe software helps organizations to validate email alerts, and login details electronically (Enablon, 2012). Besides, the software helps organizations to save resources and time that would be used gathering, finding and scrutinizing delegation information.\n\nThe main weakness of the Enablon ASM software is that it is not compatible with all hardware devices. Additionally, it would be hard for an organization to manage authorization process in event of software failure.\n\nThe software is not costly since one does not incur licensing cost (Enablon, 2012). Once a company purchases the software, it is set to start using it with the authorization of Enablon Company. The software is suited for oil companies, which run numerous firms across a region or globe.\n\nWith the software, a company would be able to manage its information system by making sure that employees only access the information they require in their areas of specialization.\n\nConclusion\n\nGrowth in computer technology and increase in cybercrimes, has led to organizations looking for ways to secure their information systems. Presently, organizations use two-factor and multi-factor authentication procedures to secure their systems.\n\nBesides, organizations are turning to authentication and authorization software to enhance the security of their information technology systems. This has saved many organizations from cyber criminals.\n\nReferences\n\nBhargav-Spantzel, A., Squicciarini, A., Modi, S., Young, M., Bertino, E. & Elliott, S. (2007). Privacy preserving multi-factor authentication with biometrics. Journal of Computer Security, 15 (5), 529-560.\n\nBirch, D. (2007). Digital Identity Management: Perspectives On The Technological, Business and Social Implications . Hampshire: Gower Publishing Limited.\n\nBishop, M. (2002). Computer Security: Art and Science . Boston: Addison-Wesley.\n\nEnablon . (2012). Enablon ASM . Web.\n\nJin, A. Ling, D. & Goh, A. (2004). Biohashing: two factor authentication featuring fingerprint data and tokenised random number. Pattern Recognition, 37 (11), 2245-2255.\n\nRatha, N. & Bolle, R. (2005). Enhancing security and privacy in biometrics-based authentication systems. IBM Systems Journal, 40 (3), 614-634.\n\nScorpion Software Corp. (2013). AuthAnvil two factor Auth Intro: AuthAnvil 2AF technical tour . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Technologies That Assist People With Disabilities Research Paper\n\nIntroduction\n\nThe disabled persons may feel isolated and excluded because of the inability to lead their lives normally. They are unable to do the simple things that a normal person does such as using the phone or moving around. In order to enable the disabled to manage their daily lives, a range of devices has been invented. These devices enable the disabled individuals to be more independent and live better lives.\n\nThis research explores the various technological devices that are available for people with all types of disabilities, and their impact on the individuals\u2019 lives. The implementation of these devices in colleges and public places is also discussed. The shortcomings of these devices as well as suggestions for their improvement are also covered.\n\nDevices for assisting the disabled\n\nThe visually impaired\n\nTo assist the visually impaired to use computers, there are Braille computer keyboards and Braille display to enable them to enter information and read it. The Braille display device produces Braille output. This device is useful for blind and deaf people. Screen reading software produces a voice output reading the text on a computer screen or prints out a Braille output.\n\nExamples of such software include Window-Eyes, Serotek and Job access with Speech, among others. Screen reader technology has been incorporated in computers and iPhones by Apple.\n\nThere are also portable reading devices which assist the blind by downloading books and reading them out. Examples of these devices are the BookSense audio book and the Victor Reader Stream. These devices make available books of all types including entertainment and education 1 .\n\nAnother device for assisting the visually impaired is the Global Positioning System device. This device assists the blind people in moving around by calculating their exact locations and giving them directions to any destination, through speech or Braille display.\n\nSome of the Global Positioning systems for the blind include Trekker, BrailleNote GPS, Mobile GEO and Street Talk. An electronic cane device is also useful in enabling the blind people to move around. This device makes the user aware of what is in the surrounding environment to enable them to reach their destination safely 2 .\n\nAccess to electronics by the blind is very important. Many household appliances\u2019 are labeled in Braille to enable the blind to use them. Some of these devices include dish washers, refrigerators, DVD and MP3 players, microwaves, radios among others 3 .\n\nThe deaf\n\nThe deaf cannot be able to make calls and can use e-mail or text messaging to communicate with others. Video phones can also help them to make a video call and communicate using sign language. The partially deaf can use high-powered speakers or amplified headphones to enhance their hearing ability when using the telephone. They can also use infrared systems to adjust the volume of television as desired.\n\nThe deaf need environmental alert systems to enable them to be aware of various situations. They can use vibrating alarms placed under the pillow to wake them up or alert them to fires. Other alarms have a lighting system to draw the attention of the deaf persons.\n\nExamples of these alert devices include Sonic boom Alarm clock, Deluxe Alarm clock, Wireless Audio Visual Emergency System., Personal Tactile Signaler, among others 4 .\n\nCochlear implants can be used by the deaf to transmit sound directly to the auditory nerve. The people who use this device are those with serious nerve deafness and cannot benefit from other hearing aids. They are useful to people whose auditory nerves are not damaged. Deaf individuals with damaged auditory nerves can use the Auditory Brainstem Implant 5 .\n\nThe mobility impaired\n\nThere are various alternative keyboards for use by those with hand disabilities. For people with small hands, the Little Fingers keyboard is used. Those people with only one hand can use the BAT One-Handed keyboard. People who cannot use their hands can either use foot pedals as a substitute or gigantic trackballs.\n\nPeople with no arms can not be able to type information using a keyboard. They can use computers with speech recognition software so that they just give voice commands to the computer which turns them into print 6 .\n\nPeople who cannot move their hands or legs can use a Tongue Touch Keypad to operate different devices like computers, telephone, bed, electronic wheelchair and music players.\n\nThis device is placed in the mouth and operated with the tongue to touch various keys. A fifteen year old boy from Maine who cannot move his legs or hands uses this device to operate various devices 7 .\n\nWord prediction devices can also be used to assist in inputting the text and increases the speed of input of the user.\n\nWheelchairs that can climb stairs have been invented to enable the mobility impaired to access buildings that are accessible only through stairs 8 .\n\nThe mute\n\nFor people who cannot speak, there are devices which can help them to express their feelings. For instance, there is a device called Tango which was invented to help a boy who could not speak. The device has pictures which the user can touch to communicate his feelings. When the pictures are touched, a voice relays the feelings of the user. These people can also use video calls to communicate with others through sign language. Speech synthesizers can also be used to enable the mute to communicate 9 .\n\nEmploying devices for disabled students in colleges\n\nIn colleges, students use computers for a variety of purposes including to conduct research, write term papers, and to take part in online activities. Disabled students need to be able to use computers for these purposes 10 .\n\nDevices for assisting disabled students enhance their productivity and compensate for their disabilities. Students with mobility impairments need to have access to special keyboards and mouse in order to be able to input information into the computers. These devices should meet the needs of students who cannot use their hands or can use one hand or one finger only. The computer equipments should also be able to be adjusted to different positions in order to meet the various needs of the disabled students 11 .\n\nBlind students can be able to use access online instructions through the use of screen readers, optical character recognition software and Braille output devices.\n\nStudents who are partially blind can enhance their vision through using keyboards with enlarged keys, magnification software and large print output. In class, blind students can take notes through note-taking cassette recorders. They can also use talking clocks to be aware of the time 12 .\n\nDeaf students experience challenges when accessing audio and visual files. Alternative formats for the audio visual files such as captioning and transcripts should be provided in order to assist them.\n\nMute students can use speech synthesizers to relay their information. Students with information-processing disabilities should be allowed to use spell-checking features in order to correct the errors in their work. All the facilities in the college e.g. classrooms, laboratories, among others, should be easily accessible by all the students. The facilities should be designed in such a way that disabled students can comfortably use them 13 .\n\nRole of public places in implementing the equipment\n\nPeople with disabilities can be assisted through making the environment more accessible to them. The designs of the environment and buildings should be able to accommodate assistive technology to help the disabled people to access them better.\n\nPublic areas, homes, buildings, recreational areas, buildings and transportation systems should be designed in such a way that the barriers that might inconvenience the disabled persons are removed. These facilities should have features like wheel-chair lifting devices, specially designed locks and door openers, ramps and approaches and elevating devices, among others 14 .\n\nDesigns used in construction of public facilities should be universal and free from all barriers to enable everyone to access them independently, freely and safely. For instance, buildings that are sonly accessible through the stairs cannot be accessed by physically disabled persons on wheelchairs.\n\nAssistive technology cannot function well in an environment whose design is not accommodating. The use of assistive technology in an environment with accommodating design features helps the disabled people to go on with their lives more easily and can also reduce the disability levels or prevent the disability from getting worse 15 .\n\nPublic policies such as the Americans with Disabilities Act have been enforced to ensure that the disabled people have access to assistive technology and an environmental design which is conducive.\n\nThis act requires public officials, employees and businesses to ensure that the disabled people have access to an accommodating environment for them to perform their activities efficiently. Such people should be provided with assistive technological devices, flexible work schedules, accessible design, training and personal assistants.\n\nThe Technology-Related Assistance for Individuals with Disabilities Act approves Federal finances to nations which support disabled persons by planning and developing technological assistance programs for them.\n\nThis act emphasizes the importance of planning in order to enable an increased access to technology by disabled persons. These states can increase the access to technology by instituting technical assistance and training to the disabled, providing assistive technological devices, and initiating projects to increase the public awareness.\n\nShortcomings of the devices for assisting the disabled\n\nThough the assistive devices have helped improve the lives of the disabled, they have some shortcomings. Most of these devices are very expensive and may be inaccessible to all the disabled persons. For instance, screen readers are very expensive, with prices starting from $900 16 .\n\nSome devices may not improve the functionality of the disabled persons and therefore become a hindrance to the individual. Others may be bulky and therefore impossible to carry around and inconvenient.\n\nThese devices are reliant on technology and may be unusable to people who have no access to the technology. These devices also require continued technological support in terms of maintenance and training. Moreover, the use of these devices requires adequate training and orientation and lack of this may lead to inability to use them 17 .\n\nUsers of the assistive technology may become over-reliant on it and be unable to develop their own skills. It may also be difficult to get the device that matches the exact needs of the user. Most of these devices are designed to cater for a single type of disability. Therefore, people with multiple disabilities may not benefit from them 18 .\n\nImproving the devices for assisting the disabled\n\nAssistive technological devices have been able to make the lives of many disabled persons easier and less independent on others. However, these devices are not the only solution for disability. The disabled can also benefit from personal assistance, adapting to new environments and learning new skills 19 .\n\nThe application and development of assistive technological devices can be enhanced through several strategies. The policy makers should be able to make these devices more accessible and affordable for all. Access to up-to-date information about the devices should also be made available. This will ensure that the correct devices are used by the disabled to enhance their functionality 20 .\n\nThe environment should be modified so as to accommodate the use of assistive technological devices. For instance, public places should be designed in a way that they are accessible by all people including the disabled.\n\nDevices that cater for the needs of more than one disability should be designed. Some of the devices that are bulky should be designed in such a way that they are convenient and portable. Moreover, the devices should be designed to match the exact needs of the users 21 .\n\nConclusion\n\nTechnological devices for assisting the disabled have had a tremendous impact on the lives of the disabled persons. People with various disabilities are now living better lives and engaging in almost all normal activities.\n\nThese devices have been implemented and accommodated in public places like buildings, offices, schools and other places to make the lives of the disabled much easier. However, these devices have some shortcomings. These devices have greatly improved the quality of life for the disabled persons and enabled them to live normal and healthy lives.\n\nBibliography\n\nAlbrecht, Gary L., Seelman, Katherine D. and Bury, Michael . Handbook of Disability Studies . USA: Sage, 2001.\n\nBatcheller, Lori . Assistive Technology for the Blind. 2011. Web.\n\nBurgstahler, Sheryl . Working Together: People with Disabilities and Computer Technology . 2010. Web.\n\nCarey, Allison C., Friedman, Mark G., Bryen, Diane Nelson and Taylor, Steven J. Use of Electronic Technologies by People With Intellectual Disabilities. Mental Retardation . 2005, Vol. 43,( 5), pp. 322-333.\n\nDavis, Barbara Gross. Tools for Teaching . San Francisco: Jossey-Bass Publishers, 1993.\n\nDay, Sheryl L. and Edwards, Barbara J. Assistive Technology for Postsecondary Students with Learning Disabilities. Journal of Learning Disabilities . 1996. Vol.29,(5), pp. 486-492.\n\nGlinert, Ephraim P. and York, Bryant W. Computers and people with disabilities. Communications of the ACM. Vol. 35( 5),1992.\n\nHardman, Michael., Drew, Clifford and Egan, Winston. Human Exceptionality: School, Community, and Family . London, Cengage Learning, 2010\n\nHersh, Marion A. Assistive Technology for Visually Impaired and Blind People.USA: Springer, 2007\n\nLewis, Rena B. Assistive Technology and Learning Disabilities:Today\u2019s Realities and Tomorrow\u2019s Promises. Journal of Learning Disabilities. 1998. vol. 31(1) pp.16-26.\n\nMcGuire, Joan. M., and O\u2019Donnell, Jennifer. M. Helping Learning Disabled Students to Achieve: Collaboration Between Faculty and Support Services. College Teaching , 1989,Vol. 37(1), pp.29-32.\n\nNakasone, Jon. Assistive Technology and Accessibility . 1999. Web.\n\nPell, Stephen D., Gillies, Robyn M. and Carss, Marjorie. Relationship between use of technology and employment rates for people with physical disabilities in Australia: Implications for education and training programmes. Disability and rehabilitation. 1997, Vol. 19,(8) , pp.332-338\n\nPress, Sedgwick. The Complete Directory for People with Disabilities . New York: Grey House Pub., 2001\n\nScherer, Marcia J. Assistive technology: matching device and consumer for successful rehabilitation. USA: American Psychological Association, 2002.\n\nTomlinson, Carole Anne. Devices to Help the Blind. 2010. Web.\n\nWren, Carol, and Segal, Laura. College Students with Learning Disabilities: A Student Perspective . Chicago: Project Learning Strategies, DePaul University, 1985\n\nFootnotes\n\n1 Marion A., Hersh . Assistive Technology for Visually Impaired and Blind People. USA: Springer, 2007.\n\n2 Lori, Batcheller . Assistive Technology for the Blind. 2011. Web.\n\n3 Carole Anne, Tomlinson. Devices to Help the Blind . 2010. Web.\n\n4 Gary L., Albrecht, Katherine D., Seelman and Michael, Bury . Handbook of Disability Studies . USA: Sage, 2001.\n\n5 Gary L., Albrecht, Katherine D., Seelman and Michael, Bury . Handbook of Disability Studies . USA: Sage, 2001.\n\n6 Ephraim P., Glinert and Bryant W., York. Computers and people with disabilities. Communications of the ACM. Vol. 35( 5),1992.\n\n7 Gary L., Albrecht, Katherine D., Seelman and Michael, Bury . Handbook of Disability Studies . USA: Sage, 2001.\n\n8 Hardman, Michael., Drew, Clifford and Egan, Winston. Human Exceptionality: School, Community, and Family . London, Cengage Learning, 2010\n\n9 Gary L., Albrecht, Katherine D., Seelman and Michael, Bury . Handbook of Disability Studies . USA: Sage, 2001.\n\n10 Carol, Wren and Laura, Segal. College Students with Learning Disabilities: A Student Perspective . Chicago: Project Learning Strategies, DePaul University, 1985.\n\n11 Joan M., McGuire, and Jennifer. M., O\u2019Donnell, Helping Learning Disabled Students to Achieve: Collaboration Between Faculty and Support Services. College Teaching , 1989, Vol. 37(1), pp.29-32.\n\n12 Sheryl L., Day and Barbara J., Edwards. Assistive Technology for Postsecondary Students with Learning Disabilities. Journal of Learning Disabilities . 1996. Vol.29,(5), pp. 486-492.\n\n13 Barbara Gross, Davis. Tools for Teaching . San Francisco: Jossey-Bass Publishers, 1993.\n\n14 Jon, Nakasone. Assistive Technology and Accessibility . 1999. Web.\n\n15 Gary L., Albrecht, Katherine D., Seelman and Michael, Bury . Handbook of Disability Studies . USA: Sage, 2001.\n\n16 Sheryl, Burgstahler . Working Together: People with Disabilities and Computer Technology . 2010. Web.\n\n17 Allison C., Carey., Mark G., Friedman, Diane Nelson, Bryen and Steven J., Taylor. Use of Electronic Technologies by People With Intellectual Disabilities. Mental Retardation . 2005, Vol. 43,( 5), pp. 322-333.\n\n18 Press, Sedgwick. The Complete Directory for People with Disabilities . New York: Grey House Pub., 2001.\n\n19 Stephen D., Pell, Robyn M., Gillies, and Marjorie, Carss. Relationship between use of technology and employment rates for people with physical disabilities in Australia: Implications for education and training programmes. Disability and rehabilitation. 1997, Vol. 19,(8) , pp.332-338.\n\n20 Rena B., Lewis. Assistive Technology and Learning Disabilities:Today\u2019s Realities and Tomorrow\u2019s Promises. Journal of Learning Disabilities. 1998. vol. 31(1) pp.16-26.\n\n21 Marcia J., Scherer. Assistive technology: matching device and consumer for successful rehabilitation. USA: American Psychological Association, 2002.\n",
        "label": "human"
    },
    {
        "input": "How Computers Work: Components and Power Coursework\n\nTable of Contents\n 1. Introduction\n 2. Major Components of a Computer\n 3. How to Power a Computer\n 4. Conclusion\n 5. References\n\nIntroduction\n\nOver the years, a number of technological advancements have occurred in various fields, thereby making man\u2019s work easier. In the field of Information Technology, one of the most stunning developments has been the invention of computers (PCs). A computer refers \u201cto a device that can receive some input and process it in order to produce an output\u201d (Microsoft, 2013). For instance, a human brain also functions like a computer.\n\nHowever, the focus of this paper will be on computers as electronic devices, which contain a microprocessor. A microprocessor refers to \u201ca small electronic device that carries out complex calculations in the blink of an eye\u201d. A number of electronic gadgets such as calculators, television sets, mobile phones among others that people use everyday contain microprocessors.\n\nNevertheless, a computer (PC) is the most commonly known gadget with a microprocessor. In1970s Ed Roberts came up with the first marketable computer, which used a microprocessor designed by Intel. Roberts\u2019 computer came to be known as the Altair 8800, thereby ushering in the era of personal computers. Therefore, this paper examines how computers work.\n\nMajor Components of a Computer\n\nA computer is made up of several components, which are usually assembled in order for it to function. Some of the most common components of a computer include a case, motherboard, power supply, Central Processing Unit (CPU), random access memory, drives, cooling devices, and cables. All these parts make up a computer\u2019s central processing power.\n\nThe case of a computer includes a keyboard and a screen in case of a laptop. On the other hand, a desktop has a separate monitor (screen), keyboard, and CPU as shown in figure 1in the appendix. The various parts of a desktop are often connected using cables.\n\nThe motherboard of a computer refers to the circuit board inside a computer. The various components of a computer connect in one way or another with the motherboard. For instance, the complementary metal-oxide semiconductor is directly attached to the motherboard. The complementary metal-oxide semiconductor \u201cstores some information, such as the system clock\u201d.\n\nIn order for a computer to function, it must have a power supply. \u201cPower supply is that component of a computer that connects it to a power source such as a battery in case of a laptop or an outside power outlet in case of a desktop\u201d. The component of a computer that contains a microprocessor is the Central Processing Unit.\n\nThe CPU of the computer determines the ultimate performance of a computer because it is the core-processing unit of a computer as shown in figure 2 in the appendix. On the other hand, the Random Access Memory acts as a buffer to the CPU, whereby it stores information while the CPU process it.\n\nAdditionally, computers have drives, which stores information. For example, the hard drive stores the operating system and software. Computers also use cooling devices like fans in order to control the amount of heat generated. Apart from the major components, computers also have ports, peripherals and expansion slots, which allows for interaction between the computer and its user, as well as, other computers.\n\nHow to Power a Computer\n\nWhen a computer user presses the power button, the computer will first undergo a number of internal processes referred to as the boot process. The Basic Input-Output System (BIOS) perform the booting process. The BIOS \u201cis software stored on a flash memory chip and embedded on the motherboard\u201d. The BIOS also links the different hardware in a computer.\n\nWhen the power button is pressed, it activates the computer\u2019s power supply, which in turn powers the motherboard and other computer\u2019s components. The computer then undergoes power-on self-test (POST), whereby the POST checks any failures in the computer hardware by giving signals through beeping.\n\nA single beep indicates that everything in the hardware is functioning appropriately whereas a series of beeps signals a failure. Afterwards the computer will display information regarding the boot process on the screen/ monitor.\n\nSome of the information usually displayed includes the size of RAM, drives detected, specifications of the processor, and the manufacturer of the BIOS. However, in most computers this kind of data has been replaced with a splash screen that only shows the logo of the producer\n\nThe BIOS will then access the boot disk that is the hard disk containing a computer\u2019s operating system. After accessing the boot disk, the BIOS feed the boot loader into the computer\u2019s short-term memory (RAM). The boot loader will then search for the operating system and launches it into the RAM. Once the operating system has been launched, the computer user can be able to interact with the PC.\n\nThe type of operating system that a computer has determines how the computer works hence the user can control the PC via the operating system. There are different types of operating systems. For instance, the most commonly used OS is the Microsoft Windows and Linux version. However, Apple computers mostly use Mac OS X.\n\nThe operating system performs a number of tasks as follows. First, the OS manage a computer\u2019s processing task whereby it determines the data for processing by breaking it down into controllable amounts for the CPU to process. Second, the OS control a computer\u2019s Random Access Memory by ensuring that the RAM contains considerable data at a given time.\n\nThird, the OS performs device management whereby it acts as a link between the internal components of a computer and other external devices that are connected to the computer. For example, the OS interprets inputs via the key board or the mouse into a form that the user can understand. Additionally, the operating system directs how information should be stored within a computer.\n\nA PC can store data in hard disk or external devices. Last but most important, the operating system performs application interface. The OS enables \u201cthe exchange of data between the software programs and the PC\u201d (Microsoft, 2013). Every application program is usually designed for a certain version of an OS.\n\nConclusion\n\nFrom the above discussion, it can be concluded that computers have made man\u2019s work easier because the PCs can process information within the shortest time possible. A PC must undergo a boot process before the user can interact with it.\n\nAdditionally, in order for a PC to function, it must have all the major components like the CPU, motherboard, and power supply among others plus software systems such as the operating system and application programmes.\n\nReferences\n\nMicrosoft . (2013). Introduction to computers . Web.\n\nWalters, G. (2001). The essential guide to computing. New York: Prentice Hall Professional.\n\nYoung, R. (2009). How Computers Work: Processor and Main Memory. New York: Springer.\n",
        "label": "human"
    },
    {
        "input": "Computer Mediated Learning Analytical Essay\n\nThere are many definitions available that attempt to explain learning processes delivered, enabled or mediated using electronic technology for the explicit purpose of learning. These definitions include such terms as distance learning which may occur even in the absence of electronic technology. Distance learning has for a long time been performed using correspondence (Fee, 2009).\n\nAn example of these terms is computer mediated learning which is used to define any learning process that is facilitated by electronic technology. Computer mediated learning therefore implies that the learning could be facilitated by both one way and two way learning exchanges as well as learner to learner exchanges.\n\nAmong the terms that are often used and erroneously taken to have the same meaning as computer mediated learning is the term computer authored learning. Computer authored learning defines a learning process that is designed or engineered through the use of computer technology (Distefano, Rudestam & Silverman, 2004).\n\nThis mode of learning involves the use of authoring tools which are computer programs. These programs are used to create powerful scripts that can create useful learning content. Prior to the emergence of the internet such tools were commonly used to ease the process of learning. In some reports it is stated that by 1993, there were over 165 authoring tools available for use by instructors (Distefano, Rudestam & Silverman, 2004).\n\nHowever, with the emergence of the internet web authoring tools have fast replaced these tools. In fact many web page developers have taken to use the authoring to describe the process of web page creation (Distefano, Rudestam & Silverman, 2004). One major advantage that is the result of the emergence of the World Wide Web is the ease with which content can be changed and updated.\n\nThis appears to have been a major short coming of the early models of computer authored learning tools. Other terms that have long been used interchangeably with computer mediated learning are computer based training and computer assisted instruction. These approaches have for long used embedded questions in an instructional module to determine paths for individual students (Ifenthaler & Seel, 2010).\n\nOne of the most common uses of this approach is in the testing of students located in various remote locations. The modern approach is to use the results from the tests to guide the provision of instruction based on student needs (Ifenthaler & Seel, 2010). As the discussion has indicated many of these different terms were used based on the needs of the vendor.\n\nThe goal appeared to have been to place the vendor at the center stage. Based on these many definitions there has been a need to come up with a single term that will provide a convenient umbrella for all these different activities. (Fee, 2009). Terms such as distance learning that emerged from learning institutions though authoritative are still quite diverse.\n\nIt is for this reason that the term e-learning is fact becoming a more convenient umbrella term than any of the previously mentioned definitions. According to the American Society for Training and Development, \u201ce-learning covers a wide set of applications such as web based learning, computer based learning, virtual classrooms and digital collaboration\u201d (Fee, 2009).\n\nThis definition appears suitable especially due to the fact that this is the world\u2019s largest professional body for professional development and learning. The organizations membership is reported to be in the range of 70,000 and the body can be found in almost 100 countries across the globe (Fee, 2009).\n\nThis large membership and global representation makes this body more suitable than most to define learning that involves the use of computers and digital technology. It is possible that as times goes by this term and the accompanying definition will replace the many confusing terms that have been used both in the past and today to describe the role and interaction between computer, students and instruction.\n\nReferences\n\nDiStefano, A., Rudestam. K. E., &Silverman, R. J. (2004). Encyclopedia of Distributed Learning . Thousand Oaks, CA: Sage Publications Inc.\n\nFee, K. (2009). Delivering E-Learning: A complete strategy for design, application and assessment . London: Kogan Page Limited.\n\nIfenthaler, D., & Seel, N. M. (2010). Computer-Based Diagnostics and Systematic Analysis of Knowledge . New York: Springer Science + Business Media LLC.\n",
        "label": "human"
    },
    {
        "input": "Computer Technology in the Last 100 Years of Human History Analytical Essay\n\nTable of Contents\n 1. Education\n 2. Work\n 3. Communications\n 4. Conclusion\n 5. Works Cited\n\nThe debate about the most significant creation within the last 100 years of human history is not straightforward because different people base their augments on various criteria leading to oppositions about the topic.\n\nTo address this debate, this analytical essay will try to persuade a mixed audience in terms of gender and age. The audience will be composed of male and female college students aged 16 to 21 years.\n\nThe argument supporting the topic will focus on a thesis articulating that computer technology is the utmost invention influencing the youth. The essay will, in addition, highlight how computers influence younger generations constructively and unconstructively.\n\nThe positive influences of computer technology on younger generations are immeasurable despite its obvious negativity. The main purpose of inventing computers was to help humans solve compound arithmetic problems.\n\nThe outcomes of the invention may be comprehended in terms of VIP communication, space exploration, the Internet, email and the web, television broadcast, movie production and almost every aspect of human living.\n\nFollowing the invention and launch of personal computers, every aspect of younger generations living has been affected. The youth utilize computers in all aspects of their living such as writing assignments, doing research, communicating, listening to music, viewing movies, and traveling.\n\nActually, the younger generations encountered their first computer before the birth through ultra sound scanning during the pregnancy scan.\n\nEducation\n\nComputer technology is the foundation of contemporary education systems across the world. Nowadays, students must possess computers skills because lecturers use computer technologies such as screen projectors and online classes to teach.\n\nFurthermore, writing assignments entails the use of personal computers and so does the submission of assignments for awarding grades (Jacko and Sears 414).\n\nWork\n\nIn the most developed nations, students look for summer and part time employment opportunities using computer technology. In fact, modern day employers and recruiting agents require job applicants to submit their credentials electronically. Electronic submission is attainable by means of using computer technology.\n\nWith the introduction and growth of computer technology, contemporary places of work have experienced remarkable changes. Most students are hopefulness that they will find job opportunities after graduating from colleges or other tertiary institutions.\n\nComputer technology, such as mobile telephone communication, video conferencing, and virtual workrooms, have created an interconnected world where workers do not leave their places of residence to go to their work (Jacko and Sears 416).\n\nStudents should appreciate the vast opportunities the computer technology presents the students with. Many students are embracing computer technology in finding work to pay for school, housing, and food (Jacko and Sears 415). In view of this, computer technology has made freelance work possible.\n\nMoreover, students who find summer season jobs in organizations, such as MacDonald\u2019s, Starbucks, Wal-Mart and other companies offering temporally work to students need computer technology to communicate with customers and senior managers.\n\nStudents, who do not have computer skills, are likely to struggle hard to find work.\n\nCommunications\n\nCurrently, almost every student has a cell phone, email address, or asocial media account. These communication tools are based on computer technology, and are the foundation of younger generations\u2019 living. The youth spend more time using computer technology than they do using non-computer technology.\n\nComputer technology has eased the harnessing of the Internet, which forms the basis of modern communication. Today, younger generations use the Internet to connect, work, and socialize with each other (Girard and Girard 265). Newspaper readership has reduced because of computer technology.\n\nIn contrast to older generations, younger generations read newspapers less because the latter prefers to read news items using computer technologies, such as PDAs, Smart Phones, and Tablets (OECD 44).\n\nResearch has confirmed that younger generations continue to spend more time using mobile phones for texting or sending SMS (Wilska 441). Mobile phones base their functionalities on computer technologies such as Java and Android programs.\n\nComputer technology continues to influence younger generation communication aspects through television broadcasting and motion pictures viewership.\n\nThe scale of the information disseminated through television transmission, movies screening and the Internet could help comprehend the influence of computer technology on human communications.\n\nYounger generations use these media source to obtain information, therefore, these generations are the most influenced by the technology. Computers have enabled continuous transmission of information across the globe (Castells, Fernandez-Ardevol and Qiu 161).\n\nWhile computer technology has presented huge benefits to the youth, it has its detriments. The major downside of computer technology is over dependence, which affects social development.\n\nA substantial number of young people in colleges and high schools are addicted to computer technologies, such as computer games, Internet pornography, and social media.\n\nThe young face the challenge of developing poor social skills as some prefer to spend their leisure time playing computer games and chatting on social media instead of interacting with people physically (Kirsh 377).\n\nComputer technologies are criticized for making people indolent and reducing their intellect. A study done in Britain established that over 50% of children aged less than 12 years could not read time on an analog watch, but could comfortably identify time on a digital watch (Womack 1).\n\nDigital watch uses computer technology to display time. What is more, obesity is another factor most prevalent in countries with high penetration levels or use of computer technology.\n\nSo, inactive lifestyles such as watching television, and playing computer games could be linked to computer technology. These two aspects of computer technology mostly influence young generations.\n\nComputer technology is detrimental to the lives of younger generations because it fails. Since these generations over depend on computer technology, a small malfunction could lead to late tendering of assignments, loss of work and money, and increase in stress levels among other negative effects.\n\nFor instance, a faculty may schedule an online examination at 14:00, but computer problems such as low Internet connectivity, malware or virus infection or a computer failing to boot (Andrews 245), could mean rescheduling of the examination for another time or in another location. Such inconveniences lead to loss of time and resources.\n\nConclusion\n\nThe very nature of modernity signifies that computer technology is the most significant invention to happen within the last 100 years of human history. Computer technology influences younger generations more than it does older generations.\n\nComputer technology has been with man for roughly 50 years, but its influence on aspects of modern societies is immeasurable. Computer technology is found everywhere from modern banks, shopping outlets, educational institutions to workplaces.\n\nComputer technology remains the influential and vital invention of the twentieth century. However, this technology is still nascent in its capabilities despite being 50 years old. Soon, computer technology will overtake other inventions and profoundly influence more human society living aspects than it does today.\n\nWorks Cited\n\nAndrews, Jean. A+ guide to hardware: managing, maintaining and troubleshooting. Boston, MA: Course Technology, 2010. Print.\n\nCastells, Manuel, et al. Mobile communication and society: a global perspective. Cambridge, Mass: MIT Press, 2007. Print.\n\nGirard, John P and JoAnn L Girard. Social knowledge: using social media to know what you know. Hershey, PA: Information Science Reference, 2011. Print.\n\nJacko, Julie A. and Andrew Sears. The human-computer interaction handbook: fundamentals, evolving technologies, and emerging applications. New York: Routledge, 2003. Print.\n\nKirsh, Steven J. \u201cThe effects of violent videogames on adolescents: The overlooked influence of development.\u201d Aggression and Violent Behavior 8.4 (2003): 377-389.\n\nOECD. News in the Internet age: new trends in news publishing. Paris: Organization for Economic Co-operation and Development, 2010. Print.\n\nWilska, Terhi-Anna. \u201cMobile Phone Use as Part of Young People\u2019s Consumption Styles.\u201d Journal of Consumer Policy 26.4 (2003): 441-463.\n\nWomack, Sarah. \u201cYoung children cannot tell time on old clocks.\u201d The Telegraph 17 September 2002: 1.\n",
        "label": "human"
    },
    {
        "input": "Computer Adaptive Testing and Using Computer Technology Research Paper\n\nIntroduction\n\nThe growing use of computer technology has increased an interest in adaptive testing when it comes to tutoring and testing systems. Computer adaptive testing (CAT) is a form of educational measurement that has been designed to adapt the examination of proficiency in testing activities. It is a computer based test that adapts to the examinee\u2019s ability level which makes it more of a tailor made test.\n\nComputer adaptive testing is slowly replacing the traditional methods of testing commonly referred to as the paper and pencil tests. The increasing developments and innovations in computer technology have began to render the paper and pencil mode of testing obsolete as more educational institutions and training facilities incorporate the use of computers in their examination testing activities (Setchi, 2010).\n\nComputer adaptive testing has been seen to be a theoretically sound and efficient method of testing large scale programs. It has also become more feasible in educational research and practice. The recent developments in e-learning technology have enabled educational institutions to start incorporating online instructions and online testing to their examination programs.\n\nThe continued computer and technology developments will move paper and pencil testing towards a more computer based testing scenario. Computer adaptive testing is mostly carried out in large educational institutions, or in the certified and licensed examination centers (Tao et al, 2008).\n\nHistory of Computer Adaptive Testing\n\nThe concept of adaptive testing can be traced back to over a century ago. The first adaptive test developed in the early 1900s was considered to be the individually administered Binet-Simon intelligence test that was developed by Alfred Binet. This test involved administering various subtests based on the examinee\u2019s current ability level.\n\nIf the examinee passed all of the subtests in the Binet-Simon intelligence test then a higher ability of subtests was administered to the examinee. If the examinee failed all of the subtests at any given level of ability, then the test was terminated.\n\nThis made the Binet test to be referred to as an adaptive test because of the different subtests that measured an individual\u2019s ability and knowledge level. After the Binet test, two more testing methods were developed in the 1940s. These were the staircase method and the sequential analysis system (Ayala, 2009).\n\nThe staircase method of testing involved the adjustment of ability levels to meet those of the examinee. This method was mostly used by psychophysicists and experimental psychologists in examining their patients. The sequential analysis system was similar to the Binet test where examinees were given tests based on the current level of ability.\n\nIn 1951, Hick designed an adaptive testing approach that would be used as a foundation for adaptive testing today.\n\nHe developed an adaptive test based on the concept that an intelligence test should be a branch process that has questions with a 0.5 chance of being answered correctly. Patterson further developed Hick\u2019s adaptive test in 1960 by taking a pool of questions and arranging then in a way that the examinee would receive a harder question if they got the previous one correct (Ayala, 2009).\n\n1970 saw Lord developing a testing theory that would create a tailored testing program. The tailored test as Lord described it would be used to attain a better measurement of the examinees ability levels by selecting and administering the relevant questions and testing criteria.\n\nHowever many critics disputed his test as not being viable and feasible because it produced poor results. This later became to be an ironic twist as his work came to mark the beginning of tailored testing procedures and item response theory testing (Ayala, 2009).\n\nMethodology used in Computer Adaptive Testing\n\nComputer adaptive tests are assessment tools that have a theoretical background with the most common underlying psychometric theory being that of item response theory (IRT). The CAT theory of testing does not only involve the use of paper and pencil test questions but also a wide range of exercises that test the examinee\u2019s ability levels.\n\nThe CAT tests mostly use dichotomous item response theory models where the examinee\u2019s answers to a question are evaluated to be either correct or incorrect. The process of computer adaptive testing involves administering questions known as items to the examinee one at a time.\n\nThe presentation of each item or question and the decision to finish the test are dynamically adopted options that are based on the examinee\u2019s responses and response rate (Lester & Paraguacu, 2004).\n\nThe computer adaptive test basically selects questions that are based on the information given by the examinee in the previous question so as to maximize the precision of the exam (Thissen & Mislevy, 2000). The CAT usually applies an iterative algorithm that first estimates the student\u2019s knowledge level.\n\nThis iterative algorithm usually involves examining all the items that have not yet been administered to the examinee so as to determine the next best question to ask the examinee. This step is usually based on the previous item estimations of the examinee\u2019s knowledge and ability levels. The next step of the CAT will involve presenting the chosen item/question to the examinee who then answers it correctly or incorrectly.\n\nThe third step will involve updating the ability estimate of the examinee based on all the previous answers to the items or questions. The fourth step involves a repetition of steps one to three until the termination criterion is met adequately (Lester & Paraguacu, 2004).\n\nThe number of questions that are usually contained in a computer adaptive test are not fixed or similar to tests that have been presented to the other examinees. Also in the computer adaptive test, nothing is usually known about the examiner of the test prior to the administration of the first item. The item algorithm is usually started by selecting an item that is of an easy, medium or difficult nature compared to the previous item.\n\nThe adaptive nature of the CAT ensures that different examinees receive different tests. The psychometric technology used in administering the test allows for the equitable computation of scores based on the item response theory (Green, 2000).\n\nOne of the major elements that are used in developing the computer adaptive test is the item response model also known as the IRT model which determines how the test taker will respond to a question through the use of their current knowledge and ability levels.\n\nThe next element used in developing a CAT is an item pool which is a certain amount of questions that have been calibrated to measure the different knowledge levels of the examinee. If the item pool is well developed and calibrated, then the computer adaptive test will be successful in measuring an individual\u2019s knowledge ability.\n\nItem selection is the other component that is used in developing the computer adaptive test where the psychometrics of the test chooses the next item to be administered to the examinee. This selection is usually based on the current estimation of the individual\u2019s knowledge ability during the test.\n\nThe termination criterion is the basis used to decide when the test will be completed. This is mostly based on the type of test that has been administered and its purpose to the examinee (Lester & Paraguacu, 2004).\n\nThe basic purpose of the item response theory in adaptive testing is to measure the response of the examinee during and after the test. The IRT models the relationship between an examinee\u2019s ability level on the trait being measured by the item and the examinee\u2019s response to the item.\n\nIt uses estimating scores to predict the items and the performance of the examinee in the test. The IRT model mathematically describes the relationship that exists between a person\u2019s trait level and the performance of the examinee in the item (Tsang, 2010).\n\nThere are three item response models that are used in the item response theory. They include the one parameter IRT model, the two parameter model and the three parameter IRT models. The one parameter model which is also known as the Rasch model presents the relationship that exists between an examinee and the difficulty of the items or questions in the test.\n\nThe two parameter model which is known as the sequential probability ratio test (SPRT) model involves selecting items in the test in a random manner. The sequential probability ratio is usually calculated according to the item response of the examinee (Tsang, 2010).\n\nThe use of IRT in adaptive testing is based on two major principles which are the measurement of the student\u2019s performance as an unknown numeric value that is explained by their knowledge level and the performance of a student which can be explained by an estimated knowledge level that answers an item i which can be predicted through the use of probabilistic theories and modeled by means of a function known as the item characteristic curve (ICC).\n\nThis curve determines the probability of a student with a certain level of knowledge, \u03b8 , being able to answer an item correctly. Each item or question must be able to define an item characteristic curve based on the previous answer given by the examinee (Lester & Paraguacu, 2004).\n\nThe three parameters that are usually used to determine the shape of the ICC curve are the discrimination factor (a), the difficulty factor (b), and the guessing factor. The discrimination factor which is represented by the slope of the curve is usually used to determine the success of the examinee\u2019s responses to the item presented to them.\n\nIf the item is difficult, then their success rate will be low and if the item is easy, the success rate will be high. The difficulty factor of the ICC curve is used to determine the examinee\u2019s knowledge levels in terms of answering an item correctly or incorrectly.\n\nThe guessing factor of the ICC curve is the probability that a student with no knowledge at all will answer an item correctly through the random selection of a response (Lester and Paraguacu, 2004).\n\nAdvantages and Disadvantages of Computer Adaptive Testing\n\nThe advantages of using computer adaptive testing are that they provide uniform scores for most of the examinees or test takers, the number of questions that are usually presented to the examinee are usually minimized by 50 percent while still ensuring there is a high level of precision. This minimization ensures that the examinee has enough time to finish the test.\n\nThe computer adaptive test also ensures that the test taker does not waste time answering difficult or easy questions. The minimization of the test time also ensures that the institution administering the exam saves on test costs and seat time.\n\nThe computer adaptive tests are more preferable when compared to the traditional testing methods as they show the examinee their results immediately after the test is taken. CAT reduces the examinee\u2019s exposure to some items or questions as different examinees usually receive different sets of items or questions (Thissen & Mislevy, 2000).\n\nOther advantages of computer adaptive tests are that the selection of items is usually tailored according to the individual examinee. This ensures that irrelevant questions are omitted from the test thereby enhancing the responder\u2019s compliance. The CAT test also reduces the floor and ceiling effect and it allows the examinee to specify the desired degree of precision to be used in the test.\n\nThe examiner can be able to identify the individuals that make inconsistent response patterns during the test. Items and groups of respondents who have a similarity in their responses can also be identified through the use of computer adaptive testing (Fayers & Machin, 2007).\n\nThe disadvantages of CAT are that comprehensive item pools have to be developed and tested before the test is designed. The calibration of the item pool is usually a major challenge when developing a CAT because all the items of the test have to be pre-administered to a sizeable sample after which they are analyzed. To achieve this process, the examinee\u2019s responses are recorded but they do not contribute to the examinee\u2019s total score.\n\nThis process is referred to as pre-testing or pilot testing and it usually presents logical and ethical challenges. All the items of the test have to be pre tested with a large sample of 1,000 examinees or more so as to obtain a stable item. Another disadvantage of the computer adaptive test is that the item algorithm usually has exposure control that is conditioned on the examinee\u2019s knowledge ability.\n\nThis exposure control algorithm is not usually controlled making it possible for some items in the test to be similar for more than two examinees that have the same knowledge ability. This is a serious problem as groups sharing the same items might as well have a similar functional ability level (Thissen & Mislevy, 2000).\n\nAnother disadvantage of the computer adaptive test is that the examinee cannot be able to review past items or a response to previous questions as the CAT does not allow it. This means that they cannot be able to review their response which is usually possible with the paper and pencil test.\n\nAnother disadvantage of adaptive testing is that if the examinee answers one item incorrectly the next item will be easier than the previous one.\n\nThis creates a situation where an examinee can be able to detect the incorrect answers and correct them. Once the questions become easier the examinee reviews the items that had an incorrect answer, assigning them a correct answer which allows them to achieve a very high score (Wainer & Mislevy, 2000).\n\nComparison between CAT and Paper and Pencil Testing\n\nWhen a computerized adaptive test is developed as an equivalent to a paper and pencil test, the construct validity of the CAT has to be based primarily on the comparison that exists between the paper and pencil test and the computer adaptive test. The items that are usually developed for a CAT test basically measure the same constructs that are contained in the paper and pencil test items.\n\nCAT, paper and pencil tests are usually highly correlated and these correlations should be similar when corrected for measuring errors. The covariance structures that exist between paper and pencil tests and computer adaptive tests are usually used to demonstrate the similarity that exists between the two types of tests. Another comparison of the two tests is that some ability measures are usually statistically equivalent.\n\nThese measures include the sequence of the items, the composition of the items, the kind of information contained in the item and the level of ability being measured by the test (Gaskill & Marshall, 2007).\n\nAnother comparison that exists between the paper, pencil test and the computer adaptive test is that the CAT is usually constructed to measure the same traits or ability levels that the paper and pencil test seeks to measure.\n\nThe paper pencil tests and the computer adaptive tests usually highlight the important parts of a question where the paper pencil tests underline the important phrase of the item while the CAT highlights the phrase and relevant section of the item.\n\nAnother comparison is the constructs that are measured by the paper and pencil and CAT are usually similar. The paper and pencil item parameters are usually used as estimates for developing the item pools that are used in the computer adaptive test (Kolen & Brennan, 2004).\n\nAnother comparison that exists between the paper-pencil and computer adaptive tests is that both of these tests are used to measure the performance of the examinee. The mode and paradigm effects that are used to test the examinee are similar for both tests. Wang and Shin (2010) have identified three criteria that can be used in evaluating the comparability between the CAT and the paper, pencil tests.\n\nThese criteria are validity, psychometric criteria and statistical assumptions or test administration. The validity criterion is mostly used to assess whether the constructs measured by both tests are similar in nature.\n\nThe psychometric criterion evaluates the comparability between the paper pencil test and the CAT by analyzing properties such as the reliability of the test, the measurement of errors and the distribution of the examinee\u2019s scores. Statistical assumption or test administration is similar for both tests as their main purpose is to measure the examinee\u2019s knowledge level and ability (Wang & Shin, 2010).\n\nArmed Services Vocational Aptitude Battery (ASVAB) and CAT-ASVAB\n\nThe Armed Services Vocational Aptitude Battery (ASVAB) test is an aptitude test that is used by many Armed Forces around the world in their recruitment and selection exercises. The ASVAB is made up of nine subtests that are used to measure the aptitude levels and knowledge abilities of armed forces recruits.\n\nThe nine subtests usually examine various subjects that range from mathematical applications to general science topics or vocabulary related topics. The results of the ASVAB usually determine whether an individual qualifies to join the Armed Forces or not.\n\nThe purpose of the ASVAB is not to test an individual\u2019s intellectual capacity but to determine the ability of an individual and how this ability can be used to train the individual for a specific job in the armed forced (Kaplan, 2009).\n\nASVAB tests were originally designed during World War II as a tool of selecting armed forces recruits to join the military. The ASVAB tests were designed to provide a general means of measuring a person\u2019s intellectual ability as well as their aptitude levels. These levels and abilities were used to determine which branch of the military the person would be placed in.\n\nThe first form of the ASVAB was the Armed Forces Qualification Test (AFQT) which was developed in 1948 to be used as a standard screening test. The AFQT test underwent various changes over the years so that it could become more accurate when screening individuals.\n\nThe changes were meant to gear the tests to provide accurate scores that would be used in measuring the success of military training programs. The results of the tests were used to help the recruits to select which branches of the military they wanted to serve in. The ASVAB was basically developed as a result of these changes to the AFQT test (Grayson, 2004).\n\nThe very first ASVAB test was administered in 1968 but it was not used for recruitment or selection purposes. It was however used in 1976 to recruit individuals to join the armed forces. The basic structure of the ASVAB remained the same for several years until it underwent some changes in 1980 that saw the items of the test undergoing changes to ensure they were up to date with technological innovations.\n\nThe computerized version of the ASVAB test was developed in 1993 and it became operationalized in 1996. The latest revision to the ASVAB test was performed in 2002 where two subtests were removed from the ASVAB creating room for the addition of one ASVAB subtest.\n\nThe two subtests that were removed included the numerical operations and the coding speed which were replaced with the assembling objects subtest. The items/questions in the ASVAB were also updated to ensure they were up to date with the technological advancements in the environment (Wiley, 2010).\n\nThe ASVAB has been viewed to be one of the most commonly used ability tests especially when it comes to measuring an individual\u2019s capabilities. The ASVAB test mostly has its background from the mental testing movement as it represents the state of art that exists in multiple aptitude test batteries. The first large scale adaptive test to be developed in the late 1980s was the ASVAB.\n\nAdaptive testing during this time was still at its infancy and had not become as common as it is now. The large scale nature of the ASVAB was designed to select two million people to join the armed forces in one year. This made it to be the largest single testing program in the world.\n\nThe ASVAB was later developed into an adaptive test by incorporating the computer based testing system into its structure. This allowed the test to save on time and money that was spent in examining the two million people in one year (Wainer et al, 2010).\n\nThere are various versions of ASVAB tests that have been developed for testing purposes and they include the institutional version, the production version, the computer adaptive screening test (CAST) and the armed forces classification test (AFCT). The institutional version of the ASVAB is a test that is mostly used by the military in high schools and institutions of higher learning.\n\nThis type of test is usually administered in collaboration with the Department of Defense and the Department of Education in the United States. The purpose of this test is to provide school counselors with the necessary information that will be used in recommending career options to high school students.\n\nThe production version of the ASVAB test is usually used in selecting and recruiting individuals into the armed forces. It also identifies which military jobs an individual is qualified for based on their test scores (Wiley, 2010).\n\nThe production version of the ASVAB test is usually administered in two forms which are the paper form and the computerized test. The computerized version of the ASVAB test is referred to as the CAT-ASVAB test and is commonly used in processing large numbers of army recruits. The computer adaptive screening test (CAST) is mostly a recruiting test that is used to screen the various applicants or individuals who have applied to work in the armed forces.\n\nThe purpose of the CAST test is to reduce the number of recruits to a more manageable number. This is accomplished by administering small mini tests that are used to determine which individuals will progress to take the main ASVAB test. The armed forces classification test (AFCT) is mostly given to those military personnel who want to be retrained for a different job within the armed forces.\n\nThe contents that make up an ASVAB subtest include word knowledge, arithmetic reasoning, general science, paragraph comprehension, mathematical knowledge, electronical information, auto and shop information, assembling objects and mechanical comprehension (Wiley, 2010).\n\nThe computerized version of the ASVAB test is known as the computer adaptive test- ASVAB. The CAT-ASVAB test has similar features and similarities to the paper and pencil test as it also measures the same aptitude levels of the examinee. The test usually displays the questions and scores of the answers.\n\nThe computerized testing version of the ASVAB is one of the most thoroughly researched tests for examining an individual\u2019s proficient capability in recent times. The CAT-ASVAB was the first large scale adaptive battery test to ever be administered in the recruitment and selection of armed forces recruits since 1976 (Wall & Wall, 2010).\n\nThe paper and pencil ASVAB test was the most commonly used recruitment tool before the CAT-ASVAB was developed. Before it was entirely launched into the market, the CAT-ASVAB test underwent some experimental designs to collect important data that would be used in examining the adequacy of the adaptive testing algorithms.\n\nThe purpose of the experimental design was to develop a full battery CAT version of the ASVAB that would measure the same dimensions as the paper and pencil ASVAB test. The items that were needed in the experimental design were the item pools, psychometric developments (item selection, and scoring) and the system of delivery.\n\nThe experimental CAT-ASVAB system was used in a large scale study between 1982 and 1984 to ascertain whether the CAT-ASVAB could replace the paper and pencil test. This exercise involved testing 7,518 recruits who were scheduled for a training exercise in 23 military training schools. They were tested by the paper and pencil ASVAB prior to recruitment and the experimental CAT-ASVAB before taking any basic training.\n\nThe results of the study showed that predictive validity and equivalent constructs could be obtained through the use of the CAT-ASVAB. These results were later used to fully implement the CAT-ASVAB in 1984 (Segall & Moreno, 2010).\n\nThe CAT-ASVAB is similar to the computerized adaptive testing in that it contains fewer questions when compared to the paper, pencil test which makes it easier to take and faster to complete. The items in the computerized adaptive ASVAB test are mostly tailored to an individual\u2019s knowledge ability level ensuring that the test does not administer simple or difficult items.\n\nThe items selected by the individual are usually based on whether they get the previous question right. The subtests that make up the CAT-ASVAB include general science, word knowledge, arithmetic reasoning, comprehension in paragraphs, electronic information, mathematical knowledge, auto and shop information, mechanical comprehension and the assembling of various objects (Wall & Wall, 2010)\n\nThe advantages of using the CAT-ASVAB over the paper and pencil ASVAB test is that the computerized test takes a shorter time to complete than the paper and pencil test. The test scores are usually released immediately and the computerized computation of the results minimizes the chances of errors. The CAT-ASVAB test can also be taken without any prior scheduling as is usually required for the paper and pencil test.\n\nThe main disadvantage of the CAT-ASVAB test that is similar to the computer adaptive test is that it does not allow the examinee to review the previously answered questions which is usually possible in the paper and pencil test. It also does not allow the examinee to skip difficult questions and move on to other questions in the test as it requires the examinee to complete the current item so as to move to the next.\n\nApplications of CAT Tests\n\nBecause of the adaptive nature of the CAT tests, these examination approaches have been used in the fields of education, organizational licensing and certification. There are an estimated 30 operational computerized adaptive testing programs in use around the world.\n\nThese programs are usually used to evaluate four to six million people every year who want to attain educational credentials or receive some form of licensing or certification. The number of CAT programs in use is increasing every year which can be attributed to the changes in technology (Fetzer et al, 2008).\n\nThe major CAT programs that are in use around the world include the armed services vocational aptitude battery (ASVAB) test which is used in the recruitment and selection of armed service personnel, the graduate management admission test (GMAT-CAT) which is administered to individuals wishing to join a masters program more specifically the Master in Business graduate program, the Microsoft certified professional exams that provides certification to information technology professionals and the American institute of certified public accountants exam (AICPA) that provides certification to public accountants, audit and tax professionals.\n\nCAT has also been used in fingerprint imaging and web based testing (Fetzer et al, 2008).\n\nEffects of Test Anxiety in CAT and Paper and Pencil Testing\n\nThe increased use of online testing methods such as the computerized adaptive testing has raised some serious concerns as to whether test anxieties can be properly managed before taking the test. While the CAT tests have proved to be more effective and efficient than the paper and pencil tests, concerns have been raised on whether CAT tests consider the cognitive and psychology demands of the test taker.\n\nPsychological and cognitive problems usually arise when the examinee has no control over the pace of the test. This is mostly so in the CAT test where the examinee lacks the ability to go back to previously answered questions. The CAT test also makes it difficult for the examinee to move on to the next question until they have answered the current question.\n\nThis increases their anxiety levels increasing the possibility of them answering a question incorrectly. When compared to the paper and pencil tests, the effects of anxiety in the CAT tests are much higher.\n\nConclusion\n\nComputer adaptive tests have become the most common form of testing for educational or professional purposes. This form of testing will continue to be in use given the increasing technological innovations and the need to produce efficient, effective and reliable test scores.\n\nThe traditional method of testing which is the paper and pencil test will soon be obsolete as more training institutions embrace the CAT and CAT-ASVAB testing methods in their recruitment or certification programs. The traditional methods will also be obsolete when it comes to examining large numbers of people in educational institutions or in army recruitment exercises.\n\nReferences\n\nAyala, R.J., (2009). The theory and practice of item response theory . New York: Guilford Press.\n\nFayers, P.M., & Machin, D., (2007). Quality of life: the assessment, analysis and interpretation of patient reported outcomes. England, UK: John Wiley and Sons Limited.\n\nFetzer, M., Dainis, A., Lambert, S., & Meade, A., (2008). Computer adaptive testing (CAT) in an employment context. Roswell, US: Previsor.\n\nGaskill, J., & Marshall, M., (2007). Comparisons between paper and computer based tests , British Colombia. Canada: Society for the Advancement of Excellence in Education.\n\nGrayson, F.N., (2004). Officer candidate tests. New Jersey: Wiley Publishing Inc.\n\nGreen, B.F. (2000). System design and operation. In Wainer, H. (Ed.) Computerized Adaptive Testing: A Primer . Mahwah, New Jersey: Lawrence Erlbaum Associates.\n\nKaplan (2009), Kaplan ASVAB: the armed services vocational aptitude battery. New York: Kaplan Publishing.\n\nKolen, M.J., & Brennan, R.L., (2004). Test equating, scaling and linking: methods and Practices. Netherlands: Springer Science.\n\nLester, J.C., & Paraguacu, F., (2004) Intelligent tutoring systems. Berlin, Germany: Springer Verlag Heidelberg.\n\nSegall, D.O., & Moreno, K.E., (2010). Development of the CAT-ASVAB. Web.\n\nSetchi, R., (2010). Knowledge-based and intelligent information and engineering Systems. Berlin, Germany: Springer Verlag Heidelberg.\n\nTao, Y.H., Wu, Y.L., & Chang, H.Y., (2008). A Practical Computer Adaptive Testing Model for Small-Scale Scenarios. Educational Technology & Society , Vol. 11 , No. 3, pp.259\u2013274.\n\nThissen, D., & Mislevy, R.J. (2000). Testing Algorithms. In Wainer, H. (Ed.) Computerized Adaptive Testing: A Primer . Mahwah, New Jersey: Lawrence Erlbaum Associates.\n\nTsang, P., (2010). Hybrid learning . Berlin, Germany: Springer Verlag Heidelberg Wainer, H., & Mislevy, R.J. (2000). Item response theory, calibration, and estimation. In Wainer, H. (Ed.) Computerized Adaptive Testing: A Primer . Mahwah, NJ: Lawrence Erlbaum Associates.\n\nWainer, H., Bradlow, E.T., & Wang, X., (2010). Testlet response theory and its Applications . Cambridge, UK: Cambridge University Press.\n\nWall, J. & Wall, E. (2010). McGraw-Hill\u2019s ASVAB basic training for the AFQT. New York: McGraw Hill Companies Inc.\n\nWang, H., & Shin, C.D., (2010). Comparability of computerized adaptive and paper- pencil tests. Test, Measurement and Research Services Bulletin , Issue.13, pp 1- 7.\n\nWiley (2010). The ASVAB in a nutshell . Web.\n",
        "label": "human"
    },
    {
        "input": "Computer R Us Company: Initiatives for Improving Customer Satisfaction Case Study\n\nIntroduction\n\nThe Computer R Us Company received numerous complaints about the services offered in their CompleteCare division.\n\nAfter thorough investigations into the complaints, the management established that the division was experiencing problems as a result of inadequacy of trained operators and problems with distribution and availability of parts.\n\nIn response to these problems, the management came up with four initiatives that aimed at improving customer satisfaction. In this paper, analysis will be carried using various tools to establish the effectiveness of the initiatives that were put in place.\n\nResearch design\n\nSampling technique\n\nThis research was conducted using research survey study approach. Data was collected using a questionnaire that had three sections. The first part required personal information, that is, age and gender. In the second section a Likert scale of ten points was used to collect some data.\n\nThe final section focused on determinants of customer satisfaction. Four questions were asked in this section and each had a Likert scale of ten points. The random sampling technique was used to select a sample of 500 customers (Kothari, 2004). The questionnaires were sent to the 500 customers and only 420 responded.\n\nIn order to collect the data necessary for this study, several steps will be taken to ensure that appropriate care is taken to protect the participants. There are no universally accepted determinants of customer satisfaction ((Verbeek, 2008).\n\nBesides, the results of previous studies do not give conclusive result on the most effective determinant. Therefore, the attributes used by the management of Computer R Us to improve the level of customer satisfaction are a sample of what other companies use (Zikmund, Babin, Carr, Griffin, 2012).\n\nAnalysis\n\nThe first test show that the overall satisfaction is statistically different from 6 out of 10. The calculated mean is 4.4881 and it is less than the goal. The result of the second question shows that the overall satisfaction of female customers is higher than that of male customers.\n\nTherefore, there is a need to improve the level of satisfaction of the male customers. The results of the third question indicate that there is no difference in the level of satisfaction across the different age groups. Further, tests on question five shows that there is no difference in gender composition across the five age groups.\n\nThe fifth test reveals that customers tend to be more satisfied with the loyalty rewards program than response times in the CompleteCare division. Therefore, the management needs to improve the response time in the division. The final test shows that all the four initiatives have a potential of improving customer satisfaction.\n\nFurther, response time of the CompleteCare division and level of advice CompleteCare staff provides on Computers R Us products have more impact than the other two initiatives (Baltagi, 2011).\n\nRecommendations\n\nThe results of hypothesis testing show that the management did not achieve their goal. For the company to achieve the target of 6 out of 10, the management needs to consider the recommendations listed below.\n\n  * Decrease the response time of the CompleteCare division. This can be achieved by increasing the number of well trained personnel and equipment that can facilitate service delivery at the division. The company should introduce a rating system that can be used by customers continuously.\n  * The management should also focus on improving the level of satisfaction of the male customers.\n  * The management should train the CompleteCare staff on a continuous basis. This will improve the quality of advice they give clients.\n\nReferences\n\nBaltagi, G. (2011). Econometrics. New York: Springer Publisher\n\nKothari, J. (2004). Research methodology: methods and techniques . New Delhi: New Age International (P) Limited Publishers.\n\nVerbeek, M. (2008). A guide to modern econometrics . England: John Wiley & Sons.\n\nZikmund, W., Babin, B., Carr, J., Griffin, M. (2012). Business research methods . USA: Cengage Learning.\n\nAppendix: Hypothesis testing\n\nDoes the current level of customer satisfaction differ from management\u2019s goal of 6 out of 10?\n\nHypothesis\n\nH 0 : The current level of customer satisfaction = 6.\n\nH 1 : The current level of customer satisfaction \u2260 6.\n\nStatistical technique\n\nIn this case a one sample t-test will be used to test the hypothesis.\n\nJustification\n\nOne sample t-test is most suitable for evaluating a hypothesis that compares the actual mean and hypothesized mean.\n\nResults of the test\n\n                              Variable 1    Variable 2\nMean                          4.488095238   6         \nVariance                      5.505824526   0         \nObservations                  420           420       \nPearson Correlation                                   \nHypothesized Mean Difference  0                       \ndf                            419                     \nt Stat                        -13.20498454            \nP(T<=t) one-tail              7.85063E-34             \nt Critical one-tail           1.64849841              \nP(T<=t) two-tail              1.57013E-33             \nt Critical two-tail           1.965641842             \n                                                      \nt Stat                        -13.20498454            \nt Critical two-tail           1.965641842             \nP(T<=t) two-tail              1.57013E-33             \n\n\nInterpretation\n\nThe results show that t-calculated is greater than t-critical. Also, the p-value (1.57013E-33) is less than alpha (5%). Therefore, the null hypothesis will be rejected at the 95% confidence level. This implies that the current level of customer satisfaction differ from management\u2019s goal of 6 out of 10.\n\nIs there any difference between the overall satisfaction of male and female customers at Computers R Us?\n\nHypothesis\n\nH 0 : Overall satisfaction of male customers = overall satisfaction of female customers at computer R Us.\n\nH 1 : Overall satisfaction of male customers \u2260 overall satisfaction of female customers at computer R Us.\n\nStatistical technique\n\nIn this case, a paired sample t-test will be used to test the hypothesis.\n\nJustification\n\nA paired sample t-test is the most suitable for testing hypothesis that compared the mean of two related variables.\n\nResults of the test\n\nt-Test: Two-Sample Assuming Equal Variances\n\n                              Female        Male       \nMean                          3.589430894   5.75862069 \nVariance                      4.27564294    4.507873231\nObservations                  246           174        \nPooled Variance               4.371757391              \nHypothesized Mean Difference  0                        \nDf                            418                      \nt Stat                        -10.47338477             \nP(T<=t) one-tail              2.98994E-23              \nt Critical one-tail           1.648507149              \nP(T<=t) two-tail              5.97988E-23              \nt Critical two-tail           1.965655464              \n\n\nInterpretation\n\nIn the results, the mean and variance of overall satisfaction for the male is greater than that of the female group. Further, t-calculated is greater than t-critical. Also, the p-value is less than alpha (5%). Therefore, the null hypothesis will be rejected at the 95% confidence level. This implies that there is a difference between the overall satisfaction of male and female customers of the company.\n\nAre there any differences in the overall customer satisfaction across the following age groups: under 20, 21-30, 31-40, 41-50, 51 and over?\n\nHypothesis\n\nH 0 : There is no difference in the overall satisfaction across the various age groups.\n\nH 1 : The overall satisfaction of at least one age group is different from the others.\n\nStatistical technique\n\nIn this case, analysis of variance (ANOVA) will be used to test the hypothesis.\n\nJustification\n\nANOVA is the most suitable technique for testing hypothesis that entails comparing mean for more than one group. One way ANOVA will be used because there is only one independent variable.\n\nResults of the test\n\nAnova: Single Factor                                                          \nSUMMARY                                                                       \nGroups                Count     Sum  Average   Variance                       \nUnder 20              47        180  3.829787  6.579093432                    \n21-30                 109       501  4.59633   6.150356779                    \n31-40                 105       466  4.438095  5.786996337                    \n41-50                 107       485  4.53271   4.647504849                    \nover 50               52        253  4.865385  4.236425339                    \nANOVA                                                                         \nSource of Variation   SS        df   MS        F            P-value   F crit  \nBetween Groups        29.52282  4    7.380705  1.344941124  0.252492  2.393438\nWithin Groups         2277.418  415  5.487753                                 \n                                                                              \nTotal                 2306.94   419                                           \n\n\nInterpretation\n\nIn the results above, the value of F-calculated is less than the F-critical. Besides, the p-value is greater than alpha (5%). Therefore, the null hypothesis will not be rejected at the 95% confidence level. This implies that there is no difference in the overall satisfaction across the various age groups.\n\nAre there any differences in the gender compositions across the five age groups?\n\nHypothesis\n\nH 0 : There are no differences in gender composition across the five age groups.\n\nH 1 : Gender composition is different in at least one of the age groups.\n\nStatistical technique\n\nAnalysis of variance (ANOVA) will be used to test the hypothesis.\n\nJustification\n\nANOVA is the most suitable technique for testing hypothesis that entail comparing mean for more than one group. One way ANOVA will be used because there is only one independent variable (Verbeek, 2008).\n\nResults of the test\n\nAnova: Single Factor                                                       \nSUMMARY                                                                    \nGroups                Count     Sum  Average   Variance                    \nUnder 20              47        20   0.425532  0.249769                    \n21-30                 109       47   0.431193  0.247537                    \n31-40                 105       43   0.409524  0.244139                    \n41-50                 107       41   0.383178  0.238582                    \nover 50               52        23   0.442308  0.251508                    \nANOVA                                                                      \nSource of Variation   SS        df   MS        F         P-value   F crit  \nBetween Groups        0.18386   4    0.045965  0.18751   0.944872  2.393438\nWithin Groups         101.7304  415  0.245134                              \nTotal                 101.9143  419                                        \n\n\nInterpretation\n\nIn the results above, the value of F-calculated is less than the F-critical. Besides, the p-value is greater than alpha (5%). Therefore, the null hypothesis will not be rejected at the 95% confidence level. This implies that there are no differences in gender composition across the five age groups.\n\nIs there any difference in customer satisfaction based upon \u2018response times in the CompleteCare division\u2019 and the \u2018loyalty rewards program\u2019?\n\nHypothesis\n\nH 0 : Customer satisfaction based upon response times in the CompleteCare division = the customer satisfaction based upon loyalty reward program.\n\nH 1 : Customer satisfaction based upon response times in the CompleteCare division \u2260 the customer satisfaction based upon loyalty reward program.\n\nStatistical technique\n\nIn this case, a paired sample t-test will be used to test the hypothesis.\n\nJustification\n\nA paired sample t-test is the most suitable for testing hypothesis that compared the mean of two related variables (Verbeek, 2008).\n\nResults of the test\n\nt-Test: Paired Two Sample for Means                                       \n                                     Response time  Loyalty reward program\nMean                                 3.242857143    5.645238095           \nVariance                             4.222502557    7.842817366           \nObservations                         420            420                   \nPearson Correlation                  -0.011950135                         \nHypothesized Mean Difference         0                                    \nDf                                   419                                  \nt Stat                               -14.09404771                         \nP(T<=t) one-tail                     1.69112E-37                          \nt Critical one-tail                  1.64849841                           \nP(T<=t) two-tail                     3.38224E-37                          \nt Critical two-tail                  1.965641842                          \n\n\nInterpretation\n\nThe results show that t-calculated is greater than t-critical. Also, the p-value is less than alpha (5%). Therefore, the null hypothesis will be rejected at the 95% confidence level. This implies that there are differences in customer satisfaction based upon response times in the CompleteCare division and the loyalty rewards program.\n\nAre any of the initiatives proposed by management related to the overall satisfaction of Computers R Us customers?\n\nHypothesis\n\nH 0 : The initiatives proposed by the management are determinants of the overall satisfaction of Computer R Us customers.\n\nH 1 : The initiatives proposed by the management are not determinants of the overall satisfaction of Computer R Us customers\n\nStatistical technique\n\nIn this case, a multiple regression analysis will be used.\n\nJustification\n\nMultiple regression analysis is used to model the relationship between one dependent variable and other explanatory variables.\n\nResults of the test\n\nSUMMARY OUTPUT                                                                                                                \nRegression Statistics                                                                                            \nMultiple R              0.965602191                                                                                           \nR Square                0.932387592                                                                                           \nAdjusted R Square       0.931735906                                                                                           \nStandard Error          0.613066164                                                                                           \nObservations            420                                                                                                   \nANOVA                                                                                                                         \n                        Df            SS              MS        F         Significance F                                      \nRegression              4             2150.962676     537.7407  1430.732  3.3062E-241                                         \nResidual                415           155.9778006     0.37585                                                                 \nTotal                   419           2306.940476                                                                             \n                        Coefficients  Standard Error  t-Stat    P-value   Lower 95%       Upper 95%   Lower 95.0%  Upper 95.0%\nIntercept               0.896561298   0.103883791     8.630425  1.32E-16  0.69235727      1.10076533  0.69235727   1.100765326\nResponse time           0.86471784    0.03836737      22.53785  4.92E-74  0.789299227     0.94013645  0.789299227  0.940136454\nLevel of advice         0.271037316   0.041145932     6.58722   1.36E-10  0.190156892     0.35191774  0.190156892  0.351917739\nLevel of communication  -0.01775345   0.021480889     -0.82648  0.409009  -0.05997837     0.02447146  -0.05997836  0.024471457\nLoyalty reward program  0.007973903   0.010696359     0.745478  0.456405  -0.01305189     0.0289997   -0.01305189  0.0289997  \n\n\nInterpretation\n\nThe F-test will be used to test the overall significance of the regression model. The p-value for the F \u2013 test is less than alpha (0.05). Therefore, reject the null hypothesis and conclude that the four determinants are significant determinants of the overall customer satisfaction. Further, the p-value for response time and level of advice are greater than alpha (0.05). This implies that they are significant determinants of overall customer satisfaction of the company.\n",
        "label": "human"
    },
    {
        "input": "Corporate Governance in Satyam Computer Services LTD Report\n\nAbstract\n\nSatyam Company is a good example of companies that have failed due to fraud and embezzlement of funds by senior officials. Based on the findings from different scholars, the company lacked independent directors, thus giving room for wrongful misappropriation of funds.\n\nIts external auditors also colluded with the directors to defraud the investors. The company had a weak internal control system and in its operations, the promoters held executive position, and thus they had great influence on the company\u2019s decisions.\n\nThis essay is prepared based on the requirement of the course as directed by the tutor. This paper seeks to advise the management team of Satyam Ltd. on the best corporate culture to adopt in its administration.\n\nThe report is based on the view that the management of Satyam Company has reported at least one incidence involving misappropriation of funds by some irresponsible financial controllers.\n\nTherefore, the aim of this study is to analyse the company\u2019s corporate governance critically and evaluate the strengths and weaknesses of its internal control system before coming up with the necessary recommendations.\n\nThe recommendations that will be given at the end of the essay will be based on the provisions of the UK code on corporate governance.\n\nIntroduction\n\nIn the modern world, the scope of corporate governance has changed due to the high number of companies that have failed due to misappropriation of funds. It has been taken to include the accountability concept especially in the accounting department.\n\nThe need for sound corporate governance has been perpetuated by the nature of modern businesses, which are run by third parties on the behalf of the shareholders or owners. A number of factors have facilitated the need for improved corporate governance.\n\nFirstly, most businesses today are in the form of companies, which are owned by a great number of shareholders who are scattered, and thus cannot control the affairs of the firms (Varottil, 2010).\n\nSecondly, the shareholders may require professionals to manage their businesses. This aspect creates a principal-agent relationship between the shareholders and the directors of the company.\n\nTherefore, the concept of corporate governance becomes essential since for the shareholders\u2019 interests to be secured there must be a control of the directors\u2019 actions.\n\nThis essay therefore seeks to identify the shortfalls in the control system of Satyam Company and advise the company accordingly.\n\nLiterature review\n\nCorporate governance denotes the way in which firms are managed effectively (Bhasin, 2013). It outlines the rights and obligations of all the interested parties such as the directors, shareholders, and creditors among others.\n\nCorporate governance also outlines the decision-making procedures and explains the role of each stakeholder in the process. It defines the corporate structure of the company clearly by outlining the role that each stakeholder is expected to perform.\n\nOwners of modern businesses employ managers to run the businesses on their behalf (Atesci, Bhagwatwar, Deo, Desouza & Baloh, 2010). This scenario results in a conflict of interest between the owners and managers as the latter would like to maximise their earnings at the expense of the owners.\n\nIn addition, the failure of large companies across the world has contributed to the need for a well-outlined set of rules defining corporate governance.\n\nGovernments in most countries across the world have recognised the need for corporate governance and thus they have established uniform laws governing the conduct of agents in a bid to reduce agency risks involved.\n\nIn this essay, corporate governance in the UK and India will be given as examples of good corporate governance and the recommendations will be based on the two aforementioned nations.\n\nCorporate governance in United Kingdom\n\nCorporate governance in the UK is based on the provisions of the UK Corporate Governance Code and the companies Act (Armour, Deakin, & Konzelmann, 2003). The code outlines the rights and obligations of the shareholders and insists greatly on the shareholders interests.\n\nIt calls for the existence of a board of directors in every company whether private or public. There is a requirement that a majority of the directors in the board to be independent directors who are elected through a poll in an annual general meeting (Aguilera & Jackson, 2003).\n\nThe board also comprises other ex-official members drawn from a cross section of professionals. In a bid to protect the shareholders\u2019 interests, non executive directors are required to dominate the board and only a few executive directors are incorporated in the board.\n\nThe Chief Executive Officer of the company in the UK serves as the chairman of the board, but his/her powers are controlled by the other board members (Dewing, 2003). Any decision involving large amounts of money must be agreed upon by all members before it is executed.\n\nThe directors have expressed and implied powers that they can use to make certain decisions. However, in most cases the shareholders must be called upon to pass certain decisions involving their interests. In the UK, companies are largely governed by the corporate charter (Denis & McConnell, 2003).\n\nOn the other hand, the stock markets govern the sale and purchase of securities in the market. Every company is required to submit a copy of its bylaws to the registrar of corporations. The bylaws must conform to the corporate charter laws, and thus any contradicting provision in the bylaws is regarded null and void (Aguilera & Jackson, 2003).\n\nIn addition to the provisions of the corporate charter, companies whose stock is listed in the London Exchange market (LSE) must comply with certain requirements.\n\nWith the adoption of the companies Act, the LSE was given the full mandate to see that the companies apply the statutory provisions regarding the exchange of stock. Among the requirements of \u201cthe LSE is that all listed companies must have independent directors dominating the board of directors\u201d (Armour et al., 2003, p.541).\n\nDecisions made by these directors are expected to be independent and none of the directors should be left to make decisions without involving the rest of the team. In a bid to maintain the independence, the independent directors cannot hold managerial positions or any other position in the company.\n\nThe companies listed in the stock market are also expected to hold board meetings at regular intervals (Denis & McConnell, 2003).\n\nThe management is barred from attending these meetings in a bid to ensure that the independence of the directors is maintained. Board members form committees from among themselves. The committees are charged with the responsibility of recruiting new members into the board.\n\nCorporate Governance in India\n\nIndia has for a long period lagged behind in the efforts to establish corporate governance principles. In 2011, the country demonstrated attempts to come up with a set of rules that would define its corporate culture (Varottil, 2010).\n\nIndia\u2019s parliament passed the Companies Bill that outlines the principles of governance in companies. Before the enactment of this bill into law, the Securities and Exchange Board of India (SEBI) governed India\u2019s corporate structure.\n\nSEBI was not effective until 2000 when a committee was appointed to offer recommendations on the changes that would make it effective in its corporate governance provisions (Henley, 2006). The committee came up with recommendations that were implemented in the year 2000.\n\nThe committee wanted the composition of the board of directors to include both executive and non-executive directors in the same ratio. The committee also recommended that an independent director should head the audit committee.\n\nThe composition of the audit committee was to be comprised of at least three directors with the financial controller regarded as a member. The recommendations placed the directors in a fiduciary relationship with the company. The salary of the directors was to be fully disclosed and approved by the shareholders.\n\nSatyam Computer Services Limited\n\nThe company opened its doors in 1987 (Henley, 2006). It was formed as a private company with B. Ramalinga as the main shareholder. In 1990s, it invited the public for subscription of its shares, and thus it turned public. The company\u2019s main objective was to offer IT services.\n\nThe Indian company was identified as one of the main suppliers of software and it was later listed in the New York Stock Exchange (Athreye, 2005), which is an indicator that the company was financially sound. During the same period, the company managed to open subsidiaries across the world.\n\nLater on, Satyam signed merger agreements with several companies across the world (Bhasin, 2013). The reputation of the company spread fast internationally since the company offered solutions to the ever-emerging IT problems.\n\nDue to its ever-growing reputation, the company managed to attract investors from all corners of the world and acquired new customers each day.\n\nSatyam scandal\n\nThe company\u2019s setbacks started in 2009 when its promoters decided to buy a stake from the Matyas firms without involving the shareholders. The law requires the directors of a company to convene a general meeting to deliberate on the viability of an investment (Pillania, 2012).\n\nThe shareholders may approve the transaction or reject it altogether. The promoters argued that the investment did not require the approval by the shareholders and they went ahead with the transaction. The directors of the company approved the deal without informing the shareholders.\n\nThe decision received opposition from the shareholders especially when they learned that Raju\u2019s relatives owned the Matyas (Varottil, 2010).\n\nOn learning the shareholders\u2019 mood, the directors revoked their earlier decision, thus making it impossible for the transaction to be executed. This decision by the directors did not go well with the company\u2019s reputation as it had the effect of lowering the market value of the company\u2019s stock.\n\nThe company\u2019s main promoter, Ramalinga Raju, later admitted that he had engaged the company in ghost transactions leading to loss of huge amounts of shareholders\u2019 capital (Henley, 2006).\n\nConsequently, the market value of the company\u2019s stock was slashed by more than 75% immediately Ramalinga confessed to the allegations (Bhasin, 2013).\n\nBefore he resigned from his position as the executive director of the company, Raju confessed of ballooning the company\u2019s profits to induce investors to invest their funds in the company\u2019s stock.\n\nThis confession resulted in a further fall in price of the company\u2019s stock. The year 2009 saw the company incur huge losses due to the disclosure by Raju (Henley, 2006).\n\nRaju was accused of falsifying the accounts of the company, manipulating balance sheet items by presenting assets that never existed, and presenting fake account balances at the end of the financial year (Khanna, 2009).\n\nThese evil acts were perpetuated by the fact that Raju held the senior most position in the company and he was thus in a better position to manipulate the company\u2019s financial accounts. There were no internal control checks, and thus the fraudulent activities could not be easily detected (Kapur & Ramamurti, 2001).\n\nGovernance issues surrounding Satyam Company\n\nInsider dealing is a common scene in the company. The laws governing the directors\u2019 dealings with a company deny them the right to participate in decision making as to whether a transaction should be approved without first declaring their interest in the transaction (Pillania, 2012).\n\nInvestigations into the Satyam saga revealed that an insider trading formed the better part of the company. Right from the promotion of the company, the promoters acquired land from the sale of shares to Raju\u2019s relatives.\n\nThe investigating committee revealed that the company raised huge amounts of money from sale of shares within a very short period. The committee further argued that such huge turnover of shares could not be achieved without an insider transaction (Afsharipour, 2009).\n\nThe company\u2019s insider dealing is also evident in the last attempt by the directors of the company to invest in the Matyas Company. The firm belonged to Raju\u2019s relatives and its price was inflated to benefit the seller. The top officials of the company initially purchased the largest part of stock of the company.\n\nThe latter then disposed of the shares at a higher price when the company was seen to be failing. The officials used the secret information they had about the company to make secret profits at the expense of the shareholders. This aspect is a clear indicator of poor corporate governance in the company.\n\nThe auditors\u2019 report was questionable since the audit fee paid to the auditing firm that had the responsibility of reporting to the shareholders on the company\u2019s financial statements was too high as compared to the fee charged by similar audit firms (Gupta, Nair & Gogula, 2003).\n\nThe financial statements were doctored with the help of the auditors of the company. The audit committee of the company was aware of the misrepresentation of accounts, but it too colluded with the auditors to defraud the shareholders (Singh, Kumar & Uzma, 2010).\n\nCompanies whose stocks are listed in the New York Stock Exchange have the obligation of establishing a nomination board manned by independent directors (Pillania, 2012).\n\nThe company ignored this provision and never set the committee as per the requirements of the New York stock exchange, which was made possible by the fact that the Indian governance structure did not expressly provide for the formation of the committee (Afsharipour, 2009).\n\nTherefore, it was not mandatory for the company to set a nomination committee.\n\nManagement control system in Satyam Company\n\nThe term \u2018management control system\u2019 (MCS) as used in this context refers to the internal checks in the management (Kapur & Ramamurti, 2001). The system involves a number of control measures to ensure that policies and strategies formulated are implemented effectively to achieve the organisations\u2019 goals.\n\nContinuous assessment of the management teams\u2019 conduct is desirable. MCS works very close with corporate governance for the benefit of the company (Nikam, Ganesh & Tamizhchelvan, 2004).\n\nDue to this close relationship between the two, failures of either of them may result in the failure of the other, thus exposing the company to the danger of extinction. MCS in Satyam Company was questionable and so was its corporate culture. The company\u2019s failure can thus be attributed largely to the lack of effective MCS.\n\nRecommendations\n\nThough the fraud can be attributed to numerous factors, the main cause of the fraud was lack of properly defined set of rules governing the conduct of business in the company.\n\nLack of proper implementation of the already existing rules inside the company\u2019s corporate governance could also be cited as a factor that led to this fraud (Khanna, 2009).\n\nIn the light of the fraud case involving Satyam Computer Company, the Indian capital market should focus on reducing such instances by applying the following two approaches, viz. the preventative and palliative measures (Bhasin, 2013).\n\nThe palliative control measures, if well invoked, will help in detecting fraudulent activities by altering the current verification procedure and implementing new improved procedures that are not known by many.\n\nOn the other hand, the preventative measures focus mainly on detecting and controlling fraudulent acts at earlier stages before huge losses are incurred. The latter is the best option since it produces effective results both in the short run as well as in the long term.\n\nThe other thing worth recommending to the company is the separation of obligations between owners of the company and directors. Clear separation should be observed between the shareholders and directors of the company.\n\nIn the case of Satyam Company, director Ramalinga Raju owned a better portion of shares in the company (Varottil, 2010). In addition, he served as the executive director of the company, which could lead to decisions that are not in favour of the company.\n\nSuch a director will have great influence on other directors since the rest of the director will look at him/her as their employer and will tend to follow his/her directions unquestionably.\n\nThere should be a group of independent directors in the company charged with the responsibility of recruiting directors based on their qualifications and expertise (Khanna & Palepu, 2005).\n\nOn the other hand, he directors should not remain in office when their terms expire. Instead, they should be elected to serve for a maximum of one year after which another recruitment exercise is carried out. However, the retiring directors should not be denied the chance to serve the company if they are re-elected.\n\nA proper code of conduct similar to the one in the UK example given above should be established with trust and honesty being the backbone of the code. Continuous assessment should be carried out from time to time to ensure that all the staff members of the company observe the code\u2019s provisions.\n\nDivision of responsibilities is also another important factor that Satyam Company should embrace. An executive director should have his/her roles defined and s/he should not be allowed to hold any other position.\n\nThe directors should work as a team and no director should be allowed to make certain critical decision on his/her own. There is also need for independent internal auditors in addition to the external auditors who will maintain proper accounts in the company and help detect fraud when being executed (Henley, 2006).\n\nThe internal auditors should work closely with their external counterparts and even work together when need arises.\n\nReporting on the financial accounts is the responsibility of the directors (Khanna & Palepu, 2005). The directors thus ought to maintain an objective and purposeful relationship with the auditors to ensure that the report presented to the shareholders reflects the true and fair view of the company\u2019s financial status.\n\nAdditionally, the directors should outline their role of ensuring that proper accounts have been prepared and present the same to the shareholders along with the auditors\u2019 report. However, the independence of the auditors should be observed closely to avoid collusion with the management.\n\nClear penalties should be well spelt for auditors who collude with the management to give misleading reports (Shamir, 2004). Directors are chosen under the watch of shareholders in the annual general meeting.\n\nTheir selection is not based on their expertise, but rather on merit and ability to hold such position (Atesci et al., 2010). The directors should be offered continuous training on the corporate culture to make them understand the scope of the fiduciary duties that they owe the company.\n\nConclusion\n\nSatyam Company\u2019s failure was due to poor governance. The company had a poor management control system. There was a separation of responsibilities between the owners and directors as the main shareholder of the company, Raju Ramalinga, held the position of an executive director in the company.\n\nLack of properly defined division of responsibility between the two gave the director a good opportunity to defraud the company of its funds since the other board members could not question his decision. The company had a weak internal control system, and thus the fraud could not be detected in time.\n\nIn a bid to cut down cases of fraud and misappropriation of funds by some individuals, there has to be strong internal controls manned by independent directors. Directors should also be trained on their rights and obligations in the companies that they run.\n\nThe company therefore need to borrow some of corporate governance practices from the UK Code of corporate governance. The code for example requires that the board of directors be comprised mainly of the non-executive directors.\n\nReferences\n\nAfsharipour, A. (2009). Corporate governance convergence: lessons from the Indian experience. Northwestern Journal of International Law & Business, 29 (2), 335- 402.\n\nAguilera, V., & Jackson, G. (2003). The cross-national diversity of corporate governance: Dimensions and determinants. Academy of management review, 28 (3), 447-465.\n\nArmour, J., Deakin, S., & Konzelmann, S. J. (2003). Shareholder primacy and the trajectory of UK corporate governance. British Journal of Industrial Relations, 41 (3), 531-555.\n\nAtesci, K., Bhagwatwar, A., Deo, T., Desouza, C., & Baloh, P. (2010). Business process outsourcing: A case study of Satyam Computers. International Journal of information management, 30 (3), 277-282.\n\nAthreye, S. (2005). The Indian software industry and its evolving service capability. Industrial and Corporate Change, 14 (3), 393-418.\n\nBhasin, M. (2013). Corporate Accounting Fraud: A Case Study of Satyam Computers Limited. European Journal of Business and Social Sciences, 1 (12), 25-47.\n\nDenis, D., & McConnell, J. (2003). International corporate governance. Journal of Financial and Quantitative Analysis, 38 (01), 1-36.\n\nDewing, I. (2003). Post-Enron developments in UK audit and corporate governance regulation. Journal of Financial Regulation and Compliance, 11 (4), 309 \u2013 322.\n\nGupta, A., Nair, P., & Gogula, R. (2003). Corporate governance reporting by Indian companies: a content analysis study. The ICFAI Journal of Corporate Governance, 2 (4), 7-18.\n\nHenley, J. (2006). Outsourcing the provision of software and IT-enabled services to India: emerging strategies. International Studies of Management and Organisation, 36 (4), 111-131.\n\nKapur, D., & Ramamurti, R. (2001). India\u2019s emerging competitive advantage in services. The Academy of Management Executive, 15 (2), 20-32.\n\nKhanna, T., & Palepu, K. (2005). The evolution of concentrated ownership in India: broad patterns and a history of the Indian software industry. In R. Morck (Ed.), A history of corporate governance around the world: Family business groups to professional managers (pp. 283-324). Chicago: University of Chicago Press.\n\nKhanna, V. (2009). Part III Corporate Governance, Trade, and Environment: Corporate Governance in India: Past, Present, and Future. Jindal Global Law Review, 1 , 171-251.\n\nNikam, K., Ganesh, C., & Tamizhchelvan, M. (2004). The changing face of India. Part I: bridging the digital divide. Library review, 53 (4), 213-219.\n\nPillania, R. (2012). Corporate Governance in India: Study of the Top 100 Firms. Journal of Applied Economic Sciences (JAES), 1 (19), 87-92.\n\nShamir, R. (2004). Between Self\u2010Regulation and the Alien Tort Claims Act: On the Contested Concept of Corporate Social Responsibility. Law & Society Review, 38 (4), 635-664.\n\nSingh, J., Kumar, N., & Uzma, S. (2010). Satyam Fiasco: Corporate Governance Failure and Lessons Therefrom. IUP Journal of Corporate Governance, 9 (4), 30-39.\n\nVarottil, U. (2010). Evolution and effectiveness of independent directors in Indian corporate governance. Hastings Business Law Journal, 6 (2), 281 \u2013 290.\n",
        "label": "human"
    },
    {
        "input": "Corporate Governance: Satyam Computer Service Limited Report\n\nTable of Contents\n 1. Introduction\n 2. Summary of the Case of Satyam Computer Service Limited\n 3. Theoretical Framework\n 4. Case Study Analysis\n 5. Recommendations\n 6. Conclusion\n 7. Reference List\n\nIntroduction\n\nFor effective operation of an organisation, monitoring and control are necessary. Corporate governance comprises one of the ways of controlling and monitoring of organisational operations (Abdulla & Page, 2009). The key agenda of corporate governance is to alleviate any clashes of demands among partners.\n\nThis task is mostly accomplished through the enactment of various customs, laws, processes, policies, and institutions, which influence the manner in which organisations are controlled.\n\nFor example, the UK has established a set of rules that guide all organisations to ensure that they do not engage in unethical practices, which harm the interest of the shareholders.\n\nState-established policies and other control structures that are established within the field of corporate governance may help to regulate employees\u2019 conducts and decisions. Alternatively, organisations may develop codes that guide the practices of their employees so that they do not engage in issues that harm their shareholders.\n\nHowever, even with the codes, directors and other employees may fail to uphold accountability as witnessed in the case of Satyam Computer Service Limited. The company misrepresented financial performance information. The act misguided the shareholders concerning the company\u2019s performance.\n\nThis paper discusses corporate governance issues in the contemporary organisations using Satyam Computer Service Limited as a case study.\n\nSummary of the Case of Satyam Computer Service Limited\n\nSatyam Computer is an Indian-incorporated information services organisation. At the dawn of 2008, the business announced its plan to secure two organisations. The first one was Maytas Infrastructures while the second one was Maytas Properties. Ramalinga Raju\u2019s folks owned these two organisations.\n\nRaju was the pioneer and chair of Satyam Computer Service Limited. However, following issues with the stock market, the organisation withdrew its acquisition deal within 12 hours. Questions began to emerge from the company\u2019s corporate governance systems.\n\nSatyam Computer Service Limited authorised an acquisition while knowing that the transaction was party-related. However, people began to take responsibility for the malpractice. Two independent directors offered to resign.\n\nAt the beginning of 2009, the chairperson exposed how Satyam Computer Service Limited had been exaggerating its returns. This case increased concerns over the company\u2019s corporate governance.\n\nIn his resignation letter, the head confessed that the company had engaged in accounting malpractices that involved the manipulation of over $1billion cash balance. The collusion to engage in the acquisition without consulting the independent director aimed at benefiting the chair\u2019s immediate family members.\n\nThe vice cost Satyam Computer Service Limited over $1.6billion.\n\nIn India, the SEBI Act of 1956 addresses the company\u2019s operations. Clause 49 of the Act specifies issues of corporate governance in relation to investments that are done with group companies.\n\nIt sets forth the provision for constituting a board of directors, independent directors, the procedures that the board has to follow, audit committees, remuneration issues, and shareholder committees among other issues that are meant to increase transparency and accountability of decisions that the organisation makes through its board of directors. Indeed, corporate governance under the SEBI Act of 1956 is founded on equality, responsibility, and intelligibility.\n\nCompanies also need to appoint people who have the appropriate qualifications to participate in the independent board. More importantly, they should demonstrate integrity.\n\nThe goal is to ensure that the directors make independent decisions in an objective manner in the process of executing their mandate of adding value to the success of an organisation (Bhagat & Black, 1999). Satyam Computer Service Limited has the appropriate number and qualified independent directors.\n\nWhat went wrong? Did they fail in their responsibility of protecting the shareholder interests by permitting the proposed acquisition? Did the Satyam board not consider their roles? Could they not detect the inflated revenues? These questions are relevant in the discussion of corporate governance issues.\n\nHowever, the debate in the next section addresses corporate governance within the context of the UK in the effort to determine whether the Satyam fraud, possibly the Indian Enron, could have been avoided under the UK 2014 corporate governance guide.\n\nTheoretical Framework\n\nThe UK Financial Reporting Council develops and implements guides to corporate governance. The latest version of the code was released in September 2014.\n\nIt addresses various issues that relate to the operations of an organisation. Some of the particular areas of relevance entail the composition and remuneration of organisations\u2019 board of directors. It addresses issues that relate to finance, regulations, and ownership systems.\n\nIn the UK, corporate governance also cuts across various relationships that exist between many organisational stakeholders in relation to a particular company\u2019s objectives and goals.\n\nThe 2014 UK version of corporate governance guide identifies four major areas of concern, namely headship, efficiency, accountability, remuneration, and shareholder relations.\n\nThe overall aim of the UK corporate responsibility guide is to ensure ardent protection of all organisational stakeholders. In the contemporary business settings, stakeholders are classified either as external or internal. The chief external stakeholders include groups such as creditors, suppliers, and debt holders (Taylor, 2006).\n\nInternal stakeholders encompass the executive board of directors and other workers.\n\nWithin the sphere of corporate governance concerns in the UK, an endeavour is made to ensure that the organisations are directed and controlled with utmost transparency, professionalism, and in a responsible manner to safeguard the interests of all stakeholders.\n\nA key interest is the increment of confidence among investors in the capital markets and shareholders. A dominant issue in the contemporary corporate governance debate in the UK is the effect of the system of corporate governance on economic efficiency (Bebchuck, 2005).\n\nTo this end, an emphasis is deployed on the shareholders\u2019 welfare within the established regulatory policy framework. The fall of high profile corporations because of accounting fraud since 2001 has fuelled the need to address the issue of the contemporary corporate governance across the globe (Masdoor, 2011).\n\nIn the discourses of corporate governance, managers are responsible to the board. The board needs to exercise utmost responsibility to the shareholders. To conceptualise Enron and Marconi\u2019s scandal, corporations are required to foster and portray high levels of integrity in conducting their business in a transparent manner.\n\nCertain rules to regulate corporate audit committees are established to promote accountability to both shareholders and other business partners.\n\nBefore the enactment of 2010 corporate code and the 2014 policy, the UK corporate accountability check was predominately pegged on demanding corporations to prepare annual reports explaining \u201cthe basis on which the company generated or preserved its long-term value and the model for delivering its objectives\u201d (Clarke & Marie, 2008, p.27).\n\nIdeally, accountability infers the provision of information that makes business stakeholders mitigate the perceived risks that result from fraud within corporations, which largely act against their interests.\n\nIn the UK, the 2014 accountability and business reporting code requires directors to explain their responsibilities in the preparation of accounts and yearly reports. Financial Reporting Council (2014) reckons, \u201cthe board should present fair, balanced, and understandable assessment of the company\u2019s position and prospects\u201d (p. 16).\n\nThis claim underscores the role of directors in guaranteeing transparency and correct representation of financial information. With the information, shareholders possess the ability to evaluate the performance of a company, the success of its business model, and its success strategies.\n\nAuditors are required to state their role in reporting (Financial Reporting Council, 2014). The report that is presented to the shareholders and other people who are interested in the operation of listed companies will have to pass through the scrutiny of different levels of organisational leadership, control, and monitoring structures.\n\nTherefore, the capacity to recognise fraud increases.\n\nThe accountability principle also establishes the risk management and internal controls as strategies for increasing transparency of an organisation\u2019s dealings and decisions of different stakeholders.\n\nUnder this principle, \u2018the board is responsible for determining the nature and extent of principal risks that it is willing to take to achieve its strategic objectives\u2019 (Financial Reporting Council, 2014). Therefore, it needs to ensure that it maintains effective risk management and internal systems control.\n\nRobustness in the evaluation of risks by directors is important before making the decision of taking risky investments.\n\nUnder the principle of audit committees and auditors, the board has the obligation for ensuring formal and transparent decisions in terms of applying corporate reporting, internal controls, and the principle of risk management.\n\nThe board for the UK-based corporations is required to form a committee that consists of at least three people for big companies and two individuals for small companies that serve in the position of non-executive directors who should also work independently.\n\nFor small companies, the chair can be a member of the committee. However, for big companies, he or she cannot form part of the committee.\n\nAs the main watchdog for the institutional investors, the committee\u2019s functions comprise monitoring integrity of organisational financial statements, reviewing internal control, monitoring the effectiveness of an organisation\u2019s auditing functions, and reviewing the independence of external auditors among other auditing-related issues.\n\nApart from accountability, which is the key issue in Satyam Computer Service Limited\u2019s corporate fraud, the UK corporate governance codes also have a principal concern in the regulation of remuneration, leadership, effectiveness, and shareholder relations.\n\nThe principle of leadership underlines the necessity of power separation in organisations.\n\nIn corporations\u2019 leadership, Financial Reporting Council (2014) reveals, \u201cThere should be a clear division of responsibilities at the head of the company between the running of the board and the executive responsibility of running of the company\u2019s business\u201d (p.5).\n\nThis separation of powers ensures that the executives become accountable to the board. As the two different entities have access to the decisions that are made by one of them, separation of powers encourages transparency. The guide also provides that no single person can demonstrate the domination of authority to make choices.\n\nTo encourage effectiveness, the 2014 UK code for corporate governance requires the board to conduct formal and rigorous yearly evaluations of its performance together with that of the directors and committees.\n\nIt becomes possible to discover loopholes in organisations\u2019 performance as the basis for establishing mechanisms for sealing them. All provisions of the 2014 UK corporate governance ensure long-term organisational success in a bid to protect the interest of all shareholders.\n\nFor example, the principle of compensation requires payments to be done such that they promote the long-term performance of listed companies where directors are not permitted to take part in the process of determining their remunerations.\n\nThis strategy is an attempt to eliminate the setting of one\u2019s remuneration in a manner that may act against the interest of institutional investors and other stakeholders.\n\nFrom the context of shareholder relations, the 2014 UK corporate governance code requires the establishment of dialogue that is founded on the principle of mutual understanding between an organisation and the investors. Indeed, this task is the primary role of the board.\n\nWhile passing resolutions, the board is required to arrange general meetings with investors to foster their participation in making key decisions that may overly influence or expose them to excessive risks. Unfortunately, this strategy was not the case for Satyam in India.\n\nCould the company\u2019s leadership have engaged in fraud if India had developed and implemented corporate governance codes that were similar to those of the UK?\n\nThe next section analyses the applicability of the UK\u2019s corporate governance codes in mitigating corporate governance challenges that are similar to those that were experienced in Satyam Computer Service Limited.\n\nCase Study Analysis\n\nDifferent nations have different theoretical corporate governance frameworks. For example, the Anglo-American corporate governance model, which is also termed as unitary corporate governance model, prescribes a system that constitutes a mixture of non-executive and administrative board of directors (Clarke & Marie, 2008).\n\nThe owners of a corporation\u2019s shareholders elect these boards of directors. According to the model, non-administrative leaders assume central ranks, including reimbursement and stocktaking commissions. They also outnumber the executive directors. In the case of Satyam, executive directors held key positions.\n\nIndeed, independent directors who served oversight roles on behalf of the institutional investors seemed to have little say concerning the board\u2019s decisions.\n\nFor example, the acquisition deal of the two companies whose owners belong to the family of Satyam Computer Service Limited\u2019s chair leaves a prudent question on how such a deal would be reached without whistle blowing by the independent directors.\n\nOpposed to the case of the UK, issues such as the unanimous authorisation of the acquisition deal and the inflation of the company\u2019s accounts suggest lack of corporate governance principles that can protect the interest of the shareholders.\n\nThe situation suggests the investment of power in few individuals who make decisions on behalf of the investors.\n\nTo this extent, although literature on corporate governance finds no relationship between the separation of powers and corporate governance, vesting power in few individuals without providing a room for sufficient ventilation is hazardous (Abdulla & Page, 2009).\n\nThe resignation of the chair and the independent directors occurred when investors complained over corporate governance approaches of the company once a risky decision had already been made.\n\nThis situation implies that the shareholders were not engaged in the decision-making process on a vital investment that would have significantly influenced their interests negatively.\n\nThe chairperson had also developed an appetite for making risky investments, which prompted him to misrepresent financial accounts to give the company a high value and ratings in the international markets. This case suggests the lack of adequate experience and knowledge on the likely implications of misrepresenting financial accounts.\n\nTherefore, the chair failed to meet the UK\u2019s requirements for corporate governance. Such demands call businesses to appoint leaders who are experienced in risk analysis and evaluation.\n\nThe codes and principles of corporate governance have immensely been developed in different nations. As Arcot and Bruno (2006) observe, in the UK \u201ccompliance with governance recommendations is not mandated by law, although the codes that are linked to stock exchange requirements may have a coercive effect\u201d (p. 57).\n\nIn fact, the applicability of the UK corporate governance codes is restricted to corporations that appear in the stock exchange listings. This observation suggests their likely application in Satyam Computer Service Limited since Indian corporate governance principles also apply to all listed companies as established in the SEBI Act of 1956.\n\nThe UK\u2019s 2014 corporate governance code is accorded statutory authority by the Financial and Services Market Act of 2000. This authority exemplifies the \u2018comply or explain\u2019 rule.\n\nThe 2000 Act persuades, \u201cpublic listed companies to disclose how they have complied with the code and to explain where they have not applied the code\u201d (Arcot & Bruno, 2006, p.56).\n\nTo this extent, the UK corporate governance deviates from the Indian approaches to corporate governance in the sense that the 2014 code upholds Indian principle-based as opposed to rule-based corporate governance approaches.\n\nThe UK\u2019s corporate governance code provides general principles, which act as a guide for the general best practices. Therefore, it can effectively help in mitigating risks such as those that were exposed to Satyam\u2019s investors.\n\nRecommendations\n\nIn the UK, corporations\u2019 operations are subject to the application of a common law in addition to relevant jurisdictional statutory laws. Within particular jurisdictions, corporations have specific individualised rules that govern their codes of ethics or authority.\n\nSuch rules even constrain the mandates of the corporation directors\u2019 decision-making capacities. The rules are provided in the corporations\u2019 constitution. In the UK, the jurisdictions are termed as corporate charter.\n\nIn India, the successful prosecution of crimes that were committed by Satyam\u2019s committee members and chairperson faces constitutional impediments. Indeed, prosecuting the chairperson requires a person to file a case against him on the grounds of fraud.\n\nThis situation is quite unlikely. Even if it were likely, there is no guarantee that vital evidence would still be preserved. Participants of corporate fraud are always intelligent enough to ensure that they conceal or destroy any evidence that can link them to wrongdoing.\n\nTherefore, the statutory law may fail to prevent future occurrence of fraud such as Satyam\u2019s fraud. Strong corporate governance policies are required.\n\nFrom the above expositions, Satyam should consider power breakdown, increased participation of independent directors in the decision-making process, and whistle blowing. More importantly, investor awareness is crucial in enhancing Satyam\u2019s future success. Organisations must serve shareholders\u2019 interest.\n\nThis role is the reason why the board of directors is formed. Consequently, in its endeavour to move forward, Satyam needs to develop policies for increasing shareholder participation, accountability, and transparency in its dealings.\n\nInternal and external auditors also need to play their roles as established by the UK corporate governance codes to ensure that any accounting malpractice is discovered and revealed to the investors before money is transferred.\n\nThese recommendations are captured in the UK\u2019s 2014 corporate governance manual. Consequently, India needs to establish a corporate governance guide that is similar to that of the UK.\n\nConclusion\n\nIn corporate governance, institutional investors have a noble role of supervising corporate managers who demonstrate any form of self-interest with organisations, which hire them. This situation occurs when investors have sufficient information on the practices of the board of directors and the chairpersons.\n\nHowever, this state of affairs was not the case for Satyam Company. The leadership made acquisition decisions, which exposed investors to high risks without their participation. The revenues of the company were inflated in a well-calculated plan to depict the company as generating high revenues.\n\nUnfortunately, the independent directors and the auditors could not reveal this malpractice until when the chairperson admitted wrongdoing when things had already gone bad. This paper has revealed how this case highlighted lack effective corporate governance.\n\nHence, it recommends the adoption of corporate governance codes that are similar to those of the UK, not only at Satyam, but also in India in general.\n\nReference List\n\nAbdulla, A., & Page, M. (2009). Corporate Governance and Corporate Performance: UK FTSE 350 Companies . Scotland: The Institute of Chartered Accountants of Scotland.\n\nArcot, S., & Bruno, V. (2006). In Letter but not in spirit: an Analysis of Corporate Governance in the UK . London: Routledge.\n\nBebchuck, L. (2005). The Case for Increasing Shareholder Power. Harvard Law Review, 118 (3), 833-914.\n\nBhagat, S., & Black, B. (1999). The Uncertain Relationship Between Board Composition and Firm Performance. Business Lawyer, 54 (3), 921-943.\n\nClarke, T., & Marie, R. (2008). Fundamentals of Corporate Governance. Thousand Oaks, CA: SAGE.\n\nFinancial Reporting Council. (2014). The UK Corporate Governance Code . London: Financial Reporting Council.\n\nMasdoor, A. (2011). Ethical Theories of Corporate Governance. International Journal of Governance, 1 (2), 484-492.\n\nTaylor, B. (2006). Corporate Governance: the crisis, investors\u2019 losses and the decline in public trust. Corporate Governance, 11 (3), 155-163.\n",
        "label": "human"
    },
    {
        "input": "Environmental Friendly Strategy for Quality Computers Limited Problem Solution Essay\n\nIntroduction\n\nAs the need to enhance environmental conservation and sustainability has continued to grasp attention in all regions all over the world, organizations have boldly come out in support of and recognition of their role in protecting as well as sustaining the very environment they operate in.\n\nThe following is an environmental friendly strategy for Quality Computers Limited (QCL) a fast-growing and one of the leading company dealing with computer systems such as desktop computers, netbooks, servers as well as work stations to individuals, organizations and other businesses. The company also deals with other computer applications and peripherals that include LCD monitors, displays, printers and USB drives.\n\nThe company that was established in 1998 has over the years continued to manufacture computers parts and products of high quality for personal, business, or even corporate use. The strategy laid out is in accordance with the company\u2019s set goals and objectives.\n\nOne such objective is enhancing environmental conservation and creating a production atmosphere that is environmental friendly. The strategy is inclusive of important business components that include social responsibility, ethical leadership practices, sustainability, as well as legal considerations.\n\nDiscussion\n\nSocial responsibility\n\nWith the unprecedented environmental disasters which have been reported in the 21 st century, the environmental protection and the conservation of the natural resources for future generations have become integral elements of the international policies.\n\nAt present, every company has to come to realization of its social responsibility before the society and develop the strategies which correspond to the principles of ethical and environmental sustainability.\n\nThe term of sustainable development is used to define the principles of integrating the ethical and environmental consideration into the companies\u2019 strategies. Though environmentally-friendly projects can be rather expensive, the principle of environmental sustainability should not be associated with the increased expenses only.\n\nThe use of the innovative strategies opens up new opportunities which allow recycling the remains and using the alternative sources of energy.\n\nAt the same time, along with the realization of their social responsibility and the importance of the application of the principles of sustainable development, the transformed legislation and regulations, as well as the accountability principles, are the factors which motivate contemporary companies to develop environmentally-friendly strategies.\n\nIn line with its set out objective that calls for the development of a program that works towards maintaining social responsibility, sustainability and society development, QCL needs to ensure that their activities, products, methods and process of production are those that aim at achieving this integrative objective.\n\nThe company will aim at reducing the external impact it has on the environment through the adoption of programs that promote energy efficiency that will result in process undertakings and manufacture of products that are energy efficient.\n\nOther than its impact on the external environment, the company through its programs will also be able to create an internal environment that is also energy efficient and that will be beneficial for the company through various ways that include the reduction of production energy costs, increased productivity and profitability.\n\nThrough power management of personal computers, for example, the company will be able to save approximately $ 1.3 million in terms of costs of energy through the use of particular soft wares for energy management.\n\nCreation of eco-friendly environment within the company will require setting up of goals geared towards this purpose as well as setting up projects that will work towards the achievement of these targets (Blomstrom, 1995).\n\nOne such goal is the promotion of product recycling whereby materials used to manufacture all computer systems; products are peripherals are those that are recyclable. Another goal of the company will be to reduce the level of carbon intensity by 20 percent by the year 2015.\n\nQLC\u2019s environmental level of performance will be reported in the Corporate Social Responsibility annual report in accordance with protocols laid out by the Global Reporting Initiative (GRI). The presented report will include information on the various components that include sustainability, corporate social responsibility issues like, for example, those to do with the protection of the environment as well as the rights of workers.\n\nAs opposed to the obsolete misconceptions concerning the negative impact of the eco-friendly strategies on the companies\u2019 profits, the concept of the triple bottom line which comprises the three P (People, Planet, Profit) shows that the human, ecological and economical dimensions of the companies are interconnected and the implementation of the principles of the environmental sustainability can be beneficial for each of them.\n\nThe application of the concept of the triple bottom line as one of the central accountability principles allows evaluating not only economical success of the firm but also taking into account its progress made in the spheres of social development and environmental protection.\n\nWith the increasing social expectations applied to business, the companies\u2019 objectives cannot be limited to meeting economical targets only and need to be based on the values-driven management and principles of sustainable development.\n\nLegal and regulatory considerations\n\nThe goals and programs set up are those that are in accordance with the various environmental standards and legal laws that have been put up to ensure environmental conservation and to protect members of the public from harm caused by toxic waste products and emissions obtained from manufacturing companies and industries.\n\nThese standards will be used as a guide not only in the planning but also in the implementation process to ensure quality performance and successful achievement of positive results.\n\nThe principles of sustainable development are declared in the international legislative documents and the integration of the component of the environmental consideration into the company\u2019s strategy has become one of the directions of the international policies. \u201cEcological sustainability was implied in the Brundtland Report on sustainable development and, more importantly, in the 1992 Rio Declaration\u201d (Bosselmann p. 41).\n\nAlong with the efforts of the international organizations and commissions, the principles of environmental sustainability are reflected in the local regulations of every state specifying the standards which are to be met by all the firms.\n\nFor the purpose of controlling the fulfillment of the proclaimed goals of environmental sustainability in concrete companies, the government has developed the reports for making the firms accountable for their actions.\n\nFilling in the Corporate Social Responsibility annual report, the firms have to provide the relevant information on the measures which were imposed by organizations for meeting the goals of protecting the environment and using the natural resources reasonably.\n\nThe implementation of the environmentally-friendly strategies into practice was followed by the transformation of the existing legislation and regulations for stimulating the companies to proceed from words to actions on their path of sustainable development.\n\nThe transformed legislation and regulations on the local, state and international levels have made the companies accountable for implementation of the environmentally-friendly strategies, enhancing their environmental responsibility.\n\nThe environmental friendly strategy for Quality Computers Limited corresponds to the current legislation acts and regulations which control the integration of the principle of environmental consideration into the company\u2019s strategies.\n\nSustainability considerations and recommendations\n\nAs a way of ensuring sustainability, a few sustainability measures will be used. One such measure will include making an estimate of the amount of greenhouse emissions of gas is produced by each of the various products they bring into the market. This estimate will be inclusive of the various factors through the entire product life cycle that starts with the acquisition of raw materials for each product through mining.\n\nAnother measure is the production of laptops and LCD screens that have glass as well as aluminum enclosures that are recyclable, displays and display glasses that do not contain any amount of mercury and arsenic respectively. Manufactured electrical components, computer cables, internal parts of the system, circuit boards and other mechanical parts will not contain any trace of PVC and BFR.\n\nDesigning laptops that are energy efficient will be another effective measure. More focus will, therefore, be placed on manufacturing laptops and screens that consume only a minimal and manageable amount of energy.\n\nIn addition to this, the level of energy consumed by QLC\u2019s computer systems will further get reduced through the enhancement of power management practices of the Central Processing Unit and by increasing the efficiency of computer hardware.\n\nAnother essential measure that if undertaken is going to be a great success in terms of ensuring that the products brought out into the market are environmental friendly will be the adoption of a system called.\n\nThe Eco Highlights label. Here, each of the products manufactured by the company and that is in accordance with all environmental laws and standards will be fixed with a label to indicate so. Through this, consumers will then be able to purchase products that are eco-friendly as per their different choices or preferences.\n\nThis system will enhance continued energy efficiency, use of recyclable materials during manufacturing as well as eco-friendly packaging for the company. Information on this undertaking will be placed on the company\u2019s website. Reduction of the level of carbon emissions will be measured through obtained quarterly reports that will be filled obtained from recorded data of the emissions.\n\nCorporate Social Responsibility has been defined as a company\u2019s continued commitment to ensure that all processes and company undertakings are done in an ethical way and make a contribution the development of the society. Through CSR, companies are able to improve the living conditions of not just their employees but also that of community members and society as a whole.\n\nCSR calls for all companies and businesses to integrate both environmental as well as social concerns in their various company undertakings and transactions with different partners and stakeholders. CSR works in what is referred to as the triple bottom line that entails people or the community, the environment and company profit-making (Visser, 2007).\n\nEthical leadership considerations\n\nCommitment to promoting an environmental friendly production process that is successful and sustainable and the promotion of corporate social responsibility requires efficient and effective leadership skills among the various managers and team leaders involved. This process will entail having considerations of the various ethical standards, good management skills as well as a good level of decision making.\n\nOther than making profit and fulfilling their economic responsibility, managers have a great role to play in ensuring that the society or surrounding community benefits from the various undertakings of the organization. This means that the society should be able to feel the positive impacts of the organization and that the actions of the organization should not in any way bring harm to society members.\n\nThe rights approach calls for the need to ensure that actions undertaken are those that have minimal or no harmful effects on individuals.\n\nBased on this approach, managers have a responsibility in ensuring that community members are not harmed by the various activities and actions that a company may be undertaking in search for profits (Singer, 132). Promotion of eco-friendly manufacturing processes is one way through which this can be done.\n\nDisregarding all the recent progress in the sphere of promoting the principles of environmental sustainability and transformation of the current legislative acts and regulations, the development of the environmental friendly strategies remains one of the ethical issues, and their settlement requires strong leadership. Dealing with the risks and opportunities of the environmental friendly project is a true leadership challenge.\n\nThe costs which are required for the implementation of the environmentally friendly strategies can be easily measured while the advantages and the positive results of the project are not that obvious. In particular cases, a substantial time lag is required for the assessment of the benefits of deciding on environmental friendly strategies.\n\nIt means that the company managers need to obtain strong ethical principles and avoid concentrating on the actual financial profits for the firm only. Though contribution of every employee to the development of environmental friendly project is valuable, the choice of the strategies and decision-making process depends upon the views of senior management.\n\nThe strength of the ethical considerations of the leaders of the company is critical for the integration of the environmental component into the company\u2019s strategies and making the ethical choices on the company\u2019s level.\n\nThe implementation of the environment-friendly strategy at Quality Computers Limited would depend upon the strength of the ethical considerations of the firm\u2019s senior management and leaders of the departments. The views of the leaders and their personal example can have a significant impact on the success of the realization of the environmental friendly project.\n\nConclusion\n\nBy creating an environmental friendly production process, QLC will be able to promote sustainability, social responsibility, business and leadership ethics as well as work in accordance with set up environmental legal standards and laws.\n\nEnhancing eco-friendliness will be one way through which the company will be able to promote Green IT and join other major computer manufacturers who have successfully been able to achieve this. The development of environmentally-friendly strategy is preconditioned with the contemporary demands of social responsibility and accountability of business.\n\nRequiring strong ethical leadership and certain expenses, the realization of the environmentally-friendly project corresponds to the international standards and regulations aimed at preserving the natural resources for future generations.\n\nReference List\n\nBlomstrom, K. (1995). Business and Society: Environment and Responsibility . New York: McGraw-Hill.\n\nBosselmann, K. (2008). The principle of sustainability: Transforming law and governance . Burlington, VT: Ashgate Publishing Company.\n\nSinger, P. (1993) Practical Ethics . Cambridge: Cambridge University Press.\n\nVisser, W. et al. (2007) The A to Z of Corporate Social Responsibility . London, England; New York, NY: Wiley.\n",
        "label": "human"
    },
    {
        "input": "Implementing Computer Assisted Language Learning (CALL) in EFL Classrooms Analytical Essay\n\nThe Internet and modern technologies have changed the nature of learning. Thousands of teachers throughout the world consider computer-aided learning as a kind of dream that will bring only positive outcomes. The implementation of the computer-assisted language learning (CALL) in EFL classrooms can be rather a challenge.\n\nMany scientists have conducted researches aiming to investigate the impact, problems, and efficiency of CALL in EFL classes. According to Afrin (2014), the computer is becoming an integral part of every classroom nowadays. The Internet and CALL are not the only modern approaches that alter the traditional educational procedures.\n\nCALL modifies the role of both the teacher and student in the process of language learning drastically. In fact, CALL presupposes the language learning and teaching with the help of additional resources such as the computer, the Internet, and other computer-based sources. The author dwells on several significant advantages of CALL.\n\nThese benefits include motivation, adapting to learning, authenticity, and the development of critical thinking. However, it can be rather a challenge to integrate CALL into classrooms. As Alkahtani (2011) states, there are three levels of CALL integration. Integration at the institutional level includes the purchasing of equipment, the formation of technical support, and providing the access to it.\n\nThe second stage is the integration at the department level. Department should formulate their policies to maximize the efficiency. The third level includes the integration by teachers. Teachers should receive necessary knowledge and skills to implement CALL in the EFL classrooms.\n\nThe implementation of CALL in EFL settings should enhance the learning outcomes of learners and facilitate the meeting of needs of both pupils and teachers. Nevertheless, the problem with teachers\u2019 attitude exists. Naeini (2012) writes that \u2018teaching staff have always been encouraged to integrate computer and the Internet into their classrooms, but very few have practically responded positively\u2019 (p. 9).\n\nDespite teachers\u2019 attitudes, several other barriers impede the successful implementation of CALL in EFL classrooms. These barriers include the lack of financial support, availability of hardware and software, the insufficient theoretical and technical knowledge (Lee n.d.). Jager et al. (2014) emphasize the importance of the combination of the computer-based language learning with face-to-face learning.\n\nThey introduce the idea of blended EFL e-learning. In most cases, teachers assume that they realize pupils\u2019 needs while it is not always so. The implementation of CALL should fill the gap between teachers\u2019 assumptions and students\u2019 needs. Finally, it is significant to evaluate some practical results of the implementation of CALL in EFL classrooms.\n\nKnowledge of sufficient amount of words is always a problem of students who learn English as the foreign language. The results of several types of researches and experiments indicate that the usage of CALL enhances the vocabulary acquisition. Students process information efficiently via computer-aided resources.\n\nIt results in a long-lasting remembering of words (Ghabanchi & Anbarestani 2008). Listening skills are also of extreme significance for foreign language learning. Most EFL students face difficulty in recognizing the foreign language in everyday conversations between native speakers. Phuong (n.d.) provides the information concerning the implementing of CALL for the promotion of listening skills in EFL learners in Vietnamese Universities.\n\nThe author pays attention to the fact that teachers who have studied necessary information concerning CALL and have changed their attitudes are more likely to teach pupils efficiently. Students improved their listening skills with the help of CALL and teacher\u2019s assistance.\n\nReference List\n\nAfrin, N 2014, \u2018Integrating Commuter Assisted Instruction in the EFL Classroom in Bangladesh\u2019, Journal of Humanities and Social Sciences , vol. 19, no. 11, pp. 69-75.\n\nAlkahtani, S 2011, \u2018EFL female faculty members\u2019 beliefs about CALL use and integration in EFL instruction: The case of Saudi higher education\u2019, Journal of King Saud University , vol. 23, no. 2, pp. 87-98.\n\nGhabanchi, Z & Anbarestani, M 2008, \u2018The Effects of Call Program On Expanding Lexical Knowledge Of EFL Iranian Intermediate Learners\u2019, The Reading Matrix , vol. 8, no. 2, pp. 86-95.\n\nJager, S, Bradley, L, Meima, E & Thouesny, S 2014, CALL Design: Principles and Practice , Research Publishing, Dublin.\n\nLee, K n.d., English Teachers\u2019 Barriers to the Use of Computer-assisted Language Learning . Web.\n\nNaeini, M 2012, \u2018Meeting EFL Instructors\u2019 Needs through Developing Computer Assisted Language Learning (CALL)\u2019, International Journal of Language Teaching and Research , vol. 1, no. 1, pp. 9-12.\n\nPhuong, L n.d., Adopting CALL to Promote Listening Skills for EFL Learners in Vietnamese Universities. Web.\n",
        "label": "human"
    },
    {
        "input": "\u201cInteraction\u201d in Human Computer Interaction: iPad\u2019s Design Framework Essay\n\niPad\u2019s Design Framework\n\nThe iPad\u2019s entry into the market signaled a new era in Human-Computer interaction. It came with enhancements that have revolutionized how man interacts with computers. The concept of Multi-touch moved touch screen technology many steps forward.\n\nSome of the novel interaction features of the iPad, which were not previously in the iphone, include the spread-or-pinch feature, bundle creation, and page navigation. The iPad\u2019s iPhoto application allows for expansion or closure of a pile of photos as part of its new multi-touch interactions through a spreading or pinching motion applied by two fingers. It is also possible to make piles out of multiple slides in order to move or delete them.\n\nThis involves selecting the first slide, then holding it, and then tapping on the rest of the slides. The slides form a bundle, which is now ready for movement to a desired location.\n\nThis applies to mail where the user can form bundles of mail for mass deletion. In addition, a page navigation function included in the iPad allows a user to preview the contents of a page before opening it by pressing and holding the scroll bar area. It brings up a viewer through which the destination page is previewed.\n\nTwo functions, currently enhanced in the iPad, that were already in use in the iPhone are the scrolling tabs and the contextual keyboards. The scrolling tabs feature allows for shifting from one tab to another while the contextual keyboards displays a keyboard relevant for the application in use.\n\nFor instance, it brings up a numeric keyboard for input of numeral based functions such as time input and mathematical calculations, while a text-based keyboard appears for input of text based applications such as messaging.\n\nMusic is one of the popular iPad uses, supported by numerous iTunes applications. Several apps meet the various needs users have ranging from the creation of music to playing it. One of the applications used to create music that is benefiting from the bigger multi-touch screen is the Groovemaker app. There was a previous release for use with the iPhone.\n\nHowever, it had a practical limit as to how effectively one could use it because of the iPhone\u2019s smaller screen. The purpose of the Groovemaker is \u201cbeat-making\u201d. It works by sequencing loops from a huge library and forming layers to make a track. The main control is by drag-and-drop, which allows a user to easily pick desired tracks and remove undesired ones.\n\nThe bigger screen now allows users to control up to eight tracks simultaneously, a feat not possible previously. After creating the music, it allows for exportation to a computer. There are three options for the application, house, hip-hop, and Drum \u2018n bass, depending on a user\u2019s preference. It is available in the different packs in the iTunes store, with a free release pack and other enhanced packs, costing at least four dollars.\n\nThe iPad fits the description of a bigger iPhone because it came with many of the apps that were present in the iPhone, while providing enhancements to take advantage of the new capabilities of the iPad. Edge and Faas (2010) confirmed this when they said, \u201cMost applications that run on the iPod Touch and the iPhone can run on an iPad\u201d. An iPhone user would easily recognize the apps used by the iPad.\n\nWhen compared to the MacBook, the two devices\u2019 designs are for different uses, which complement each other. The iPad has its own unique strengths such as better surfing experience and better photo display, as compared to the Macbook, which is essentially a computing device that allows for storage of larger files and supports document processing better than the iPad.\n\nReference list\n\nEdge, C. & Faas, R. (2010). Enterprise iPhone and iPad administrator\u2019s guide. New York: Apress.\n",
        "label": "human"
    },
    {
        "input": "Computer Games: Morality in the Virtual World Essay (Article)\n\nTable of Contents\n 1. Is It an Addiction?\n 2. Players as Moral Beings\n 3. Product Placement in Computer Games\n 4. Are There Gender Differences?\n 5. Honour among Gamers\n 6. Reference List\n\nIs It an Addiction?\n\nThe computer game industry is now one of the fastest growing industries worldwide. Millions of people play various computer games. Clearly, children and teenagers are major users of these products but there are many gamers among adults as well.\n\nThere is a wide range of games and genres, each person chooses a game or even a virtual world to his/her liking. Some people are too absorbed by the virtual world and computer games become an addiction.\n\nNonetheless, it is rather an issue of the human psychology than of the product, as only certain people prefer the virtual to the real world.\n\nPlayers as Moral Beings\n\nIt is necessary to note that ethics and morality of computer games have been a focus of scholarly research for a number of years (Ford, 2001). Researchers state that the virtual world is a moral world and, hence, players are moral beings.\n\nThis assumption is justified by the fact that there are some rules in games and players follow those rules, which creates a certain moral paradigm (Sicart, 2005). An example of such moral rules is the peculiarity of many games where players cannot destroy players of their team or other characters (for example, hostages) (Sicart, 2011).\n\nTherefore, it is possible to state that players are moral beings that have a specific code of conduct and this code creates the bonds and makes up a community.\n\nProduct Placement in Computer Games\n\nObviously, being a multibillion industry, computer games are seen as a world of many opportunities for advertisers. Product placement has become quite a common thing for cinematography and television. It is clear that such type of media as computer games can also be of interest to advertisers.\n\nProduct placement is quite appropriate when it comes to advertising aimed at adults. However, there can be certain ethical issues related to advertising aimed at children. Clearly, in such games, product placement should be minimum and developers of games have to be very responsible (avoid advertising potentially harmful products).\n\nAre There Gender Differences?\n\nIt is possible to note that virtual worlds that have been created are very diverse. They respond to needs and aspirations of different groups of people. Of course, there are certain gender differences in this sphere as well.\n\nSicart (2011) notes that the majority of virtual worlds are dominated by males, and females who also want to enter the world and participate in its creation may face certain difficulties (for example, some kind of discrimination, lack of support or communication and so on).\n\nHowever, there are various worlds where females are dominating (Sims can be seen as one of the examples of the game for females). Males prefer action and creating stories (races, shooting, analysing positions) while females prefer storytelling and details (adventures, searching for details, creating worlds).\n\nHonour among Gamers\n\nSince it has been acknowledged that virtual worlds are moral, it is but natural to assume that there is honour among gamers. Sicart (2005) argues that there is a certain honour system among participants and co-creators of virtual worlds.\n\nEach game (each virtual world) has a set of rules that are expanded by gamers and these rules constitute the honour system of the particular game. It is necessary to note that these sets of morals are often employed in the virtual world only as many people create alter egos when playing games (Ford, 2001).\n\nAt the same time, many morals can often appear in the real world.\n\nReference List\n\nFord, P.J. (2001). A further analysis of the ethics of representation in virtual reality: Multi-user environments. Ethics and Information Technology, 3 , 113-121.\n\nSicart, M. (2005). Game, player, ethics: A virtue ethics approach to computer games. International Review of Information Ethics, 4 , 13-18.\n\nSicart, M. (2011). The ethics of computer games . Cambridge, MA: The MIT Press.\n",
        "label": "human"
    },
    {
        "input": "Computers and Information Gathering Essay\n\nTable of Contents\n 1. Introduction\n 2. Discussion\n 3. Conclusion\n 4. References\n\nIntroduction\n\nComputers are computing devices that are used in receiving, storing, transmitting, and retrieving information in a business. Computer application knowledge has become one of the best skills to acquire in the modern society.\n\nNotably, computers are used in every sector for a variety of reasons. For example, gathering information, storing information that would be used for future reference, and making transactions in businesses. This paper assesses whether the application of computers in gathering information is good or bad.\n\nDiscussion\n\nThe utilization of computing devices in information gathering has a primarily good development in economics and business. According to Johnson (2011), utilitarianism rule states that a thing is good if it creates the highest value for the largest number of people. Computers have been used to gather information in organizations in a convenient way, reducing the time that would have been spent if a manual way had been used (Lane, 2004).\n\nNevertheless, their utilization in data acquisition has minimized errors that had been occurring because they are automatic. Data are computed with within the shortest period possible, increasing utility. Records about events would last longer when stored in computers than when they are kept in manual systems.\n\nThis implies that computer applications add value to human life, improving development. In addition, the application of computing devices in collection of data would enhance privacy in various sectors (Lane, 2004). They could gather information without an individual\u2019s consent. As a result, people in an organization would be keen on what they would be doing to avoid being spied on by computers.\n\nWith regard to alteration of one\u2019s behavior, gathering information through computers would promote good social interrelationships among companies, groups, and people. It is crucial to note that, due to fear of being spied on, people would do what would be right. Digital information has been used to come up with new products that are beneficial to society (Johnson, 2011).\n\nFor example, inventing new drugs that would be used to save lives is a brilliant idea. God Himself gave this duty when He commanded human beings to protect life. Therefore, from the evidence above, it can be said that the use of computers in collecting information has many merits.\n\nOn the other hand, it would be correct to say that application of computers in gathering information has led to negative impacts in firms. Kant argues that for something to be good, it must conform to the morality of the society where that act is being practiced. The application is contrary to this philosophy. Computing devices infringe on human rights, which are basic for survival (Arnold, Beauchamp & Bowie, 2012).\n\nThe right to expression is violated because citizens do not express themselves freely due to the fear of being spied on in the workplace. Moreover, people have used data stored in devices to sue individuals (Arnold et al., 2012). This is morally wrong because every citizen has a right to privacy and no one should access another person\u2019s information without his or her consent.\n\nAlthough drugs are invented through digital computers, accessing them is not easy because they are expensive to purchase and only rich people can afford them. This would mean that citizens would continue losing their lives, which would be against moral principles of fairness in which there should be equal treatment to all people (Johnson, 2011).\n\nConclusion\n\nBasing our argument on utilitarianism and Kantian arguments, it would be vital to indicate that gathering of information using computers has both good and bad developments. For example, large volumes and different types of data are stored using computers. On the other hand, those who access personal information of individuals without their consent abuse their rights.\n\nReferences\n\nArnold, D. G., Beauchamp, T. L., & Bowie, N. (2012). Ethical Theory and Business . New York, NY: Pearson Higher Ed.\n\nJohnson, D. G. (2011). Ethics online. Communications of the ACM, 40 (1), 60-65.\n\nLane, F. (2012). The Naked Employee: How technology is compromising workplace privacy . New York, NY: AMACOM.\n",
        "label": "human"
    },
    {
        "input": "How Computer Based Training Can Help Teachers Learn New Teaching and Training Methods Research Paper\n\nIntroduction\n\nThe introduction of technology in various aspects of the day to day activities in administration, government, business, and education, among other fields has brought about both new opportunities and challenges in the twenty first century. The incorporation of technology in the academic setting has rattled the conventional teaching and learning paradigms.\n\nThis use of information and communication technologies in educational practices has posed new challenges to the teachers and trainers, which involve the acquisition of necessary skills in the implementation of IT products, and the appropriate tools and channels to convey IT knowledge to learners (Sansanwal, 2009).\n\nThe current societal trends require citizens to be well versed in accessing and communicating information through the newest technologies (Sansanwal, 2009). As a result, both teachers and students need to use the best means to build their competency in ICT. The use of telematic networks in various fields enables users to have unlimited access to information, as well as, flexibility of time barriers (Sansanwal, 2009).\n\nICT has changed the traditional learning community due to the introduction of new learning and teaching settings that are based on virtuality (Mikre, 2011). These new teaching modalities require trainers to change their attitudes towards the teaching paradigms in order to take up the new educational concepts that incorporate flexible learning processes, and interactive bi-directional communication systems (Mikre, 2011).\n\nOne of the key benefits of using ICT in education is the ability to interact and collaborate with other participants on a global platform in both real and asynchronous time. The application of ICT in the high school context requires extra effort from the educational authorities through the incorporation of \u201cdepth of ICT in the curriculum as a central to teaching and learning processes\u201d (Mikre, 2011).\n\nIn addition to this, educational authorities should also focus on increasing the capacity of teachers to educate the learners through the use of training programs. Such programs familiarize trainers with the new learning scenario, and allow them to take a central role in the development of a knowledge society (Mikre, 2011).\n\nOverview\n\nThe creation of computers was not focused on enhancing the quality of teaching; however, researchers found them to be useful in the profession through various applications such as Computer Assisted Instruction (CAI), Computer Managed Instruction (CMI), and Computer Based Instruction (CBI) among others. These applications have been used to teach various subjects in schools, as well as, in higher learning.\n\nStudies on the efficacy of teaching using CAIs over conventional paradigms that employ lectures showed a significant improvement in the former for multiple subjects. For instance, supplemental CAI were observed to be effective in enhancing mathematics and spelling for mentally handicapped students through the interactive dialogue of language impaired children with computers (Leung, Watters, & Ginns, 2005).\n\n\u201cThe CAI activities involved setting goals, following instructions, accessing information to accomplish the task, and evaluating performance\u201d (Leung, Watters, & Ginns, 2005). CAIs have also been found to be effective in enhancing Science Process Skills, teaching reference skills, and building Meta-cognitive writing skills in elementary school children (Leung, Watters, & Ginns, 2005).\n\nSome of the shortcomings with the use of CAIs in education include the lack of sound functionalities, rapid changes in the school curriculum, and lack of adequate training for trainers (Gamage, Adams, & McCormack, 2009).\n\nMost of these limitations were overcome through the introduction of ICT, which was not limited to text, but also included audio, video, and other formats to relay information. This has provided opportunities for online learning, e-coaching, and e-education among others (Gaible & Burns, 2005).\n\nThese forms of education are fast, convenient, and cost effective in providing rich material to the classroom and library for access to both teachers and learners. ICT is used in high schools for various functions including teaching, diagnostic testing, remedial teaching, evaluation, psychological testing, online tutoring, development of reasoning and thinking, and instructional material development (Gaible & Burns, 2005).\n\nThe most significant use of ICT in high schools is to fulfill the teaching function, which entails the development of expression ability, reasoning and thinking power, comprehension, speed and vocabulary, proper study habits, judgment and decision making ability, and application of concepts (Gaible & Burns, 2005).\n\nThis paper looks at the development of a teaching plan for trainers in order to build their capacity for the integration of ICT in the teaching of foreign languages.\n\nTeacher training in ICT\n\nThe current technological advances provide trainers with a variety of educational contents that allow them to offer both autonomous and diversified learning opportunities. Training in ICT requires teachers to abandon the conventional techniques and adapt to the new guidelines while facilitating cooperation and collaborative efforts between various entities.\n\nTeachers are expected to acquire various skills including operating new media and managing different learning environments where skills and knowledge are complemented (Leung, Watters, & Ginns, 2005). Teachers also need to be familiar with the various applications to be used, and know how to select and assess learning materials, as well as, solve practical problems.\n\nPrevious efforts to engage teachers in learning ICT have been faced with numerous challenges including negative attitude towards the new paradigms, and lack of ideological content, which makes the teachers unsure of their ability to cope with the change.\n\nHowever, these challenges have been addressed as seen with the increasing number of teachers participating in training courses, and number of necessary equipment within the learning environment (Leung, Watters, & Ginns, 2005).\n\nThe wide variety of technology that learners need to be familiar with imply that more is needed to ensure satisfactory operations besides adequate equipment and teacher training. The high school educational setting is still overly reliant on the transfer of knowledge through textbooks and teachers.\n\nHence, the challenge of introducing ICT in the learning environment can best be addressed through specific training that is relevant to the curricular, and the usage of technological tools through the design and development of modules that adapted to the educational requirements (Leung, Watters, & Ginns, 2005).\n\nThis requires the planners to move away from trends that focus on technical and instrumental criteria, and focus on vital aspects such as references to their utilization, organization and didactic design.\n\nCompetencies of teachers, which are determined by their training, need to be modified because: the teacher has to handle intricate scenarios where students have diverse needs and new communication patterns; there are modifications to content being taught and instruments being used to deliver; and the requirement for lifelong learning and regular updating of knowledge (Gamage, Adams, & McCormack, 2009).\n\nTeacher training involves two components namely \u201ctraining for the media\u201d, which entails the trainers familiarizing themselves with the use of the means, and \u201ctraining with the media\u201d, which involves a devoted means of training in order to build on cognitive abilities and enhance the comprehension of information, as well as, the development of differentiated environments for learning.\n\nAchieving success with the students requires trainer competence, which is determined by the teacher\u2019s readiness and capacity (Gaible & Burns, 2005). Hence, the various requirements that should be identified before training commences include:\n\n 1. Cultural competency: knowledge of the areas that the teacher will teach\n 2. Pedagogic competency: research techniques, didactic abilities, social and psychological knowledge that enables them to comprehend conflict resolution, treatment of diversity, and group dynamics.\n 3. Instrumental capabilities and awareness of new languages in ICT and audio-visual language\n 4. Personal traits such as maturity, self-esteem, empathy, self-confidence, emotional intelligence components\n\nStudies show that these are the vital factors that should be used to design a training plan in order for teachers to be well suited to work with ICT (Sansanwal, 2009). With this in mind, the teacher trainer should take into consideration various contents when training teachers in ICT as detailed below:\n\nType of content                                                    Content                                                                                        \n                                                                     * Society of knowledge and ICT: open but necessary attitude about their uses and implications\n                                                                     * Knowledge of hardware and basic maintenance of the equipment                               \n                                                                     * Essential functionality of the operating system: copying, saving, recording                \n                                                                     * Audio-visual language, hyper textual structuring of the information                        \n                                                                     * Texts: word processor, dictionaries, OCR, creation of simple web pages                     \nOn ICT \u2013 for professional, didactic, personal use                    * Image and sound: graphic editor, scanner, sound editor, video, photography,                \n                                                                     * Presentations: applications                                                                \n                                                                     * Internet services: navigation, electronic mail, chats                                      \n                                                                     * Searching techniques, evaluation, process and transmission of information with ICT         \n                                                                     * Spreadsheets and graphs                                                                    \n                                                                     * Management of databases                                                                    \n                                                                     * Local networks                                                                             \nThematic \u2013 related directly to the subject matter of the teaching    * Sources of information and ICT resources: location, access                                 \n                                                                     * Use of programs specific to the fields of knowledge of the subject                         \n                                                                     * Integration of ICT into the curricular design                                              \n                                                                     * Objective assessment of ICT resources                                                      \n                                                                     * New teaching and learning strategies using ICT, cooperative work, self-instruction         \n                                                                     * Selection of ICT and design of formative interventions set in a context                    \nPsycopedagogic \u2013 involving deep attitudinal components               * Using ICT to analyze students in the educational action                                    \n                                                                     * Using ICT to guide and identify the main traits of the students, reports, follow-up        \n                                                                     * Using ICT for the management of the educational centers                                    \n                                                                     * Management of the ICT resources in the classroom centers of resources, coordination        \n                                                                     * Elaboration of simple didactic materials: open programs, authoring tools                   \n\n\nDelivery methods\n\nThe channels used to provide training to the teachers will be in the form of web-based tools and in-service teaching materials. Some of the internet tools include email communications, discussion lists, and bulletin boards. Forms of synchronous communication between the teachers-trainer and the teachers include audio-chats and audio-video conferencing (Leung, Watters, & Ginns, 2005).\n\nOther forms of communication include virtual reality like action mazes, authoring tools like puzzles and quizzes, and collaborative environments. The World Wide Web (WWW) will be useful for teachers as a source of referencing through providing access to various resources including dictionaries, encyclopedias, news articles and search engines.\n\nInternet resources include educational portals, virtual resource centers, on-line writing labs, cyber listening labs, web-based English courses, virtual libraries, creative teaching websites, teacher\u2019s websites, student websites, on-line English grammars, and help centers on English related questions (Leung, Watters, & Ginns, 2005).\n\nTeachers will be given the option of choosing the web-based tools that they will be most comfortable using. Teachers who are going to use the Internet will have to start looking for appropriate web sites or resources that they can use in their teaching (Leung, Watters, & Ginns, 2005). However, there are many instances in which teachers become overwhelmed by the amount of information they find.\n\nHence, studies have proposed a model for selecting material on the World Wide Web, enabling the user to evaluate web pages critically without getting lost in all the information. The criteria provided below will be used to facilitate the evaluation of websites for resources due to the versatility of the information available on the internet.\n\nPre-evaluate the learning objectives. Set goals as to what to find. Narrow down the subject as much as possible depending on what you are looking for in terms of opinions, facts, stories, interpretations, or statistics (Mikre, 2011). By knowing the responses in advance, teachers will create their own private set of criteria by which they can screen their sources quickly before evaluating their contents (Mikre, 2011).\n\nTest the sites that have been selected by a number of criteria. For this purpose, a set of criteria can be used that can be divided into two groups: criteria pertaining to the contents of web pages and those pertaining to design and user-friendliness.\n\nThe first category is the most important one since only reliable information can be used. Therefore, it is necessary to look at the sources, accuracy and objectivity (Mikre, 2011). However, in limited lesson time slots, it is also imperative that the sites are user friendly, well designed and easy to access. Furthermore, students should not be distracted from the contents by poorly designed or poorly structured sites (Mikre, 2011).\n\nThe SCAD checklist (Source evaluation, Contents, Access, and Design) is an easy-to-use set of criteria which incorporates both aforementioned categories:\n\nSource evaluation\n\nIt can be helpful to look for the author\u2019s credentials in order for the learner to find out if the author is knowledgeable and reliable. For instance, is there biographical information (education, training, relevant experience)? Is contact information (email, snail mail) provided? An anonymous publication is never a good sign. Is the site reviewed or rated by an organization?\n\nOr is the article published on the site of a well-known and reputable institution? If this is the case, then one can be sure that the publication was reviewed thoroughly before being put on the Web. What sort of meta-information is provided? Evaluative meta-information is harder to come by. It includes reviews, comments, ratings and recommendations (Gamage, Adams, & McCormack, 2009).\n\nContents\n\nIt is necessary to find out if the information is correct, in depth, truthful, precise or complete. The audience a publication is meant for, and the purpose it was written for can be beneficial too. A publication should be well balanced, moderate and not emotional. It should not comprise wild and irrational claims or arguments (Gamage, Adams, & McCormack, 2009).\n\nAccess\n\nStudents easily get bored if they have to wait too long for a page to download to their computer. So it is good to check if a site is not cluttered with unnecessary graphics that help to slow down download time (Gamage, Adams, & McCormack, 2009).\n\nIt is also necessary to check if a site is so popular that it can become overcrowded and difficult to access. Furthermore, teachers should make sure they have all the plug-ins and software that are required for using the site and that it uses standard multimedia formats (Gamage, Adams, & McCormack, 2009).\n\nDesign\n\nA web page should be well structured, and the individual pages should be concise and short enough to avoid having to scroll. In addition, a website should be easy to navigate and allow the teacher to use it spontaneously. Teachers should also ask themselves if the design is functional or just fancy (Gaible & Burns, 2005). Does it support the contents or does it constitute a distracting element?\n\nDo all parts of a page work and are the hyperlinks up-to-date? A last consideration is the degree of interaction. One of the benefits of using the Internet is that students can interact and not merely passively sit back and consume information (Gaible & Burns, 2005).\n\nHence, the SCAD list can be summarized as shown below, as proper guidance for students (teachers) in identifying the most appropriate resources\n\nSource evaluation  Trustworthy source: information obtained from author\u2019s credentials, e-mail, organizational support, rating, grammar, meta data            \nContents           Provides accurate information that is relevant: up-to-date, comprehensive, objective, reasonable, consistent                              \nAccess             Standard multimedia formats in that it does not require extra plugins or applications to view it, it is free, easy to access, downloadable\nDesign             Well structured, easy to navigate, interactive, no distracting visual elements, functional design, no broken links.                       \n\n\nProgram assessment\n\nThe content will be piloted in one of the high schools, in order to use the teachers as trainers for a reaching more schools with the same methodology. The materials comprise a series of duties whose achievement requires the use of ICT tools and resources.\n\nThe primary objective of the delivery method is to inform the teachers of the ability of the tool in order to favor teaching-learning (Leung, Watters, & Ginns, 2005). Assessment will enable the teachers\u2019 trainers to determine:\n\n  * Whether the teachers are receiving the right training regarding information and communication technologies\n  * Which ICT competencies the different teachers should emphasize in order to account for the successful integration of ICT in teaching and learning processes\n  * Which ICT-based tools and resources account for the successful integration of ICT in the various classrooms\n  * The elements that enhance the efficacy of the integration of ICT in teaching learning processes.\n\nKeeping up with technology\n\nOne of the training techniques involves the establishment of an educational portal for the institution. The portal provides teachers with necessary resources on all subject areas, reference materials and manifold activities that can be conducted online.\n\nThe website will also provide teachers with access to guidelines and support, a catalogue of the available resources, and innovative school projects regarding the integration of ICT tools in the curricula. In order to optimize the benefits and reduce the disadvantage of integrating ICT, various initiatives will be introduced including:\n\n  * The development of workshops to enhance the exchange of experiences among colleagues\n  * Teacher training initiatives to provide a principled, meaningful approach to the creation and harnessing of new literacy such as dedicated websites.\n  * Creation of platforms that provide links to providers and users of various online classes and materials\n  * Creation of new training plans whose contents can be updated and revised periodically.\n\nReferences\n\nGaible, E., & Burns, M. (2005). Using Technology to Train Teachers: Appropriate Uses of ICT for Teacher Professional Development in Developing Countries. ICT and education series , 3(1), 2-17.\n\nGamage, D., Adams, D., & McCormack, A. (2009). How Does a School Leader\u2019s Role Influence Student Achievements? A Review of Research Findings and Best Practices. International Journal of Educational Leadership Preparation , 4(1), 23-43.\n\nLeung, K. P., Watters, J. J., & Ginns, I. S. (2005). Enhancing Teachers\u2019 Incorporation of ICT in Classroom Teaching. Educational Leadership , 40(1), 4-10.\n\nMikre, F. (2011). The Roles of Information Communication Technologies in Education Review Article with Emphasis to the Computer and Internet. The Role of Information communication , 12(1), 1-36.\n\nSansanwal, D. N. (2009). Use of ICT in Teaching \u2013 Learning & Evaluation. Educational Technology Lecture Series , 14(1), 21-29.\n",
        "label": "human"
    },
    {
        "input": "Hands-on Training Versus Computer Based Training Case Study\n\nTable of Contents\n 1. Introduction\n 2. Similarities\n 3. Differences\n 4. Conclusion\n 5. References\n\nIntroduction\n\nThe success of any modern organization depends on how well its personnel are trained. Most American organizations spend a lot of resources annually in training their human resources. The choice of a training methodology or technology is influenced by a number of factors. First, most companies usually consider the cost of the training methodology.\n\nSecond, the relevance of the training methodology or technology to a company\u2019s needs is considered. Many organizations or institutions like a training methodology that consumes little time. This paper compares and contrasts hands-on training verses computer based training methodologies as they relate to domestic and global business environments.\n\nHands-on training refers to an instructional methodology that is used by companies and educational institutions to train students or employees. Hands-on training is a more effective way of giving instructions than teaching in a conventional classroom situation. This mode of training enables a trainee to perform the task that is being taught at a particular time.\n\nIn most instances, the trainer practically guides the trainee on how he or she should go about a certain task. Both the trainer and the trainee ought to be well prepared for the actual training activities. For example, the trainer should prepare a set of objectives or goals and a list of tasks for a given training session.\n\nThe trainer should also set enough time for the training. On the other hand, the trainees should properly research on the task to be taught. Good preparation enables both the trainer and trainee to have a successful hands-on training session.\n\nComputer based training is an instructional methodology in which a trainer imparts skills on a trainee through a computer. There are different forms of computer based training. First, there are computer-assisted instructions that involve practical sessions.\n\nSecond, there are computer-managed instructions that evaluate students\u2019 performances and track their progress in learning. Third, there are computer-enriched instructions whereby the computer serves as a programming tool. Computer based programmes are \u201cdeveloped by teams of five specialists: an expert in the particular subject, a technician expert, an instructional designer, a graphics artist and a programmer\u201d.\n\nSimilarities\n\nBoth hands-on training and computer based training programmes should be well formulated for effective training activities. Instructors should set objectives which show the desired outcomes. Learning objectives make the training sessions more focused.\n\nHands-on training and computer based training are both effective in training students and employees. Therefore, any of these training methodologies can be used to train individuals on a given task.\n\nDifferences\n\nThrough hands-on training, trainees can gain first hand practical experience in a given area of study. Trainees can easily understand and recall what has been taught because there is opportunity for demonstration. This training methodology also allows trainees to seek clarification from their instructors.\n\nFor example, \u201corganizations like Occupational Safety and Health Administration require that employees should be given opportunity to ask questions\u201d. According to Occupational Safety and Health Administration, trainees should be given applied experience in working with equipment.\n\nHands-on training also enables an instructor to assist trainees when they do not get a concept right. However, computer based training does not provide trainees with such an opportunity. In most instances, the trainees will only watch a demonstration on a computer screen. This may result into boredom that can hinder effective learning.\n\nHands-on training gives new employees an opportunity to interact with their colleagues in the company. New employees are likely to receive a warm reception from the officials of a company if they are inducted into the system through hands-on training.\n\nThis is contrary to computer based training in which new employees might not have a chance to interact closely with their senior workmates. Consequently, the new employees \u201cmay perceive the company as faceless and uncaring for their needs\u201d.\n\nComputer based training is more uneconomical compared to hands-on training because it compels a company to spend extra amount of money in developing and maintaining their own customized computer based programs. For example, a company may part with not less than $ 300,000 to $ 500,000, excluding the upgrading cost.\n\nOn the other hand, computer based training is more flexible than hands-on training because it offers flexible training sessions to new employees. For example, an employee can train at home. In the case of hands-on training, one has to be present at the training venue.\n\nConclusion\n\nFrom the above comparison of hands-on training and computer based training, it can be concluded that a company or an institution should choose its training methodology carefully. The choice should be made on the basis of cost, time, and need.\n\nA company should also make a comparison between different training methodologies and technologies. This will enable it to make a wise decision because the quality of training that is given to employees or students will either boost or lower their productivity.\n\nReferences\n\nKoppett, K. (2001). Training To Imagine. New York: HRD Press, Inc.\n\nPike, R. (2003). Creative Training Techniques Handbook. New York: HRD Press, Inc.\n\nSisson, G. (2009). Hands On Training. Sanfrancisco: Barrelt Koehler.\n",
        "label": "human"
    },
    {
        "input": "Apple Computer, Inc.: Maintaining the Music Business Case Study\n\nIntroduction\n\nApple Computers Inc. was incorporated in early 1977. Its primary business was the manufacture of personal computers. Apple\u2019s primary target was to make privet individuals to own a computer. This strategy has changed over the years. It has incorporated other devices into its production line (Chapman & Hoskisson, 2012). Over the years, Apple\u2019s innovations have revolutionized the smart devices market.\n\nThe company was reluctant to venture into the mobile phone market at first. Steve Jobs once said that Apple would not manufacture a phone. His reason was the company\u2019s incapacity to control what went into the device (Meyer, 2010). A few years later, Apple produced the iPhone.\n\nThe iPhone is just one of Apple\u2019s leading products. ITunes allows subscribers to download music and movies at a fee. This innovation fundamentally changed music distribution worldwide. The company does not earn substantial amounts from this service. Apple capitalizes on its influence on other products instead.\n\nIt significantly increases iPod\u2019s sales as many customers use it to store their music and movies (Chapman & Hoskisson, 2012). The iPod is Apples cash cow, generating up to 40% of its revenues (Meyer, 2010). Apple\u2019s iPhone combines mobile phone and iPod features.\n\nThe company operates in a highly competitive industry, competing directly with cell phones, computers, digital content and consumer electronics. Apple combines innovation and smart marketing strategies to maintain its competitive advantage (Meyer, 2010).\n\nStrategic challenges facing Apple Computer\n\nApple Computer keeps a tight control over the quality of its products (Chapman & Hoskisson, 2012). This has assured its customers of high quality products and services from the company. Keeping this control and maintaining quality is a challenge facing the company.\n\nVarious companies supply its product components. These include; Texas Instruments, Phillips and Samsung. Some of these suppliers have expressed concern over Jobs\u2019 controlling nature (Chapman & Hoskisson, 2012). Maintaining a balance between quality controls and good relationships with suppliers is a major challenge for the company.\n\nApple\u2019s innovative ability has always been the hallmark of its success. It produces attractive pieces which competitors keep reproducing (Chapman & Hoskisson, 2012). The challenge to Apple is maintaining these abilities.\n\nRemaining innovative helps to keep its products unique and different from its competitors. As it expands globally, keeping its innovations secret will be another challenge. Information on designs and other important details may leak from its factories worldwide.\n\nApple\u2019s traditional clients were technology savvy fanatics. Its customer base has widened with an increase in its products. Some of its products aim at specific consumer groups such as students (Chapman & Hoskisson, 2012). Marketing to these diverse markets and meeting their expectations is going to be a huge challenge. Eye-catching marketing ads have always been associated with Apple.\n\nWith a wider and varied customer base, the company needs to remain relevant. Satisfying a varied customer base is not going to be easy for Apple. There are customers who prefer products that are compatible with different devices. Apple has always preferred services that are only compatible with its products. Meeting these expectations require a change of tactics.\n\nThe company is expanding globally (Chapman & Hoskisson, 2012). This will require a professional team to handle the increased exposure. It will also need to create and maintain good relationships with all the stakeholders. This is a significant challenge for Apple.\n\nThis is because it has always relied on Steve Jobs for this. It needs other faces to show passion for the brand and boost its popularity. Jobs could not do it alone after the expansion. There should be a succession plan in place.\n\nMeasuring company success\n\nThere are various ways by which a company\u2019s success can be measured. Accounting measures are popular methods of measuring company success along financial lines. These include; profitability, return of assets and return on equity ratios among others.\n\nThey give a snapshot of the financial status of a company. Data that can be quantified is used in this method. The problem is that these methods do not take qualitative data into account. A company\u2019s success is only measured in financial aspects (Hitt, Ireland & Hoskisson, 2010).\n\nUsing a balanced scorecard can give a balanced assessment of a company\u2019s success. Accounting measures are applied alongside three other measures. This method considers different perspectives. These are; the internal processes, accounting measures, learning and growth, and customer perspectives. By considering both qualitative and quantitative data, one gets a balanced analysis s of the company.\n\nThe balanced scorecard reveals the weaknesses and strengths of a company. This measure is similar to the triple bottom line method of measuring company success. When applying the triple bottom line, one considers both the environmental and social successes of the company (Hitt, Ireland & Hoskisson, 2010).\n\nBenchmarking focuses on the current success of a company in different aspects. It also looks at ways of improving or maintaining that success. Significant factors affecting a company\u2019s success are identified. These are then measured and used as a yardstick. The company must aim at meeting and surpassing these measures. Customer satisfaction, return on assets, profitability and return on equity are commonly used as benchmarks.\n\nInternal and external environmental factors affecting Apple\u2019s future\n\nA company\u2019s future significantly depends on its competitive advantage in the market (Hitt, Ireland & Hoskisson, 2010). This is heavily influenced by factors in its internal and external environment. Apple has a rich well of resources, both tangible and intangible.\n\nThe company\u2019s wealth, however, lies in its intangible resources. Apple\u2019s employees have a knack for innovative designs. Its CEO-Steve Jobs is a good example of an innovative mind within Apple. He has been the brain behind many of its innovations.\n\nThe company\u2019s brand is another internal resource. It is associated with attractive, high quality devices (Chapman & Hoskisson, 2012). It has also created a network of good relationships with other players in the industry. This has enabled it to make agreements with music record companies and movie makers.\n\nThese partnerships have been the bedrock of iTunes\u2019s success. The personal qualities of Steve Jobs have helped the company to maintain a good public image. He has also helped to maintain good relations between the company and its customers.\n\nApple\u2019s internal capabilities are in advertising, production and marketing. It produces products that cater for its diverse markets. Its creative adverts and marketing strategies reward it with high sales. Its core competence is innovation. The company comes up with new technology and devices regularly. This gives it a healthy competitive edge over its competitors (Chapman & Hoskisson, 2012).\n\nIn the external environment, a variety of factors are likely to affect Apple\u2019s future strategies and success. The digital media industry that Apple has ventured into is fiercely competitive. Entry barriers are low. The capital required is not prohibitive and many firms with resources can easily invest.\n\nITunes, iPod and the iPhone face stiff competition from new and existing entrants in the market (Chapman & Hoskisson, 2012). The future of Apple depends on how well it maintains its competitive advantage.\n\nApple\u2019s products face competition from cheaper brands. Its customers have easy access to products that can substitute Apple\u2019s products. Its competitors, who produce and sell similar products at a cheaper price, threaten Apple\u2019s dominance. Global markets such as Japan are saturated with competitors. Apple has found it hard to penetrate this market because local manufacturers have a strong hold over it (Chapman & Hoskisson, 2012).\n\nIts operations are affected by copyright laws. It sells protected material on iTunes (Chapman & Hoskisson, 2012). It, therefore, has to protect these products from copyright violations. This is a significant factor in the arrangements it makes with music records and movie companies.\n\nTechnological advancements also affect Apple\u2019s business significantly. As technology changes, it has to be innovative to remain relevant. Consumers want the latest technological devices. Its survival depends on how well it keeps pace with changes in technology.\n\nApple\u2019s strategy in the face of rivalry\n\nApple\u2019s marketing and innovative strategies make it a leader in the industry. It consistently produces innovative products whose popularity is unrivaled in the industry. Though similar products exist, they do not match apple in quality. Apple\u2019s customers have become accustomed to superior experiences (Chapman & Hoskisson, 2012). However, the incompatibility of its services with other products could be its undoing.\n\nCompetitors such as Microsoft and Samsung manufacture products that are compatible with many devices. The company\u2019s advertising and marketing strategies maintain its competitiveness. It operates its own outlets that increase its products\u2019 exposure. Apple\u2019s strategies have fared favorably even with fierce rivalry within the industry.\n\nRecommendations\n\nThe popularity of the iPhone and other products exposes Apple to increased malware threats. The company should increase the security of its products. Assuring customers of the safety of their devices will maintain customer confidence.\n\nAs the company enters the music industry, it should maintain the core values of the Apple brand. Innovative, high quality products should be maintained. Customers\u2019 expectations of the Apple brand should be satisfied even in new markets.\n\nCurrent products; phones, iPods computers and other should be updated. This will differentiate them from the competing products. It will also ensure that they remain relevant as technology changes. The company should craft a succession plan.\n\nIt relies significantly on Steve Jobs for its public image. He is Mr. \u201cfix it\u201d when things go wrong. The company should groom other persons for the role. The press and other stakeholders should have other faces to associate the brand with.\n\nApple has a reputation of concentrating on its customers and not on competitors. This is a good strategy as the company establishes an intimate bond with them. It learns what customers prefer and focuses on satisfying them. However, the competition it faces is increasing by the day. It faces threats not only from established companies but also startups.\n\nIt should pay more attention to competitors without becoming obsessed with them. As the company seeks to partner with different companies, it should not lose its control over quality. These partnerships are necessary in its global expansion plant. As it grows big, it may lose its original strengths. It should guard against this to maintain customer loyalty.\n\nReferences\n\nChapman, R & Hoskisson, R (2012), Apple Computer Inc.: Maintaining the Music Business While Introducing iPhone, Case Study.\n\nHitt, M , Ireland, R & Hoskisson, R (2010), Strategic Management: Concepts and Cases: Competitiveness and Globalization, South Western Educational Publishing, Sydney.\n\nMeyer, C (2010), 7 Challenges Facing Apple after Surpassing Microsoft\u2019s Market Cap, Retrieved from http://www.workingwider.com/strategic_innovation/7-challenges-facing-apple-after-surpassing-microsofts-market-cap/\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics and Digital Evidence Expository Essay\n\nConducting investigation of computers does not differ much from traditional public investigation. It also implies search for material evidence, including word documents, photos, pictures, or graphic data. The only difference lies in methods of extracting these computer artifacts (Nelson & Phillips, 2010).\n\nIn this respect, to disclose the facts of kidnapping, computer forensics could be of great value because it can allow the policy office to find all necessary information about the forthcoming crime or about the crime that has already been committed (Nelson & Phillips, 2010).\n\nIn case with kidnapping, the computer files and hard disk should be investigating for possible photos of the kidnapped victims, stenography, a process by means of which an image is concealed behind another digital image, and deleted files that are possible to extract and recover.\n\nWhen electronic data has been collected to identify the kind of the incident and introduce evidence of the crime, it is important to organize a meeting with the witness who can provide details of the incident (Kannellis, 2006). The one-to-one interview can help me to get into more detail about the case and compare the testimony with the files and pictures found on computer (Anson & Bunting, 2007).\n\nOne of the most frequently used tools for concealing information is stenography, which is also considered the major threat to governmental security (Caloyannides, 2004). This way of information coding is used by most criminals who strive to conceal the images and photos (See Appendix 1). Before evaluating and deliberating the evidence, it is essential to carry out baseline data network analysis to understand the crime scene.\n\nFurther steps will determine the normal usage and topology of the network, interview with system administrators, logs of affected and associated systems, and outputs received from IDSs.\n\nThe data that can be obtained from computer is stored in a running system memory, which constitutes as great evidential value (Anson & Bunting, 2007). As soon as the running processes are determined and connections are active, it is possible to generate a clear picture of what actions have been performed by a user, a potential suspect.\n\nInterestingly, in case the kidnapper does not realize the face of the crime, it is reasonable to build in specific equipment that can permit the investigator to control subsequent course of action provided by the criminal. Such an approach will increase chances for discovering new evidence proving the fact of kidnapping (Anson & Bunting, 2007).\n\nWhile examining the evidence, it is purposeful to refer to various types of evaluation, including live analysis and cross-sectional assessment. The first step to be taken is detecting the first activity that was carrying out on computer at the moment of the accident (Anson & Bunting, 2007).\n\nUsing cross-sectional analysis through acquisition, analysis, and reporting, it is possible to detect the fact of criminal\u2019s using several computers for storing data about the victims (Mohay, 2003). There is a great probability that the attacker can use more than one database for allocating pictures, documents, and stenography to distract the crime scene investigators.\n\nFinally, although some of the resources detected on the computer of the accused, they can be applied to characterize and define the background, including his activities and hobbies. Therefore, this information will also be helpful for forensic professional who make the psychological profile of the kidnapper.\n\nReferences\n\nAnson, S., & Bunting, S. (2007). Mastering Windows Network Forensics and Investigation . New Jersey: John Wiley and Sons.\n\nCaloyannides, M. A. (2004). Privacy Protection and Computer Forensics . US: Artech House.\n\nKannellis, P. (2006). Digital Crime and Forensic Science in Cybe rspace. US: Idea Group Inc.\n\nMohay, G. M. (2003) . Computer and Intrusion Forensics . US: Artech House.\n\nNelson, B., & Phillips, A. (2010). Guide to Computer Forensics and Investigation . US: Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Human Computer Interaction \u2013 Heptic Technology in PlayMotion Essay\n\nThe traditional PlayMotion era in gaming is over. With the introduction of haptic applications of in gaming, PlayMotion does not constitute the ultimate gaming experience any more. PlayMotion makes it possible for gamers to experience the gaming environment in enhanced ways.\n\nIt translates their actions into shapes and patterns by applying gesture recognition technology and a graphics engine to process the resultant environment for on screen display.\n\nA video projector displays images on a screen. In itself, play-motion is a unique concept to gaming, and provides gamers with the ability to create their environment. Playmotion (2008) stated, \u201cThrough colorful, positive, real-time, high performance visualizations, the invisible is made manifest, and imagination is made real\u201d.\n\nThe one thing PlayMotion does not achieve is providing tactile perception to gamers. The sense of touch is fundamental to the human sensory experience. Without it, the experience remains incomplete no matter how graphically advanced. Haptic technology is finding application in all sorts of places, from flight simulation to military training.\n\nIn the medical field, the technology finds application in the training of surgeons, who get a virtual feel of a surgical process before they ever lay a scalpel on a patient. Phone manufacturers such as Nokia are applying haptic technology to handset design to enhance the user experience.\n\nAs Robles-De-La-Torre (2008) states, \u201cHaptic perception in virtual environments (VEs) relies on sensory signals arising from computer controlled mechanical signals produced by haptic interfaces\u201d. Nintendo 64\u2019s Rumble Pak implemented an application of haptic technology. What haptic technology promises gamers tactile is feedback on their gaming experience and a multisensory involvement in the gaming environment.\n\nWhen these two advantages combine with the possibilities available today from computer graphics, there is a tremendous improvement in human-computer interaction. Newer concepts such as haptic touch screens are already in the market. Using haptic technology, a gamer will \u2018feel\u2019 his actions.\n\nBy using controls that provide haptic stimuli, a gamer can realistically feel elements such as the texture of a road, the shooting from a gun, flying in bad weather among others.\n\nThis connects the gamer better with the environment. The gamer will experience vibrations when they hit a bump or when they crash into something in the course of their gaming. Currently, Sony\u2019s DualShock technology, and falcon, from Novint are some of the gaming options in the market that use haptic technology.\n\nThe enhancements proposed for the PlayMotion technology include inclusion of hepatic gloves and a hepatic headgear. These two enhancements will add a completely new dimension to the experience for PlayMotion Gamers by providing them with tactile feedback and input to the virtual environment created by PlayMotion.\n\nThe hepatic glove will also serve as a control unit for game elements. This is to say that it will have an input function and a feedback provision function. The game design will include the following elements\n\n 1. Enhanced Playmotion features, including gesture recognition and video projection capabilities, using the latest available technologies because of the advanced computer graphics\n 2. Hepatic gloves, with input and feedback functions since hands provide the best surface for sensory experience of hepatic stimuli\n 3. An optional hepatic helmet, which will enable users to feel hits and to enhance the sense of motion\n\nReference list\n\nRobles-De-La-Torre, G. (2008). Principles of hepatic perception in virtual environments. In Gr\u00fcnwald, M. (Ed.) Human haptic perception: Basics and applications. Berlin: springer.\n\nPlaymotion, (2008). Playmotion is founded upon ideas of magic, transformation, and empowerment . Retrieved from: http://www.playmotion.com/\n",
        "label": "human"
    },
    {
        "input": "Human Computer Interface: Evolution and Changes Essay\n\nHuman computer interface development has brought considerable change in many sectors such as political, social, cultural and economically. Some of these changes have affected the community either positively or negatively. The continuous evolving of human interface computer highlights substantial changes that are certain to happen in the future.\n\nIn entertainment one can do a lot in images when they are digital because computer interface has enabled users to produce digital images in many times as one wishes and broadcast to a lot of people around the world through the use of websites (Schindler, 2002). It is highly possible to take and share a photo to millions of people, a thing that was not there before.\n\nSocially the world becomes like a small village whereby it is possible to communicate to people in different geographical regions. This will make the world unite more and have a rich culture due to cultural exchange. People will exchange, share and discuss ideas thus solving world problems easily.\n\nSome human computer interface such as brain computer interfaces\u2019 helps disabled persons use their brain waves to control digital movies, switches the lights and turn on the music. Entertainment would be less cumbersome as it provides clear and fast devices to carry out some entertaining tasks.\n\nThese human interfaces include the use of hot hand device which is a wing worn by electric guitarist, which uses wireless, transmitter to come up with various sounds effects by a different hand gestures (Mills, 2010). Socially ones movements will be remarkably safe as ones locations will be easy to be monitored. This is possible through the use of GPS as well as capturing ones locomotion by use of CCTV cameras.\n\nThis will ensure the cases of insecurity have reduced drastically. People in the future will be making their office be at home, the spacious offices can be discarded as workers will not be required to travel to work.\n\nThe conferences will not require members to be there in person as the events can be transmitted to people in their homes and they can make suggestions too (Bushnell, 2008). Families will be stronger since they will communicate to one another through computer-based video connection.\n\nThe aged will benefit more as they will use computers and contribute to the economy. They will continue working using computers and these will change the way people view aged members in the society. Some people will use computes to spy others.\n\nParents will be in able to monitor their children activities even when they are not together. This will also apply to other people such as policemen as well as thieves. Future automotive will have an apartment for computer games whereby commuters will be able to keep themselves busy along the way.\n\nPerson\u2019s centralized information can be accessed by many individuals without the owner\u2019s permission. All additional software and cables will not be needed to synchronize ones laptop and palmtop a thing that will lead to more social connections (Atkin, 2007).\n\nThe cost of entertainment will reduce as one will only buy the song he wants instead of buying the CD. This will make many individuals afford and access only what they want. There will be no much digital divide in the computer use as many people will already have used them before. The aged have used the computer in one stage during their lifetime making them adopt new development easily.\n\nReferences\n\nAtkin, D. ,2007, Communication technology and social change:theory and implication. Chicago: Lawrence Erlboum press.\n\nBushnell, D. ,2008, The role of computer in future instruction system. London: University of Michigan.\n\nMills, C. ,2010, Computer-based testing:building the foundation for future assessment . Mahwah: Tayler & Francis.\n\nSchindler, J. ,2002, Graphic user interface for control of a home entertainment system. Computer and entertainment , 13-14.\n",
        "label": "human"
    },
    {
        "input": "Computer Manufacturer: Apple Computer Inc Essay\n\nIntroduction\n\nApple Computer Inc. has been in music business for over 30 years. The company has emerged as one of the highly recognized players in computer industry. The giant computer manufacturer is mostly known for its famous apple\u2019s iTunes online music store, iphones and ipods among other products such as laptops and desktop computers.\n\nThe company has quite often endeavored to maintain its market share despite myriad of internal and external factors which have negatively affected its profitability. Factors such as stiff competition from both legal and illegal online video and audio music download services as well as competition from rival companies like Dell and Microsoft have equally downsized its operations.\n\nFurthermore, the company has not eluded the effects and threats of possible substitutes and low-priced products that tend to unfavorably compete with its products such as iPhone, apple TV and the iPod. As such, it operates in a very dynamic market where video-on-demand, mobile phone and music technology is rapidly changing thereby forcing all the players in this industry to devise very effective strategies for survival.\n\nThere is no single company in the computing industry that has full control of new technology and therefore, each player in the market has to change in the same pace as technology in order to remain relevant in the market. It is imperative to note that many companies in this industry are in a state of dilemma to either invest in their traditional modeling such as computers or invest in the newer products such as iphones and ipods.\n\nExternal analysis\n\nCompany\u2019s profile\n\nApple Inc. began in 1976 through a partnership between stephen Wozniak and Steve Jobs. In 1977 it was publicly known as Apples Company Inc. As a public company, it has employed thousands of employees who are skilled personnel working as highly trained human resource managers and marketers.\n\nThe Company\u2019s profile showcases an experienced team of executives with management officers. Its sale in music alone has risen over the years. For instance, cash equivalent in 2007 was $6,392 million. It handles both software and hardware products such as computers and laptop accessories as well as music and video products such as iphone and iPods,\n\nKey success factors\n\nIn order to maintain the music business, Apple Inc. has built a robust customer base. The company has millions of customers who are loyal to its music products that are distributed across the globe. The company has maintained a competitive edge in the market due to the quality of video and music products it offers.\n\nThe management at Apple Inc. has continued to attract new customers as well as retaining the existing one by offering satisfactory services. This has enabled the company to attract large crowd whenever they introduce a new product in the market. Such products include Apple TV and iPhones.\n\nAdditionally, the company uses creative and authentic designs when advertising its products. The adverts of the company are designed in a way that they not only persuade the customers, but also convince them to buy the products.\n\nFor instance, the company has over the years successfully used celebrity musicians to advertise its products such as iPhone and iPod to attract new customer\u2019s particularly young generation. These types of adverts have continued to build good reputation for the company\u2019s products.\n\nInternal analysis\n\nFrom the financial statement of the year ended September 2010, the profit of the company rose from $ 42.9 billion reported in 2009 to $65.2 billion in 2010. The sales and revenue of the company have also been growing steadily for the past few years.\n\nThis positive financial growth of the company can be attributed to different strategies that the management of the company has undertaken in order to increase financial performance of the company despite changing economic conditions that are experienced worldwide.\n\nMoreover, offering a wide range of products and services in order to increase its sales has been used as an internal marketing strategy. For instance, the company has come up with various types of music products and services that suite different types of customers. These products include commercial music softwares such as Mp3 players, iPods, iPhones and Apple TV.\n\nOffering a wide variety of music products has enabled the company to retain their customers who feel that all their needs and preferences are well catered for. Again, different products and service perform differently in the market depending on internal and external forces. Therefore, offering variety of product will ensure that the sales of the company are maximized in all the seasons despite some products performing poorly in the market.\n\nStrategic issues\n\nThe Company has a highly competent and innovative team. It has employed a team of researchers and innovators who have made it possible for the company to be a market leader in the world of music technology.\n\nThrough research, this team has been able to keep at pace with the changing music technology where they have managed to invent new devices that meet their customers music needs. Each an every year, the company is coming up with new products such as iPhone and iPods as well as more advanced laptops that have enabled the company to compete effectively with the rival companies in the market such as IBM.\n\nManagement\n\nThe company has efficient management team comprising of board of governors, CEO and other management staffs who have spearheaded the company to greater heights despite global economic turn down.\n\nThe management team is driven by the vision of the company and it is committed to accomplish goals and objectives of the company. This team overseas all the operations of the company at all levels and make strategic decisions such as expansion strategies that have enabled the company to be a market leader in the world of technology.\n\nHuman resource department of the company has created a good working environment for all the employees which have enabled it to increase the productivity in all departments. A good working environment has been created through well defined rules and regulations that stipulate how employees should behave in the work place.\n\nWhat the employees are expected to do is clearly defined and what is not expected of them as well as disciplinary actions that will apply if one misconduct himself or herself in the work place.\n\nClear rules and regulations have ensured that all employees conduct themselves properly and they treat their fellow employees with dignity and respect. This enhances good personal relations among the employees which can eliminate all forms of discrimination enabling employees to work as a team for the common goal of the organization.\n\nHuman resource department has also played a crucial role in motivating employees of the apple company. The department has recognized the importance of a well motivated workforce in the achievements of goals and objectives of the organization.\n\nThe company has come up with initiatives to increase morale of the employees such as a rewarding program, recognizing hardworking employees, promotions and above all good compensation that have motivated them to work hard.\n\nThe human resource department assesses the performance of all the employees and ranks them according to their performance in a given period of time. Promotions and recognition is then based on those ranking where at times employees are given an opportunity to choose employees of the year for awarding.\n\nVRIO Analysis\n\nApple Inc. has been able to gain competitive advantage by identifying strategic capabilities that are appropriate and comprehensive in planning and allocation of resources. According to competitive advantage theory, high market price for products should be directly proportional to high- quality of the given products.\n\nThe ability to sell music products of high quality at low prices and still maintain competitive advantage indicates that the company has restructured its production capabilities in such a way that mass production is done at relatively low cost.\n\nFurthermore, Apple\u2019s competitive advantage has been visualized through its ability to by outperforming its competitors through developing combination of attributes such as skilled personnel, highly trained human resource in marketing skills. The Company showcases an experienced team of executives with the management officers.\n\nIts competitors find it cumbersome to imitate its customer service, durability, firm reputation, brand image, reliability, innovativeness and technology as well as product quality. This can be attributed to its differentiation strategy. This latter has helped it gain competitive advantage over other competing companies due to customer loyalty and shrewd marketing techniques, quality advertising and other special offerings.\n\nIt is important to note that competence and differential strategy secure a company\u2019s market leadership through competitive advantage. Other important aspects at Apple Inc. include organizational resources, human resource management, innovation and technology.\n",
        "label": "human"
    },
    {
        "input": "The Usefulness of Computer Networks for Students Essay (Critical Writing)\n\nComputer network is the interconnection of many resourceful computers through domains which provide users with the possibility to freely share ideas, information and other useful resources. In my class setup, computer network has played a big role in construction of knowledge among my fellow learners. One major role which computer network plays in imparting knowledge is implemented through sharing of ideas.\n\nAccording to Resnick (1996), learning is described as an active process where knowledge is constructed from various experiences in the world, thus making ideas rather than getting them. Through computer network, my fellow learners share various ideas they have made in the process and this became available to all of us. We are able to access various ideas, disseminate, and improve them which results in the splendid end product.\n\nComputer network has also enabled us to collaborate in real-time through chatting and using newsgroup forums where we communicate freely among learners which results into enhancement and construction of new knowledge. The network has enabled us to make computer simulations in various projects we are undertaking and which are tested by other learners who act as users of the constructed simulations.\n\nThe feedback obtained from the users helps us improve on our simulations through active participation on the internet. In the absence of computer network in the school environment, learners can use physical discussion forums where we can meet in study rooms, meeting places and still share ideas thus extending distributed constructionism.\n\nLearners can also record their findings and work with electronic tapes, e.g. flash disks, compact disks and post them via postal mails to their colleagues. Mobile phones text messages can also be used to share ideas and resources hence promoting distributed constructionism.\n\nComputer network and the Internet act as Rorschach test for educational philosophy whereby it can be interpreted into many ways. Through my own educational philosophy, I see the internet as a way of life throughout the world.\n\nTo defend my philosophical view, the internet has enabled the entire world to become a global village whereby users share their daily life experience hence making the internet as a virtual way of life. The information shared on the internet can be used by various users depending on their area of interest.\n\nPea\u2019s idea of enabling educators to facilitate individuals from tool-free cognition is a feasible and realistic idea which can be adopted at any level of learning (Pea, 1993). Distributed intelligence and cognition are defined as a theoretical approach that focuses on interaction between individuals and other factors that are depend on how they are approached and implemented.\n\nThe theory is realistic in that no individual is an island of his own information and students get to know things through interaction with other people. This can range from interaction with cultural influences, material artifacts and with any other social activity. To facilitate Pea\u2019s idea, educators need to understand the level of grade they are teaching.\n\nLower grade students need to use simple simulations and resources which they can easily understand as opposed to adult learners who can understand complex simulations. Learners who will have access to information will learn faster than those who will have limited information.\n\nComputers cannot replace the role of adults in guiding the children for 100%, but they can play a very big role in imparting knowledge to youngsters. This notion is applicable where computer intelligence is as a result of A customized software which has been programmed by various technocrats through distributed constructionism, tested by various users and finally presented to the educators for the purpose of teaching children.\n\nThe use of computer intelligence should not be limited to children and their ZPD only, but should also be applied to adults as well. The determinant factor when using computer intelligence should be the selection of learning tools.\n\nIn this case, educators should select learning tools depending on the level of the learners to facilitate distributed cognition. Educators should also break down a problem in question into simple manageable levels for the learners to understand.\n\nIn my learning environment, I have personally experienced computers substituting the skilled other. In this case, computer simulations were used to state and represent a problem. This was done by using whiteboard diagrams which broke the problem down into manageable levels that can be understood.\n\nReferences\n\nPea, R.D. (1993). Practices of distributed intelligence and designs for education. In G. Salomon (Ed.), Distributed cognitions, Psychological and educational considerations (pp. 47-87) . NY: Cambridge University Press. peaDistrib.pdf\n\nResnick, M. (1996). Distributed Constructionism. Proceedings of the International Conference on the Learning Sciences Association for the Advancement of Computing in Education Northwestern University . Retrieved from\n",
        "label": "human"
    },
    {
        "input": "Theft of Information and Unauthorized Computer Access Essay (Article)\n\nTable of Contents\n 1. Introduction\n 2. Discontented Employees\n 3. Hackers\n 4. Journalists and Political Activists\n 5. Information Agents\n 6. Conclusion\n 7. Reference List\n\nIntroduction\n\nInformation plays a vital role in successful organizational management. Information management using modern information technology enhances organizational survival and decision-making.\n\nIndividuals with ill intentions can utilize any security flaw existing in the organization\u2019s information systems to pursue their selfish interests; this, in other words, can render an organization susceptible to risks, which are costly or lower the integrity of the organization.\n\nUnauthorized computer access is only possible when an organization information security strategies are weak thus leaving an organization\u2019s information resources vulnerable or accessible. When there are loopholes in the information security management system, malicious individuals take advantage of these ostensible flaws to achieve their ill intentions of either stealing or corrupting an organization\u2019s databases.\n\nThis paper discusses some of the key threats to an organization\u2019s information security system or the persons that compromise an organizations computer system through unauthorized entry. Some of the highlighted threats are discontented employees, journalists, and political activists, hackers and information agents.\n\nDiscontented Employees\n\nAn organization\u2019s information systems can be compromised in a variety of ways. The most notable being actions by discontented employees. Employees become a threat when they have been laid off or when they have left their jobs voluntarily in search of greener pastures.\n\nSuch people might attempt to steal information with a motive of using it to exploit the company by selling confidential information to the competitors or to leverage on the same to get a new job. Owing to their familiarity with the computer system employed, such unscrupulous staff stands a better chance of successfully sabotaging, hacking or distorting information (Salehnia, 2002).\n\nIdeally, every organization should have an information security policy. Unfortunately, most companies lack proper policies to govern storage or management of sensitive and confidential information against theft and fraud. One of the most basic and yet reliable measure towards curbing employee related risks is doing an employee audit.\n\nAn employee audit can be done electronically or manually to ascertain that no employee is not in custody of vital company information as they leave the company premises. In addition, magnetic chips should be fixed on crucial organization documents such as reports and files to control information movements.\n\nHackers\n\nA hacker is an individual who is well equipped with programming skills and uses these technical skills to gain unauthorized access to an organization\u2019s information systems. Hackers do this for personal gigs or monetary gain. Hacking is a serious threat to many organizations\u2019 computer systems. Hackers break into computer systems by compromising the integrity and privacy of data (Cross & Shinder, 2008).\n\nBy obtaining this information, they easily use it to trade with interested parties pretending to be the genuine users. Unlike, other security risks, hackers, pose the greatest risk and can completely jeopardize the operations of the firm. However, several measures such as regular change in passwords and encryption of passwords and user names are important in containing the risk.\n\nJournalists and Political Activists\n\nJournalists or political activists might be interested in the operations of an organization with intentions of discrediting or damaging the organization\u2019s reputation. While there are codes of ethics that govern journalism, practical codes vary giving room to substantial overlap in information dissemination.\n\nThus, some journalist and political activist take advantage of this mostly through bias reporting of facts, misrepresentation of figures and blatant deception to the public. Sometimes they will suppress the information even disregarding facts through mischief. Overly, they attempt to shape and influence the opinion of the public (Bosworth & Kabay, 2002).\n\nInformation Agents\n\nThese people use subversive tactics to gather proprietary information. This information could be anything from new product ideas, bid information, proposals, marketing strategies or research and any other corporate data.\n\nThey penetrate the information system of a given organization with the intention to find information that they can trade. They sell this information to the highest bidder especially in financial or consultancy or brokerage industry. It is, therefore, important for all organizations to exercise vigilance and secure their systems to any such security threats.\n\nConclusion\n\nIt is critical for all organizations to embrace various security strategies to guarantee the safety of vital information and resources in the organization. This can be done by having back up files, changing passwords regularly, use of encryptions, installing antivirus scanners to prevent any unauthorized access to information against persons or foreign software programs.\n\nPhysical security such as employing guards, using biometrics doors, and controlling unauthorized access can also be deployed. Finally, each organization requires a meticulously formulate information security policy. Failure to institute measures against information security threats, opens up an organization to much information security threats, which can easily compromise an organization\u2019 information systems.\n\nReference List\n\nBosworth, S & Kabay, M. (2002). Computer Security Handbook. New Jersey, NJ: John Wiley & Sons.\n\nCross, S. & Shinder, D.L. (2008). Scene of Cybercrime. Burlington, MA: Syngress Press.\n\nSalehnia, A. (2002). Ethical Issues of Information Systems . Hershey, PA: IRM Press.\n",
        "label": "human"
    },
    {
        "input": "Supply Chain Management at Dell Computers Essay\n\nAccording to Klapper et al, supply chain management is the integrated set of functions that seek to ensure that the products of an organization reach a customer in a timely manner. Therefore, the roles of the supply chain manager at Dell computers are to ensure that the customer is satisfied with the product distribution utilized by the company.\n\nThe main roles of a supply chain manager are to ensure the effective selection and management of suppliers and to supervise efficient transportation of products from the production location through storage, though storage, and finally to the consumer (Chomilier, Samii, and Wassenhove).\n\nTherefore, at Dell, the responsibilities of the supply chain manager are to ensure that there is effective communication between the customer, the supplier, and manufacturer of the final product.\n\nThis role leads to a high demand for supply chain managers by firms such as Dell because the supply chain managers ensure that the company\u2019s products are efficiently distributed to various consumers in the market (Kale, 2004). These managers are also in high demand because the company has to distribute its products to diverse populations, and to do this, effective communication in the supply chain is needed.\n\nSWOT Analysis\n\nStrengths                                                                                                                                                                                                                                      Weaknesses                                                                                                                                                                                                                                                                                                  \n  * From the Case Study onDell Computers, it is evident that the company is the largest computer maker in the world, so marketing the computers is simplified because they have a set brand name.                                                * The main weakness with the marketing potential of Dell computers is that they have a weak relationship with retailers since the company prefers marketing its own products. This means that the company does not penetrate the market effectively, and the retailers do not market the company products.\n  * Another strength from the customer\u2019s perspective is the use of custom designs to make computers; some customers will order custom-made computers from the company thus increasing their brand loyalty.                                       * The other marketing weakness of the company is the fact that they do not have unique technologies to offer the market, so new products do not increase the market potential of the company.                                                                                                             \n  * The shipment strategy used by the company is an effective method that ensures that the computers reach the customer with minimal delays, which increases the marketing potential of the company.                                             * The issue of dealing with a large amount of suppliers from different countries means that the company faces many potential problems when their products are recalled from the market (Cohen and Rousell, 2004).                                                                                         \nOpportunities                                                                                                                                                                                                                                  Threats                                                                                                                                                                                                                                                                                                     \n  * The main opportunity that the company has in the market is the opportunity to improve on the brand created by the founder; it presents a chance for the company to expand its market capabilities to regions outside the current domains.    * The main threat to the marketing potential of Dell computers is the increasing number of popular brand names in the market, which increases the competitive factor and reduces market share.                                                                                                            \n  * The other marketing opportunity come from the projected expansion to new markets like China and India, which are projected in the situation analysis to grow in the next three to five years (Clay, 2006).                                   * Since Dell is a company that is focused on international business, fluctuations in world currencies can affect the profit potential and marketing capabilities of the company.                                                                                                                          \n                                                                                                                                                                                                                                                 * Developing strong relations between retailers and competing companies that reduce the distribution capability of the company, also affect the marketing potential of the company.                                                                                                                       \n\n\nReferences\n\nCase Study: Dell. Can The Icon of The Logistics Industry Survive in India? Chomilier, B., and Samii, R., and Wassenhove, L., The Central Role of Supply Chain Management at IFRC, [Online] Web.\n\nCohen, S., and Rousell, J., 2004, Strategic Supply Chain Management . New York: McGraw Hill.\n\nKale, S., n.d., Global Competitiveness: Role of Supply Chain Management, [Online] Web.\n\nKlapper, L., and Hamblin, N., and Hutchison, L., and Novak, L., and Vivar, J., 2000, Supply Chain Management: A Recommended Performance Measurement Scorecard. London: Logistics Management Institute.\n",
        "label": "human"
    },
    {
        "input": "The Computer-Mediated Learning Module Essay\n\nTable of Contents\n 1. Methods of Assessment\n 2. Instructional Strategies\n 3. Learner Activities\n 4. References\n\nMethods of Assessment\n\nThis study develops a computer mediated module in project assessment, utilizing several design techniques to enrich the learning experience.\n\nThe computer-mediated learning module will be used via a handheld device or through a computer, which will represent content in linear fashion (not very different from how people read online publications).\n\nStatic processes will be mainly taught using this process and softwares will be an important tool for undertaking respective tasks; however, it should be noted that, the difference between this computer mediated learning process and the web based learning model is very small, but the latter uses the internet, while the former uses CD ROMs (and such like data transfer mechanisms).\n\nThis module is designed in a manner that does not involve any internet usage.\n\nSince this module is computer based; assessments will be done through multiple choice questions where students will answer respective questions related to the module and the computer will easily perform the task of assessment (through softwares) without any external input.\n\nA drag and drop structure, radial button, simulation and other interactive means, will be used as complementary assessment tools, to be undertaken electronically, and the feedback communicated to the user, almost immediately.\n\nHowever, the major assessment criterion to be used will be the formative assessment test which works by sifting through several incorrect answers to come up with the right answer, as an evaluative criterion. This strategy will be used to assess what the students have learnt (or not learnt) regarding project management.\n\nTo enhance the students\u2019 understanding under the module, the instructor will explain how each question should be approached, to give the student an understanding of the practice to be studied (with every variation evidenced when sifting out the questions).\n\nThis first part of assessment encompasses the formative stage, but the second part is the summative assessment stage, where an assessment will be done to evaluate only the topics that were taught.\n\nAfter the second assessment, the data will be reported to the students in several ways. For example, different formats will be used to communicate the students\u2019 scores, and this will include percentages, marks and grades. Multiple scores and grades will also be used to communicate different aspects of the student\u2019s performances, through a remote out-of service application.\n\nA runtime data model will be used to encode the information received into a standard format of programming (BASIC), and a standard metadata will also be used to identify a particular activity in the performance criterion which will articulate the performance data to be sent back to the student.\n\nHowever, before the feedback is communicated to the students, alpha and beta testing will have to be carried out; both at the developer\u2019s sight and at the end user computer. Alpha testing will be used for application evaluation softwares, but beta testing will be adopted after the assessment results are assessed by another independent party (which in this case would be the administration).\n\nIn the beta testing stage, the instructor will have no control over the evaluation process, and feedback will be communicated to the students regarding their performance. This will be a way to ensure the systems credibility is in check.\n\nInstructional Strategies\n\nThe primary instructional strategy to be used will be the case-based learning strategy. This strategy will be used to solve learning dilemmas which can be best solved by analyzing previously existing cases. The case-based learning strategy will be used to induce meaningful discussions among students about real world examples that the computer-aided model may fail to expose (Herreid, 1997, p. 163).\n\nThe inducement for discussion is one advantage of this technique because its superiority emanates from the fact that, the strategy is learner-centered.\n\nThe approach will therefore enable students build a huge body of knowledge, without much input from the instructor because the instructor\u2019s role will be reduced to that of a facilitator only. Since the teaching methodology is computer-mediated, such a learning strategy is very practical because students can easily undertake meaningful discussions through computer-aided techniques.\n\nThis approach will be undertaken alongside the simulation technique to replicate real life examples in the computer environment (Sokolowski and Banks, 2009, p. 6). Inhibitive or impossible learning styles which are difficult to explain in the real world environment will therefore be easily explained in the computer environment.\n\nIn using the simulation technique, various softwares will be used, including the Monte calo simulation software and the stochastic modeling technique; these softwares will enable the simulation process become almost effortless (Sokolowski and Banks, 2009, p. 6).\n\nThis approach is also useful to the learning process because it allows for student mistakes, without necessarily having to suffer the implications of such mistakes in the real life environment (Sokolowski and Banks, 2009, p. 6). This platform therefore allows for a speedy correction of student mistakes before they can be experienced in the real life environment (Sokolowski and Banks, 2009, p. 6).\n\nThe cognitive tools to be used will be a combination of chunking and graphic organizers. Both strategies will be used to \u201cincrease the student\u2019s ability to recall what was taught, increase student understanding, eliminate boredom and organize student thoughts\u201d (Sokolowski and Banks, 2009, p. 6). Graphic organizers as a cognitive tool will take many forms, including concept mapping flow charts, webbing and matrix structures.\n\nWebbing will specifically entail brainstorming as the core activity and for comparison purposes; the double cell diagram will be used. As the name suggests, the concept mapping strategy will use the concept map in comparing or contrasting.\n\nThe comparison matrix will be used to compare the attributes and characteristics of the subjects or topics of discussion to establish which learning item share the same qualities and which ones have different qualities. This cognitive tool will complement the goal of the case-study approach because it is also analytical in nature and it can be used to describe specific attributes of an object without making conclusions about it.\n\nIn other words, it makes the process of analysis very simple. The comparison matrix will also be used a visual aid, and in this regard, students are likely to develop deep analytical skills, and in the same regard, the cognitive tool can be used to analyze different objects at the same time.\n\nThe flowchart will be the last cognitive tool to be used and it will be used to represent learning contents in folders and subfolders, which are linked by the subordinate relationships they share. The flow chart will be used to analyze, collect and distribute information. The following example represents it:\n\nSource: Ali Baig (2010)\n\nLearner Activities\n\nThere are several learning activities that will be used to make the learning experience more enriching to the students. However, these learning activities will be specific to the learning stage or task in question. The first task in the learning process is remembering facts which were taught through the primary instructional strategies identified above.\n\nIn ensuring students effectively remember such facts, performance aids will be the primary activity to be adopted (Clark, 2004). Performance aids will be used by the students to describe the tasks they have learnt in an electronic manner, through a software which will give instant feedback regarding the student\u2019s performance.\n\nThis process will therefore be done in an electronic performance support system, which unlike the conventional type, does not need to be subjected to printing, shipping or mailing to assess the student\u2019s performance in the learning activity. Results will be availed instantly.\n\nOther traditional learning activities will also be incorporated into the learning module and first among them is the questioning session where students will be asked questions by their colleagues (and the instructor) about what they have been taught (Clark, 2004). This will be done in a real-time forum which resembles a debate, where participants answer questions in front of their colleagues and are given scores immediately.\n\nThe design of this interactive session will be in a group format where students will be clustered in various groups and assigned several random questions that they will be supposed to answer within a specified time period. Scores will be availed immediately and the right answers availed to groups with the lowest scores.\n\nThe second stage in the learning process to be analyzed will be the understanding of important concepts of study. In this stage, students will be required to draw computer-aided animations to represent certain concepts of study. Such animations will be accompanied by explanatory texts. A drag and drop exercise will also be incorporated in the study module to analyze the student\u2019s comprehension abilities (Clark, 2004).\n\nFor example, questions will be categorized under one category of boxes and answers will be categorized in another. Students will therefore be required to match the right boxes, with questions that resemble the right answers. The following diagram shows an example:\n\nSource: Clark (2004)\n\nThis strategy will also be used in the application phase of the learning process, but a demonstration exercise will also be introduced to expose how well students have understood the learnt concepts by demonstrating they understand all required processes to make a system complete.\n\nThe next group of learning activities will be aimed at ensuring students have mastered the right analytical skills; say, by troubleshooting a given problem and determining the right course of action to be undertaken.\n\nComputer aided discussions will be encouraged as the primary learning activity which allow students to create, modify or distribute the contents learnt to solve specific dilemmas regarding various learning problems (Clark, 2004). This will be an interactive exercise among students.\n\nIn this context, file sharing becomes an important concept of information exercise among students because certain medium such as \u201cflickr\u201d and \u201cslide share\u201d will be used (Clark, 2004). As the second learning activity in this category, charts and matrix structures will also be used to evaluate the student\u2019s analytical skills because here, students will be required to draw several steps regarding the various processes analyzed.\n\nReferences\n\nAli Baig, M. (2010). Syllabus for Computers in Education . New York: Vclassroom.\n\nClark, D. R. (2004), Instructional System Design Concept Map . Web.\n\nHerreid, C. (1997). What Makes a Good Case? Some Basic Rules of Good Storytelling Help Teachers Generate Student Excitement in the Classroom. Journal of College Science Teaching , 12, 163-165.\n\nSokolowski, J.A. & Banks, C.M. (2009). Principles of Modeling and Simulation . Hoboken, NJ: Wiley.\n",
        "label": "human"
    },
    {
        "input": "Turing Test From Computer Science Research Paper\n\nThe importance of the Turing Test\n\nFirst of all, there is a need to point out that the Turing Test was created to determine artificial intelligence of the machine. In other words, the original aim of test creation was to understand whether the machine can think or no. Oppy Graham and Dowe David (2011) are of the opinion that, \u201cThe phrase The Turing Test is sometimes used more generally to refer to some kinds of behavioural tests for the presence of mind, or thought, or intelligence in putatively minded entities\u201d (para. 2).\n\nWhile investigating artificial intelligence, one is to determine the basic aims of the test. So, the primary goals are to understand the nature of the process of thinking , and the ways intelligent entities are built. While analyzing human intelligence in relation to the Turing Test, one is to determine the functions, the degree they are fulfilled, and the ways these functions are carried out.\n\nMachine learning, automated reasoning, knowledge representation, and natural language processing are the required capabilities to succeed in passing the test. Objective idea of intelligence is considered to be the most important advantage of the test. Tyler Cowen and Michelle Dawson (2009) state that, \u201cTo pass the test, the machine has to be intelligent but it also should be responsive in a manner which cannot be distinguished from a human being\u201d (p. 1).\n\nWhile speaking about thinking humanly , one is to keep in mind the so-called cognitive science approach. In other words, this approach should reflect the way the human mind works and the process computer systems should reflect such work . So, computer is to mimic human intellect. Thinking rationally is another approach, which is to be discussed. This includes logic and main obstacles. The last category means \u201cinformal knowledge translating into logical notation\u201d (\u201cArtificial Intelligence\u201d, n.d., p. 20).\n\nStuart M. Shieber (2006) is of the opinion that, \u201cthe Turing Test is founded on the idea that ability to produce sensible verbal behavior is an indication of intelligence\u201d (p. 1).\n\nThe ways to improve the test\n\nWhile speaking about the ways the Turing Test can be improved, one is to remember that logic programming, machine learning, and cognitive compatibility are the basic positions the Turing Test is based on. In my opinion, the most important improvement, which can be made, is to adapt the artificial intellect to human beings\u2019 process of thinking.\n\nSo, in other words, the machine is to contact with the persons and to mimic their behavior. Gilles Deleuze and Felix Guattari (2000) are sure that, \u201cWith current advances in computer graphics, virtual reality, biomechanics and many other fields, it is possible to create an Enhanced or Virtual Turing test\u201d (para 6.1).\n\nGenerally, I suppose that the improvements depend upon the development of new technologies. Unfortunately, \u201cknowledge-based systems, though they have achieved marketable engineering successes, still have many limitations in the quality and generality of their reasoning\u201d (\u201cAl: Early History and Applications\u201d, n.d., para. 80).\n\nThe tasks the Turing Test consists of can be also improved. For instance, taking into account Mundane tasks (perception, natural language, common sense reasoning, and robot control); one can state that there are the constituents of the tasks, which could be improved.\n\nThus, natural language is based on understanding, generation, and translation; so, these constituents may include some new improved features, in order artificial intelligence could be developed in a proper way. Formal tasks (games, mathematics) and expert tasks (engineering, scientific analysis, medical diagnosis, and financial analysis) must be also improved. However, it is necessary to remember that all the improvements depend upon new technologies development.\n\nReferences\n\nAl: Early History and Applications. The Turing Test. Web.\n\nArtificial Intelligence. CS 4633/6633 Artificial Intelligence . Web.\n\nCowen, T. & Dawson, M. (2009). What Does the Turing Test Really Mean? And How Many Human Beings (Including Turing) Could Pass? Web.\n\nDeleuze, G. & Guattari, F. (2000). Everything is a Machine. Web.\n\nOppy, G. & Dowe, D. (2011). The Turing Test . Web.\n\nShieber, S. (2009). Does the Turing Test Demonstrate Intelligence or Not? Web.\n",
        "label": "human"
    },
    {
        "input": "Apple Computer Inc. \u2013 History and Goals of This Multinational Corporation Essay (Article)\n\nApple Computer Inc. currently referred to as Apple Inc. is a multinational corporation based in the U.S., which designs electronics like personal computers, computer software, and distributes them for sale. Macintosh computers, iPhone, iPad, and iPod are the commonly known hardware products produced by the company. Its software includes aperture, Mac OS X operating system, iTunes, Logic studio, a web browser (Safari), and an operating system (iOS) for the media.\n\nThe company holds annual Worldwide Developers Conference (WWDC) in June during, which new products, and upgrades are announced. In 2012, the conference was held between 11 th and 15 th in San Francisco during which iOS 6 and Macbook Pro were introduced by the CEO of the company, Tim Cook (Kaneshige, 2012).\n\nAccording to Kaneshige (2012), the price of the new Macbook Pro starts from $2199. The machine cannot be upgraded and has the following features: a 4GB memory, quad-core i7 processor with a Ram ranging from eight to 18 GB, HD camera, Facetime for chatting, software, and a retina display of high density.\n\nMacbook Pro is used in organizations and individuals for work. It makes work easier by handling much work, which could have consumed much time and energy if it were to be done manually. Signing an agreement with Apple to purchase laptops for employees is the necessary measure to implement Macbook Pro.\n\nApple Macbook Pro is an exceptional computer with fast performance, sharp display screen, a sleek design, and it is unique. Its battery is long-lasting. After charging, it lasts up to seven hours when used continuously. It offers a variety of connectivity options like thunderbolt, Wi-Fi, Bluetooth, and USB. The packaging, which includes glass and aluminum is good. It is costly because the HD display is combined with the glass, thus expensive display has to be bought or the machine is send for repair at the company.\n\nIt is impossible to upgrade the RAM because the memory modules and RAM are fused with the logic board. In case of battery replacement, the Macbook should be send to Apple because the battery is fused to the case. The price of buying Macbook Pro is very high compared Ultrabooks from other dealers. Last, it does not include Ethernet port. According to iFixit.org, the new Macbook is the least repairable laptop (Kaneshige, 2012).\n\nThe company introduced iOS 6 in replacement of Google maps on iPad and iPhone. To create their own maps, the company bought three mapping technology companies.\n\nThe features of Apple maps include flyover feature, interactive 3D views, rotation, real-time and local search information. Advantages of iOS 6 are: it is upgradable, and it is offered freely for iPhone and iPad. The following are features of iOS 6, Facebook, Siri, passbook, phone, safari, Apple maps, shared photo stream, and it is accessible. The use of iOS 6 is to be implemented in 2012 but its advantages over Google maps are not clear.\n\nPeople learn the use of technology through free tutorials that come with products of the company. Apple keeps on updating their products making the company user friendly to the customers. The technology of using Apple maps may be disadvantageous to the company. Users of iPad and iPhone are likely to shift to android products if Apple Company chose Apple maps in favor of Google maps but fail to succeed in their plan. The success or failure of iOS 6 is yet to be known (Kaneshige, 2012).\n\nReference\n\nKaneshige, T. (2012). What not to like about Apple\u2019s new Macbook Pro, iOS Maps App . Retrieved from: https://www.cio.com/article/2395087/what-not-to-like-about-apple-s-new-macbook-pro\u2013ios-maps-app.html\n",
        "label": "human"
    },
    {
        "input": "Dependency on Computers Essay\n\nComputers have revolutionized the world we live in today by making it possible for people to engage in many different activities (Mann 6). It is easier now to connect with people in other parts of the globe passing over information without any difficulties. Over 3 billion people are using computers today with a significant portion of the population able to access computer partially.\n\nComputers have made people to become extremely creative even though most people have put great emphasis on the negative impacts of technology while ignoring the positive side. One of the negative aspects is for example the fact that crime has been improvised using computers even though crime existed even before computers were discovered. However, the computers elevated crime to new levels (Dawn 21).\n\nIn this argumentative paper, the author notes that as a society, we are becoming more and more dependent on computers. The question is, is it a good thing or is the dependency detrimental to the wellbeing of humanity? This essay is going to argue along these lines.\n\nThe paper will address dependency and over dependency on computers today. In their arguments, the author will look at both positive as well as negative effects of this dependency.\n\nDependency on Computers and Human Wellbeing\n\nComputers were invented to be used for different purposes and this application has both negative to positive impacts. Regardless of the negativity or positivity of this application, the fault is not with the device itself but with the user (Lea 33). The user gets paralyzed by the computer if they become dependent on it. People depend so much on computers in contemporary society.\n\nThis is clearly seen when things grind to a halt if the computer is not included in the operations. For example, even the author of this paper is not only using the computer to type the essay but they are also relying on the grammar checker to correct any grammatical errors in the paper. This is not a good thing since the author is now being controlled by the computer in their academic endeavors.\n\nThe use of computer in education is widespread. It is noted that in many countries today, students no longer write on the wall or in their notebooks. Instead computers are used to type notes for the students and even smart writing boards are used instead of the manual writing boards (Higgins 26). Instead of controlling the computer, the education sector is now controlled by these machines.\n\nTechnological innovations have peaked especially with the emergence of computers. Technology is an effective transformation tool but human kind is misusing it by depending too much on it. A lot of people are unable to keep a mental track of the various tasks they need to accomplish without the use of computer technology.\n\nFor example the spelling check has led to the deterioration of grammar among students and even among those people perceived as being intelligent. They have to consult the computer programs to get it right (Higgins 31).\n\nThe author acknowledges the positive impacts of computers in our schools today. Computers have positively turned around the education system in many parts of the world where majority of teachers develop their teaching models using computers.\n\nAs the advancement of technology takes root in the society, there is need to expand the skills of the educators in teaching students. In the absence of technological advancement, the learning process may be compromised.\n\nBut it should be noted that for computers to have a positive impact on the education sector, the technology should be used sparingly. Overdependence will make it hard to attain the envisaged goals of the learning process (Higgins 32).\n\nIn the urban centers, computers have literally taken over everything. From the transport sector to the education sector, computer is a part and parcel of every aspect of life in the cities. What about in the rural and remote areas of the society? Overdependence on computers is getting there too. At this juncture, the author notes that the urban centers have a higher dependency on computers than the rural areas.\n\nThis dependency discrepancy will make it easy to appreciate another negative aspect of relying so much on computers. Dependency on computers is detrimental to our wellbeing as it creates indirect or direct dependency on other things. For example, over dependency on computers inevitably leads to over-dependency on electricity and other sources of energy to operate the computer.\n\nFor example, when power goes off in an urban hospital which has computerized all its activities, operations comes to a standstill. In a hospital where computers are not part of operations\u2019 central nervous system, operations will still go on even after a power failure. From this perspective, it can be argued that dependency on computers may lead to inefficiencies as the computers themselves are not reliable 100 percent.\n\nOrganizations have to incur additional costs when they invest in machines and other resources that are meant to support the computers. This is for example incurring additional costs in electricity, procuring standby generators to cater for power black outs, hiring IT experts among other things. This clearly shows the negative aspects of dependency on computers in contemporary society.\n\nThis author is not blind to the positive aspects of computers in health institutions and other organizations in the society today. For example, it is a fact beyond doubt that business operations have become more efficient with the use of computers. Surgical operations and other medical processes have improved with the use of computers. However, the use of these devices should be regulated to avoid over dependency.\n\nOver dependency on computers has turned the benefits of this technology into a catastrophic phenomenon. Relying so much on online shopping will turn many emerging shopping malls into useless structures that are worth less to many. The consumer no longer needs to access the mall physically.\n\nFrom the comfort of their sitting room, the consumer can order for whatever they need from the shops. Traffic in these malls will reduce. Many people will be rendered jobless given that owners of the shopping malls will require fewer people to attend to the reduced number of clients visiting the shop physically (Lea 9).\n\nRelegation of shopping malls and other physical shopping structures raises another relevant issue on the negative impacts of computers. Computers are taking over operations that were hitherto the preserve of human operators. In the near future, the computer is inevitably going to take away hundreds of millions of jobs leaving many people without a source of income (Shotton 13).\n\nDependency on computers is also a threat to the privacy of individuals in contemporary society. Privacy is an important aspect in the life of an individual and no one is comfortable when their private life is an open secret to the whole world. Computers are the major source of this problem.\n\nIn spite of regular assurances from those online customer care personnel that confidential information will not be revealed, many people around the world have been exposed to danger and even cash lost after hackers have accessed personal information from databases.\n\nDependency on computers has caused a new form of addiction to these machines. Studies have shown that a large number of teenagers cannot \u2018survive\u2019 without the use of computers at regular intervals during the day. This has crippled their way of thinking and in some cases the addiction has caused some illnesses such as loss of eyesight, radiation from laptops and many others.\n\nMany teenagers are addicted to social sites such as Facebook and such others. They remain glued to the computer for almost 15 hours a day (Shotton 24).\n\nHowever it should be noted that computers have made it possible for people to connect socially across the globe but people are abusing the technology by depending too much on it. Dependency on computers is churning out a new generation of zombies and unbalanced individuals who are relying so much on the technology (Shotton 25).\n\nConclusion\n\nThe author will like to reiterate that computers have tremendously improved the quality of life in contemporary society. Medical operations are now more efficient, shopping have been made easy while education is now more interesting. However, over dependency on these gadgets neutralizes the positive impacts of the technology.\n\nAs a result of this, the author argues that the use of computers should be moderate. Human beings should have some level of autonomy when it comes to their interaction with computers.\n\nWorks Cited\n\nMann, Steve. Cyborg: Digital Destiny and Human Possibility in the Age of the Wearable Computer. London: Oxford Press, 2007. Print.\n\nHiggins, Jon. Calculators, Computers and Classrooms : Ohio: Eric Clearing House, 2009. Print.\n\nShotton, Mark. Computer Addiction? A study of Computer Dependency. New York: Taylor & Francis Publishers, 2010. Print\n\nDawn, Heron. Time to Log Off. Florida: University of Florida, 2009. Print.\n\nLea, Goldman. This is Your Brain on Clicks . London: Forbes, 2007. Print.\n",
        "label": "human"
    },
    {
        "input": "Computer Technology in Education Report (Assessment)\n\nTable of Contents\n 1. Behavioral Learning Objectives\n 2. Assessment of the ABCD Objective 1\n 3. Assessment of the ABCD Objectives 2 and 3\n 4. References\n\nBehavioral Learning Objectives\n\nInstructional Issue: Students have problems with memorizing new Spanish vocabulary and pronouncing new Spanish words.\n\nTechnology-Based: Multimedia software is going to be used for helping students remember new vocabulary and practice pronunciation.\n\nMajor Outcome: Voki and Digital Dialect flashcards will help students remember new words by means of constant repeating those and practical exercises based on listening and repeating will help students memorize how new words should be pronounced (Moore 1996).\n\nABCD Objective 1: after using multimedia software, Voki and Digital Dialect flashcards, students will be able to memorize new vocabulary (15 words) and match those new Spanish words with cards on the screen with 100% accuracy.\n\nA = audience; the students\n\nB = behavior; students will be able to remember all new words which comprise vocabulary from one unit.\n\nC = condition; after having used the multimedia software students will be able to name 15 words of the new vocabulary which appear on the screen.\n\nD = degree; 100% accuracy\n\nABCD Objective 2: having used multimedia software, teachers will be able to increase participation of students up to 100% and reinforce lessons which involve the use of the computer technologies at least twice a week.\n\nA = audience; the students\n\nB = behavior; students will have more desire to attend classes as the use of computer technologies increases their interest in learning\n\nC = condition; visiting computer classes students will get a first hand experience from using computer technologies in learning (Korsvold 1997).\n\nD = degree; 100% attendance of lessons, reinforcement of the lessons which involve innovative technologies twice a week.\n\nABCD Objective 3: students will be able to pronounce new Spanish words correctly with 100% accuracy by means of using practice exercises on the basis of multimedia software.\n\nA = audience; the students\n\nB = behavior; students will read and pronounce new Spanish words correctly.\n\nC = condition; after using the practice exercise directed students are able to pronounce new Spanish words correctly\n\nD = degree; 100% accuracy\n\nAssessment of the ABCD Objective 1\n\nWriting Selected Response Assessment Items\n\nThe main idea of this assessment is to check how well students have remembered 15 new words from a new vocabulary after having used Voki and Digital Dialect flashcards. The assessment procedure should be conducted with the use of the following steps.\n\n 1. A teacher prepares a short a text (about 200 words) which contains new words. The teacher reads it to the audience for the first time with pointing at the pictures which define the new words. Students just listen without making notes.\n 2. When a text is read, a teacher asks general questions about the context of the text trying to use new words for students to hear them.\n 3. A teacher reads the text for the second time without pictures. Students are allowed to make notes to make sure that they remember everything.\n 4. Students are offered three types of tasks, Multiple Choice, Matching and True/False ([Anonymous] n.d.). The tasks should be organized in such a way that students are to use all 15 new words they have already learnt.\n\nMultiple Choice Task. Students are offered a statement and a list of answers which are to be of the similar length, plausible, and without \u201call options\u201d response.\n\nMatching Task. This assessment tool should consist of two columns. The first column should be a list of the new words, and the second column should be the list of the definitions of those words. The task for students should consist in matching a word with its definition.\n\nTrue/False Task. The task should be based on the content of the text, however, the stress should be made on the new words and the use of the vocabulary by students ([Anonymous] n.d.).\n\nAssessment of the ABCD Objectives 2 and 3\n\nStudents are to use the computer for completing this task. Students should be notified that the task should be completed any time they want during a week after the new vocabulary was learnt. Students are offered a list of new words in Spanish.\n\nUsing a microphone, students are to pronounce a word and give a system an opportunity to record it. Each student has only one attempt to complete one vocabulary unit. Thus, at the end of the week a teacher will have a list of students who have completed the task and the results of their pronunciation skills.\n\nMoreover, here is a list of more assessment tools which may be used to check how well students have learnt to pronounce new Spanish words:\n\n 1. Vocabulary quizzing (one student says a word in Spanish and another one should respond in English, students should switch roles after one has pronounced all 15 words correctly.\n 2. Picture identification (a teacher shows pictures and students are to pronounce the words drawn at the picture correctly.\n 3. Pronunciation and spelling (one student says a word and another student should spell it without looking at the word, a student is interrogated until he/she is tested for all the words). Students should not have the vocabulary list before their eyes while being tested (Buttner 2007).\n\nReferences\n\n[Anonymous] n.d. Selected response. Classroom Assessment. Web. Available from http://fcit.usf.edu/assessment/selected/responseasi.html .\n\nButtner A. 2007. Activities, games, and assessment strategies for the foreign language classroom. Larchmont (NY): Eye on Education. 189 p.\n\nKorsvold A. 1997. New technologies in language learning and teaching. London: Council of Teaching. 156 p.\n\nMoore Z. 1996. Foreign language teacher education: Multiple perspectives. New York (NY): Multiple Press of America. 337 p.\n",
        "label": "human"
    },
    {
        "input": "Telecommunication and Computer Networking in Healthcare Research Paper\n\nOverview of the healthcare and security systems\n\nInformation security as well as well as privacy in the health sector is an issue whose prominence has been swelling day by day. Healthcare sectors have been called to adopt information technology systems to aid in speeding activities and securing records. Efforts to adopt information technology in healthcare have been practiced for a long time. Several developments point to the need for better security systems in healthcare.\n\nThese developments include the use of patient records that are digitalized, the consolidation of healthcare providers and the rise in healthcare regulation. Also, there has been an increasing need for the exchange of healthcare information between healthcare providers, patients and healthcare institutions. All these developments are strongly bonded on security thus making security a paramount issue in healthcare (Symantec, 2009).\n\nInformation systems are seen as the available and efficient options of improving the quality of services and security of information transfer and exchange in the healthcare sector. Many countries, including the United States, have created strategies to adopt automated systems in healthcare. Therefore, a lot of research has been done about how best information systems can be incorporated to improve information exchange and security (Appari & Johnson, 2010).\n\nThe requirements of HIPAA for network security to maintain confidentiality of patient records\n\nPatient information is extremely valuable in healthcare. This information has become increasingly vital not only to the doctors but also to other healthcare givers providers like health insurance firms. The confidentiality of this information in the US was boosted through the formulation of a law in 1996.\n\nThis piece of legislation is referred to as the Health Insurance Portability and Accountability Act. This piece of legislation was formulated by the federal government hence it is applicable in all states. It regulates the security as well as privacy of health information. The key areas emphasized in this legislation are the maintenance of confidentiality, the protection of patient information and the respect of the rights to privacy of the patients (Keir & Keir, 2008).\n\nHIPAA contains regulations on communication mediums to be used in medical institutions. These rules require staffs in the healthcare institutions to know privacy and security rules of the electronic media tools used. Under this piece of legislation, the securing of office computers must be a priority in healthcare institutions. Appropriate information technology safeguards must be acquired by the institutions.\n\nThey include virus protections, firewalls and antispyware which help in locking out cyber hackers. They also prevent virus attack on the downloaded information and even people with ill motives who may want to intercept in the exchange of information. The presence of the secured network systems provides the medical staffs with the green light of using their emails for sending and receiving of patient information (Keir & Keir, 2008).\n\nThe other regulation measurers in HIPAA are that emails must contain email notices. Email notices must be embedded on patient information. These caution users about sending messages to unintended people. It also helps senders in identifying messages that are sent to un-intended recipients. Other security measures include the encryption of the emails containing the information of patients. Subject lines in the emails have to be generic and not specific. Patient information is not to be included in the subject line of the emails (Keir & Keir, 2008).\n\nHIPAA also contains auditing requirements for healthcare institutions. Healthcare organizations must enhance their capacity to control the people who access patient records. The code of control of access to patient records is contained in the American Academy of Family Physicians.\n\nA system network is used in discharging this function. Such a system must have the ability to assign usernames and passwords that are unique and in accordance to the people who are allowed to access the accounts. The users are saved according to the access levels. Different user levels are assigned to different users. The security level limits the access of a person to a person. The legislation allows health organizations to develop their own system networks according to the way their activities are structured (Lindh, 2010).\n\nTechnical Risk Assessment of the healthcare security networks\n\nRisks are inherent in security network systems. Therefore, risk management systems have to be put in place in order to help reduce the risks that can be posed in the implementation of security systems. The following risk reduction steps can be followed to ensure effective implementation of security systems (NEMA/COCIR/JIRA Security and Privacy Committee, 2007).\n\n  * The first step entails the listing of all the assets to be used in establishing health security systems. Each of the items in the security system has to define in terms of its uses.\n  * The second step entails the collections of the entire security-related requirement for the assets to be used in setting us the security system for the company. Most of the assets have various security specifications which must be availed for the proper working of the security system.\n  * The third step involves the identifying and elaborating on the threats and applying these threats to the system to determine the vulnerabilities of the system. Threat paths are easily identified.\n  * The probable risks are then scored.\n  * Mitigations measurers are then proposed for the inherent vulnerabilities in the healthcare security system.\n\nLastly, the residual risks are summarized and the details of coming up with details of coming up the\n\nCompliance with state laws concerning privacy of patient records\n\nThe protection of health information is guarded by the federal government through the federal law. The federal government through the federal law defines how the information on patients should be protected by the actors in the healthcare sector. The federal law is affected or enforced in different states.\n\nThe states are the enforcers of the laws that are set by the federal government. The states set their own standards as well as sub-legislations to help them in enforcing the laws that are set by the federal government. The state laws may differ from the federal laws. However, the state laws are proactive.\n\nWhen state privacy legislations promise to be more protective of patient information, then they are preferred over the state laws. Healthcare networks in such cases are implemented basing on the most proactive legislation. States pose detailed and private patient records which are the reason as to why they are better placed in enforcing the healthcare security network (NEMA/COCIR/JIRA Security and Privacy Committee, 2007).\n\nCompliance with Meaningful Use requirements for managing IT security\n\nFor healthcare security systems have to be efficient and meaningful to the healthcare institutions, they must be implemented systematically. They must be coherent with information technology standards as per the standardizations of the International Standards Organization. Standardization of the systems is a crucial issue which will help in bringing about well-coordinated use of healthcare IT security networks.\n\nThe first consideration is the identification of IT security system which meets the industry standards. All the organizations in the healthcare sector must then be notified of It standards of the IT systems which will be required. The organizations must also be made to understand the regulations and how they can keep to the required stands.\n\nThis will help these organizations to equip themselves with the standard or right healthcare Information Technology tools to be used in implementing healthcare networks. The healthcare sector must also find proficient IT dealers to help them in the management of the Healthcare information security systems (NEMA/COCIR/JIRA Security and Privacy Committee, 2007).\n\nPolicy and procedures required to manage ongoing security principals and provided security guidance to staff\n\nAs mentioned earlier, there is a strong need to have the staffs in the healthcare sector informed of the policies that govern the use of healthcare security networks. This is because most of the staffs in this sector are not experts in information technology. Therefore, they must be guided on how to attain the best results out of the use of healthcare security networks (Cooper, 2007).\n\nTherefore, three main key issues are addressed as it concerns to the adherence of the healthcare security principals by the healthcare staffs. The first thing that is addressed is the training of the staffs on the importance and the use of the security tools. The training on how to share patient information on a security networks then follows.\n\nThe staffs are made to understand all the regulations on information sharing and exchange including the risks that might be posed and remedies to these risks. The HIPAA clearly stipulates the guidelines on effective use of healthcare security networks (Cooper, 2007).\n\nReferences\n\nAppari, A & Johnson, M. E. (2010) \u201cInformation security and privacy in healthcare: current state of research\u201d. International Journal of Internet and Enterprise Management , 6(4):pp. 279-314.\n\nCooper, T. (2007). Managing Information Privacy & Security in Healthcare Privacy and Security Principles . Web.\n\nKeir, L., & Keir, L. (2008). Medical assisting: Administrative and clinical competencies . Clifton Park, NY: Thomson Delmar Learning.\n\nLindh, W. Q. (2010). Delmar\u2019s comprehensive medical assisting: Administrative and clinical competencies . Clifton Park, NY: Delmar Cengage Learning.\n\nNEMA/COCIR/JIRA Security and Privacy Committee. (2007). Information Security Risk Management for Healthcare Systems . Web.\n\nSymantec. (2009). Security and Privacy for Healthcare Providers. White Paper: Best Practices Series for Healthcare. Web.\n",
        "label": "human"
    },
    {
        "input": "The Convergence of the Computer Graphics and the Moving Image Essay\n\nThe concept of new media has thus emerged and entails an amalgamation of two distinct historical paths: computing and media technology (Manovich p.1).\n\nThese two phenomena have contributed to the emergence of contemporary media technologies that facilitate the storage of sounds, images, image series and text utilizing diverse materials- film stocks, photographic plates, and gramophone documents.\n\nConsequently, the conversion of whole still pictures into motion images using computer graphics has been realized (Manovich p.2).\n\nThe identity of media has transformed significantly over years. The whole new media objects, whether generated from scratch on computers or transformed from analogue media fonts, comprises of digital code. This fact has two vital outcomes: one, new media items can be illustrated mathematically.\n\nFor example, an image can illustrated through a mathematical function; two, new media images can be manipulated using algorithms. For example, by using suitable algorithms, sound can be automatically eliminated from a photo, locate the outlines of the shapes, enhance its contrast, or alter its size.\n\nIn nutshell, media arts have become programmable. Modularity, also known as fractal structure of new media, is a novel technology used in media arts. Media elements, such as sound, are depicted as sets of discreet sample (Manovich p.2).\n\nThese elements-sounds, shapes, images or behaviours- are amassed into large scale objects although they preserve their distinct identities. The modular make-up of a media object and the numerical coding of media art enable the computerization of various operations engrossed in media creation, manoeuvring and access.\n\nTherefore, human characteristics can be alienated from the creative process to some degree. For instance, Photoshop converts pictures into Van Gogh\u2019s.\n\nCurrently, media experts are developing what is known as \u2018high level\u2019 automation of media design, which will enable a computer to recognize, to a certain level, the language entrenched in the objects being created (Manovich p.2).\n\nAs the 20 th century came to a conclusion, majority of online users were familiar with software that produced human conversation. Researchers at New York University were able to develop practical plays that featured inherent actors.\n\nThese actors would modify their acts in real-time when a player altered his deeds. The MIT Media Lab designed several diverse projects dedicated to high level computerization of media design and use: a smart camera that, when given a screenplay, mechanically follows the action and structures the shots; ALIVE, a practical environment where the user interrelates with animated actors; and a new type of human behaviour.\n\nThe character, produced by a computer instantaneously, converse with the user via a natural language; it also attempts to deduce the user\u2019s emotional status and to alter the mode of communication appropriately (Manovich p.2).\n\nThe historical perspective of media art and computer graphics\n\nThe history of media art is place in a course of the pedigree of the media technology as opposed to art history. For example, in The Automation of Sight: From Photography to Computer Vision, Manovich discusses the innovation of perspective to computer made images. He also consigns this path within a history of automation.\n\nThe architect of the algorithm which makes perspective reproduction on computer a reality had a noble objective than merely generating a tool for art. The computer should thus be able to render and understand 3-D images via a blueprint of recognition.\n\nTherefore, the project of 3-D computer created images was a component of the project of Artificial Intelligence (AI) within the framework of Cold War (Medosch p.27). The field of AI emerged in 1980 via unified practical and theoretical efforts in cybernetics, computer science and biology re-defined as life science.\n\nThe cybernetic sphere had facilitated the parallelisation of organic and non-organic systems as open and variable processes. Both organic and nonorganic processes can be conceptualized as comprising of changeable components whose features can be devised with respect to communication and theoretical models.\n\nThis enables life to be integrated with technology (Medosch p.21). Lev Manovich argues that the artistic principles employed in new media art were invented by German and Russian film makers in 1920. For example, the film \u2018The Man with a Movie Camera, can be used as a blueprint to understand the language of new media art (Medosch p.28).\n\nAccording to Manovich, computers are interactive by their design and he argues further that all forms of media art are interactive in principle.\n\nHis thesis essentially dismisses interactivity as a field synonymous with media art. By ignoring interactivity and establishing the screen as the key feature of new media art, Manovich posit that Russian avant-garde movies established the groundwork for media art.\n\nHe seems to propose that artistic innovation stopped over 70 years ago. The innovative strategies in the 1920s of zooms, pans, montage, and of the unshackled and accelerated kino-eye have turned out to be the list of options in Photoshop.\n\nThe U.S. software designers are merely offering the public with watered-down menu access to the artistic innovation of the early 1920s (Medosch p.29).\n\nBatchen Geoffrey censures Manovich for using cinema as the main conceptual lens via which the language of new media is addressed, overlooking the historical development of telegraphy and photography.\n\nThe exploit of 35mm rejected motion picture by Zuse, the German architect of the computer, is sufficient proof for Manovich to accept the fact that the entire present media transformed into numerical data available for the computer (Medosch p.29).\n\nBatchen states that \u201cthe plausibility of this particular historical metaphor depends on two particular claims: 1, that computing and photo-media have no interaction until the 1930s and 2, that cinema is the key to any understanding of the forms and development of new media\u201d (Medosch p.29).\n\nBatchen\u2019s account demonstrates the close relationship of the histories of telegraphy, photography and computer. According to Medosch, four interconnected technologies and their conceptual tools-mechanical weaving, photography, photo-mechanical printing and computing-were initially envisaged in the 19 th century and thus must be comprehended with respect to modernity, which implies industrialism, patriarchy and colonialism (p.29).\n\nMedosch claim that Cubist, Futurist, Cub futurist, Dadaist and supremacist were the theoretical forerunners of media art. The new media art form that emerged after World War II- Fluxus, Pop Art, action painting, performances and actions- were employed as media platforms to set the centre stage for the liberated digital image (p.30).\n\nHowever, Christine Paul contradicts this assertion by alleging that the concept of interactivity and virtuality in media art were discovered by artists such as Marcel Duchamp with respect to objects and their optical effects.\n\nPaul posits that the contribution of Duchamp was highly influential within the sphere of digital art due to a transformation from object to concept. She devises a lineage of digital art that differs from Manovich\u2019s by laying emphasis on the effect of Duchamp through an OULIPO, French writers to conceptual art.\n\nThe conceptual connection here is that OULIPO writers, Duchamp and Fluxus artists habitually generated art works that were grounded on the implementation of a set of rules, which are comparable to computer algorithm.\n\nThis observation is augmented by Peter Suchin who asserted that the nature of art in1960, amassed under the shared heading of Conceptual art is a vital determinant of the contemporary new media art practises.\n\nOther conceptual connections between art activities in the past and the current media art centred on the exhibition Cybernetic Serendipity, at the ICA, London in 1968.\n\nOn the contrary, there is no connection between the emergence of cybernetic art in 1960 and its resurfacing in980. Moreover, there is minimal connection Burnham\u2019s Software Art show and the software art in 2005 (Medosch p.30).\n\nDistinctive features of new media art: The principle of variability\n\nOne distinctive feature of new media art is that it can be expressed in diverse unlimited versions due to the effects of modular structure of the object and numerical coding of media. Previous cinema involved computer experts who manually assembled optical, documentary and audio features into a logical sequence.\n\nNew media art, in contrast has an element of changeability that enable media objects to generate several versions. Instead of being generated entirely by humans, these versions are usually amassed in part by a computer. Therefore, the principle of changeability is highly related to automation.\n\nVariability would also not be feasible in the absence of modularity. Media elements preserve their distinct features and can be amassed in to several series under program control. Moreover, since the elements are split into concrete samples- for example, an image is displayed as an assortment of pixels), they can be generated and modified on the fly (Manovich p.3).\n\nThus, the principle of variability is vital in that it allows the combination of numerous critical features of new media that may appear unconnected at first sight. Specifically, popular new media compositions such as interactivity, branching and hypermedia can be viewed as examples of the principle of variability.\n\nFor instance, during branching interactivity, the user ascertains the sequence in which elements are accessed. This is the easiest type of interactivity. The complex types are also likely where the structure and elements of the object are altered or produced on the fly in reaction to the user\u2019s interface with a program (Manovich p.4).\n\nThe selection of ready-made elements to form part of the content of a new media object is merely one facet of the \u2018logic of selection\u2019. As the designer develops the object, he also naturally selects and uses a number of filters and \u2018effects\u2019.\n\nAll these filters, whether altering image facade, generating a conversion between moving images, or using a filter to a piece of music, entail algorithmic alteration of the present media object or its components (Manovich p.7).\n\nThe technological shift from a material object to a signal achieved by electronic technologies signalled a fundamental conceptual move towards computer media.\n\nThe phrase \u2018digital compositing\u2019-commonly used in the area of new media- is defined as the process of merging a series of moving images and possibly still pictures, into a solo series with the aid of special compositing software for example Cineon (Kodak), Adobe (After Effects) and Wave front (Compositor).\n\nDigital compositing signifies a common operation of computer culture- amassing several elements to generate a distinct seamless object. Thus, it is feasible to discriminate compositing in the general sense from compositing in limited way by accumulating motion pictures to produce moving images (Manovich p.8).\n\nThe link between the artistic of postmodernism and the operation of selection is related to compositing. These two processes concurrently facilitate the current practise of quotation and pastiche. One function is used to choose styles and elements from the database of culture while the other is used to amass them into new objects.\n\nThe basis of the postmodern artistic in 1980s and the rationale of the computer based compositing in 1990s are dissimilar. For example, in 1980s, the past references and media quotes were preserved as separate elements and boundaries between elements were defined aptly.\n\nThis aesthetic matched the early digital and electronic tools of the era, for example, DVE, keyers, video switchers and computer graphics cards with a restricted colour resolution. These tools allowed hard-edge copy and paste operations but not silky, multilayer composites (Manovich p.8).\n\nThe use of computer graphics to generate special effects\n\nThe contemporary proliferation of special effects in Hollywood film industry has brought about the age of digital revolution. Digital technology has been praised as a revolutionary phenomenon in the history of media arts.\n\nA cursory analysis of media technology shows that substantial research is devoted the social, economic and political effects of the virtual reality, internet, and other types of new media. For example, the use of computer graphics in Avatar has taken cinema industry to new heights of technological innovations, especially in the IMAX 3D.\n\nCameron, the director of Avatar, employed game-changing special effects. For instance, in The Abyss, the producers developed whole digital 3-dimensional effects in the movie.\n\nIn Judgement Day, the producer created digital characters with human traits that had practical movement while in Titanic; unique effects were generated using modern computer technologies to create an impression of a large mass of water (Eisenberg p.2).\n\nIn the same manner we cannot indulge in a discussion about suspense without referring to Hitchcock, we cannot discuss the evolution of computer graphics without talking about Tron Legacy .\n\nWhile Hitchcock pioneered the use of animated 3D CGI, Tron Legacy used new technology comprehensively, including more than 10 minutes of totally computer generated images. In the Star Trek: The Wrath of Khan (1982), the movie director was able to create black holes using green lines and arcs.\n\nNext, the computers were able to replicate millions of year\u2019s value of planetary evolution in a short span of time. This special effect was developed by Lucas Film\u2019s Computer Graphic department and involved the use of microscope to create minute features of the movie, especially with regard to the placing of stars as observed from the camera\u2019s viewpoint, light-years away.\n\nThe landscape is also the first time in cinema industry where a fractal generated scene was employed to simulate practical landscape (Eisenberg p.4).\n\nIn the movie The Matrix (1999), the producers were able to use numerous cameras placed on an arc around the scene (also called green screen).\n\nThe cameras were synchronized to generate the appearance of slow motion. Although this effect was employed previously in Blade, it was the application of interpolation software that enabled the Wachskis to slot in CGI outlines that made the motion extremely fluid.\n\nIn the movie Lord of the Rings Trilogy (2001), the producers employed A.I. to create digitalized movie characters. One of the special effect was the movie\u2019s colossal orc army. It is obvious that the producers could not have employed over 99,000 extras required to bulk up the army\u2019s ranks; instead, they created digital orcs with brains.\n\nBy employing a computer software known as MASSIVE (Multiple Agent Simulation System in Virtual Environment), the producer was able to generate thousands of orcs at no cost (Eisenberg p.6).\n\nIn \u2018The Legible City\u2019 by Shaw Jeffery, the user takes a ride on a domestic trainer bicycle and goes through a 3-D city of words and sentences displayed before him. The structural design is based on real cityscapes in which the shape and size of letters matches with those of buildings.\n\nIn the Manhattan edition (1989) the manuscript tell over seven different narratives, from ex- mayor Koch to Donald Trump to a cab driver.\n\nThe piece is said to ascertain an express relation between real and abstract city whereby the textual element of the city virtually deciphers the features of hypertext and hypermedia into structural design where readers create their own story by selecting paths via the non hierarchical text warren.\n\nThe art work thus shows post modern theories about textuality and offers user a first hand of cyberspace. In the \u2018The Legible City\u2019, the planet is viewed as text, a proposition that has been propagated by post modern philosophers (Medosch p.36).\n\nMachinima movies-the creation of animated movies in real time via the use of computer game technology-have altered game play via performance, subversion, spectatorship, and modification.\n\nThe manner in which previous machinima projects described the unification of animation; filmmaking and game development has been enlightening. They definitely educate us something concerning the impact of advancement in game technology and computer graphics (Lowood p.1).\n\nThe phrase \u2018machinima\u2019 was taken from \u2018machine cinema\u2019. It thus implies the creation of animated movies in real-time by using a special software to design and play computer games. The game creators\u2019 uses software called game engines to control complex real-time graphics, camera views, lighting and other facets of their game.\n\nGames are interactive; pre-rendered computer graphics has narrow use in software that must instantly respond and reshape the screen in response to player action (Silva p.3). Games such as first person shooters increase the stakes of this technological challenge.\n\nIn order to plunge the player in the swift action of the game, the virtual environment must be rendered in 3-D format from the user\u2019s viewpoint, constantly, doing so at high frame-rates as the user moves through that space. Ever since 1990s, specific software and hardware solutions have been used to create these views on the fly (real-time).\n\nMachinima creators have discovered ways to re-deploy this intricate software for producing movies, relying on their knowledge of the games and game software.\n\nStarting as players, they discovered that they could convert themselves into directors, actors and even cameras to generate these animated movies cheaply on a personal computers employed to frag friends and monsters in Quake or Doom.\n\nOf late, as machinima develop artistically and technically, the inventors are now focussing on how to use the technology efficiently to generate animated films that compete with digital frame-based methods (Silva p.3).\n\nThroughout the 20 th century, the conceptualization of 19 th century insinuation of what would emerge later as computing machinery was basically not documented. Other than electronic arts and IT disciplines, there was obviously little interest with regards to the development of computer based art.\n\nHowever, the interface between computing and arts began to emerge, from the period John Whitney used analogue computer to create animation in 1958 to Edward Zajac\u2019s computer made movie five years later. The combination of media art and computer graphics was more prominent in the 1990s.\n\nHowever, its significant progress was evident as a result of a richer and extensive durability of technology and media art.\n\nThe concept of amalgamating electro-mechanical processes with fine and applied arts and architecture liberated the theory of an idiosyncratic contemporary and homogenous tradition based on the complex relationship between human and machine, science and art (popper p.11).\n\nComplementing its substantial retrospective, Phantasmagoria included latest work by new media artists Toshio Iwai and Agnes Hegedus and places their contribution within the spectacle and magic of Melies pioneering films.\n\nAs a matter of fact, Callas and Watson noted that the creative contributions of these new media artists revitalize admiration and delight experienced by the initial movie audience in Melies \u2018magic movie\u2019.\n\nIf early movie was a historical perspective in which it was appropriate to situate these modern artists operating with interactive media, it was also manifest to both Watson and Callas that if Melies was alive today, he would not fail to adopt the digital pleasure offered by computer graphics and virtual reality (Hughes p.6).\n\nThe convergence of media arts and computer technology has played a significant role in the advancement of film industry, especially with respect to motion pictures.\n\nThe ability to use computer graphics to generate special effects as seen in movies such as Avatar, Troy and Lords of the Rings have taken the cinema industry to unprecedented level.\n\nThe game creators\u2019 have used special software called game engines to control complex real-time graphics, camera views, lighting and other facets of their game.\n\nSo as we sit back to enjoy watching animated characters in a movie, we should remember that the major achievements seen in the media arts industry is due to computerisation of the processes involved in creating moving image.\n\nWorks Cited\n\nEisenberg, Eric. How Avatar Happened: Light cycles and Giant Lizards on the path to Innovation. Dec 15 2009. Web. <www.cinemablend.com/\u2026/How-Avatar-Happened-Lightcycles-And-Giant- Lizards-On-The-Path-To-Innovation-16162.htm>\n\nHughes, Robert. The Shock of the New: Art and the Century of Change . London: BBC Publishing, 1993. Print\n\nManovich, Lev. The Language of New Media . New York: The MIT Press, 2001. Print\n\nMedosch, Armin. Technological Determinism in Media Art . Sussex: Sussex University, 2005. Print\n\nLowood, Henry. \u201cHigh-Performance Play.\u201d The Making of Machinima. Stanford: Stanford University.\n\nPopper, Francis. Art of the Electronic Age. London: Thames & Hudson, 1993.\n\nSilva, Michelle. Digital Alchemy; Matter and Metarphosis in Contemporary Digital Animation and Interface Design. Pittsburgh: University of Pittsburgh, 2005.\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics Related Ethics Term Paper\n\nAbstract\n\nComputer crime which is referred as an illegal access to a computer system is on the rise in the recently. Computer crimes ranges from; hacking, phishing, cyber stalking, computer viruses and identity theft. Due to advancement in technology, data in various institutions is no longer kept in file papers but in computers.\n\nHuman beings have found a way of reaching this information without permission with an aim of using it for personal gain. To counter all these evils, the US congress had to introduce some ethical values in form of laws which act to protect all important forms of data and to ensure privacy security of personal information.\n\nSuch laws include; Health Insurance Portability and Accountability Act (HIPAA), Sarbanes-Oxley Act of 2002 (Sarbox), Children\u2019s Online Privacy Protection Act of 1998 (COPPA).\n\nCalifornia Database Security Breach Act of 2003, Computer Security Act, Privacy Act of 1974, Uniform Electronic Transactions Act, electronic Signatures in Global and National Commerce Act and Uniform Computer Information Transactions Act. All these were introduced with an aim of protecting information in various departments from being accessed.\n\nComputer crimes have various negative results such as alteration, modification and loss of important information through computer crash. Therefore there is an agent need for measures to protect such forms of impacts and to promote privacy of personal information.\n\nIntroduction\n\nThe rate of computer crime committed at the present is alarming, especially with the advent of the internet. Computer crime can be defined as a criminal act whereby information technology is used to reach admission into a computer system with the sole intention of damaging, erasing or modifying data in an illegal manner. Electronic plagiarism, data theft and copy right manipulations are also activities considered to be crime.\n\nApart from the physical destruction, alteration of private data and important information, software theft whereby privacy settings are interfered with, is also considered to be crime. Due to these crimes, measures have been put in place to ensure security. These crimes that are committed can be categorized into different types as described below;\n\nHacking; this is a case whereby one breaks into a computer system and illegally accesses the information saved in there. It is the most common crime and it involves an authorized disclosure of passwords and IP addresses with the aim of carrying out business transactions with a vague identity.\n\nPhishing; this is where sensitive information such as usernames, passwords and credit card particulars are obtained through the use of popular websites; usually, the user is requested to give such details. Because the trust entrusted to such websites, one ends up being lured to give such useful information.\n\nCyber stalking; cyber stalkers usually gather information about the user through social sites such as face book, chat rooms and websites and use this information to harass the users. False allegations, giving of threat, spoil of records and equipment, offensive phone calls and obscene mails can be put under this category.\n\nComputer viruses; these are programs that deletes files, replicates them or completely crashes a computer system. They are transferred through removable devices such as the CDs, flash disks and others. Virus infections lead to ultimate crash down of the computer resulting into loss of a huge amount of data.\n\nIdentity theft; a false identity is used in order to steal money from other people\u2019s accounts. This involves use of someone else\u2019s credit card details or pretence of being someone in order to obtain what is not rightfully he\u2019s/hers. The above forms a list of identifiable crimes today.\n\nInformation technology is of great importance but when such acts come into play; it can be considered a curse (Oak 2011). Due to these crimes, some acts have been enacted to look into security issues, they include the following:\n\nHealth Insurance Portability and Accountability Act (HIPAA)\n\nIt was enacted in 1996 by the US congress due to the increasing need by the patients to keep their health records in privacy. Title I of HIPAA is concerned with the health insurance cover for workers, in case of job loss or shift while Title II or Administration Simplification (AS),ensures security sand confidentiality of health record.\n\nHere, institution of nationalized principles for electronic healthy care business, identifiers for providers, health cover policy and employers are required. All these protocols are aimed at promoting the health care service delivery. They improve it by providing a widespread utilization of electronic data exchange in the United States.\n\nTitle II of HIPAA identifies various offenses pertaining to health care and provides its civil and criminal punishment. It also safeguards against fraud in the health care system by requiring the Department of Health and Human Services (HHS) to provide standards for information pertaining to health care use and distribution. Some of the rules set are given below which only apply to the covered entities.\n\nPrivacy rules; it is mainly for controlling the use and revelation of information such as medical service givers and health insurer involved in a business transaction. It also looks at the security of Protected Health Information (PHI).This information can only be disclosed to an individual 30 days after a request has been made, when required by law for example when filling a case regarding child abuse to the welfare agencies.\n\nTransaction and code set rules; this concerns medical givers who file electronically. HIPAA helps track their payments. Various EDI are used in this transaction such as EDI Health Care Claim Transaction set, EDI Retail Pharmacy Claim Transaction and many others.\n\nSecurity rule; it concerns Electronic Protected Health Information (EPHI). A provision for administrative, physical and technical security is made here. An example of violation of HIPAA is the case of a health officer who reached out records on celebrity in California, Los Angeles and transacted the information with the tabloids.\n\nIn this case those who were affected were required to file a case with the HHS. Heavy punishment of not less than $2,500 is executed on this law violates as it can jeopardize everyone\u2019s health data (Privacy rights clearinghouse 2003).\n\nSarbanes-Oxley Act of 2002 (Sarbox)\n\nThis act is concerned with exposure and control of fraud in the financial statement. Acts of fraud has greatly affected the world\u2019s economy over the past recent. It is more evident in popular and big companies such as the Enron Corporation and the WorldCom.\n\nThe US congress passed the law to protect investors against fraud by improving the precision and trustworthiness of company disclosure.\n\nThe above two mentioned corporations collapsed due to recompense plans that were set by the senior management and the executives whereby they wanted to have large gains by incorporating liability receptors such as the special purpose entities (SPEs) which scrubbed off large amount of dollars of debts in the balance sheet.\n\nAfter which they abandoned the companies leaving the shareholders with nothing thus contributing to their collapse. Such acts reduce shareholder\u2019s trust in the share markets.\n\nIt was after the collapse of Enron that the then US president George W Bush commissioned senator Sarbanes and Congressman Mike Oxley to come up with tough rules that would counter the chances for such scandals.\n\nSuch laws are evident in various sections of the constitution such as the following few; section 406 requires executive managers to sign a code of ethics, section 409 calls for a timely announcement in the change of financial materials, section 802 and 1101 forbids any modification, damage or fabrication of any documented information so as to hinder inquiry on any issue.\n\nThese and many more ensures that all companies or shareholders in the US stock market comply to the SOX (Holt 2008).\n\nChildren\u2019s Online Privacy Protection Act of 1998 (COPPA)\n\nThis act became effective on 21 st April 2000 in the US. It looks at online gathering of personal information by an operator from children of thirteen years and below. It sets clear of what an operator should consider in privacy policy, what time and which manner to acquire information with approval from a parent or guardian. The act relates to electronic services, operated for money-making intentions.\n\nHere, children under thirteen are involved whereby they are requested to give information. The Federal Trade Commission (FTC) is accountable for giving regulations and effects COPPA.It also ensures a secure harbor such as that given to TRUSTe, ESRB, CARU and Privo. It ensures a sliding scale on which the consent of the parents is based, giving accountability of the way information is acquired and the uses it is put into.\n\nViolators of COPPA regulations are subject to fines, for example, Xanga websites was made to pay USD $1 million because of letting children under 13 to sign up for service on several occasions without their parents knowledge and UMG Recordings gave a fine of USD $400,000 for promoting the Lil Romeo and supporting children games.\n\nWebsite operators should ensure that they deal with the parents by maybe doing one of the following; making use of the credit card particulars, a digital signature via emails, parent\u2019s signed forms through poster and calling them through a suggested telephone line. Following such rules safeguards against disclosure of private information by a person disguising parenthood (Bro 2004).\n\nCalifornia database security breach act of 2003\n\nThis is meant to protect citizen\u2019s private information against theft whereby the details can be used to carry out a transaction with the thief disguising identity. The act came into effect on 1 st July 2003 and it was warmly welcomed by the citizens due to the then increased cases of identity theft.\n\nThe act concerns organizations that are accountable for guarding vital information; these organizations are to report any case of crime in the most appropriate time. It does not only apply to the businesses in California but also to those outside though affiliated to the state.\n\nThe act is also responsible for alerting the citizens over a suspected security breach, this is at times feared as it can jeopardize a company\u2019s reputation and encourage hackers as they delight in causing customer panic. Sensitive information processed via the internet is also controlled by this law.\n\nVarious methods have been adopted to guarantee security; such include, installation of certain software which detects unusual behavior and prevents any access to the server and gateways which are used to sense any unauthorized access to personal information.\n\nCustomer alerts through email and web postings on breaches is also sponsored by this act. An example of a protection company that offer such protection is the Andy Lawson and Southland shredding (Tendick 2010).\n\nThe Computer Security Act\n\nThe computer security act of 1987 aims at offering security and privacy of personal information for the federal computer systems and also giving acceptable measures to effect security. It was enacted by the united State Congress through the Federal Information Management Act of 2002, section 305 (a).\n\nThe act regards sensitive information as that which when lost, altered or destroyed with no permission, can lead to adverse effects on the reputation of the centralized system thus leading to loss of interest by the public. In the past, the act has not satisfactorily provided security to this kind of data in the government systems thus their exposure remains a challenge.\n\nUnder this act, the following are the provisions made; National Institute of Standards and Technology (NIST) which is required to lay down approved laws to safeguard information. Security plans are established and the owners or users of the system are required to undergo some training on how to implement the laid down measures.\n\nAnnual and independent assessment of the security plans is carried out to determine their efficiencies and conformity with Federal Information Security Act, (FISMA) necessities. FISMA also requires the establishment of an incident center whose aim is to offer technical help to federal agencies in the detection, analysis and compilation of federal data.\n\nExamples of federal computer crime includes; accessing a federal computer system without permission, accessing security, financial or credit information from a state computer system illegally or transmitting a code that can cause destruction to a protected computer.\n\nAny detected crime is to be forwarded to the congress through a court order. It is not the role of the federal government to protect information stored in non-governmental computers and therefore the act does not consider illegal access to such information to be crime though it requires certain information in the same non-governmental system to be safeguarded.\n\nAn example is the November 1999 case with the Social Security Administration (SSA) which deals with the largest percentage of the federal government expenditure which was threatened with hacking. Also in July, Department of Transportation (DOT) system was illegally accessed through the internet, putting the information on health, benefits and other areas at risk (Willemssen 2000).\n\nThe Privacy Act of 1974\n\nThis act was put into place due to security issues pertaining the development and use of computerized information. To offer security, four practical and substantive privileges were created. Firstly, an individual can be shown information stored on him/her.\n\nSecondly, it required that the agencies be fair in their dealings by following given laws. Thirdly, how the agencies share an individual information with others is restricted. Lastly, an individual is in a position to file cases against the state upon violation of their personal information.\n\nThe act also exempts some cases where ones information can be disclosed, such situations include; when there is need to enforce a law, when the information needs to be stored in archives for historical purposes by the united government, when carrying out statistical analysis by the census bureau, for daily uses with the government agency for administration uses and investigation purposes.\n\nDue to advancement in technology, individual information can be kept in databases, the risk of accessing this data is evident and this necessity the need of such a law so as to ensure security.\n\nIn 1973, the department of Health, Education and Welfare (HEW), gave a report that required the congress to adopt a code that could ensure fair information practice of personal information. The code had several guidelines which included the following;\n\n  * No system for recording personal details should be kept in secret\n  * An individual has the right to know what information is kept on him/her and intentions for the same\n  * An individual is entitled to the right of ensuring data collected on him/her is used for its original purpose and not for other reasons\n  * An individual can correct or alter information kept on him/her\n  * Any organization concerned with record keeping of identifiable data should ensure its rightful use.\n\nThe privacy act only protects information in the centralized systems. Violation of this act has both the civil and criminal penalties such as follows; suing an agency on refusal to access, correct some data on request where the court passes an order for amendment and causes the US government to pay certain fee such as the litigation and Attorney\u2019s fee.\n\nA fine of $5,000 maximum is also passed to any person or agency that intentionally asks for identifiable data about an individual in disguise, keeps this information in secret or discloses the information without the owners consent (The privacy act of 1974 2009)\n\nUniform Electronic Transactions Acts\n\nThe uniform electronic transactions act (UETA), was established in 1999 with the sole purpose of countering barriers against ecommerce. Due to many states agreeing to use different forms of signature, there was a need to come up with a uniform one so as to facilitate ecommerce. This was to be effective by including signatures that could have the same impact on the electronic data as it had on paper.\n\nThe esignatures are associated with the electronic record and a person carrying out a transaction should adopt it. The word record here is used to denote information that is stored electronically and can be retrieved. The act does not create any new rule but fosters for the inclusion of contracts, signatures and records to be included in electronic data. In 2000, after the enactment of EUTA, US congress passed the E SIGN act.\n\nThe act applies to parties who agree to carry out their transactions electronically. This becomes significant because when one involves in a business, signature is very important for clarification and easy tracking when it comes to payment, receiving a mare email cannot be a good evidence of the terms and conditions agreed on.\n\nWithout the signature, it is easier for the involved party to deny the conditions agreed on in the transaction, as such, this evidence is vital for smooth and fair transactions.\n\nThe act covers those transactions that concerns business, commerce and state. Section seven of this act allows use of electronic signatures, records and contracts in business transactions as long as there is an agreement between the parties involved to use them. The act is also concerned with ensuring that the person who sends an electronic item is the one whose signature is used.\n\nThis is termed as attribution ,as the signature is always attributed to the sender of the records. Timing is also an aspect under UETA, a record is said to have been sent when it reaches the recipient in a manner that his/her computer system can read or when the record is no longer in the hands of the sender.\n\nUETA also ensures that records are received in the recipient business place on residence if there is no such business place available (Miller.Jentz 2009).\n\nElectronic in Global and National Commerce and the state\n\nThis act was enacted by the US congress on 30 th June 2000 and its aim was to promote the use of electronic records in both the local and international business transactions. The act ensures the legality of any business carried out online. Confirmation of the consumer consent is required to be put in writing by the law. This act came into effect in October 2000.\n\nThe act allows for information required by the law in writing to be passed electronically with the consent of the customer and with a confirmation that he/ she can access this information if transferred electronically.\n\nThis act has several requirements such as; that the Esign, which is only effective when the concerned party has intended to sign, that a party not only uses electronic signatures and records but also, facilitate their legal use, that a federal state be neutral in terms of technology used in carrying out transactions and that no need of other embossing devices or seals is required as long as there is provision for them in electronic means\n\nLike any other act, ESIGN has exceptions such as; when supply of a utility like water or electricity has been cancelled, in case of court orders requirement to carry out an execution and when there is need for a document in the process of handling hazardous material.\n\nThe act concerns business involving two or more entities and records that do not involve regulations governing wills such as divorce and adoption. Two steps are involved in the signing of documents electronically; firstly, revelation of the right by the consumer to use paper followed by a notification of the electronic procedures. Secondly, the actual capture of the signature.\n\nFiles stored electronically should be accurate and made available to both parties; this is achieved by all the involved parties saving their work in their computers. This calls for businesses to choose an ESIGN that is flexible and can be easily accessed and is readable (Bowman 2009).\n\nUniform Computer Information Transactions Act\n\nThe Uniform Computer Information Transaction Act (UCITA) was passed by the United State law to set a precise and standardized principles to control the transaction of computer information such as software licensing. The failure of Uniform Commercial Code to provide coverage of the software business facilitated to UCITA birth.\n\nIt was presented by the National Conference of Commissioners on Uniform State Laws in 1999 (NCCUSL) in collaboration with the American Law Institute (ALI) as a alteration of the standardized Commercial Code to guarantee uniformity. It was introverted in 2002. It aims at making the rules that govern the transactions in information technology into a common ground like the Uniform Commercial Code applies in business.\n\nUnder the same act, clarification of rules regarding good use, consumer protection, shrink-wrap licenses and their timing and ability to make transfers is ensured.\n\nThe consumers are allowed to return goods if only their licenses are invalid. It has not been embraced by all states because, some fill that the Act is not adequate in its provision of security to the software transaction as there other organization which can not only offer protection but also manufacture them. This act applies majorly when computer information is used in a transaction.\n\nThe act defines computer information transaction as a contract to generate or alter computer information. It defines computer information as digital information that can be processed by a computer. This act does not deal with services on financial business, animations or image programming.\n\nIt is the first law developed to ensure uniformity of the technology economy, though it has been embraced partially it is only in Virginia and Maryland that the act has succeeded. Efforts to enforce it in other states have failed due to controversies over its effectiveness (Uniform Computer Information Transaction Act n.d).\n\nConclusion\n\nDue to technological advances that are evident in the current world, there is a great need to keep records that were previously stored in papers in an electronic medium. Business transactions are also carried out through electronic media.\n\nThis method of data storage and transaction is not very secure as computer crime is also on the rise thus causing risk of alteration, modification, misuse and eventual loss of very large amount of useful and sensitive data. Privacy of personal information is also at threat of being illegally accessed. It is all these reasons that have seen to the enactment and enforcement of the above discussed laws so as to safeguard these information.\n\nReferences List\n\nBowman, I. (2009). Electronic Signatures in Global and National Commerce Act (\u201cESIGN\u201d). Web.\n\nBro, H., R. (2004). The E-business legal arsenal: practitioner agreements and checklists. Washington, DC: American Bar Association.\n\nHolt, F., M. (2008). The Sarbanes-Oxley Act: costs, benefits and business impact. Burlington: Butterworth- Heinemann.\n\nMiller, L., R. Lentz,A, G.(2009). Fundamentals of Business Law: Excerpted Cases. Wodsworth: Cengage Learning.\n\nOak, M. (2011).Types of computer crimes. Web.\n\nPrivacy rights clearinghouse.(2003). Web.\n\nTendick, R. (2010). California Data Security Breach Act Helps Protect Private Information. Web.\n\nThe Privacy Act of 1974. (2009). Web.\n\nUniform Computer Information Transactions Act . Web.\n\nWillemssen, J., C. (2000). Computer Security. Web.\n",
        "label": "human"
    },
    {
        "input": "Information Security Fundamentals: Computer Forensics Term Paper\n\nIntroduction\n\nThe main objective of computer forensics is to examine the validity of electronic evidence in a manner that is acceptable in a court of law. The basic procedures involved in computer forensics are the identification, preservation, recovery, analysis and preservation of digital evidence gathered.\n\nComputer forensics does not play a significant role in alleviating computer crime but also an integral process in civil proceedings that involves the application of techniques and practices aimed at the establishment of a legitimate examination trail (Blackley et al., 2003).\n\nThis paper outlines the steps required in making electronic evidence acceptable in court, the various crimes, and incidents involved in electronic forensic investigations and the importance of security and computer policy applications.\n\nIn addition, the paper provides an overview of the techniques used in obtaining evidence from the internet and web resources, the types of evidence that can be recovered from electronic and computer resources, and the importance of documentation and chain-of-custody in the judicial process.\n\nSteps necessary to make electronic evidence acceptable in court\n\nThe practices involved in the examination of digital evidence and evidence collected because of computer forensics investigation are usually the same. The admissibility of digital evidence in a court of law is impeded by the fact that most digital evidence is collected without legal authority. This implies that digital investigation is problematic in making collected evidence acceptable in a law court (Brenner, 2007).\n\nWith the chief objective of acquiring and analyzing digital evidence, there are three fundamental steps in making the collected evidence acceptable in a law court: evidence acquisition, authentication and relevance, and analysis.\n\nA typical example to illustrate this is during the seizure of a suspect\u2019s hard drive, a copy of the hard drive is made, after which it is analyzed to ascertain relevance to the court case and identify potential evidence such as deleted files (Bunting, 2007).\n\nEvidence acquisition for electronic evidence varies depending on the type of evidence. The significant challenge is acquiring electronic evidence to ascertain its location. For instance, some computer forensic processes require the examination of data stored in hard drives and log files, which are stored in the Random Access Memory of the computers.\n\nThere is standardized procedure in gathering electronic evidence, implying that the investigator must deploy suitable evidence collection methodology in order to secure the electronic evidence. It is also imperative that the investigator must collect the evidence in its raw state to not to temper the integrity and value of the evidence.\n\nIntegrity and value of the evidence play a significant role in making electronic evidence acceptable in a law court. Some of the steps involved in collecting digital evidence include the chain of custody, identification, preservation, and finally transport and storage.\n\nThe chain of custody serves to protect the evidence and ensure that the evidence was not subjected to alteration and modification during the period that the evidence was in custody. Identification of evidence requires immense expertise concerning computer hardware and digital media (Clarke, 2010).\n\nIt is essential to collect the evidence immediately after its identification in order to avoid modification of the evidence due to subsequent computer usage. Duplication and imaging are sometimes done in order to facilitate a systematic analysis of the evidence. The forensic investigators have the responsibility of ensuring that the duplicating utility does not alter or introduce new features into the originally collected evidence.\n\nDuplication of evidence is bound to affect the admissibility of evidence in a court; this implies that the forensic investigators have to ensure that the copy is an exact replicate and a valid one. Also, they must ascertain the repeatability of the imaging process (Clarke, 2010).\n\nThe second step in making digital evidence admissible in a law court is authentication of the evidence. This entails ensuring that the gathered evidence represents the exact copy during the time of identification of the crime. In this context, the forensic investigators have responsibility of ensuring that the collected evidence is from a computer or any digital media that was available at the crime scene during identification of the crime.\n\nThe evidence must not be altered or destroyed in order to prove its authenticity. One vital technique used in evidence authentication is time stamping, whereby the duplicated evidence is compared with the original copy of the evidence (Cowen, 2009).\n\nA third step in making electronic evidence acceptable in a court of law is to evidence analysis. This involves using validated tools that are not bound to taint the evidence collected. Some of the most common activities during evidence analysis include searching the database files for any pertinent data, searching and recovery of deleted files and noting the changes in the system states (Cowen, 2009).\n\nReport generation accompanies the analysis process whereby all the steps involved during the computer forensics investigation processes are documented in a manner that they depict the relevance of the evidence to the case. The report generated must be able to counter any legal challenges in the courtroom (Kruse & Heiser, 2002).\n\nCrimes and incidences involved in computer forensics investigation\n\nVarious kinds of crimes and incidences warrant the deployment of computer forensics investigation in order to uncover the offenders and the nature of crime. One of the most common crimes that require computer forensics is network intrusion and hacking crimes. These entail gaining unauthorized access to people\u2019s networks and computer systems.\n\nHacking and network intrusion are typical examples of violation of computer security policies (Kruse & Heiser, 2002). The main objective behind hacking and network intrusion is to steal and modify information without the knowledge of the owner. The application of computer forensics, in this case, is to examine the log trails in order to identify the nature of information stolen and trace the hacker.\n\nIt is therefore essential for network administrators to have prior knowledge of computer forensics in order to counter hacking and network intrusion crimes. In addition, network and information systems administrators require fundamental computer forensics skills in order to enhance the survivability of an organization\u2019s network (Kruse & Heiser, 2002).\n\nThe second kind of crime involved in computer forensics investigation is cyber terrorism, whereby an attacker uses a computer attack to government agencies with the aim of destroying or modifying critical information concerning the activities of such agencies.\n\nCyber attackers may be influenced politically or socially depending on the context of the attack. In most cases, it entails the use of the internet resources to administer such attacks. Computer forensics, in this case, serves to identify the nature of the crime, the motive of the attacker and possibly trace the source of the attack.\n\nThe third type of crime associated with computer forensics is computer fraud, which refers to deceitful misrepresentation of information with the intent of gaining benefit from such actions. Fraud is diverse and entails the use of false identities to instill identity theft, consumer-based frauds and other cybercrimes (Cowen, 2009).\n\nComputer fraud can be initiated in various ways, with the first approach being modifying of computer data without authority. This needs minimal technical knowhow; it is mostly evident in cases whereby by data entry staff input the wrong data intentionally or modifying the data before entry into a computer system.\n\nThe second manner in which computer fraud can be initiated is through destroying or modifying the output with the intent of hiding unauthorized computer-based transactions.\n\nDeletion and modification of stored information is also a way of initiating computer fraud. Another common type of computer fraud is misusing computer software for dishonest purposes. The detection of such crimes requires computer forensics (Newman, 2007).\n\nCrimes that result to loss of computer information and affecting the integrity of such information warrant the deployment of computer forensics in attempt to determine offender and the nature of the crime. Computer viruses are a typical example of such crimes. The creation and spread of computer viruses are illegal.\n\nComputer forensics serves to identify the source of computer viruses, the objectives of creating the malicious code and the individual behind it. Other crimes that require computer forensics include phishing frauds, which involves posting links that appear to be somewhat trusted and requesting users to input their personal information (Ross, 2000).\n\nComputer forensics also play a significant role in detecting sex crimes and child pornography carried using the internet and other web sources. Computer forensics can also be applied in crimes relating to intellectual property theft (Newman, 2007).\n\nImportance of security and computer use policies\n\nComputer use and security policies are an integral part of ensuring information systems security within an organization. With the increasing internet use, organizations have the need to implement various security policies in order to guarantee the security and integrity of their data within and outside the organization\u2019s environment.\n\nThe first importance of computer use and security policies is that it ensures confidentiality breach of data in an organization (Newman, 2007). Confidentiality is a computer security concept that aims at alleviating releasing of information to individuals and systems that have no permission to access such files.\n\nComputer use and security policies ensure confidentiality by limiting the access to critical information in the organization\u2019s information system. This implies that effective computer use and security policies are essential in ensuring privacy of information deemed critical and confidential to the organization or any individual (Vacca, 2005).\n\nComputer use and security policies ensure the integrity of information. Data integrity is ensured when the information cannot be edited due to limited access to the data, and in cases whereby the data has been modified, it is easy to identify and track the changes made to the data.\n\nComputer use and security policies foster data integrity by limiting the personnel and their respective access rights to an organization\u2019s database. Integrity can be implemented through formulation of administrative and logical control policies when accessing information a database (Wall, 2007).\n\nPhysical controls and separation of duties are essential in ensuring that data integrity and confidentiality are maintained in an organization.\n\nAnother important aspect of computer use and security policies is facilitating availability of information. Information availability means that the data should be available when its use is needed. Availability of data is achieved through the implementation of security control policies aimed at protecting the various access channels and ensure that computer systems are functionally as expected.\n\nSecurity policies facilitate availability through detecting of the various intrusion programs that may impede the functionality of computer systems and network within an organization (Newman, 2007).\n\nAccountability is an essential concept in the information systems of organizations. Accountability means that individuals are answerable for any operations that take place in the information of an organization.\n\nFor instance, if a network administrator is the only one who has the access rights of modifying the database, he is the one accountable for cases relating to modification, or he breached confidentiality by issuing his credentials to allow the modification of information. It is, therefore, evident that computer use and security policies play an essential role in ensuring accountability in an organization (Wall, 2007).\n\nAnother importance of implementing computer use and security policies is that it helps to mitigate the costs associated with data losses, computer crimes, and computer security problems. Computer crimes are significantly increasing resulting to loss of valuable data and electronic sabotage.\n\nDue to this, organizations that have not implemented effective computer security policies and strategies are more vulnerable to risks associated with computer security. The present state of the internet filled with malicious users, untrusted links, and employees warrants the deployment of effective computer security policies to protect an organization\u2019s information assets.\n\nThe costs associated with network breaches and information disclosures are high, this means that preventive strategies such as computer use and security policies are a requirement in order to mitigate such costs. It is therefore important for organizations to deploy appropriate computer security policies as a risk management strategy against confidentiality breaches, data losses and integrity and ensuring accountability (Wall, 2007).\n\nTechniques used to obtain evidence from the internet and web resources\n\nComputer forensics significantly relies on the internet and other web resources for gathering evidence regarding computer crimes. It is important to ascertain the admissibility and relevance of evidence before embarking on evidence collection strategies.\n\nThe techniques deployed during evidence collection must ensure that they do not tamper or alter the digital evidence. There are no standard procedures used by investigators to collect the evidence from the internet. However, there are standard guiding principles that can be applied in order to collect evidence over the internet and other web resources as described below (Blackley, Peltier, & Pelitier, 2003).\n\nThe first approach of collecting evidence from the internet is to put into consideration the order of volatility of the evidence. With this regard, the investigator should first gather evidence from sources that are more volatile and proceed to sources that less versatile.\n\nFor instance, sources that are more volatile can be temporary internet files, registers and the routing table. Sources such as archival media and physical configuration are less volatile and should be considered last (Ross, 2000).\n\nIt is essential to incorporate evidence collection steps that facilitate the gathering of transparent and reproducible evidence. With this regard, the first stage is to identify the evidence by listing the various systems involved and a detailed description of the incident.\n\nThe next procedure is to establish the admissibility and relevance of the evidence and establish the volatility rank of the various system elements and potential sources of evidence. It is essential to eliminate any elements in the network that may introduce changes in the evidence. The next procedure is to collect the evidence putting to consideration their order of volatility.\n\nDocumenting each procedure and the evidence collected is essential since it eliminates the possibility of the collected evidence being challenged in a court of law in admissibility grounds. Generation of checksums and deployment of cryptographic signs is important because it helps in the preservation of evidence and creating of audit log of the computer crime (Ross, 2000).\n\nArchiving evidence is another important technique in collecting evidence from the internet and web resources. Archiving involves the securing the evidence and documenting the necessary procedures during evidence collection. Computer forensic investigators should use the common storage devices and access to archived evidence should be limited.\n\nThis implies that the evidence security measures should be deployed in order to facilitate the detection of any unauthorized access of the evidence. There are diverse set of tools required for collecting evidence in various operating systems platforms. For instance, it is vital for a forensic investigator to have an application program used for evaluating the processes, the system state and carrying out bit-to-bit duplication.\n\nAlso, the investigator should have application programs for creating checksums and digital signatures, applications for creating images for analysis and scripts to facilitate the evidence collection process through automation. It is important to put into consideration the authenticity of the evidence collection tools and application programs in order to alteration of evidence (Ross, 2000).\n\nTypes of evidence that can be recovered from computer and electronic devices\n\nComputer and electronic devices are potential sources of evidence for a criminal case proceeding. Some of the digital devices that can hold evidence include hard drives, flash memories, Personal Digital Assistants (PDAs), printers, mobile phones, and floppy drives. One of the types of evidence that can be collected from electronic devices and computer systems is physical evidence.\n\nPhysical evidence is broad an entails all the material evidence such as digital cameras, storage media, video footages and audio trails, which attempt to provide a link between the offender and the criminal case at hand. It is imperative for forensic investigators to gather physical evidence that can be reproduced.\n\nThe most common type of physical evidence that can be gathered from a computer and digital storage media is documentary evidence, which refers to evidence in the form of business logs, manual, printouts, and files that are computer-generated.\n\nTherefore, during evidence collection, the computer forensic investigator has the responsibility of collecting all the available physical evidence such as the computer system itself, the documentations, and systems logs (Vacca, 2005).\n\nComputers can provide computer-generated evidence, which are divided into four basic categories: visual output, printed evidence, film, and audio footage, which can be stored on optical disks and hard drives. A legal drawback of computer-generated evidence is that it is perceived to be hearsay by most of the legal courts (Clarke, 2010).\n\nComputers and electronic devices can also provide digital evidence, which refers to information regarding a criminal case that can be used during a trial in a court case. Digital evidence is broad an entails any files that the computer and other electronic devices generate during the course of their usage.\n\nFor instance, emails, histories and logs of internet browsers and network activities respectively, and documents such as notepads. Digital evidence is somewhat similar to the traditional physical evidence but has an added advantage of being difficult to extinguish and its duplication is easy (Brenner, 2007).\n\nAnother type of evidence that properly configured electronic devices and computer systems provide is log evidence. Logs are trails of activity on computer system such as network activities and operating system activities.\n\nWithin the context of computer forensic investigation, logs play an important role in facilitating the integrity of evidence, normalization, data reduction, and time stamping. This means that log files serve as a potential source of evidence during the analysis of computer crimes activities (Cowen, 2009).\n\nImportance of documentation and chain-of-custody in the forensic process\n\nDocumentation is an integral step in any forensic investigation process. This means the computer forensic investigation procedures should put into consideration the documentation of all the people involved during the process and the roles they played during the evidence collection phase.\n\nA chain-of-custody represents the chronological account of the investigation procedures; as a result, they are used in ensuring accountability during the investigation process.\n\nDocumentation and chain-of-custody reveal all the steps undertaken in identifying and collecting evidence and reports the conditions put into consideration to make the evidence relevant to the case at hand. In addition, the chain-of-custody provides all the activities undertaken in ensuring security of evidence and storage (Ross, 2000).\n\nThe first significant importance of documentation and chain-of-custody in the forensic process is that it helps in ascertaining the admissibility of digital evidence in a court of law.\n\nDocumenting all the procedures and people involved during an investigation is one of the key requirements in ensuring integrity of evidence; this means that presenting evidence with chain-of-custody increases the admissibility of forensic evidence (Vacca, 2005).\n\nAnother importance of documentation and chain-of-custody is that it helps the evidence to withstand any legal challenges regarding the originality of the collected evidence during the judicial process. This is fostered through ensuring accountability of the personnel responsible for handling the evidence and a record of all the conditions that the evidence was initially collected.\n\nIn a court of law, it is a requirement that the chain-of-custody should report all the personnel responsible for handling the evidence and that it should not have been subjected to modification, in other words, the presented evidence should be the one that investigators collected in a crime scene without any discrepancies (Wall, 2007).\n\nThe third importance of documentation and chain-of-custody is that it facilitates the analysis of forensic evidence by the investigators. Having a systematic documentation of processes involved in evidence collection makes evidence analysis easier.\n\nIn addition, it facilitates the making of jurisdictional judgments easier. This means that documentation and chain-of-custody has a significant role and value in establishing the origins of evidence and its influence on the case at hand. Also, the documentation facilitates the process of criminal scene reconstruction, making the forensic investigation process less difficult (Kruse & Heiser, 2002).\n\nEnCase\n\nEnCase is one of the computer forensics tools developed by Guidance Software. The first use of EnCase is to provide an analysis of digital media for forensic investigations, data investigations, and recovery. Most of the law enforcement agencies consider the EnCase forensic tool as a standard for analyzing digital media during evidence collection.\n\nSome of the services available in the software include data acquisition, parsing of file, and retrieval and recovery of data. A typical example where EnCase was used in the criminal court was the case of BTK killer, whereby EnCase was used for data recovery (Bunting, 2007). EnCase forensic was developed for computer forensic practitioners who want to collect evidence in a repeatable manner.\n\nProduct features and functionalities of EnCase Forensic tool\n\nOne of the most outstanding features of EnCase Forensic is that has inbuilt automated tools aimed at speeding up the forensic investigation process. Some of the automation tools include EnScript, which enables user to write scripts and at the same time use prebuilt scripts. It also has an Active Directory for extracting data and recovery partitions used in the recovery of deleted files (Bunting, 2007).\n\nThe EnCase forensic tool comes with analysis features such as hash analysis, a log parser used in the study of Windows events, analysis of digital file signature and a file finder, which is used to detect and extract files in unallocated disk spaces.\n\nThe EnCase facilitates reporting through generation of automatic reports. This is done through providing a list of all the Uniform Resource Locators and the respective dates that the sites were visited by a user. Besides, the EnCase supports the generation of log reports and incidence response reports (Bunting, 2007).\n\nThe EnCase forensic tool has features that can facilitate the investigation and probe processes on email and internet. Some of these tools are used for analysis of browser history and providing email support. Other features of the EnCase forensic tool include bookmark features, searching and data acquisition support. The starting price for purchasing EnCase Forensics tools is $ 9995.\n\nHow EnCase facilitates the forensic investigation process\n\nEnCase functions by generating an exact copy of the authentic digital media gathered in a crime scene. After the creation of a copy in binary form, EnCase does the verification through generation of hash values that are used in revealing when the collected evidence has been tampered with.\n\nEnCase can be customized in order to facilitate the automation of investigative processes that may be consuming too much time. Report generation is an essential aspect of the EnCase that makes the whole investigation process easier (Kruse & Heiser, 2002).\n\nReferences\n\nBlackley, J. A., Peltier, J., & Pelitier, T. (2003). Information Security Fundamentals. New York.\n\nBrenner, S. (2007). Law in an Era of Smart Technology. Oxford: Oxford University Press.\n\nBunting, S. (2007). EnCase Computer Forensics, Includes DVD: The Official EnCE: EnCase Certified Examiner Study Guide. John Wiley and Sons.\n\nClarke, N. (2010). Computer Forensics. New york: IT Governance Ltd.\n\nCowen, D. (2009). Hacking Exposed Computer Forensics, Second Edition: Computer Forensics Secrets & Solutions. New York: McGraw Hill Professional.\n\nKruse, W., & Heiser, J. (2002). Computer forensics: incident response essentials. New Jersey: Addison-Wesley.\n\nNewman, R. C. (2007). Computer Forensics: Evidence Collection and Management. New york: Auerbach Publications: Taylor and Francis Group.\n\nRoss, S. (2000). Digital archaeology? Rescuing Neglected or Damaged Data Resources. London: British Library and Joint Information Systems Committee.\n\nVacca, J. (2005). Computer forensics: computer crime scene investigation, Volume 1. New York: Cengage Learning.\n\nWall, D. S. (2007). Cybercrimes: The transformation of crime in the information age. New York: Cambridge University Press.\n",
        "label": "human"
    },
    {
        "input": "Computer-Mediated Communication: Study Evaluation Essay (Critical Writing)\n\nIntroduction\n\nThe impacts of intercultural awareness in the use of electronic email exchange between Hungarian learners and native English speakers seem to correspond with Laura (2009) that use of email as a communicative device provides a platform needed to authentic communicative events, but the researcher neither tests the hypothesis nor evaluates the program.\n\nLaura (2009) also states that email communication enhances exchange of ideas between native speakers and L2 learners, which improves reading and writing skills and development of cultural awareness in the long run.\n\nThis is inconsistence with Kern (1998) reports that indicated no communication improvement of students who learn from interaction with their virtual peers.\n\nResearcher deviation from scientific research procedures in developing research design, population and sample collection can not gain the status of real and useful investigation.\n\nBackground\n\nUnfortunately, although research shows that the sample group consisted of 13 pairs of both Hungarian and American students, it did not indicate how the samples were chosen or the demographic location of the research. It is also unclear of what \u201cL2 learners\u201d are despite being used throughout the research.\n\nAlso, the research did not indicate the range of sample group composition in terms of age, demographic location, gender and race, which suggests that the study findings provide unclear perspective on the selection of sample group.\n\nIn examination of social interaction such as an academic environment for his case, a researcher needs to gain first-hand information about social process, essential measurement tools and appropriate research methods.\n\nFor this reason, constant comparative method used as a qualitative analysis tool may hold more promise than their use since the researcher only looked at the end-product.\n\nHowever, little research on constant comparative method supports the coding process of what Strauss and Corbin (1990) states as \u201cbreaking down, examining, comparing, conceptualizing and categorizing data\u201d (p.61) as effective data collection method, little research has reported their efficacy in social environment.\n\nThe study did not however describe how participants were trained or record data collected. No published studies have reported on the usefulness of computer-mediated communication as a platform for development of English language speaking and writing skills even though email communication has become common in consumer culture during the past decades.\n\nThe theoretical basis for this research included principles drawn from social-based practices of a learning environment to elucidate participants\u2019 needs. Since the researcher was also a teacher of the Hungarian students, results of the study could have been manipulated to fit her perspective.\n\nLiterature review\n\nThe research should have begun by introducing the reader of what intercultural communication is and how it relates to intellectual awareness. The author fails to mention what intercultural communication and competence in relation to academic learning environment.\n\nLiterature review provided here in discussion of intercultural communication is very limited and also fails to provide substantial evidence on intercultural communication as intellectual awareness tool.\n\nFor example, the author emphasises on the importance of computer-mediated communication to help students learn and interact with one another through debate and discussions, but many questions such as is email communication as the primary communication tool in schools remain unanswered in this area.\n\nIt is also unknown from this literature whether the learning outcomes frequency or duration of email exchange influences cultural awareness or rather intellectual awareness and how long the effects lasts. In addition, it is unclear whether the positive influence of electronic emailing is affected by professional (the school instructor) and where the training of use of email is used.\n\nNone of the articles reviewed discussed specific learning outcomes of electronic emails in terms of the number of students who reported to have benefited from the program; therefore whether specific learning outcomes in relations to intellectual awareness are more or less effective in positively influencing intercultural awareness, language skills and elimination of stereotype remains unclear.\n\nFuture research should place emphasise on these variables if our understanding of the related factors influencing the use is to be expanded.\n\nProject description\n\nResearch period of the project is reported to have started in spring 2005 and does not indicate the period in which the project was completed.\n\nThe study sample was randomised, but the criteria on which the participants were selected in terms of age and race and gender were not mentioned which means that the population as a whole was not fully represented.\n\nWhen it comes to research objective, the author presents two objectives of the project which is confusing to a novice researcher. First; Laura (2009), states the objective of the project to be to develop better intercultural awareness and positive attitudes toward the target cultures, as students discussed information about the cultures, languages, and teacher education systems of the two countries (3).\n\nSecondly, towards the beginning of the essay, in the introduction part, the Laura (2009) stated the objective to investigate the development of intercultural awareness in an electronic mail (email) discussion project between Hungarian learners of English and American speakers (1).\n\nEven when known, neither of the objectives represents the underlying philosophy or may lead to tangible results. This makes it difficult for a reader to determine variables to be measured against each other in the research project.\n\nAlso, since the sample consisted of people who were community dwelling (a school setting), who were Hungarian learners and Native American speakers, pursuing language education and needed access to authenticate communicative events.\n\nThe results should be not be applied to other factors of learning population since its unclear how the samples were chosen. Future research plan should include randomised sample with clear variables in terms of age and race and use controlled trials with a larger and more inclusive sample than the selected 13 participants.\n\nThe author should have introduced measurement tools to be used in data collection and participants should have had a thorough knowledge of specific progression of communicative approaches used in the research. This case report describes the outcome of email communication to have improved their English language and writing skills for students who participated in the study.\n\nIt illustrates the importance of electronic email communication to have helped the students developed effective writing skills, improve critical thinking and reasoning skills. All these claims are based on assumptions as the researcher did not introduce variables to be used in the study objective.\n\nPeriod of research project was not however included in evaluating numbers of individuals at regular intervals. Laura (2009) also required students in the study group to freely choose topic of their preference, which did provide controlled trials of the research hypothesis.\n\nTo begin with, she has too many research questions. For example, the study groups are asked by Laura (2009) to choose topics based on teachers education program, personal experience, personal communication, experience of school, experience with other cultures, teacher education program, travelling, minority-majority cultures, festivals, fashion, sports, environment, foreign language learning and teaching (4) to mention just a few.\n\nIt seems to me that she decided to implement emails as a teaching tool and at some point she realised \u2018ahhh this email exchange thing is very good for raising intercultural awareness too! Oh I can publish it.\n\nDesign and Methods\n\nResearch methodology only uses the reflective papers of her students which is qualitative data and does not include triangulation. Combining both qualitative and quantitative methods (triangulation) may provide enough information that may improve validity of the results.\n\nTriangulation approach uses multiple sources of data collection which extends the quality of data and reliability of the results (Robinson 1992).\n\nThe research should have also used methodological sampling paradigms of quantitative exploration for using observable, measurable and quantifiable facts which could have helped explain why some diverse principle and knowledge about the simple world (Perry 2005, p.55; Glense and Peshkin 1992).\n\nAnother important potential benefit of using quantitative paradigm is that it produces knowledge that is free from manipulation that can be generalized.\n\nInformation on such concerns are meaningful and may assists researchers during their evaluation like in guiding evaluation interviews and develop particular explanations and predictions to generalize from the sample population.\n\nThis information may also help a novice researcher to maximise his analytic skills such as structured observation and content analysis (Dorney 2007).\n\nResearcher did not include additional criteria to help rule out difficulties in using language development in relation to vocabulary, grammar and style that were not related to email exchange. She does not take into consideration the Hawthorne effect despite the fact that she is the participants\u2019 teacher.\n\nToo many variables were used in the research. For example, participants belonged to different age groups, are at various language proficiency levels. She does not mention how their level was determined either before or after the project (pretest/ posttest).\n\nShe does not tell us how their intercultural competence was measured before and after the project. What were participants\u2019 motives for taking part in the project? Were the Hungarians tested/ marked on their performance or participation? What were the US participants\u2019 motives? She does not question that.\n\nBoth intellectual awareness and computer-mediated communication are represented in the studies reviewed, but the constant comparative method did not develop a theory that explained the mergence of electronic email and how they connected to the reality of the research of promoting cultural awareness (Glaser and Strauss (1967, p.28).\n\nIn coding procedure, Strauss and Corbin (1990) states that constant comparative method mentioned in open coding involves \u201cthe process of breaking down, examining, comparing, conceptualizing and categorizing data \u201c (p. 61) but Laura (2009, p.6) in this case has only compared the participants texts and refinement of the categories and failed to conceptualize and break down the data.\n\nThis means that the data obtained here can not guide subsequent theoretical sampling decisions. As a result, studies that follow constant comparative methods of data dissemination after collecting initial data will allow use of insights from that analysis and offers full perspective of future research of data collection.\n\nThe researcher should have allowed contacts with participants spaced over time or tract changes in the text messages until a strong theoretical understanding of the event or phenomenon has emerged.\n\nFollow up times that range from certain period should also be specified. The research methodology adopted raises questions about the validity, reliability and transferability of the findings.\n\nFindings\n\nGeographic region: The studies reflect intellectual awareness in an electronic email between Hungarian and American learners findings and offer a broader perspective on how the communication technology improves cultural awareness among other things.\n\nThe findings however, failed to provide geographical location of the study and since the author specifically mentions the research to be international email exchange, it would be impossible for one to consider findings only from one country to be reliable and conclusive. The study reviews were completed from February to April, making a total study period of 8-9 weeks.\n\nThis study shows incompetence in determining the exact study period, which should be either 8 or 9 weeks. Future research should follow participants beyond the stated period after questionnaires were issued to determine accurately of data obtained (Laura 2009, p.4).\n\nThe findings would have been useful if;\n\n 1. it was a longitudinal study\n 2. used learner diaries\n 3. was not based on a one-to-one interaction through email (some of the participants did not even get on with each other!). A discussion board would have been much more useful (and perhaps more fun for the participants). This would also make the task of cultural exchange more authentic, as it is a totally made-up situation. In real life, we don\u2019t really use emails to learn about each other\u2019s culture. Although the descriptive information of how variables were selected is valuable, it raises questions about what is the expected the learning outcomes and how they can be used for future research. Knowing what to expect during the research analysis may help in evaluating the impacts of intellectual awareness in an electronic email and quality of data obtained and identifying areas for improvement.\n\nThe findings on the rates of efficacies of electronic email emphasised the need for development of hypothesis which could be measured at the end of the study. For example, Laura (2009, p.5) reported improved writing skills on Hungarian learners as reported by their American counterparts.\n\nParticipants demonstrated their ability to learn English proficiency skills through email exchange, but these findings did not support the use of electronic email communication as the primary learning tool for developing reading and writing skills among Hungarian learners.\n\nWhereas the research is focused on intellectual awareness in electronic email, the study provides evidence that students experienced difficulties in language development in relation to vocabulary, grammar and style.\n\nThe research did not show how the school teacher trained in using constant comparative method and the arrangements of significant association with use of email exchange in academic environment to improve communication skills, which is inconsistence with the research objective to determine their relationship. These discrepancies could be attributed from different study population, or types of measurement used.\n\nAs indicated earlier in the literature review, language learners can learn effectively and more directly through email exchange, and the research relies on accuracy of data obtained from respondents. The research did not make efforts to reduce response bias in email communication in both Hungarian and American native speakers, the results should, therefore, be applied cautiously.\n\nThreats to biasness can be reduced by having assessment and training administered by professional people and not the students instructor for this case. Additional research is required to determine the extent to which computer-mediated communication led to the observed benefit of improved writing skills and cultural awareness (Laura 2009, p.4).\n\nData analysis\n\nLack of training in email use among Hungarian learners and American natives will not have beneficial effects on the ability of students to speak and learn English skills, which could have strong negative effect on research results obtained.\n\nSeveral study features that were required to determine email features in both written and spoken languages were inappropriate for specific design of the participants (Laura 2009, p.5). The author also reported that the Hungarian learners claimed to have improved their general writing, but did not provide any measurement to determine the credibility of these results.\n\nAll these findings indicate critical need for developing specific measurement tool regarding improved language skills before the results could be published.\n\nThe findings also suggested that suitable quantitative tool such as ethnographic fieldwork should have been determined at the beginning of the study to eliminate some instances of assumptions. Also, no calculations conducted to determine whether a statistically improvement was provided.\n\nDiscussion\n\nThe main research is to develop ethnographic emphasis that understands and interprets participants\u2019 views of the social world, academic environment for this case. Qualitative tool used was not appropriate for addressing views in social phenomenon because it requires people to interact more between perspectives and experiences (Firestone 1987) as opposed to how Laura (2009) generalized all the results.\n\nThe difficulties most often cited by participants were different perceptions in discussing education problems due to cultural diversity and age difference (Laura 2009, p.7), which might be the problem most affected by the interventions in this study.\n\nResearchers are encouraged to consider diversity on education difficulties interpretation for participants who present with these issues for future research. The research should also include ethnographic fieldwork to collect data about the way participants acts when interacting.\n\nAccording to Wolfson (1983), application of ethnographic fieldwork as a primary study measurement would provide most varied data and satisfactory results for research in pragmatics.\n\nConclusion and Recommendations\n\nThe adaptation of appropriate methods for research project is very important. Use of multiple perspectives such as triangular methods increases reliability of data collected, which can be used for future research. Much research has to be done in identifying research design, population and sample collection otherwise we would be far from understanding the complicated nature of research methodology.\n\nAll factors identified in this review that includes the relationship between humans and communicative devices must be considered if the expectation of concrete and reliable research is to be met. The fulfilment of this obligation can be accomplished through research, choosing desirable measurement tool, training on measurement toll to be used and addressing concerns of the students.\n\nAlthough the sample size for this research was small, further research is recommended to refine this area. The researcher also specify if the study used interviews, surveys, questionnaire or observable data at the very beginning of the research and include participants characteristics such as gender, gender ratio and race.\n\nList of References\n\nDorney, Z,.2007, Research Methods in Applied Linguistics , Oxford University Press, Oxford.\n\nFirestone, W. A. !987, \u2018Meaning in method: the rhetoric of quantitative and qualitative research\u2019, Educational Research , vol. 16, no.7, pp.16-21.\n\nGlaser, B. G. & Strauss, a. L. 1967, The Discovery of Grounded Theory: Strategies for Qualitative Research. Aldine De Gruyter, New York.\n\nGlesne, C., & Peshkin, A. 1992, Becoming qualitative researcher: an introduction , Longman, London\n\nKern, R. 1998. Technology, social interaction and FL literacy . In J. Muyskens (Ed.), New ways of learning and teaching: focus on technology and foreign language education , Heinle & Heinle Publishers, Boston.\n\nLaura, F. 2009, \u2018Outcomes of an intercultural e-mail based university discussion project, Language and Intercultural Communication, vol. 9, no.1, pp. 1-10\n\nPerry, F. L. 2005, Research in Applied Linguistics , Routledge.\n\nRobinson, M. A 1992, Introspective methodology in interlanguage pragmatics research , HI University of Howaii, Honolulu.\n\nStrauss, A. & Corbin, J. 1990, Basics of Qualitative Research: Grounded Theory Procedures and Techniques , Sage Publications, Newbury Park.\n\nWoldson, N. 1983, Rules of speaking: language and communication , Longman, London.\n",
        "label": "human"
    },
    {
        "input": "Computerized Systems: Material Requirements Planning Report (Assessment)\n\nTable of Contents\n 1. Introduction\n 2. Operation of MRP in UAE\n 3. Challenges of implementing MRP in Local UAE Organizations\n 4. Works Cited\n\nIntroduction\n\nIn the quest to handle inventory and to run firms efficiently, entrepreneurs have resolved to computerize their operation activities. The commonly used system is the Material Requirement Planning (MRP). According to Groover (741) MRP is a computer-based system that translates the operation master plan into series of activities and operation from the final product to the raw material acquisition point.\n\nBy adopting the current state-of-the art technology, firms have been able to manage their inventory effectively and consequently improved inventory related decision-making.\n\nMRP ensures that three critical goals in inventory management are met. First, through use of MRP, organizations ensure a stable supply raw material for processing and availability of finished products to meet customer demand. Secondly, application of MRP systems helps to keep a low stock level in order to avoid tying-up cash in terms of inventory.\n\nFinally, MRP governs key operation activities such as manufacturing, delivery and purchases activities to avoid overlapping and interrupting of production (Stevenson and Hojati 16). MRP is tailored to meet specifications of the enterprise depending on main activity of each firm.\n\nOperation of MRP in UAE\n\nMRP is an evolution in line Economic Order Quantity (EOQ) stipulations, which is aimed at maintaining the lowest level of inventory. It also states the replenishing amount, which is termed the re-order level while stating the maximum stock level as well as the minimum stock level.\n\nUnlike the EOQ system, which is manually executed, the MRP is automated and provides a systematic response depending on the level of inventory and recommends the remedial measure to each fluctuation.\n\nDue to the dynamic nature of demand and uncertainty in the market, production decisions are often difficult to project and therefore MRP systems help production managers in making their decisions.\n\nGenerally, all manufacturing firms face production dilemmas because consumers require supplies promptly while a long time is required to manufacture them. To meet these conflicting time demands, MRP remains the most reliable tool that enhances continuous production.\n\nMRP system is designed to work backward from an estimated finished product amount to determine the number of labor and raw material requirements. The preceding processes are systematically scheduled and subdivided into subassemblies, components, parts, and materials (Groover 742).\n\nEach process is assigned a specified time limit and cost function, by integrating all the sub-processes the operation manager is able to understanding the duration and cost of producing each product. According to Stevenson and Hojati (16), MRP helps the management to answer three production questions. The three questions relate to what is needed, what quantity is required and at what time.\n\nThorough understanding of what to produce, amount to produce and at what time, simplifies the operation decision design to ensure that only the required supplies are bought in their right proportions. Childe (77) emphasizes the importance of counter checking the data entering the system to avoid erroneous end products. For instance, inaccurate input compositions will not produce the expected final products.\n\nMRP sub-divides inventory needs into planning periods to ensure timely and systematic production process. All the planning periods are optimal in nature and they strive to maintain low inventory level and the subsequent carrying costs at their minimal.\n\nEffective adoption of an optimal MRP helps the management in understanding how to step up production capacity depending on the quantity of capital available and the time considerations.\n\nIn addition, a successful implementation of the MRP helps the firm to avoid contractual conflicts with consumers and suppliers because only the required raw materials and supplies are ordered and supplied. Stevenson and Hojati (18) highlight the importance of understanding inventory requirements in order to operate within the operation range.\n\nMRP input is generated from three sources; the bill of materials, master schedule and inventory records. The bill of materials contains all the details of the inventory such as raw materials, components and assemblies needed to complete each unit of production.\n\nBragg (22) asserts that the bill of material is the bedrock of all operation and a high accuracy level should be maintained to avoid spill over effects to the subsequent operations.\n\nBearing the importance of this item, each manufacturer should maintain bill of materials depending on the type of products one produces. MRP system deducts the quantity of materials, which are used to product a complete product from the inventory records in order to achieve the ordered requirements.\n\nAccording to Groover (742), the master schedule forecasts the production activities of the firm. The production projections are estimated using both the internal projections and consumers orders rates. The master schedule indicates the outstanding orders in terms of the products and the time they are due for collection.\n\nThe schedule is designed to cover a long time enough to produce the final product. In other words, the schedule is a function of demand and it does not entirely depend on the production capacity of the plant.\n\nIt is worth noting that the period should accommodate all the sub-assemblies ant operation until the final product is generated. Nevertheless, the feasibility of the schedule is not ascertained not unless the operation managers tried the prototypes until their reliability was proven.\n\nInventory record files supplies information that pertain inventory in go-down against the amount ordered. In inventory accounts, current stock is deducted from the material requirements. Bragg (20) adds that inventory records captures all inventory items on time basis such as gross requirements, planned receipts and projects stock level. Other information recorded, include suppliers\u2019 details, re-order level and the re-order quantity.\n\nChallenges of implementing MRP in Local UAE Organizations\n\nThere are several challenges that hinder successful implementation of the MRP system. The main challenge remains to be data integrity; this challenge is often overlooked but its consequences are massive. A thorough screening of the data input should be done in order to realize the expected output. For instance, a wrong composition of input ingredients would result in inferior quality of final products.\n\nGenerally, computerized systems produce quality depending on whatever it has been fed with. Childe (76) commented that the data integrity is conversely hampered by inaccurate phase adjustments, errors in the input and delivery of outputs, by-products unrecorded, spoilt materials and arithmetical errors.\n\nSimilar sentiments were shared by Groover (758) who emphasizes on the need to counter check data enter to avoid distortion of latter processes. All these errors lead to a wrong assessment of the inventory and the overall process and may eventually cause collapse of the firm.\n\nThe other challenge that faces MRP systems is the determination of time required for the development of the product from the initial stage to the final product (Childe 73). The assumption of a uniform lead-time is habitually imposed on all the products although this is not usually the case due to changes in demand and other uncertain issues.\n\nThe assumption is also wrong since the lead-time is also affected by the quantity of products being produced simultaneously in the plant -large quantities require longer lead-time required. Hence, the management needs to allow some spare time to cater for risks and uncertainties, which characterizes the nature of random elements such as demand and supply.\n\nMost firms, which operate various branches in different regions, are left in dilemma of whether to order for new supplies, when they have a huge backlog of such supplies in other branches. However, each factory needs to maintain a separate MRP to avoid emergence of future challenges and mix up between the branches.\n\nNevertheless, with proper communication, MRP can still interlink various branches; but this is only possible if systems adopted by the firm work efficiently before the use the MRP system. Besides, a keen attention should be borne in mind concerning the lead-time, which may be adversely affected by the carriage eventualities.\n\nThe other challenge that inhibits successful implementation of MRP is the lack of technological integration. Technology is dynamic and each firm should embrace the new technology to ensure a steady flow of operations without any delays and maintaining the quality of output. For big firms, the number of components and process involved becomes exceedingly enormous and only specialized computers should handle such transactions without major hiccups.\n\nWorks Cited\n\nBragg, Steven M. Inventory Accounting: A Comprehensive Guide. New Jersey: John Wiley & Sons, 2005\n\nChilde Stephen J. An Introduction to Computer Aided Production Management. London: St. Edmundsbury Press, 1997\n\nGroover, Mikell P. Automation, Production Systems, and Computer-Integrated Manufacturing. New Jersey: Prentice Hall, 2008\n\nStevenson, William J. & Hojati, Mehran. Production/Operation/ Management . New York: McGraw-Hill, 2001\n",
        "label": "human"
    },
    {
        "input": "Dell Computer Company and Michael Dell Research Paper\n\nIntroduction\n\nThe experiential leadership project will observe Dell, Inc. with its headquarter in Texas, United States, its missions and products, its roles and strategies, which lead to taking higher positions in the technological market, and the impact of Dell\u2019s founder, chairman, and CEO, Michael S. Dell on the development of this organization.\n\nDell is considered to be one of the leading PC manufactures, which provide population with reliable computers and all the necessary computer-related products since the end of the 20 th century. Many employers play significant roles within this company, however, the role of Michael Dell remains to be the major one as he is the person, who founded Dell and believed that this organization can achieve unbelievable success.\n\nDell Computer Company and its leader Michael S. Dell is a good example of the organization that has achieved success and gained the reputation of a really reliable organization that provides the best computer technologies and never wants to stop but to develop day by day.\n\nDiscussion\n\nGeneral Description of the Company\n\nNowadays, computer industry plays a very important role in the life of every citizen. Many computer makers try to use all possible opportunities to present proper technologies and computer products and satisfy each customer. Dell Computer Company is \u201cthe second worldwide in market share and consistently the leader in liquidity, profitability, and growth among major computer systems companies\u201d (Fulmer and Conger 160).\n\nEach organization has to have a mission in order to complete it and improve own actions. The mission of Dell is to sell computer technologies directly to their customers, enlarge their services, and provide only personalize assistance to show the customers their sincere care for each client.\n\nTaking into consideration the visions of the company like extensive care about the customers, the development of new products and services is obvious for this organization. If primary products were personal computers and the obligatory for these computers devices, nowadays, Dell provides people with various computer technologies, which may improve and make easier customers\u2019 lives.\n\nMore than 50,000 people work at Dell Computer Company, and more than 2 billion of interactions happen between Dell and its customers ( Company Facts para.6). These numbers prove successful reputation of the company and make the organization improve their work in order to attract the attention of more people and help them make the right choice during the selection of the computer technologies.\n\nMethodology\n\nIn order to gather reliable facts and information about Dell Computer Company and its leader, Michael Dell, certain data collection and analysis methods have been used. With the help of the interview with Mr. Dell, it turns out to be clear that his work and contributions to the development of computer technologies are really great and indispensable.\n\nAccording to Mr. Dell (2007), the work of their company has to be changed to reinvent their business and get success among their customers and shareholders.\n\nThe observation of the official website of Dell Computer Company is another helpful means that provide us with a chance to get more quantitative data. www.dell.com presents certain facts about Dell\u2019s cooperation with other business organizations, introduces numbers concerning how many companies run on Dell, and tells about the reasons of why Dell is regarded as a significant part of computer technologies development.\n\nData analysis methods are used to evaluate gathered information and represent the already processed data in an appropriate way. The interview with Mr. Dell has been re-read for several times in order to get clear understanding of his true intentions as for the role of his company in the lives of each citizens and the development of the technological world.\n\nNumerous competitions take place very often, this is why Dell Computer Company has to use its potential and best ideas to win the competitors and present the best services. To evaluate properly this kind of information, event analysis, discourse analysis, and content analysis have been used.\n\nExternal and internal analysis\n\nComputer technology is one of the largest fields, where many competitions between the manufactures take place. The major competitors of Dell Computer Company are Apple, Acer, Sony, Samsung, Hewlett-Packard (HP), and Asus. HP is the organization that mainly deals with printers; and Mr. Dell (2007) states that in spite of the fact that HP is the number one worldwide, their concentration on printers provides Dell with a chance to return their leading positions and produce suitable in all senses computer machines.\n\nCompetitions between the companies of the same field is a good chance for each organization to prove own abilities and skills. For example, one of the most effective steps, which have been taken by Dell Computer Company, was the creation of Idea Storm, a web site, where customers share their propositions, suggestions, and ideas of how to improve Dell\u2019s services and make them really helpful for the users.\n\nWith the help of online access, Dell may also provide everyone with the necessary information about its products and services. Illustrations and examples of Dell\u2019s technologies is a good decision to attract the customer and prove that this organization and its products are worthy of attention.\n\nThe major point is that Dell Computer Company always thinks about own customers, this is why the employers try to produce the best services with the best guarantees in order to present to their consumers the best time with these technologies.\n\nIf the company wants to achieve success on the computer market, it is also necessary to create appropriate atmosphere within a company and between the employers and employees.\n\nOrganization\u2019s culture and chosen methods play a very important role in success achievement: (1) reduction of bureaucracy should encourage workers to pay attention to such ideas like honesty and respect; (2) increasing of services capabilities should happen due to the development of the strategy BUILD-COOPERATE-BUY; (3) broadening of the company is caused by too complicated organization, and only well-intentioned people can be hired.\n\nAnalysis of Michael Dell\u2019s leadership style\n\nThe philosophy of Michael Dell is spread over the whole company: Dell will listen to each customer and will respond. Many things have to depend on customers, and in order to succeed, it is obligatory to listen and to meet customers\u2019 demands. Such philosophy is the one that helps Dell Computer Company to amaze its customers and never disappoint them with their choice.\n\nThe point is that Michael Dell is one of those leader, who completely understands own role and own place in the company. He left his position for a couple of years; however, within a short period of time, the board asked him to come back and to make certain changes to improve the company (Dell para. 4). Mr. Dell even does not want to hide his emotions and his proud of being back to the company as CEO, because his return is a great please for both his company and himself.\n\nHe, as a founder of Dell, creates such kind of strategy that may certainly satisfy his workers and the customers. From the very first days of the company\u2019s existence, Mr. Dell offers to direct all actions of their organization on meeting their customers\u2019 demands. His behavior and his actions mostly dealt with direct reports in order to utilize human resources, solve conflicts, and motivate people to change and improve.\n\nWhen Mr. Dell takes CEO position for the second time, he comprehends that the company spread too fast, and it is not that rational to rely on direct reports. More people are hired to different departments in order not to change the attitude to the work and to each employee but still to control the situation and make necessary improvements (Dell papa. 8).\n\nConclusion\n\nAfter a thorough analysis of Dell Computer Company and the style of Mr. Dell leadership, I come to the conclusion that the chosen by this person way of organizing the work is effective enough and may serve as a good example of proper vision, philosophy, and attitude to customers and employees.\n\nThe idea to create a web site, where customers\u2019 demands are taken into account in order to improve the quality of Dell\u2019s products and serves is really brilliant. Another successful move was to hire more people in order to reduce direct CEO\u2019s control and get more time to analyze and to evaluate.\n\nTo my mind, Dell Computer Company is one of those, who does not actually need any additional recommendations, because all the necessary actions have been already made, and the only advice that could be given is not to lose the chosen philosophy, pay more attention to clients\u2019 needs, and demonstrate how properly organized plan become real and bring positive results.\n\nWorks Cited\n\nCompany Facts. Dell . 2009. Web.\n\nDell, Michael. We\u2019re Willing to Change Everything . SPIEGEL. 2007.\n\nFulmer, Robert, M. and Conger, Jay, A. Growing Your Company\u2019s Leaders: How to Great Organizations Use Succession Management to Sustain Competitive Advantage . New York: AMACOM, a Division of American Management Association, 2004.\n",
        "label": "human"
    },
    {
        "input": "Microsoft Operating System for Personal Computers a Monopoly in the Markets Essay\n\nDefinitions and explanation of terms\n\nTechnology has evolved since the emergency of microprocessors. The sizes of computers have reduced rapidly over time due to innovations in technology. With the reduction of the sizes of the computers, software development has followed the same trend.\n\nAn operating system (OS) \u201cis an integrated set of programs that is used to manage the various resources and overall operations of a computer system which is designed to support the activity of a computer\u201d (Das 2010, p. 88). A team of programmers work together with microprocessor engineers to design a set of programs that work together to avail various resources of a computer.\n\nIn fact, the most important part of the computer is the operating system. While a typical boot process doesn\u2019t call the operating system right away, it hands over to the operating system before any real work can be done (Watson 2008, p. 60). Despite the fact that a computer system constitutes of the hardware, live ware and software, the operating system is the most important software that the computer system must contain.\n\nThe operating systems have been developed from time to time. The major differences in the operating systems revolve around the interfaces each operating system use. There are three major types of interfaces namely: command driven, menu driven and graphical user interface based operating systems. In the market today, there are a number of operating system software developers.\n\nThe most common personal computer operating system software developers or operating systems are: Windows by Microsoft Corporation, Mac by Apple computers, Solaris by Oracle, Linux and others. Old operating systems like MS DOS were command driven types of operating systems which are not reliable because of the difficulty encountered in remembering the commands.\n\nThe menu driven operating systems were an improvement of the command driven operating systems. This led to the invention of the graphics interface based operating system. Operating systems have two modes namely, user mode and the Kernel mode. The user mode is what the human being interacts with while the Kernel mode is what interacts with the computer hardware.\n\nMicrosoft operating system has penetrated most of the markets and is considered to be the most popular of the operating systems in use today. The Microsoft Operating system (OS) has also been changing the look and feel of the graphical interface and addition of some functions. Microsoft Windows 95 gave way to Windows 97 and Windows 98, Windows Me, Windows 2003/ XP and Windows Vista. The latest versions of Microsoft windows are the windows 7 and anticipated windows 8. The following statistics illustrate the trend in market penetration of the operating systems.\n\nCharts showing the market share of the operating systems\n\nSource: Net Market Share\n\nSource: W3Counter\n\nSource: Stat-Owl\n\nRationale behind the monopoly\n\nFrom the information above, Microsoft Windows has taken the largest market share in operating systems. Jain & Khanna (2010, p. 77) argue that, \u201c\u2026 pure monopoly is a market structure in which a single firm is the sole producer of a product for which there are no close substitutes\u201d. With this definition and the market share of the Microsoft Windows operating system (about 85% average), it is enough ground to state that \u2018it has a monopolistic market share.\u2019\n\nNow the question in mind is how the corporation has managed to retain its market share for long. Most businesses engage in dubious means of maintaining their monopoly in the market, is it so with Microsoft? It is argued that, \u201cMicrosoft has engaged in a carefully designed and extremely successful campaign to protect and extend its monopolies.\n\nMicrosoft has repeatedly made market allocation proposals to its competitors and has used a broad range of other anticompetitive and unlawful tactics to eliminate potential rivals, including tying, predatory product design, and intentional deception\u201d (Schestowitz 2009, p. 93). Most of these activities may sound lawful but they are not. Reports indicate that Microsoft makes about 77% profit every year from its two main monopolistic products namely: Windows Operating system and Microsoft Office suite.\n\nReasons why it is believed that Microsoft is using uncompetitive techniques\n\nThe reasons why Microsoft is using anticompetitive practices in order to have dominance in the market are quite elaborate. Microsoft tried their best to eliminate a number of applications that are used to accomplish day to day activities. The applications are: web browsers, office application packages, DOS and media players.\n\nMicrosoft also used other methods to discourage sale and development of other technologies. These allegations are explored below so as to support the fact that Microsoft uses anticompetitive practices to maintain a monopoly in the market share.\n\nFirst, Microsoft worked hard to eliminate the Netscape browser with its Windows explorer. In 1996 Netscape\u2019s group Chief Executive Officer said, \u201c\u2026 Microsoft\u2019s strategy for Windows and Internet Explorer involved nothing short of eliminating Netscape\u2019s Navigator\u201d (Pride & Ferrel 2006, 112).\n\nTogether with other CEOs from other firms they complained that Microsoft was anticompetitive and violated federal antitrust laws because it sold Windows with a reinstallation of Windows Explorer. The fact that everybody who bought and installed Microsoft Windows had no choice but to install Windows Explorer as well was a cause of alarm. The Windows Explorer is uninstall-able hence, the conclusion that Microsoft wanted to limit its users to its products only.\n\nMcKenzie & Lee (2006, p.18) indicates that, \u201cIn 1998, the Justice department took Microsoft to court for violation of antitrust laws. \u2026the justice department maintained that Microsoft was a monopolist as evidenced by its dominance in the market share in the operating system and Microsoft was using \u2018predatory\u2019 pricing of its internet explorer\u201d.\n\nWith the entire public outcry and the fact that Microsoft Windows Explorer was being sold as inbuilt software, was truly reason enough to conclude that Microsoft was using anticompetitive practices in order to sustain a large market share.\n\nSecondly, Microsoft had to make sure that Word Perfect is eliminated by the introduction of Microsoft Office suite. It indicated that, \u201cMicrosoft launched an anticompetitive campaign to extinguish WordPerfect, an office productivity application owned by Novell and competing with Microsoft\u2019s Office suite\u201d (Fergusion, 2002).\n\nIn fact Bill Gates (Microsoft Founder and group CEO) stated that, \u201cI have decided that we should not publish these [Windows 95 user interface] extensions. We should wait until we have a way to do a high level of integration that will be harder for likes of Notes, WordPerfect to achieve, and which will give Office a real advantage\u2026. We can\u2019t compete with Lotus and WordPerfect/Novell without this\u201d, (Schezowitz, 2011).\n\nWith this campaign Word perfect gained popularity but in retaliatory, Microsoft came up with measures to suppress it. For instance, Microsoft required that Novel must improve their technology in order to avoid incompatibility. Microsoft also narrowed the marketing opportunities of Novel by use of OEM licensing. As time went by, Microsoft office gained ground and Word perfect virtually disappeared from the market.\n\nThirdly, Microsoft introduced a Per Processor license fees. Original Equipment manufacturers (OEMs) are manufacturers of computers with preinstalled operating systems. Since these companies bought the operating systems from Microsoft and others, Microsoft required that each computer shipped has a license for the operating systems.\n\nThis was uncompetitive to other operating systems. In fact, it later emerged that, \u201cIn 1994, the U.S. Department of Justice (\u201cDOJ\u201d) filed an antitrust suit against Microsoft challenging this conduct, resulting in a consent decree under which Microsoft agreed to stop using per processor license fees\u201d (). Clearly at this point Microsoft admitted that it was using an uncompetitive technique and withdrew from some of its activities.\n\nMicrosoft went on to eliminate other commonly used software. Microsoft\u2019s introduction of the Windows media player was a way of trying to avoid exposing their API. Media players are middleware products that expose APIs to software developers which were the worry of Microsoft.\n\nThe fact that, \u201cmiddle ware could reduce the application barrier to entry by serving as an applications, taking over some of the platform functions provided by Microsoft and thereby weakening the applications\u2019 barrier of entry\u201d, (Shelaniski 2003 p. 223). This was a major concern for Microsoft.\n\nThis was the big reason why they had to introduce as many middleware in their operating systems so as to avoid the risk of exposing their code. The best way it combated this was by tying its media player with the operating system which gave users no reason to look for other media players. This was also an anticompetitive idea that Microsoft used to protect its operating system.\n\nThe server systems were no spared as well. According to Mowery (1996, p. 9), \u201ca server is a software program, or the computer on which that program runs, that provides a specific kind of service to client software running on the same computer or other computers on a network.\u201d With this definition, it is clear to note that an operating system on a personal computer on a network needs to send a request to the server computer.\n\nAgain, the request sent must be understood by the server operating system. In this regard, Microsoft decided to \u2018punish\u2019 other server software developers by making their operating system incompatible. This clearly is not a good practice to software development because it slowed down the development of other server software.\n\nThis seems to have worked well for Microsoft since it is the most popular server operating system to date. However, the desire to make it only compatible to Microsoft products was reduced drastically. This has enabled development and slow growth of other server software.\n\nAn operating system works with an underlying microprocessor; that is why there is a strong relationship between Microsoft and Intel and AMD. Since Intel was the most acceptable microprocessor during the inception of Microsoft systems, there was enough reason for Intel to cooperate with Microsoft. With this in mind and the aggressiveness of Microsoft to protect its market, it organized collective boycott against Intel.\n\nOne of Intel\u2019s main goals was to develop a Native Signal Processing System (NSP) that would enable the customers enjoy, \u201c\u2026 robust multimedia systems at lower prices because NSP in many cases would eliminate redundant expensive multimedia add-in cards\u201d, (InfoWorld Jun 5, 1995). This did not go down well with Microsoft and they decided to define their own development environment that would thwart the plans of Intel, which is sabotage.\n\nIn conclusion, Microsoft has managed to keep its market share at an extremely high value of 85% average. Its browser Internet Explorer and Microsoft Office suite are leading in market share as well. The means it uses are dubious but somehow Microsoft is cleverer to maintain that they are within the specifications of law.\n\nHowever, it has also failed in applications it has developed. The Windows version Vista is considered to have failed because it did not reach the market as anticipated. This was witnessed by users sticking to using Windows XP rather than the new and fancier version Vista. Microsoft also developed and Antivirus that could only be used on computers running its operating systems but they did not get the popularity they expected.\n\nThe behavior of Microsoft for the last two decades has illustrated its willingness to participate in unlawful conducts that extend its dominance in the market. As a result of this practice, consumers in the market are bound to use the same products with little room for innovation if the same was to be in a competitive market.\n\nIf authorities were to closely monitor the trends of Microsoft\u2019s monopoly from the beginning, they would stop them from manipulating the market. The latest measures taken by European Commission to curb Microsoft misbehavior is seen by many people as the start of stopping Microsoft participating in anticompetitive behaviors for the long term.\n\nReference List\n\nDas, S., 2010. A Complete Guide to Computer Fundamentals . New Delhi: University Science Press.\n\nJain, T. & Khanna, O., 2010. Managerial Economics . New Delhi: Vimla Kumari Jain.\n\nMcKenzie, R. & Lee, D., 2006. Microeconomics for MBAs: The economic way of thinking for Managers . Cambridge: Cambridge University press.\n\nMowery, D., 1996. The International Computer Software Industry: A Comparative Study of Industry Evolution and Structure . New York: Oxford University Press.\n\nPride, W. & Ferrell, C., 2006. Marketing: Concepts and Strategies . Boston, MA: Congage Learning, Inc\n\nSchestowitz, 2009. \u201c 20 Years of Microsoft Anticompetitive Behavior \u201d.\n\nWatson, J., 2008. A History of Computer Operating Systems . Ann Arbor, MI: Nimble Books LLC.\n",
        "label": "human"
    },
    {
        "input": "People Are Too Dependent on Computers Essay (Critical Writing)\n\nComputers are an essential part of everyday life. They are useful in carrying out various tasks and jobs and have infiltrated almost every aspect of people\u2019s lives. When computers were invented, a little over half a century ago, they were a technological innovation mainly for industrial use. Computers were primarily used for scientific purposes (Ifrah, 2001).\n\nHowever, over the years, new ways of using the computer have seen the computer rapidly evolving into a household item. Some of these technological advancements mainly include the multimedia applications such as the internet into the computer. Currently, there is hardly any task that can be accomplished without employing the services of a computer.\n\nThese range from carrying out the smallest office task, to communicating with friends and family. This paper outlines why people are too dependent on computers and why this could be problematic.\n\nFirst of all, computers are useful because they have revolutionized tasks that would otherwise have been thought to be impossible. For instance, the internet has made instant communication between two people on different sides of the globe possible. The fact that one can actually chat via video with a person thousands of miles away is a convenience that would not be possible without computers.\n\nThis has made it easier to pass urgent and important messages that have actually been able to alleviate disasters and save lives. For example, the use of the internet to announce terror alerts or pandemic breakouts is much more efficient than the traditional use of news stations and leaflets. Over 80% of the adult American population can comfortably use and navigate their way through the world wide web, even though most do not know how to use the rest of the computer applications (Ifrah, 2001).\n\nSecondly, computers have contributed greatly to the development of the global economy. Through the development of computer applications and programs, business transactions have been made much faster and more convenient. For example, the field of accounting has been revolutionized by the development of accounting programs which solve, in a few seconds, problems that would have taken several people many days to accomplish.\n\nComputers have also minimized the chances of human error in important financial calculations and decisions (Dijk, 2005). Common tasks such as shopping for items have also been made easier through online shops whereby one is able to save on time that would have been spent in traffic or queuing at a mall. Computer programs used in industrial applications such as Engineering have also ensured increased safety in constructed buildings and the speed with which objects are designed and manufactured (Dijk, 2005).\n\nThe aforementioned two categories of computer uses are just a tip of the iceberg. One thing which is common in computer use is that computers have increased efficiency and minimized the time spent on tasks. However, the problem begins with how the relieved time is used.\n\nOriginally, computers were meant to reduce the number of working hours spent at the office. This was envisioned to lead to an increase in the time that people spent with friends and family. However, the opposite case is what has happened. When people realized that they could accomplish much more with computers, they ended up using the extra time to accomplish extra tasks.\n\nIn the end, computers have destroyed the friendships, families and the very relationships that it was thought to be helping. The following few paragraphs show how the advantages mentioned above have contributed to increased over-dependence on computers.\n\nFirst of all, by increasing the efficiency of many normal tasks, computers have promoted laziness. This is clearly evident in the educational system. Students can now access all the information they need for their assignments by typing the questions in the search engines. They no longer have to read through the volumes of books recommended in the classroom.\n\nThis phenomenon eventually leads to overdependence on computers for any subsequent assignments (Bonfadelli, 2002). This is because the students had not read any book and have to keep returning to the internet for answers. Furthermore, the overdependence on computers has reduced the cognitive abilities of many students (Bonfadelli, 2002). The students know that they do not have to memorize or retain whatever they learn in the classroom since it is available on the internet. Such dependency can only lead to more dependency.\n\nSecondly, the computers were initially meant to reduce the burden of work that a single employee can do. This ended up creating more free time. However, people realized that additional tasks could be carried out in this free time, and that these tasks could actually be carried out using the same computer.\n\nTherefore, computers and the various software facilitated multi-tasking and ended up increasing the amount of time a person spends in front of a computer screen. The fact that a student can accomplish his assignments while listening to music using the same computer has increased overdependence on the machine.\n\nFurthermore, the fact that an employee is able to accomplish another non-work related task using the same computer has also resulted in increased time spent before a computer screen (Dijk, 2005). The effects of computers on its users have thus ended up exhibiting a domino behavior.\n\nFinally, the internet\u2019s latest innovation is the social networking functionality. Invented at the close of 20 th Century, social networking sites where people could interact with friends have increased dependency on computers. This is because using websites such as FriendFinder.com , people with poor social skills can now meet new friends in the online world and even prospective dating partners (Kelsey, 2010).\n\nSocial networking sites such as Facebook and Twitter have conveniently removed the need for people to meet in person in order to have a conversation. This has increased laziness and eventual social incompetence. The new generations of children who are growing up in the social networking age do not know alternative ways of interacting with people (Kelsey, 2010). This means that their dependency on computers will end up being even more than that of the pioneer social network users.\n\nIn conclusion, it is obvious from the above analysis that people are too dependent on computers. This dependence is actually increasing as more new ways of using the computer are being invented. However, there are also dangers associated with this overdependence on computers.\n\nCases of online fraud and cyber-bullying are on the rise. The overdependence on computers is a call for alarm because it is slowly evolving people into a society that cannot effectively function without the machine. It is therefore essential that people re-examine their lives and just how much of it is spent on the computer. If this overdependence is not checked, more problems will continue to plague the society, and yet they could have been easily avoided through simple self-reflection.\n\nReferences\n\nBonfadelli, H. (2002). The Internet and knowledge gaps: a theoretical and empirical investigation. European Journal of Communication 17, 65\u201384.\n\nDijk, J. (2005). The network society: Social aspects of the new media . Thousand Oaks, CA: Sage Publishers\n\nIfrah, G. (2001). The universal history of computing: From the abacus to the quantum computer . New York: John Wiley & Sons\n\nKelsey, T. (2010). Social networking spaces: From Facebook to Twitter and everything in between . Associated Press\n",
        "label": "human"
    },
    {
        "input": "Apple Computer, Inc. Organizational Culture and Ethics Research Paper\n\nAbstract\n\nThe management of human resource in organizations is a significant influence to the performance that the organizations display. The paper focuses on Apple Computers, Inc. and assesses the human resource practices, the management styles, communication, ethical and social responsibility, and the approach to diversity and globalization.\n\nThe results indicate that the organization has had significant success in the establishment of basic human resource principles, and the employees are happy at the organization.\n\nThe management style in the organization has also contributed to the positive attitude and satisfaction of employees at the workplace.\n\nCompany Research Paper: Apple Computer, Inc\n\nIntroduction\n\nThe management of human resource in organizations is crucial for the success of any business. In fact, successful organizations view their employees as the most valuable asset. In the current global economic outlook, organizations are using a number of strategies to ensure that they stay ahead in terms of competition.\n\nInformation technology is one of these strategies. Its frequent use by almost all organizations on the global front has led to the development of the information age.\n\nCompanies in the information technology industry have become some of the largest in the current age, with such companies running some of the largest global brands. Since the invention and use of the first computer, a multibillion-computer industry has been created.\n\nOne of the largest computer companies in the industry is the Apple Computer, Inc. that has had significant success in the industry since it started. The company makes billions of dollars in profit and revenue each year. It has featured in the Fortune 500 list severally.\n\nThe organizational structure and ideals that the company has established over the past century have placed it among the best technology firms in the world. This paper looks at a brief history of the organization, the human relations operations, its management styles, and communication conflict and credence.\n\nIt also examines the application of ethical and social responsibility and the organization\u2019s approach to diversity and globalization. Finally, the paper provides a conclusion on the discussed issues.\n\nBackground\n\nApple Computer, Inc. was first incorporated in 1997 to establish the current organization that is known world-over (Savage, 2013). The company made the first computer in 1976. However, this machine did not perform as well as the second one, which was launched in 1980.\n\nSteven Jobs and Steven Wozniak who were the co-founders of the business watched as the organization grew to a very viable one. It was primarily traded as a public company in 1980 when the initial public offer (IPO) took place (Savage, 2013).\n\nThe company established itself after the IPO. Nevertheless, that decade was initially marked with competition from a number of companies producing cheap personal computers. The company also faced the failure of only the third Apple computer that it had made.\n\nThe problems compounded and necessitated a change in the management (Savage, 2013). The market in which the company established itself is the PC market. However, there was stiff competition in this market in 1983 with the entry of IBM (Savage, 2013).\n\nThe company pioneered in the use of the mouse to drive the computers, with the first machine of this kind being produced in 1984 in the company (Cusumano, 2010).\n\nBy 1990, the PC market was flooded with cheap products in the industry, with Apple launching PowerMac (Cusumano, 2010). There were a series of problems in the company especially in the 90s, with an example being the loss of over $60 billion in the period between 1995 and 1996 (Cusumano, 2010).\n\nHowever, the second part of this decade was a bit better, with the return of Steven Jobs as the CEO to spur the company to productivity. The company made agreements with other companies such as Microsoft, with the acquisition of other companies such as Power School (Savage, 2013).\n\nAccording to Cusumano, alliances and collaboration with companies such as Erickson and Sun Microsystems were useful in the positioning of the organization into the current position and size (2010). The other measure that Apple utilized to find its foothold in the PC market was the acquisition of other related companies that were not necessarily in the PC market although offering complementary products.\n\nExamples of these organizations that Apple bought include Emagic, Prismo Graphics, Zayante, and Silicon Grail, with most of the acquisitions taking place in 2002 (Cusumano, 2010). The company also acquired the assets of a number of other companies in the industry, thus propelling it to success through the increase in the asset base.\n\nAn example of the firms that sold their assets to Apple includes the privately owned \u2018Nothing Real Company\u2019 that was involved in the digital image creation market (Moren, 2013).\n\nAccording to Moren (2013), one of the products that Apple has been known for over the last decade is the iTunes, which the company launched in 2003 for music enthusiasts to download albums and singles. The company also introduced a portable digital music player that came to be popular with the young generation that is dominated by music fans.\n\nSome of the other collaborations that have seen the company rise to the present glory include its alliance with Intel to use its microprocessors in the Macintosh computers, with other collaborations such as Acura, Volkswagen, Audi, and Honda allowing a platform for Apple products to be positioned in the motor vehicles (Moren, 2013).\n\nPartnership with phone companies has also provided a platform for the iTunes service that is provided by the company. The company has continued on its expansion and provision of products that are suited to the needs of its customers.\n\nAn analysis of the factors that led to the success of this organization is important to provide a strategy that other similar organizations should follow to be successful.\n\nHuman Relations Operations\n\nSeveral authors have described the human resource department at Apple as being a significant part of its operations. The company has adopted the policy of having employees the most precious asset, as exhibited in the way the organization, especially the management, treats its workforce.\n\nMost of Apple\u2019s employees are software developers and engineers, with this group being among the most highly trained and experienced in its respective fields.\n\nBased on the high competition in the industry and demand for technical knowhow, the organization holds these employees dear, with a number of incentives being awarded to encourage their stay (Lashinsky, & Burke, 2009).\n\nThe company offers free meals to employees. Among the most influential of the employees was Steven Jobs who was the CEO for a long time (Lashinsky, & Burke, 2009). Jobs introduced the use of computers in all operations of the Apple Company, thus eliminating the use of typewriters and paperwork.\n\nThe employees are also described as being highly motivated and satisfied with their jobs at the company. They do not frequently experience stress out of the ordinary work at the company. The motivators that the company provides to its employees include the presence of rewards for those that participate in experimenting and creativity (Lashinsky, & Burke, 2009).\n\nThe management values innovation and its application in the organization, with employees being encouraged to participate in innovation and risk-taking (Lashinsky, & Burke, 2009).\n\nEmployees are also encouraged to stay in the organization because of the existing organizational culture that allows them to participate in decision-making at the organization. The workload at the company and its branches is also reasonable.\n\nEmployees are only entrusted with work that they can manage (Lashinsky, & Burke, 2009). However, Apple upholds secrecy in its operations, with the employees being frequently involved in the protection of information regarding Apple\u2019s operations (Lashinsky, & Burke, 2009).\n\nManagement Style\n\nThe management style practiced at Apple has been constantly shifting with evident changes in chief executives over the years. One of the most successful CEOs at Apple was Steven Jobs. His management style was discussed throughout the world.\n\nAt the time Steven Jobs took over the operations of Apple as the CEO, the company was underperforming, with the products not being suited for the market and failing to satisfy the demands in the market.\n\nThe organization was also facing stiff competition that threatened to affect its productivity (Karlgaard, 2012). Within a year at the helm of this organization, Jobs managed to turn round these negative developments, thus enabling Apple to regain its former glory.\n\nThe management style that Jobs used to put the company back on track has been the subject of many studies. During his tenure, the organization managed to get into partnerships with a number of other organizations. Every product that the company developed and introduced into the market was a hit with consumers (Karlgaard, 2012).\n\nJob managed to motivate his employees through rewards, thus managing to reduce their turnover rate. Tim Cook who took over after Jobs faced the challenge of matching up to his predecessor. Immediately he took over as the CEO, he set the ball rolling to ensure that the organization remained viable (Karlgaard, 2012).\n\nThe CEO has frequently been described as tough. Unlike his predecessor, his management style involved more of delegation of authority (Karlgaard, 2012). Tim Cook is also described as being tough to employees, with some other observers describing him as methodical (Karlgaard, 2012).\n\nHe is said to set high standards for employees and other managers. In meetings, this strategy is known to affect the outcome of any subject under discussion. Employees were divided in terms of the style of management that was observed with the entry of Tim Cook at the leadership of Apple (Karlgaard, 2012).\n\nAccording to Karlgaard, some of the employees were happy and satisfied with the management style displayed at the organization, with others describing this king of management as tough and unnecessary (2012).\n\nFew of the employees were able to talk about the management style at the organization based on the secrecy that was practiced as a form of organizational culture.\n\nHowever, currently, most of the employees who have left the company to pursue other interests state that the management style practiced at Apple was satisfactory and that any employee would be comfortable working in the company (Karlgaard, 2012).\n\nMost of the employees were also satisfied with the management style in the organization as a whole, with many of them opting to stay in the organization based on this strategy. Therefore, the management style is sufficient for any employee based on the little complaints from the organization.\n\nCommunication Conflict and Credence\n\nCommunication is one of the most important aspects of any leader and organizational culture in the organization. At the Apple Compute Company, communication is a key element in the organization since it forms the basis of the industry in which it operates. Apple uses several methods to communicate to its employees.\n\nThese methods are applied at the various levels of the organization. For instance, at the highest organizational level, managers report directly to the CEO. The management style that Job left in the organization is mostly being followed. This means that the people reporting to the CEO are almost a hundred (Karlgaard, 2012).\n\nThe organization\u2019s management holds regular meetings to strategize and communicate the company\u2019s intentions to the concerned individuals. Another way in which the company communicates to employees and the management is through the advanced network connection within the organization.\n\nThis means that managers can talk to any of the employees at any time of day (Tan, 2013). However, communication is vertical and horizontal. In the communication between employees, the basic avenues are utilized. However, where communication issues are official, use of the official documents and notice boards is made.\n\nConflicts and disputes arise in the organization just as it is witnessed in any other organization. The company has a special way of dealing with these issues. The use of disciplinary measures against employees is applied in the organization, with those liable to punishment being punished according to the laid down organizational rules.\n\nSome of the most common issues in the organization include the sharing of information between individuals in the organization and other individuals outside the organization in contravention to the code of secrecy (Tan, 2013).\n\nExamples of measures that the organization takes include subjection to a disciplinary committee, sacking, and fines (Tan, 2013). However, the company builds trust with its employees through the promotion participation of employees in major decision-making processes.\n\nThere are wide consultations within the organization before development of new products. This plan makes employees feel important to the organization, hence building on their trust in the institution (Lashinsky, & Burke, 2009). The communication strategies in this organization are successful and useful as a template for a similar organization.\n\nEthical and Social Responsibility\n\nApple has built its reputation as one of the leading firms with respects to ethics at the workplace and in its operation world over. According to Lashinsky and Burke (2009), the work ethics at Apple dictate that employees should exhibit the right conduct at all times.\n\nApple also has a legacy of maintaining and demonstrating integrity in its operations. This strategy has been a key factor in the establishment of a large market base. The main principles that the company follows with regard to integrity include compliance, honesty, confidentiality, and respect (Karlgaard, 2012).\n\nTo ensure that all employees follow these principles, the organization has created a code of conduct that all employees in the organization and subsidiaries in the world should follow.\n\nThe company\u2019s website has a detailed description of the policies that dictate the corporate governance besides advising customers on the procedures to follow while reporting issues such as questionable conduct by its employees (Tan, 2013).\n\nA business conduct helpline is also functional. Customers and other individuals can call to report any issues of misconduct. Since some of the components of the company\u2019s products are produced in parts of the world with cheap labor, the factories may not be adherent to the international policies.\n\nTo ensure that the organization\u2019s policies are not interfered with, Apple makes these companies and factories producing the components sign the \u201cSupplier Code of Conduct\u201d (Tan, 2013). According to Tan (2013), factories that do not adhere to the codes may have their contracts terminated.\n\nThe company also participated in social responsibility. In the last decade, it participated in the planting of millions of trees in areas that have had significant deforestation (Tan, 2013). Apple has also sponsored a number of individuals in the societies in which it works for higher education as well as other levels of education (Tan, 2013).\n\nDiversity and Globalization\n\nApple is a well-diversified company that majors in the production of various electronic goods. The company initially started with the production of PCs for family use. It participated in the PC market for a long period. However, the last few decades have seen the company diversify its operations, with adoption of other services and products that are not related to their original computer business.\n\nExamples of diversification in the company include the iTunes platform that allows users to buy and download music, the iPad, the iPhone line of Smartphones, the portable MP3 players, and other products that the company has engaged in their development (Tan, 2013).\n\nThe company has also embraced globalization. It is among the top global brands. Apple\u2019s products and services can be found at any corner of the world.\n\nThe main markets for its products are in Europe and the United States, with other markets also recording considerable growth over the last few years. Apple Computer Company also has offices in over 20 countries. The factories that make the components for the various products are spread all over the world.\n\nRecent Development\n\nA recent development in the organization that has occurred in the past year is the launch of the TV application that the company plans to launch in the near future ( Apple Inc. SWOT Analysis , 2013).\n\nMost of the customers and clients of the organization welcomed the developments, with of them most willing to purchase it once it was fully developed. The company also released the iPhone 4 series. This invention generated interest from the traditional as well as the new customers that were anxious to try out the new technology.\n\nConclusion\n\nIn conclusion, the maintenance of an appropriate organizational culture is important in the profitability of organizations. The policies applied in the human resource management and the types of management exercised are discussed as being important in the success of organizations.\n\nApple is one of the companies that had humble beginnings. However, with effective human resource management policies and management styles, the organization has encountered profitability and growth. Some of the other issues discussed include the diversity and globalization methods used in the organization together with developments over the past year.\n\nThe company has applied ethics in its operations. It is evident that it uses appropriate methods in solving conflicts in the workforce. Employees are also satisfied with the management practices at the organization. The above measures may explain the company\u2019s positive performance on the global front.\n\nReference List\n\nApple Inc. SWOT Analysis. (2013). Apple, Inc. SWOT Analysis . London: Routlege.\n\nCusumano, A. (2010). Technology Strategy and Management Platforms and Services: Understanding the Resurgence of Apple. Communications of the ACM, 53 (10), 22-24.\n\nKarlgaard, R. (2012). Apple without Jobs first trillion-dollar company? Forbes, 190 (8), 34.\n\nLashinsky, A., & Burke, D. (2009). The Decade of Steve. Fortune International, 160 (9), 7-10.\n\nMoren, D. (2013). Apple Goes on an Acquisition Spree. Macworld, 30 (11), 20.\n\nSavage, N. (2013). More than a Mouse. Communications of the ACM, 56 (11), 15-16.\n\nTan, J. (2013). A strategic analysis of Apple Computer Inc. & recommendations for the future direction. (Company overview). Management Science and Engineering, 1 (2), 94.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Information Systems and E-Business Strategy Essay\n\nInformation needs for business managers tend to be similar for most businesses when one considers the same level of management. There are usually three levels of managers whose information needs are slightly different from each other.\n\nThese include operational managers, middle managers, and executive or senior managers. Computer-based information systems provide the information needed by the aforementioned categories of managers, which assists them in their work. The following are the information needs of the three categories of managers.\n\nOperational managers usually benefit from such systems by getting all the information related to the short-term or daily operations of the organization. For instance, the computer-based information system provides such managers with information related to attendance of employees, shifts of employees, and so forth. Such information helps operational managers make structured business decisions.\n\nOn the other hand, middle management benefits from a computer-based system by getting managerial or tactical information. The middle-level managers use this information to make short plans or decisions. Such information includes annual production details or even quarterly sales analysis.\n\nThe information also helps them supervise the operational level by giving managers specific activities that they should perform. Lastly, computer-based information systems help senior management make decisions. The systems provide these managers with strategic information, which the managers use to make strategic or unstructured decisions.\n\nSuch decisions may include setting the objectives and policies of the enterprise. The information they get also helps them to manage other managers within the organization. Senior management gets information from the system detailing trends in costs incurred by the enterprise or even the revenue trends.\n\nWith this kind of information, senior managers can make decisions aimed at increasing revenues and reducing the costs incurred by the enterprise.\n\nAn e-business strategy is an elaborate plan that an organization develops detailing how it intends to conduct its business over the internet. Most people wrongfully view the strategy as just internet business. However, the e-business strategy is an elaborate and skillfully developed document defining the long-term and short-term e-business goals of an enterprise.\n\nE-business strategy should ideally form part of an enterprise\u2019s business plan, and it should be contained in the organization\u2019s corporate strategy. The strategy should interconnect with virtually all strategic plans of the organization including the IT strategic plan, the marketing strategic plan and the organizational strategic plan.\n\nThe adoption of an e-business strategy is likely to affect the information systems function of a company. This impact takes the form of threats as well as benefits. Adoption of an e-business strategy is likely to enhance the involvement of the information systems functions in supply distribution. It is however important to note that not all middle managers will be eager to accept the adoption of an e-business strategy.\n\nMiddle managers are likely to oppose the adoption of an e-business strategy the organization brings other middle managers or IT consultants from outside to implement the e-business strategy. In this case, middle managers in charge of the information systems function are likely to frustrate the adoption of the e-business strategy because they may feel that the new team is taking over their functions.\n\nIt is therefore imperative that as an organization adopts an e-business strategy, it ensures that it implements sufficient change management activities to get everybody\u2019s support. Otherwise, an organization can fail miserably in its attempt to adopt an e-business strategy.\n",
        "label": "human"
    },
    {
        "input": "Analyses and Model Forms: Computer Sciences Corporation Case Study\n\nExecutive Summary\n\nLiability capping is the major concern to any organization that deals specifically with providing services to people. The staff introducing intangible products could undergo unfair treatment, ambiguity, and conflict situation on the part of the customers whose discontent can lead to a lawsuit.\n\nMoreover, it can also threaten the safety and protection of the employees, as well as hamper the culture of retention. In this respect, the report will focus on the case study revealing the story of Michael Horton, a vice-president for the natural resources for C.S.C., an Australian Computer Sciences Corporation.\n\nBy delivering IT services to various legal entities, the company faces a challenge of developing professional standards legislation that can limit the set of liabilities and duties in relation to customers. Despite the fact that the company gains recognition and trust from their customer, the company managers should work out the legislature standards that can provide their employees with sufficient support and security.\n\nAs alternatives to the case problem, three solutions could be provided. First, the company could enhance its mission and strategy and emphasize the importance of human factors while dealing with people-to-people business. Second, the company can introduce a specific set of moral and ethical principles that would allow employees to be more aware of the restrictions imposed by law in terms of relationship with clients.\n\nFinally, the company can also integrate training programs that can measure the employees\u2019 awareness of contract laws and international relations. The latter seems to be the most efficient and viable solution to the emerged problem because human factor stands at the core of understanding which restrictions should be imposed on liabilities and duties of employees.\n\nA detailed analysis of the company\u2019s processes and approaches to liability capping, as well as the alternative solutions will be introduced in the report.\n\nAlternative Solutions to the Issue\n\nEnhancing company\u2019s mission and strategy can serve as a starting point for developing new information bases for employees and clients to be more aware of the legal issues and liabilities. In particular, because the company operates at a global level, the managerial staff should pay closer attention to the cultural diversity issues and the international standards that are accepted worldwide.\n\nIn the context of liability capping, the organization should take into consideration purely commercial aspects, which specifically refer to the subject of the negotiation between the parties. The security issues should also be stated in the company\u2019s mission to ensure that all values, norms, and standards have been presented to the IT departments.\n\nThe main problem is that the organization\u2019s previous philosophy did not incorporate higher level of legal regulations. The focus, therefore, was on specific industries, corporations, and client groups. Changing the foundational ideology can allow the managers to reconsider their policies towards the liability capping.\n\nFor instance, it is possible to integrate legislation that can raise payment for the possible economic losses with both the potential customers and regular clients. According to Rascoe, the companies operating offshore encounter barriers to insurance of a liability cap 1 .\n\nThe second alternative implies developing a set of ethical and moral principles that strengthen the organization\u2019s strategic basis and ensure employees\u2019 with higher level of protection 2 .\n\nDue to the fact that liability capping refers to the injurer\u2019s responsibility to reimburse the losses, it also implies that \u201c\u2026the injurer is legally held fully liable for the damages in case of an accident, but he is only partially liable economically\u201d 3 . Under these circumstances, Mr. Horton is aware of the fact that conflict situation can be resolved through implementation of personal perspective and efficient communication devices, including listening and understanding.\n\nDuring negotiations, listening is the decisive factor contributing to the success of the agreement, as well as to the reduction of the potential risks. Cultural diversity can become a company\u2019s liability, as far as the personnel transmittance is concerned 4 . Appropriately, developed codes of conduct will reduce the risk of irrelevant behavior leading to legal infringement.\n\nMrs. Horton realizes that the development of a strong ethical commitment is not possible unless a liability adviser is invited to the company. In the majority of cases, liability capping seeks to restrict the degree of adviser\u2019s liability in case the client succeeds in defining what has been negligent 5 .\n\nThe problem is that most of the liability caps are associated with monetary values. Therefore, the company should develop a scheme in accordance with which the clients should provide a sufficient explanation why they should cap their liabilities. Therefore, the clients should inform the organization about their discontent.\n\nThe third perspective involves development of new metrics and training programs that would evaluate employees\u2019 competencies in terms of knowledge of contract law with reference to liability capping. Within the perspectives of contracts between the parties dealing with IT field, the draft of an exclusion clause, or liability-capping essentials, refers to the main three conditions.\n\nTo begin with, the organization should ensure that the clause is included into the agreement. Second, the liability caps should be enforceable and, finally, the clause should restrict losses and responsibilities that employees and clients usually take 6 . The operative wording of liability caps should comprise tested and tried sheet that should meet the above-mentioned requirements.\n\nWith regard to the presented arguments, the company should integrate new approaches to constantly upgrade the requirements to carrying out business internationally. Specific attention should be given to the analysis of recruitment culture in various headquarters. The performance analysis committee should focus on social and cultural backgrounds of the employees and client to evaluate their readiness to follow the exclusion clause.\n\nIn other words, the clients should ensure that the core indemnities are excluded from clause because the obligation will be undermined if not vitiated 7 . In this respect, the IT manager should not only rely on trustful relations and employees\u2019 good negotiating and communicating skills, but also on the development of a strong legal ground to conclude agreements in future.\n\nRationale for the Chosen Solution\n\nIntroducing stronger organizational strategies, mission, and statement is vital for ensuring rich corporate culture and providing employees with a set of obligations and responsibilities. Full commitment to organizational culture could be achieved as soon as the company\u2019s managers decide what legislature should be integrated into a global business environment.\n\nTherefore, emphasis on legislature and international standards will provide a strong foundation for developing new metrics and training programs for employees. Their knowledge of contract law and liability capping process should advance their understanding of how IT business can be improved.\n\nThe first solution to strengthen organizational mission and objectives excludes the possibility of analyzing the marketing potential of the global environment. Moreover, such a strategy fails to refer the human factor, which is the key to success. Therefore, this decision does not provide foundational ground for promoting legislature into a culturally diverse environment 8 .\n\nRather, the alternative will be pertinent at later stages of change management. Changing the awareness and consciousness of the personnel should be the priority.\n\nAs per the second alternative, the set of ethical and moral principles can contribute greatly to the enrichment of organizational culture, but they serve as a secondary instrument to handle the issue of liability capping. Moreover, knowledge of ethics does not release them from legal obligations 9 .\n\nTherefore, greater competencies in legal issues and regulations can enhance protection of confidential information and predict all possible risks during delivery of services to consumers.\n\nDue to the fact that the above-presented solutions are not essential for delivering improvement and change to the current global strategies introduced in the company, the third solution will encompass all the issues and factors that could undermine previous approaches. The liability capping deals primarily with possible risks, including technical problems, communication challenges, etc.\n\nThe primary task of the employees is to define the terms, or liability caps, that will allow them to protect their rights and responsibilities. Although the company has a number of reliable customers, the risk of failure always exists.\n\nIn the majority of cases, licensors negotiate commonly accepted liability caps and exclusion clauses. However, licensees insist on \u201cthe licensor\u2019s liability under any indemnification obligations\u2026from any cap\u201d 10 .\n\nHowever, the company should realize that the breach of the intellectual property rights should be considered by licensors before it is accepted if the remedies are presented as exclusive and sole. Alternatively, the licensor can have an unlimited liability. Similar to these principles, the company should also be aware of the unlimited options of the clients who can avoid the agreement provisions.\n\nConnecting the Case to Michael Porter Framework\n\nWhen it comes to the issue of liability capping, it is necessary to apply a specific framework to access the former properly. In the given case, Porter five forces analysis seems to be the most appropriate framework to choose. In the light of Porter\u2019s theory, one will be able to assess the company liability in a more accurate way.\n\nBargaining power of customer\n\nWith the help of liability capping, the company will be able to provide the clients with a flexible pricing policy\n\nThreat of new entrants\n\nAs Ahlstrom and Bruton explain, \u201cPotential new rivals believe that they may be able to make similar or better returns.\u201d 11 However, with more competitors in the market, the influence and authority of Horton\u2019s services are going to rise. Hence, following the liability capping strategy, Horton will be able to control new entrants and remain in the leading position.\n\nThreat of substitutive products\n\nWith new entrants, substitutive services are likely to appear. However, the given problem can be solved by increasing the quality of the original product.\n\nCompetitive rivalry within an industry\n\nSince the rivalry within the company is practically unavoidable, it is necessary to come up with the means to control it, which the liability system will be able to offer. s\n\nBargaining power of suppliers\n\nIn the light of the liability capping issue, it is especially important to consider the bargaining power. Developing an efficient liability system, Horton will be able to control the process making sure that the suppliers do not pressure him into lowering prices. Likewise, the suppliers will be able to see that the company offers fair pricing policy.\n\nHence, it can be concluded that the approach that Michael Horton uses in his company is very efficient. Putting the emphasis on the relationships between the company management and the clientele, Horton makes a very wise decision.\n\nImplications\n\nWith regard to the above-presented solutions, the company can significantly enhance its organizational strategies, as well as adopt a new legal foundation for conducting business. In particular, Mr. Horton should take into consideration these issues to create a new ethical and legal framework.\n\nMore importantly, he should address the international standards for developing the set of rules and principles according to which IT contracts should be conducted.\n\nIn addition, because the Computer Corporation deals with software licensing, liability capping is an integral component of daily practices. Finally, the main objective of Mr. Horton is to determine what indemnification obligations will be removed from the liability cap.\n\nBibliography\n\nAhlstrom, David & Bruton, Garry D. International Management: Strategy and Culture in the Emerging World . Stamford, CT: Cengage :Learning, 2009.\n\nBattersby, Charles. Licensing Update . US: Aspen Publishers Online. 2010.\n\nClassen, Ward. A Practical Guide to Software Licensing for Licensees and Lisensors: Analyses and Model Forms . US: American Bar Association. 2007.\n\nFaure, Michael G. Tort Law and Economics . Edward Elgar Publishing, 2009.\n\nFerrell, O. C., Fraedrich, John, and Linda Ferrell. Business Ethics: Ethical Decision Making & Cases . New York: Cengage Learning.\n\nHowson, Peter. Due Diligence: The Critical Stage in Mergers and Acquisitions . US: Gower Publishing, 2003.\n\nLoos, Alexander. Directors Liability: A Worldwide Review . US: Kluwer Law International, 2010.\n\nRamseur, Jonathan. Liability and Compensation Issues Raised by the 2010 Gulf Oil Spill . US: DIANE Publishing, 2011.\n\nRascoe, Ayesha. U. S. Pushes to Life Oil Spill Liability Cap . Reuters. 2010. Web.\n\nFootnotes\n\n1. Ayesha Rascoe U. S. Pushes to Life Oil Spill Liability Cap.\n\n2. Michael G. Faure. Tort Law and Economics. (Edward Elgar Publishing, 2009). 418.\n\n3. Michael G. Faure. Tort Law and Economics. (Edward Elgar Publishing, 2009). 418.\n\n4. O. C. Ferrell, John Fraedrich, and Linda Ferrell. Business Ethics: Ethical Decision Making & Cases. (New York: Cengage Learning, 2008). 265.\n\n5. Howson, Peter. Due Diligence: The Critical Stage in Mergers and Acquisitions. (US: Gower Publishing, 2003) 40.\n\n6. Charles Battersby. Licensing Update. (US: Aspen Publishers Online. 2010) 304.\n\n7 Ibid., 304.\n\n8. Jonathan Ramseur. Liability and Compensation Issues Raised by the 2010 Gulf Oil Spill. (US: DIANE Publishing, 2011) 10.\n\n9. Alexander Loos. Directors Liability: A Worldwide Review. (US: Kluwer Law International, 2010) 70.\n\n10. Ward Classen, Ward. A Practical Guide to Software Licensing for Licensees and Lisensors: Analyses and Model Forms. (US: American Bar Association. 2007) 72.\n\n11. David Ahlstrom & Garry D. Bruton, International Management: Strategy and Culture in the Emerging World (Stamford, CT: Cengage :Learning, 2009), 133.\n",
        "label": "human"
    },
    {
        "input": "The Role of Computer Forensics in Criminology Essay\n\nAbstract\n\nComputer forensics also known as digital forensics is one of the most exciting fields of science. This field has tremendously grown from a comparatively murky tradecraft into an important platform for carrying out investigations. Various departments use computer forensic tools to carry out investigations and research.\n\nFor example, experts use computer forensic tools to carry investigations on crime. Additionally, computer forensic tools are also common in military applications. Researchers and experts are busy developing new computer forensic tools in order to retrieve actionable information, which is vital in investigations and problem solving.\n\nHowever, the development of new technologies has brought new ways of capture and analysis in computer forensics. This paper discusses the single most technology that affects digital forensics known as the Virtual Machine Hypervisor Technology (Peterson & Shenoi, 2009).\n\nIntroduction\n\nThere is no doubt that digital forensics is one of the most exciting fields of science that many people like working in but not for the obvious reasons. Indeed, over the last two decades, people have been relying on digital forensics so much. Digital forensics has made crime investigation easier.\n\nBy relying on live evidence, experts have managed to unravel secrets behind certain activities. However, due to the evolving technology, scientists have encountered new challenges that require advanced tools to investigate. For instance, new file systems that require a different approach and operation is an example of how emerging technologies are affecting digital forensics.\n\nThese game changer technologies are not only instrumental in investigations, but also advantageous to the digital forensics industry. Since the development of solid-state hard drives, new technologies have emerged in the field of digital forensics that have made the field more interesting (Mocas, 2004).\n\nVirtual Machine Hypervisor Technology\n\nOver the past five years, experts managed to develop one of the most exciting technologies, which is instrumental in digital forensics. Indeed, the development of the virtual machine hypervisor technology has brought many changes in digital forensic analysis and operations.\n\nMany IT and security experts may be quick to point out security concerns and condemn this technology. However, this technology is important in digital forensic analysis. Thus, this paper calls upon all stakeholders in digital forensics to pay attention to this new technology because it is vital in carrying out capture and analysis techniques.\n\nA virtual machine is a computer-generated operating system developed to perform certain functions in digital forensics science. We understand that Virtualization helps an operating system to run on server hardware or sometimes on the computer desktop. However, for this to happen we must have software.\n\nThe \u201chypervisor\u201d software directs and manages all activities within the system. In particular, this software performs three major functions. To start with, the software generates different independent partitions comprising of operating systems, software operations and appliances. Secondly, the hypervisor software creates boundaries that separate the partitions. Lastly, the software is able to entrap and direct commands among the three partitions (Casey & Stellatos, 2008).\n\nIn some articles, some writers use \u201cvirtual machine monitor\u201d (VMM) instead of hypervisor. The two are similar. Although developed over three decades ago for mainframe computers, VMMs have been instrumental in digital forensic investigations. For example, their role in addressing security is undoubtedly great.\n\nAdditionally, VMMs are useful in addressing administrative and reliability issues that are dominant in distributed computing systems. The main task of the hypervisor is to perform decoupling. This is important in generating live-state images for forensic analysis.\n\nIn other words, the hypervisor provides channels of communication between the computer hardware and the installed software. In this way, the VMM has control over all operating systems running on the machine. The software also ensures that the system utilizes the available hardware resources effectively (Sutherland, Tryfonas & Blyth, 2008).\n\nSource: (Sutherland, Tryfonas & Blyth, 2008)\n\nLive-state analysis vs. Capture and Analysis Technique\n\nMany forensic experts believe that the virtual machine hypervisor technology is a march towards obsolescence. In digital forensics science, these experts agree that it is important to adopt the live-state analysis and do away with static digital forensics.\n\nIn fact, since the development of the virtual machine monitors, the live-state analysis in digital forensics has become common and easy to understand. Although many research institutions across the globe still emphasize on capture and analysis techniques for search and seizure processes, they perform investigations on stored information under offline mode.\n\nThis is because they want to protect the original sample. Nevertheless, due to the ever-growing memory capacities, anti-forensics and drive encryption, there is big chances of losing the original sample of evidence because of pulling or plugging. This will undoubtedly affect the case under investigation.\n\nMoreover, retrieving data from the volatile memory requires enormous expertise. Otherwise, it is easy to lose original information. Most importantly, the volatile memory has miscellany of anti-forensic techniques, which are useful in investigations. Moreover, the volatile memory has malware that is important in investigations but can easily disappear in case of a power outage (Rosenblum & Garfinkel, 2005).\n\nThese are some of the concerns that the live-state analysis technique addresses using the virtual machine hypervisor technology. Furthermore, the need to observe privacy of information is pushing many digital forensic institutions to encrypt their mobile computing devices in order to protect vital information from damage.\n\nIn fact, many of them are doing this by encrypting the entire disk of their systems using tools such as Pretty Good Privacy and TrueCrypt. Indeed, the best way to access information is to retrieve the forensic picture of a live-state system while leaving the data in untouched or unencrypted condition.\n\nBefore the development of virtual machine monitors, forensic investigator relied heavily on traditional techniques to carry out investigations on the sample of evidence. For instance, from the sample source, they created a bit-by-bit replica while at the same time making sure that they do not tamper with the original data.\n\nHowever, in the current virtual environment, it is not possible to carry out forensic analysis on the original data without tampering with it. This is simply because there are numerous virtual machines that scurry concurrently on a huddled storage area network. Thus, many organizations have adopted virtualization technology on their enterprise server backgrounds in order to improve efficiency of investigations and protect the original data.\n\nSource: (Rosenblum & Garfinkel, 2005)\n\nElimination of legal huddles to pave the way for live-state analysis\n\nAlthough this emerging technology is the most effective in digital forensics, many players (including courts and echelon society) are yet to accept VMM images as the primary source files for carrying out forensic analysis. However, as the world continues to experience digital revolution and virtual machines become increasingly popular in forensic analysis; those opposed to this technique will finally accept it.\n\nIt is also possible to adopt new and efficient dimension techniques, which are analogous to those deployed in forensic DNA analysis. In forensic DNA analysis, the technique blotches biological evidence. Consequently, this can protect original data from damage. Nonetheless, according to research experts, forensic DNA tests always destroy the original sample.\n\nHowever, these experts believe that the evidence is still important for future analyses. In most cases, enhanced flash memory faculties, drive encryption and providential anti-forensics make virtual machine hypervisor technology important in the current digital age. Undoubtedly, these three characteristics have moved digital forensics science into another level of efficient live-state analysis (Bill, Amelia & Steuart 2010).\n\nVirtual Machines and Captured Image Analysis\n\nIn 2006, researchers came up with one of the most popular tools in digital forensics science known as LiveView. This is a Java-based tool that is able to generate VMware virtual machine from the normal disk. It does this freely on the VMware desktop by generating swiftly and effortlessly numerous images for forensic analysis.\n\nIn fact, many organizations have adopted this program and encapsulated it into their systems that run on virtual machines. Therefore, forensic experts will not require LiveView tools for their analyses in the future. Instead, virtual machine technology will provide the most efficient and faster way of obtaining images for forensic investigation and analysis.\n\nThis will also enable forensic experts to generate more copies for forensic analysis within a short period compared to traditional methods. Additionally, this technology enables the incarceration of malware and root kits within the exchange file of the perched operating system of a virtual machine picture.\n\nMany traditional computers that have their operating systems running on hardware make it exceptionally intricate and unfeasible to evaluate the fortifications or vulnerabilities, which are common in very many virtual machine environments. This is the reason why it is vital to separate processes and leave them to work separately. In fact, similar processes running on a single platform means that these processes enjoy some similarities.\n\nThis is dangerous because shared processes not only share similar vulnerabilities, but also make the system weak and vulnerable. On the other hand, virtual machines isolate processes into different partitions. This means that there is enhanced consistency and security of all applications. Moreover, ports and process identities are always active when they work in isolation. This makes forensic analysis easier as experts can locate with easiness the information they need for analysis (Anson, et al., 2007).\n\nSource: (Sutherland, Tryfonas & Blyth, 2008)\n\nVirtual Machines enhance Intrusion Detection and Network Isolation\n\nVirtual machine monitors enhance intrusion detection by creating channels of indirection and interaction between the hardware and the virtual machine software. This isolates networks and makes them effective. In other words, virtual machine monitors have the ability to run adjacent to the operating system in order to ensure that the processes remain separated (Carrier, 2006).\n\nVirtual Introspection\n\nSince the development of the virtual machine hypervisor technology or the virtual machine monitors (VMM), many experts believe that virtual introspection will control future digital forensic investigations. Across the globe, forensic experts and researchers continue to explore the efficiency of virtual introspection in digital forensics science. To start with, virtual introspection is principal in digital forensics because it enables live system analysis.\n\nForensic analysis methodologies together with the virtual machine systems are important in ensuring that evidences remain unchanged even after performing forensic analysis. Remarkably, this cannot occur in traditional forensic tests were tampering of original data is so common.\n\nNonetheless, by using the virtual machine hypervisor technology, experts can stop the virtual machine for a while and retrieve the information they want and then continue it. Definitely, the state of the virtual machine remains unchanged during data acquirement even if one suspends the process for a while (Bem & Huebner, 2007).\n\nChallenge facing the virtual machine environment\n\nJust like any other emerging technology, virtualization also has high-tech challenges that may be quite demanding. We have seen that while some digital forensics investigations are easy to perform, others are complex and need intricate methods of analysis. For instance, virtual introspection enhances forensic analysis through live-state analysis.\n\nHowever, many scientists concur that it is not easy to carry out forensic investigations using this technique. In fact, due to its complexity, it requires advanced knowledge and expertise. The hypervisor software is an example of a central processing unit micro cipher. It therefore, means that it does not appear on the operating system.\n\nThus, it is complex to operate. On the other hand, although it exercises jurisdiction on the hardware, this can pose security challenges. In case, an attacker encounters a flaw in the virtual machine environment, entry into the system becomes easier, and someone can perform malicious damages compared to other technologies. Nevertheless, virtual machine environment tools such as software as a service (SaaS) can help in protecting the systems from hacking (Carpenter, Liston & Skoudis, 2007).\n\nConclusion\n\nNew technologies such as the virtual machine monitors have taken the world of digital forensics by surprise. There is no doubt that virtual machine monitors are more effective in forensic analysis than traditional methods. Unlike the traditional forensic images, virtual machine images are portable and easy to analyze. Moreover, the development of virtual introspection products has forced many organizations to adopt new technologies for forensic capture and analyses.\n\nReference List\n\nAnson, S., Bunting, S., Ryan, J. & Scott, P. (2007). Mastering Windows Network Forensic and Investigation. New York, NY: John Wiley and Sons.\n\nBem, D. & Huebner, E. (2007). Computer Forensic Analysis in a Virtual Environment. International Journal of Digital Evidence , 6(2), 1-8.\n\nBill, N., Amelia, P. & Steuart, C. (2010). Guide to Computer Forensics and Investigations (4th ed.). New York, NY: Cengage Learning.\n\nCarpenter, M., Liston, T. & Skoudis, E. (2007). Hiding Virtualization from Attackers and Malware. IEEE Security and Privacy , 5(3), 62-65.\n\nCarrier, B. D. (2006). Risks of live digital forensic analysis. Communications of the ACM, 49(2), 56\u201361.\n\nCasey, E. & Stellatos, G. (2008). The impact of full disk encryption on digital forensics. SIGOPS Operating System Review , 42 (3), 93-98.\n\nMocas, S. (2004). Building theoretical underpinnings for digital forensics research. Digit Invest, 1(1), 61-68.\n\nPeterson, G. & Shenoi, S. (2009). Digital Forensic Research: The Good, the Bad and the Unaddressed. Advances in Digital Forensics, 306, 17\u201336.\n\nRosenblum, M. & Garfinkel, T. (2005). Virtual Machine Monitors: Current Technology and Future Trends. Computer , 38(5), 40-46.\n\nSutherland, I., Tryfonas, T. & Blyth, A. (2008). Acquiring Volatile Operating System Data Tools and Techniques. SIGOPS Operating System Review , 42(3), 65-73.\n",
        "label": "human"
    },
    {
        "input": "Doing Business in India: Outsourcing Manufacturing Activities of a New Tablet Computer to India Report\n\nIntroduction\n\nThe main aim of the report is to explore and state the conditions according to which it is possible to choose India as the most appropriate country for outsourcing in relation to the tablet computer industry. The justification of the choice is provided in the report with references to the criteria which are significant for operating within the industry successfully.\n\nIndia can be discussed as the most suitable country for outsourcing because its labour market is characterised by the highly educated and skilled work force, high quality services provided, possibilities for cost savings, and developed technologies along with favourable government policies, and the fast growing infrastructure, and these factors can be considered as beneficial for implementing effective outsourcing policies (Bullen, LeFave, & Selig, 2010; Thite & Russell, 2007). Moreover, India\u2019s location is strategic for transporting the materials and products easily.\n\nThe report also includes a study on suppliers and competitors as well as the nature of the market, and it provides the expert recommendations on these aspects in relation to the specifics of the industry\u2019s micro- and macro-environment. Another aim of the report is to analyse the requirements for the establishment of the company in India, studying the competitors in the industry and their experience.\n\nThe study can show the level of the technology required and accentuate the particular features of the capital project. Presenting the analysis of the correlation between the company\u2019s required characteristics of outsourcing effectively and India\u2019s market\u2019s possibilities, this report aims to convince the Board to choose India as the country for outsourcing the company\u2019s production activities.\n\nThe Requirements for Choosing the Country\n\nSocio-Economic Requirements\n\nTo make the process of producing goods profitable, the companies\u2019 leaders are inclined to orient to outsourcing to the countries which can provide the lower costs in relation to labour costs and production. To compete within the industry successfully, it is necessary to reduce labour costs. Thus, cost saving is one of the main reasons for companies\u2019 seeking for outsourcing services from India.\n\nFurthermore, there are significant differences between labour costs in India and in the other third world countries (Bullen, LeFave, & Selig, 2010). The producers expect to obtain benefits from hiring the Indian population because of the Indians\u2019 level of education and their competence in the English language (Davies, 2004). Moreover, the companies in India have the definite freedom to manage their own budgets and gain huge profits (Thite & Russell, 2007).\n\nIndia can be also discussed as the attractive destination for companies willing to offshore their IT operations and manufacturing processes to other countries in order to lower the costs (Vestring et al., 2005). The particular features of the high technology industry are the orientation not only to the cheap labour and pricing flexibility but also to the highly skilled worker to operate the equipment, machinery, and other technologies (Lacity & Rottman, 2008).\n\nThe country\u2019s labour market consists of engineers of different specialisations who can work with high technologies and of a large number of highly skilled and educated people who can perform the various technical activities (Thite & Russell, 2007).\n\nThe fact is important for companies producing tablet computers because they need skilled labourers to manufacture the innovative products which can compete favourably with the competitors\u2019 goods. Moreover, the Indian government worked out the acts on the intellectual property to guarantee the absence of the information\u2019s leakage to the competitors because of members of the staff (Chesser & Cohen, 2006).\n\nDemand/Buyer Characteristics\n\nTo organise the production of the tablet computers in India successfully, it is necessary to refer to such aspects as proximity, ethical issues associated with the working conditions, infrastructure, and buyers\u2019 attitudes to the other brands. According to the first point, the location of the country provides the company with opportunities to organise quick transportation of the materials and products within the country and from the countries-suppliers (Bullen, LeFave, & Selig, 2010).\n\nThus, the country\u2019s closeness to Korea and Taiwan, the major suppliers of processor chips and memory cards for the industry, is advantageous for producing the tablet computers in time according to the quality standards and the company\u2019s expectations (Salwan, 2007). Furthermore, the developed transport connections with the mentioned countries make the distribution of the product more efficient, faster, and cheaper.\n\nThe next point of the discussion is the working conditions. The third world countries are often considered to be the territories with the worst working conditions round the globe (Lacity & Rottman, 2008). In spite of the fact India has the reputation of the country where foreign companies can find the highly skilled labourers, it is important to pay attention to the issue of contributing to the satisfied working conditions in order to meet the ethical requirements.\n\nDuring the last decades, the Indian government tries to control the situation in the labour market, supporting the companies invested by the foreign corporations. New regulations and ethical laws were developed to protect the labourers\u2019 rights and provide the satisfied working conditions for the labour force (Davies, 2004).\n\nIt is important to note that buyers\u2019 brand associations are based on the success of the advertising campaigns and on the quality of the producing goods. The tablet computer industry is highly competitive, and the role of the buyers\u2019 interest in the product is significant.\n\nToday, the tablet computers are at the \u2018growth\u2019 stage of the development cycle (Thite & Russell, 2007). That is why, the present situation in the economic and technological spheres is the best time to operate within the industry and try to gain the great profits, competing in the market. The following graph illustrates the potential growth of the tablets\u2019 sales.\n\nFigure 1 ( Tablet Sales Trends , 2010)\n\nIn spite of the fact the large percentage of the company\u2019s products should be oriented to the international market and Western countries, Indian people can be also discussed influential consumers of the tablet computers with references to the latest tendencies.\n\nThe Indian government has offered subsidies for college students to buy the tablet computers at a lower price in comparison with the average price set for the other countries\u2019 markets ( Akash tablet\u2019s commercial variant, 2011). Thus, it is possible to discuss India not only as a producing country but also as a potential consumer. This fact can encourage firms to seek for outsourcing services from India.\n\nBarriers to Entry into the Market\n\nThe company, which orients to producing the new tablet computers, operates within the developed Hi-Tech and IT industry. The problem is in the fact the success of the company depends on capitals investing into the production (Thite & Russell, 2007). The establishment of factories in India specialised in producing tablet computers requires much financing, and it is the first barrier to entry into the industry.\n\nThe first barrier is closely associated with the fact that the production of a tablet computer is very intensive in relation to labour and capital issues. The production is based on using expensive equipment and machinery along with hiring highly skilled workers (Vestring et al., 2005).\n\nHowever, today the contribution to the development of the Hi-Tech and IT industry in India is a top priority of the Indian government, which offers the attractive and effective IT policies to support the industry by giving tax-related and other benefits to the company which is inclined to invest in the industry (Chesser & Cohen, 2006). The government also continues to improve infrastructure and communication networks (Thite & Russell, 2007). From this point, India is the best variant for outsourcing to it.\n\nEconomies of scale can be also analysed as the important factor for the company\u2019s progress. The scales\u2019 principle can be used to reduce the company\u2019s costs and increase the productivity. The availability of the technical staff allows speaking about maximising the company\u2019s profits through economies of scale.\n\nThe costs\u2019 minimisation is the main reason for outsourcing and it provides the firms in India with the possibility to gain from the industry\u2019s competition (Thite & Russell, 2007). It is important to not that there will not be significant retaliation from competitors against a newcomer because the size and competitiveness of the industry reduce the retaliatory reactions of the already established companies such as Apple and Samsung (Sonaje, 2012).\n\nThere are very little or no protectionist policies in the High-Tech and IT industry in India. No tariffs are applied to companies willing to export their products (Thite & Russell, 2007). It reduces the cost of production leading to the companies\u2019 gaining profits (Gay & Essinger, 2000).\n\nSupply Issues\n\nIt is possible to determine such important supply issues as the transportation of processor chips, memory cards, display screens, and GPS trackers significant for producing high-quality tablet computers from different countries.\n\nThe factory located in India can have the opportunity to discuss its location as the advantage in relation to the question of transporting the necessary components from Korea and Taiwan as the major suppliers within the industry. Thus, few big suppliers for the mentioned components are located in Korea and Taiwan, and these manufacturers have the bargaining power in the industry (Chesser & Cohen, 2006).\n\nTechnological Requirements\n\nThe production of the tablet computers requires the orientation to using the developed technologies. The contemporary tendencies to support the progress of the Hi-Tech and IT industry in India are important for organising the production of the tablet computers which can be discussed as a high-tech consumer product (Thite & Russell, 2007).\n\nThe advantages of India for being chosen for outsourcing are in the quality of the labour force and a number of highly skilled and experienced engineers who are ready to work within the industry (Davies, 2004).\n\nIn spite of the fact the research and development can be based on the human resources of the head company, the qualification of the workers is important for producing the new tablet computers and for controlling the process organised according to the innovative technologies\u2019 improvements (Chesser & Cohen, 2006). Thus, to produce the tablet computer, it is necessary to hire highly skilled workers to operate the machinery and equipment and follow the instructions.\n\nCompetitors\n\nThere are few suppliers of the tablet computers and a relatively large number of buyers that is why the market can be discussed as Oligopolistic. This means that a new company in the industry can develop in the market with references to competitive pricing and controlling quality to entice the large number of consumers and win the industry\u2019s rivalry (Vestring et al., 2005).\n\nThe main company\u2019s competitors are Apple, Samsung, Amazon, ASUS, and HTC. These companies offer tablet computers in a number of different sizes and functionalities. The share of ASUS in the market is 2.3%, RIM presents 2.3%, Amazon -4%, Samsung \u2013 7.5%, Apple \u2013 63% (Sonaje, 2012).\n\nThese companies are some of the largest multinational companies in the world, and there are challenges for the successful entry to the industry (Chesser & Cohen, 2006). The accents should be made on the non-price competition as effective in the tablet industry. That is why, it is important to implement innovative technologies to follow the world tendencies and compete successfully.\n\nOther Important Factors\n\nTo provide the complex analysis of the industry and determine the advantages of choosing India as a country for outsourcing the manufacturing activities, it is necessary to pay attention not only to political, economic, socio-cultural, technological, and legal aspects but also to the problem of the environmental sustainability as the key factor of the industry\u2019s macro-environment (Bullen, LeFave, & Selig, 2010).\n\nIt is important to note that the issues of the environmental sustainability are traditionally accentuated by Western companies, and the main methods to solve the problem can be also used in India in relation to following the government\u2019s recommendations in the field (Chesser & Cohen, 2006). Thus, the problem of the environment protection is urgent for India that is why the technological processes should be organised to meet the government\u2019s requirements and the principle of the environmental sustainability (Davies, 2004).\n\nConclusion\n\nTo conclude, it is possible to note that India can be chosen for outsourcing the manufacturing activities in relation to the tablet computer industry because the conditions provided in the country meet the basic requirements stated by the company. Thus, the characteristics, which are required by the producer, are the supporting micro- and macro-environment.\n\nThe company pays attention to such forces typical for the macro-environment as political, economic, socio-cultural, technological, environmental, and legal factors and to the components of the industry\u2019s micro-environment where the accents are made on the role of competitors, suppliers, partners, and customers.\n\nThe findings of the industry\u2019s analysis and the position of India in relation to the Hi-Tech and IT industry allow speaking about India as the beneficial variant for outsourcing.\n\nIt is possible to determine such reasons as the favourable government policies developed to respond to the contemporary intellectual property issues, the orientation to the highly-skilled labourers with references to the engineers and technical workers, the availability of cheap labour while reducing the labour and production costs, the advantageous location of the country to contact with the suppliers and customers successfully, and the presence of the ethical laws and labour regulations controlled by the government.\n\nModern India is oriented to the progressive participation in the foreign business with references to providing the sources for offshore manufacturing because of the variety of benefits for the companies-producers and for the development of the Indian economy.\n\nReferences\n\nAkash tablet\u2019s commercial variant in November . (2011). Web.\n\nBullen, C., LeFave, R., & Selig, G.(2010). Implementing strategic sourcing . Zaltbommel, Netherlands: Van Haren Publishing.\n\nChesser, W. L., & Cohen, S. A. (2006). Outsourcing in India: Practical approaches to intellectual property issues from the Indian company perspective. The Indian Journal of Law and Technology, 2 (1), 65-86.\n\nDavies, P. (2004). What\u2019s this India business? London: Nicholas Brealey International.\n\nGay, C. L., & Essinger, J. (2000). Inside outsourcing . London: Nicholas Brealey Publishing.\n\nLacity, M., & Rottman, J. (2008). Offshore outsourcing of IT work: Client and supplier perspectives . London, UK: Palgrave Macmillan.\n\nSalwan, P. (2007). Best business practices for global competitiveness . New Delhi: Sterling Publishers Pvt. Ltd.\n\nSonaje, H. (2012). Top 5: World\u2019s tablet PC manufacturing companies till 2012 . Web.\n\nTablet Sales Trends . (2010). Web.\n\nThite, M., & Russell, R. (2007). India and Business Process Outsourcing. In J. Burgess & J. Connell, (Eds.), Globalisation and work in Asia (pp. 38-59). Oxford: Chandos Publishing.\n\nVestring, T., Rouse, T., Reinert, U., & Varma, S. (2005). Making the move to low-cost countries . Web.\n",
        "label": "human"
    },
    {
        "input": "Review: \u201cComputers Learn to Listen, and Some Talk Back\u201d by Lohr and Markoff Essay\n\nTable of Contents\n 1. Introduction\n 2. Computer Functionality\n 3. How Do the Authors Address the issue of Computer Functionality?\n 4. Implications\n 5. Weakness of the Article in addressing the Thesis\n 6. Challenges Facing Technology Functionality\n 7. Recommendations\n 8. Conclusion\n 9. Reference List\n\nIntroduction\n\nLohr and Markoff\u2019s article Computers Learn to Listen, and Some Talk Back is a must-read composition that strategically presents various arguments concerning the changing trends in use of computers. Nowadays, computers have changed the day-to-day lives of people. The potential of computer keeps on moving strides higher and higher with most business transactions and deals being done through computers.\n\nEven as technology advances with scientists researching for the most advanced forms of technologies, there has been the failure to consider the plights of intellectuals and professors. The advancement of this technology is posing a threat to such people with high levels of skills especially in the education sector.\n\nMany professors will be rendered jobless. Their relevance in the society will be of no use because students will have an opportunity to log in to the computers or learn everything that they want using computers.\n\nThe information present on the internet is enormous with a likelihood of making many of the professors and other people in the academic arena obsolete. Therefore, the paper critically examines the functionality of computer and its application in the day-to-day people\u2019s interactions based on the views and opinions presented in the article.\n\nComputer Functionality\n\nThe authors of the article express their optimism in computer technology. In the coming years, computers will replace human beings, as they will be used in solving problems and performing various tasks. Technology will function as an artificial intelligence because computers will have to understand what human beings say.\n\nAll countries are receiving these changes positively even to the extent of integrating them in their systems. The use of speech software among doctors in the developed countries like America has made it easier for doctors to communicate with their clients. However, such software has challenges. One of them is its inability to recognize and interpret some words. The risk may distort the meaning of the words communicated.\n\nOther challenges that this computer software has is that it will eliminate millions of jobs and probably create many different ones besides changing the daily routines of people and the nature of how they work (Zhao, Sophia, & Mishra, 2001, p. 348). Computer intelligence technology is finding its way in almost all spheres of life. Most customer-call centers, hotline companies, and technical supports have made computer their listener.\n\nWhen a customer calls, the computers identify the emotions and the issues that the customers is raising and relays this information to the specific assistant to provide a solution.\n\nFurthermore, companies are able to identify the strategies of their competitors through such technologies, which help them to readjust their own strategies to remain competitive. Currently, technology is becoming the ideal way of doing businesses by ensuring a successful social life. Life cannot go on without technology.\n\nPeople have become used to technology. This trend is still increasing. The authors in the article therefore indicate that, in the future, technology will be the overall way of life. People\u2019s life will change drastically. Children will be communicating and responding towards technologies.\n\nTherefore, the authors target the young generation and companies. These people must be acquainted with various technologies because technology will have to affect their way of doing things. Currently, there has been a progress in the way people adopt and use the computer technology.\n\nSo far, tremendous improvements have been witnessed with how people carry out their day-to-day transactions. The trend is not going back: it is advancing (Peters, 1995, p. 32). For instance, many companies that have call centers to attend customer issues are able to automate their machines to answer to the millions of queries across the world.\n\nHow Do the Authors Address the issue of Computer Functionality?\n\nThe framing of the title of the article, as used by the authors, enhances easier understanding of what they are discussing. The core issue or theme in the story is about the potential of technology and the advancement of technology. The world is moving at a faster rate in terms of technology advancement.\n\nThis is all they are trying to project in the story. They have provided salient examples and illustrations of how the computers are turning out to be listeners besides being able to talk.\n\nLiterally, computers or technologies that are used in the call centers and technical support are designed in a manner to listen or to decode what the callers are saying or transmitting this valuable information hence answering various queries (Berry, 2005, p. 399).\n\nTherefore, the framing of the article is appealing. It is aimed at creating understanding as well as arousing the readers\u2019 interest to dig deeper in the story to find out what it is all about. The message has also been presented in a well and objective manner.\n\nThe authors not only present the positive sides of embracing technology but also go ahead to talk about some implications that such advancements are likely to cause in the life of people. Firstly, it should be taken with cautions because these technologies may not be 100 % correct. It may not be able to satisfy or achieve the goals of its users. Some flaws will be factored in when using these technologies.\n\nAccording to Bakardjieva (2003), technology such as the computer technology has many issues that need to be considered to ensure that it achieves its objectives (p. 133). One of the concerns is the sustainability of technology. It should be self-sustainable besides having an inbuilt security.\n\nMany technologies are exposed to security threats. For instance, these technologies may lead someone to crack or hack into someone\u2019s systems thus leading to information proliferation and hence a threat. Likewise, others allow people to hack into others databases, which may put the company\u2019s confidential information at risk.\n\nThere are various claims made in the article by the authors relating to computers\u2019 potentiality in the current times and in the future. The major claim is that these computers will be able to communicate with human beings because they will have the capability to hear, speak, learn, and reason.\n\nThe way machines and human being will collaborate and communicate with each other will be transformed. This transformation is likely to come with its consequences. The way people work and carry out their duties will have to change. Their conventional way of doing their businesses is also destined to change.\n\nScientists have not relented in their efforts of looking for software-artificial intelligence technologies to ensure that this becomes a reality. The word today is revolutioning as many parts of the country in the world have already put into test these technologies and found that they work.\n\nFurthermore, there is a high attention directed to speech recognition technology. Even as this attention is directed to the technology, no body is looking at these software and technology limitations. More focus is drifted on the social implication of these technologies. Therefore, there is lack of balance on how people view and think about these technologies.\n\nImplications\n\nAccording to Murphy (1995), there are various social implications that come with these technologies (p. 336). These include issues concerning privacy of an individual and accessibility to information that is deemed confidential. These technologies are also a threat to social behaviors of the society if no adequate measures are instituted to guard against such misuse.\n\nThere has been a remarkable improvement in speech devices in the car industry, as well as hotels. For instance, in duration of three years, Ford Motor Company has seen a remarkable improvement in its speech software in its vehicles. One of the software is the Nuance that has improved, and now is able to recognize over 10,000 phrases and words from the earlier 100 words.\n\nThese systems are becoming popular in cars with many people liking them because of their enhanced security feature. Some of the successful areas that machines have triggered include in the area of banking where ATM machines are used to serve customers. They are efficient and convenient as opposed to standing in queues for service. Furthermore, in most of the call centers, automated machines have helped to enhance service delivery.\n\nThe increasing usefulness of artificial intelligence machines in answering questions, assisting professionals, and completing tasks form a clear indication that these machines are going to spread regardless of the risks that are inherent (Knyaz, Georgiadi, & Bogiv, 2012, p. 119).\n\nIt is apparent that, in the future, there will be a machine intelligence as opposed to human intelligence as machines will do most of the tasks. Even with this development, all sorts of errors and problems will be mandatory.\n\nTherefore, human beings will be of great importance in correcting such errors to balance them. It is much better to have this artificial intelligence in place as opposed to not having it because it is of great assistance in the operation of the human activities. The fact that information system and invention of more intelligence technologies is on the course, there must be challenges that will come with it (Peters, 1995, p. 32).\n\nPeople will not see the essence of sitting in a class to be given lectures by their professors since such information will be easily available in the internet. Therefore, the authors of the article have not been able to reason on this wavelength.\n\nAccording to Brantlinger (2003), there are changing relationships that are happening among intellectuals, media, and public, which are complicated by the coming of new technologies (p. 124). This therefore makes the public intellectuals to experience or witness a renaissance.\n\nRenaissance may not be out of the picture because of the potentiality of the internet and computers due to their broad communicative access, interactivity, ubiquity, and democratic potential. Furthermore, many computers and the internet technologies are mostly aligned on commercial interest.\n\nCapitalism is the major focus driving most of these technologies. Many services and functions provided by these technologies are aimed at increasing the profitability of the companies or people who invent them.\n\nTake an example of a call centers. This technology has enabled many customers to raise their queries besides seeking for more clarification on various issues affecting them. The companies providing these technologies aim at making a catch to make them continue using their products for their benefits (Heath, 2005, p. 66).\n\nWeakness of the Article in addressing the Thesis\n\nTechnology has a wide scope since various issues concern it. The article has only delved on the ways of improving technology to enable it function more or less as human beings. Machines have an artificial intelligence, which is able to perform tasks that otherwise would have been performed by many people.\n\nTherefore, it is a fundamental discovery in the life of many. However, the authors have not explained in depth the requirements and the benefits of such technologies. Furthermore, they have not provided the readers with the likely consequences that are associated with such technologies.\n\nFor the article to be good and appeal more to the audience, they could have gone a little far to explain some of the limitations that adoption of such technologies will have on the users. The world is moving forward with everyone being concerned about technological advancements. Companies are competing to come up with the most sophisticated software to beat others in the market.\n\nThese issues need to be addressed. The effects of such technology on the users and even on the environment should come into perspective. Furthermore, the consequences such machines will have on people and the general living standards of people should come into perspective.\n\nIt is understandable that there is no technology that is error-free (Cooper, Javier & Woo, 1994, p. 371). The magnitude of these errors should be closely investigated to ensure that they do not arouse suspicion among the users.\n\nChallenges Facing Technology Functionality\n\nThere are various challenges that the advancement of technology causes or is likely to cause to people. One of the challenges is dealing with the issues of insecurity in the technologies (Rutenbeck, 2000, p. 30). As some technologies advance, scientists are finding ways to disrupt them.\n\nFor instance, there are some software that have been created, which are able to allow individuals to hack and gain accessibility to persons\u2019 confidential information in their database. These are some of the issues that need to be addressed to ensure that technology is secure.\n\nFurthermore, implication that concerns the effects of political, economic, and social consequences should be addressed. Technology, if not well scrutinized, is likely to be used in a wrongful way, for example, in perpetuating crimes and immoral behaviors.\n\nAnother challenge related to the development of these technologies, artificial intelligence machines, for instance, is that they are prone to errors. Therefore, because they are operated and advanced by men, they will require a close attention to ensure that they continue to provide best services.\n\nTherefore, these machines cannot supersede the intelligence for human beings because it is them who programmed and came up with software and programs that are able to execute various services and or tasks. Major forces evidenced in this article are aimed at promoting and supporting new technologies.\n\nThe authors are optimistic that machine artificial intelligence is the best way to go in order to attain positive results. Individuals and organizations should adapt these technologies because of the perceived benefits they have on the individuals as well as on the economy.\n\nThe most important contribution of technology is its ability to enhance efficient and effective performance of tasks besides increasing the level of production of companies and organizations (Knyaz, S., Georgiadi, N., & Bogiv, 2012, p. 119).\n\nAccording to Grossman and Zuckerberg (2010), technology is important because it helps to reduce costs. Therefore, organizations that adapt it are better placed to increase their productivity besides cutting the cost of production. They are likely to have economies of scale (Para. 3). Therefore, many organizations that have the capacity to install technology in their operations should not relent.\n\nMost of the people have accessibility to technology through their phones and laptops. Therefore, many people are able to transact their businesses on the internet. Advertising and marketing products over the internet is therefore a good strategy to reach many potential buyers across borders.\n\nIt has a positive impact on companies\u2019 production and revenue through their increased level of sales. Therefore, the authors should have considered the potentials, which the technology has in terms of widening or expanding the market of the producers besides increasing productivity in the companies and individuals.\n\nRecommendations\n\nTechnology has become an integral part of human beings in their lives. Therefore, with the changing social systems, it is very important for people to understand various issues that concern these technologies to be able to use them appropriately.\n\nThe society, for instance, should know that these technologies are not advanced or adopted for misuse but rather to help in promoting co-existence of people besides facilitating operations (Grahame, 2009, p. 31). Therefore, some of the meaningful linkages that can be explored in this courses concern proper use of technology in fostering positive relationships and economic growth in the country.\n\nComputers are useful when well used especially in furthering the interests and needs of the people. However, it may not be effective if it is used to cause harm to others. Therefore, the society at a large has the responsibility of endeavoring to know the uses and applications of technology in a useful and positive manner.\n\nPeople\u2019s lifestyles are always changing. Therefore, people should not allow technology to influence them. They need to use it sparingly in meeting their goals and ends.\n\nConclusion\n\nIn conclusion, the rate at which artificial technology necessitated by machines is moving is raising concerns. As the authors of the article, \u2018Computers Learn to Listen and Some Talk Back\u2019 argue, it is going to move at rapid pace. However, what is needed is caution in its adoption. There are various flaws and problems that will emerge and which need to be corrected by human beings to ensure that it provides better services.\n\nTherefore, as technology moves and advances, the society should be cautious while employing it in use for their benefits but not for their destruction. Therefore, as pointed out in the paper, computer functionality has largely taken the roles that were being played by people before their invention.\n\nTasks that took hours to accomplish are taking only a fraction of a second. However, there are several implications and challenges that the technology has, which people need to be aware of as discussed in the paper with the most significant being that of depriving many people of their jobs as the computers take over.\n\nReference List\n\nBakardjieva, M. (2003). What Knowledge? Whose Fingertips? Negotiating and Serving Diverse Identities. Information Technology, Canadian Ethnic Studies, 35 (3), 133-149.\n\nBerry, W. (2005). Local Knowledge in the Age of Information. Hudson Review, 58 (3), 399-410.\n\nBrantlinger, P. (2003). Professors and Public Intellectuals in the Information Age: An Interdisciplinary. Journal of Jewish Studies, 21 (3), 122-137.\n\nCooper, A., Javier, G., & Woo, C. (1994). Initial human and financial capital as predictors of new venture performance. Journal of Business Venturing, 9 (5), 371\u2013395.\n\nGrahame, J. (2009). Changing Ads: Envisaging Media Study in the Post-Digital Age, English Drama Media, 14 (1), 31-36.\n\nGrossman, L., & Zuckerberg, M. (2010). Person of the Year 2010. Time Magazine . Retrieved from https://www.nytimes.com/2010/10/04/business/media/04carr.html\n\nHeath, M. (2005). Are You Ready to Go Digital? The Pros and Cons of Electronic Portfolio Development. Library Media Connection, 23 (7), 66-70.\n\nKnyaz, S., Georgiadi, N., & Bogiv, Y. (2012). Business planning innovative projects: the essence of technology, advantages and disadvantages. IN: Marketing \u00ec Mened\u017ement Innovac\u00ecj, 3( 2), 199.\n\nMurphy, J. (1995). Communication in the information age: a critique, ETC. A Review of General Semantics, 52 (3), 336-343.\n\nPeters, E. (1995). Information age avatars.(cover story). Library Journal, 120 (5), 32-35.\n\nRutenbeck, J. (2000). The 5 Great Challenges of the Digital Age. Library Journal, 125 (14), 30.\n\nZhao, Y., Sophia, H., & Mishra, P. (2001). Technology. Journal of Adolescent & Adult Literacy, 44 (4), 348.\n",
        "label": "human"
    },
    {
        "input": "Computer-Mediated Communication Term Paper\n\nComputer-Mediated communication (CMC) is critical in human communication due to its affordability, reliability, and connectivity. As the world steadily becomes a global village, communication experts have embraced computer-Mediated communication in providing communication solutions and enhancing interaction between socializing agents.\n\nTherefore, what factors directly affect the success of different CMC modes in relational maintenance and behavioral patterns in human communication environment?\n\nThus, this reflective treatise attempts to explicitly applaud on the modes and behavioral patterns observed across the gender divide and how the same can be related to success of CMC in provision of communication solutions to socializing agents drawn from a cross different cultural divides, gender, age, and exposure.\n\nIn order to comprehensively understand and interpreter the research question, the treatise review three previous literature on the topic by examining the articles.\n\nThe Cyber Factor: An Analysis of Relational Maintenance Through the Use of Computer-Mediated Communication by Houser, Fleuriet, and Estrada, IMing, Text Messaging, and Adolescent Social Networks by Bryant, Sanders-Jackson, and Smallwood, College Students\u2019 Use of Relational Management Strategies in Email in Long-Distance and Geographically Close Relationships by Johnson, Haigh, Becker, Craig, and Wigley, and Assessing the Measurement Invariance of Relational Maintenance Behavior When Face-to-Face and Online by Andrew Ledbetter.\n\nThe article, Cyber Factor: an Analysis of Relational Maintenance Through the use of Computer-Mediated Communication by Houser, Fleuriet, and Estrada dwells on the link between interpersonal communication and CMC. Adopting both qualitative and quantitative analysis, the article identifies significant relationship between gender variance and use of CMC modes.\n\nIntrinsically, the authors conclude that success of different CMC strategies are largely influenced by mode adopted in the process of relational management.\n\nThus, as opined by the authors of this article, there is a quantifiable relationship that actively operates in the interaction between relationship type; as determined by gender of the client, and CMC mode for that specific gender divide. Women are noted to be more reliable and consistent that men (Houser, Fleuriet, & Estrada, 2012).\n\nOn the other hand, the article, IMing, Text Messaging, and Adolescent Social Networks by Bryant, Sanders-Jackson, and Smallwood is a continuation of research on the link between CMC networks and adolescent age group in development of technologies that are socially interactive.\n\nConcentrating on quantitative research, the article is specific in identifying the aspect of inconsistent or quantifiable between teenage participation in socially interactive technologies (SIT) and traditional mode of communication (Jackson, Bryant, & Smallwood, 2006).\n\nHowever, the authors identified socially isolated teenagers as major users of SIT due to passiveness and other minor reasons. Here, use of SIT which is part of CMC among teenagers is largely determined by the behavioral patterns of the subjects. Specifically, those who are weak socially in this group are likely to use this mode of communication that those that are socially active.\n\nThe third article, College Students\u2019 Use of Relational Management Strategies in Email in Long-Distance and Geographically Close Relationships by Johnson, Haigh, Becker, Craig, and Wigley examines the significant of emails in enacting maintenance behaviors exhibited in the daily interpersonal interactions and relate this to the aspect of geographical distance between subjects interacting.\n\nThrough embracing maintenance strategy topology, the study identifies the aspects of self disclosure, positivity, and social networking as the assuring factors that promoted effectiveness of CMC modes used across the generational divide (Johnson, Haigh, Becker, Craig, & Wigley, 2008). However, the geographical distance had negligible influence on assurances and social networking through CMC.\n\nThough not as comprehensive as the other articles, the article, Assessing the Measurement Invariance of Relational Maintenance Behavior When Face-to-Face and Online by Andrew Ledbetter examines existing differences in the five identified relational maintenance behavior (RMB) types in different instant messaging and \u201cface-to-face communication\u201d (Ledbetter, 2010, p. 4).\n\nSpecifically, the article borrows heavily from the Relational Maintenance Strategy Measure (RMSM) in statistical analysis of the existing variance in answering the research question. Interestingly, the findings are in line with the RMSM in extrapolating the intrinsic relationship that exist in the interaction of instant messaging, CMC, and relational maintenance.\n\nFrom the literature review as indicated by the articles, the main factors that affect the success of CMC modes of communication are the aspects of social activeness, age, gender, purpose and relationship between the subjects communicating. Notably, it is apparent that cordial relationships would prefer assurance in use of CMC modes that casual acquaintances.\n\nBesides, gender determines reliability of the users noting that those females are generally more consistent than their male counterparts. Among the younger generation, that is adolescent and teenagers, use of CMC is highly influenced by confidence and social activeness.\n\nAs noted in the research by Bryant, Sanders-Jackson, and Smallwood, it is factual that CMC models are likely to be explored more by those teenagers that are not socially interactive. Due to their \u2018loneliness\u2019, they would rather prefer this mode of communication to the traditional face-to-face communication that requires a lot of confidence in expression.\n\nTherefore, in designing an appropriate CMC model for human communication, it is critical to factor in behavioral patterns noted in the above studies to create practical and reliable communication designs for reaching different gender, age, culture, and exposure.\n\nReferences\n\nHouser, M., Fleuriet, C., & Estrada, D. (2012). The Cyber Factor: An Analysis of Relational Maintenance Through the Use of Computer-Mediated Communication , Communication Research Reports , 29(1), 34\u201343.\n\nJackson, A., Bryant, A., & Smallwood, A. (2006). IMing, Text Messaging, and Adolescent Social Networks , Journal of Computer-Mediated Communication 11, 577\u2013592.\n\nJohnson, J., Haigh, M., Becker, J., Craig, E., & Wigley, S. (2008). College Students\u2019 Use of Relational Management Strategies in Email in Long-Distance and Geographically Close Relationships , Journal of Computer-Mediated Communication, 13(1), 381\u2013404.\n\nLedbetter, A. (2010). Assessing the Measurement Invariance of Relational Maintenance Behavior When Face-to-Face and Online, Communication Research Reports , 27(1), 30\u201337.\n",
        "label": "human"
    },
    {
        "input": "Paralinguistic Cues in Computer-Mediated Communications in Personality Traits Report\n\nAbstract\n\nComputer based communication is an integral communication modality within the social, educational and professional environments. However, when using text-based CMC, it is common for people to form prejudice impressions about the personality traits of another individual.\n\nThis experiment investigated the effect of the presence or absence of paralinguistic cues in computer-mediated communications on personality traits.\n\nThe findings suggest that the impressions formed about a personality trait of an individual rely on the presence or absence of paralinguistic cues during interaction.\n\nIntroduction\n\nComputer based communication has become more popular and vary greatly with the ways in which individuals communicate within social, educational and professional environments. The differences not only lie in the surrounding environment, but also in the method of communication (Storms, Grottum & Lycke, 2007).\n\nCommunication entails both verbal and non-verbal aspects. While verbal communication involves exchange of words between individuals, non-verbal communication takes in different forms (Walther, Deandrea & Tong, 2010).\n\nComputer-mediated communications (CMC) refers to the communicative transaction involving the use of computers and communicative networks.\n\nThe scope of computer mediated communications cuts across sociopsychological elements, particularly the topic on online interactions and their relations to everyday life, and to the application of paralinguistic aspects like emoticons (Storms, Grottum & Lycke, 2007).\n\nParalinguistic cues play a significant role in human speech communication. A lot of studies have been carried out in attempts to elucidate how the use of paralinguistic features such as body language, facial expression and posture affect the level of communication (Amant, 2007).\n\nIt is common for human beings to form prejudices about others even before speaking to them when socializing on a face-to-face basis. These preconceptions are often based on paralinguistic cues like gestures and appearance (Epley &Kruger, 2005).\n\nHowever, in the computer-mediated communications, these impressions would only be formed based on text message interactions. According to DeLamater and Myers (2007), the accuracy of communication is greatly enhanced by the use of multiple cues, as opposed to a single communication channel.\n\nComputer-mediated communication features such as lack of social context cues often make this form of communication less personal (Holland, 2008).\n\nThere is need to gain more insight on how the presence or absence of paralinguistic cues affects the expectations or preconceptions that people form of each other in CMC interactions.\n\nVarious theories have been proposed in attempts to explain the role of paralinguistic cues in influence the stereotypes and expectancies over computer-mediated communication.\n\nSome of these theories include the social context cues theory and the social information processing theory.\n\nThe social context cues theory proposes that the absence of paralinguistic cues in CMC makes it highly ambiguous (Epley & Kruger, 2005). As a result, people depend on their personal stereotypes to make preconceptions about the other person\u2019s character.\n\nCMC, thus, allows the persistence of expectancies or stereotypes due to the absence of paralinguistic cues ordinarily the question then (Holland, 2008). A highly standardized experiment was performed by Epley and Kruger (2005) to investigate impressions on different communication channels and the resultant stereotypes and expectancies.\n\nThe researchers performed three experiments, whereby they moderated the participant\u2019s anticipations regarding the interviewee. The interviews were done either on phone or e-mail.\n\nPhone communication was found to confer features reminiscent of face-to-face interaction, even though the conversations relied on simple, preset questions and rapid responses.\n\nIn contrast, communication through e-mail conferred no actual interactions between the parties, though the answers the interviewees gave were similar to those from phone communication.\n\nThe results indicated that the preformed notions about the interviewee persisted more over e-mail than over the phone (Epley & Kruger, 2005).\n\nConversely, the social information processing theory suggests that potential deficiencies of CMC are indemnified by the use of text based non-verbal cues like \u2018Laughing Out Loud\u2019 (LOL) and \u2018mhhh\u2019. The usage of emoticons in CMC provides an emotional setting to users (Walther & D\u2019Addario, 2001).\n\nCMC users can, thus, express socio-emotional content with only written text via these non-verbal cues and timing of the messages. A recent social experiment showed that live CMC chats could challenge pre-interaction stereotypes better than phone communication (Walther, Deandrea & Tong, 2010).\n\nIn this study, the researchers let the interviewees respond naturally to questions posed by the interviewers through phone and live chats. The study demonstrated that live chats provide a variety of non- or marginally verbal expressions surpassing those provided by voice communication.\n\nThis is because people can use live chats intelligent features, involving verbal and non-verbal cues, to deliver precisely what face-to-face could achieve (Walther, Deandrea & Tong, 2010).\n\nHowever, these experiments were hampered by a variety of limitations. One limitation was the use of faulty experimental designs, which did not correctly simulate natural CMC interactions (Epley & Kruger, 2005).\n\nOther experiments lacked control parameters, hence making it difficult to establish causal relationships (Walther, Deandrea & Tong, 2010).\n\nThe aim of the current study was to investigate whether paralinguistic cues in CMC interactions were sufficient to challenge the expectation that the target individual was introverted on personality trait rating.\n\nIt is predicted that the presence or absence of paralinguistic cues in computer-mediated communication interactions will not have an effect on personality trait ratings, according to the social cues theory.\n\nIt is also predicted that the presence or absence of paralinguistic cues in computer-mediated communication interactions will have an effect on extroversion ratings, according to the social information processing theory.\n\nMethod\n\nParticipants\n\nThe participants were undergraduate introductory psychology students at Monash University from Clayton, Caulfield, Peninsula, Sunway and South Africa campuses.\n\nMaterials\n\nInternet connected computers were used to conduct the experiment. An online profile for the CMC interaction was created. The profile, called MINGLE.COM, consisted of an individual\u2019s photo and personal details such as date of birth, relationship status and work details.\n\nA questionnaire with 20 questions was designed to provide extraversion ratings. The CMC interaction to be observed involved a series of questions and responses between the target and the interviewer.\n\nThe interviewer questions were like, \u201cwhat would be your favorite way to spend the summer holiday? Examples of the responses from a target were \u201cDEFINITELY at the beach. \u2026..: D, LOL\u2026..sure thing:), yep.\n\nJust be warned\u2026dun dunduuuuuun! Extraversion ratings were on a scale of 1-7, depending on the responses of the participants. The responses to the questions had seven options to choose from.\n\nTo measure the personality, the extraversion scale was utilized whereby individuals with introvert personality would have a score of 20 to 80 while those having an extrovert personality would have a score range of between 80 and 140.\n\nProcedure\n\nThe sample was divided into three groups A, B, and C who observed a CMC interaction. Group A was the paralinguistic cues group with 120 men and 342 women (M=20.7 years, SD=5.3). Group B was the plain text group with 125 men and 308 women (M=21.2 years, SD=5.4).\n\nGroup C was the control group with 131 men and 329 women (M=20.9 years, SD=5.1). The total sample had 1355 participants (M=20.9, SD=5.23). Convenience sampling was adopted as this study was part of the course requirement.\n\nInitially, the participants were na\u00efve regarding the true nature of the study. However, they were debriefed and instructed online as to the intent and procedure. The participants were directed to observe a past CMC activity.\n\nA profile stimulating the expectation in participants that the target individual was introverted was designed. Participants looked at this profile, and then observed the target individual communicating with an uncontrolled third party. Participants were then divided into two groups.\n\nOne group looked at the basic text interaction while the other looked at a realistic paralinguistic communication with many cues hinting that the target was an extrovert.\n\nThe text used for the interactions was made from a real paralinguistic interaction from which all cues were removed to produce a basic text interaction.\n\nTherefore, the conversations had identical contents apart from the paralinguistic introduced. The major aim was to establish if paralinguistic information in CMC provided enough evidence to clear the preformed ideas that the target was an introvert.\n\nDesign\n\nThe study used an independent measure design. There were two levels of independent variable conditions used. These were the group with the basic text interaction and the group with the paralinguistic interaction. The dependent variable was the extroversion rating.\n\nResults\n\nThe participants were asked questions, which they were to respond to from the given options such as strongly disagree, disagree, somewhat disagree, neither agree nor disagree, somewhat agree, agree, and strongly agree.\n\nFrom the 20 questions posed to the participants, responses were measured on an extraversion scale of 1-7, with the highest possible personality score being 140 and the lowest being 20.\n\nThe higher the score, the more the extraverted rating an individual was given. It was predicted that presence or absence of paralinguistic cues would not influence the extroversion ratings when considering the social cues theory.\n\nIn relation to the social information processing theory, it was also predicted that the presence or absence of paralinguistic cues would influence extroversion ratings.\n\nGroup A, which had participants observing a CMC interaction for paralinguistic cues, rated the targets as extrovert (M=91.74, SD=37.42) while group B, with participants observing plain text interaction, rated the targets as introvert (M=79.84, SD=29.08).\n\nGroup C, which was the control, indicated that the targets were introverts (M=72.16, SD=29.08).\n\nDiscussion\n\nThe results indicate that the presence of paralinguistic cues in computer-mediated communication can influence the impressions formed about the personality traits of an individual. In group A, the participants rated the target as highly extrovert in spite of the fact that the targets were presented as introverts.\n\nThe findings further advance the social information processing theory which emphasizes more on the style of communication as an important aspect of impression formation.\n\nUse of non-verbal, text based cues such as emoticons, ellipses and hyphens can help in deciphering the personality trait of a person in a CMC interaction (Hancock & Dunham, 2001; Walther, Deandrea & Tong, 2010).\n\nIn group B, most participants used their preformed concepts to rate the target as being introvert. This is because plain texts may often undermine the impressions about the personality traits as a result of the inadequate information it relays.\n\nThese results, therefore, support the social cues theory proposition that absence of social cues in a computer-mediated communication environment increases ambiguity, hence people use their stereotypes to form preconceptions about personality traits.\n\nThe findings are in agreement with the arguments by Epley and Krugler (2005) that absence of nonverbal cues hinders people from discerning another person\u2019s characters.\n\nConclusion\n\nThe data from group C, the control, rated the targets as introverts. Given that this data falls between the data for groups A and B, then group C suggests that the profile was successful in measuring the objectives of the study.\n\nIt can, therefore, be concluded that the presence or absence of paralinguistic cues in computer-mediated communication can influence the impressions about the personality traits of a person.\n\nThe impressions formed depend on the communication style.\n\nReferences\n\nAmant, K. (2007). Linguistic and cultural online communication issues in the global age . Hershey, PA: Information Science Reference.\n\nDeLamater, J. D.,& Myers, D. J. (2007). Social psychology , 6 th edn. Belmont, CA: Thomson Higher Education.\n\nEpley, N., & Kruger, J. (2005). What you type isn\u2019t what they read: The perseverance of stereotypes and expectancies over email. Journal of Experimental Social Psychology, 41 , 414-422.\n\nHancock, J. T., & Dunham, P. J. (2001). Impression formation in computer-mediated communication revisited: An analysis of the breadth and intensity of impressions. Communication Research, 28 , 325-347.\n\nHolland, S. (2008). Remote relationships in a small world . New York, NY: Peter Lang.\n\nStorms, H. I., Grottum, P. P., & Lycke, K. H. (2007). Content and processes in problem-based learning: A comparison of computer-mediated and face-to-face communication. Journal of Computer Assisted Learning, 23(3), 271-282.\n\nWalther, J. B., & D\u2019Addario, K. P. (2001). The impacts of emoticons on message interpretation in computer-mediated communication. Social Science Computer Review, 19 , 324-347.\n\nWalther, J. B., Deandrea, D. C., & Tong, S. T. (2010). Computer-mediated communication versus vocal communication and the attenuation of pre-interaction impressions. Media Psychology, 13 , 364-386.\n",
        "label": "human"
    },
    {
        "input": "Are We Too Dependent on Computers? Essay (Critical Writing)\n\nIntroduction\n\nA computer has become a very important device in modern days. Usage of computers has generally been increasing since invention of the device back in the nineteenth century. This has resulted from the possibility to modify the device so that it suits different situations and thus ends up performing most of the jobs that were initially performed by people.\n\nFlexibility in the use of the device has lead to integration of computers in nearly all fields in which human beings have been involved. Usage of computers ranges from performing simple tasks such as arithmetic operations to complex logical tasks like analyzing data. Recently, Artificial Intelligence has been incorporated by companies to enable them make valid decisions (Kizza 2011).\n\nThis is beneficial in that decisions and recommendations made by a computer are more reliable and accurate, and therefore the companies\u2019 policies are likely to improve. The ever decreasing prices in computers have made them more affordable to many people. More varieties of computers have come up.\n\nThese range from simple hand-held mobile communication devices to serves and super computers that are mainly used in large companies. The decrease in price and ready availability of the computers in the market has made many individuals, companies, organizations and other institutions integrate them in their operations. Dependency on computers has therefore gone up, whether an individual is directly working with the machine, or he/she depends on services provided through a computer.\n\nComputers play a central role in the modern world, as outlined in the following sectors:\n\n  * Communication\n\nComputers have played a very integral part in communication sector. As tools for communication, they have been employed in so many fields such that it is seemingly impossible for communication to take place without them. They play a central role which cannot be overlooked in communication. People have become so dependent on them such that any little failure affects the whole communication system.\n\nTelecommunication companies use super computers called serves that provide a link of communication between different client computers. All voice calls have to go through these serves so as to reach the other person at the receiving end.\n\nIn addition to providing voice call services, the telecommunication companies also provide internet access to companies and institutions such as colleges and banks. The companies charge a subscription fee for the service on monthly or yearly basis. These servers also act as message centers for transmitting messages between different client units.\n\nLike any other man made machine, these servers are subject to occasional breakdowns (Rosenberg 2004). The duration taken to restore the machine varies depending on the cause of the breakdown, expertise of the repairing engineer and the resources needed to restore the machine.\n\nIt may therefore, take as short time as a few seconds to rectify the fault, and as long as several hours or even days. Breakdowns of these machines imply a stoppage in the delivery of services to clients. This alters the operation of companies that do not have alternative providers of the essential communication service.\n\nCompanies whose operations entirely rely on communication networks such as banks end up in not functioning. For instance, customer transactions cannot be processed, which implies that the customers are not able to withdraw or make deposits in the institution.\n\nBreakdown of these servers also hinders communication between different individuals, and this may lead to misunderstandings. Business people, for instance, are not able to reach their customers and vice versa, which can adversely affect the business. Other services that rely on availability of communication network such as money transfer services are also hindered.\n\nDespite the risk that these breakdowns pose in daily lives, many companies have automated their operations. These calls for services of telecommunication companies, and therefore they end up in awkward situations when there is a breakdown.\n\nApart from communication network, computers must be supplied with power in order for them to function. Whenever there is lack of power supply, companies, individuals and organizations that entirely depend on computers for their operations are forced to virtually close down. This results from the fact that all operations can only take place through the computer.\n\nIf the company does not have an alternative source of power, its operations forcefully come to a halt. Customers served by this company end up in inconveniencies and unnecessary delays. Essential services provided in pharmacies, supermarkets, banks and other institutions become unavailable.\n\n  * Medicine\n\nComputers have also been employed in medicine to facilitate carrying out different tasks. For instance, programmers have designed special applications that are applied in medicine to perform diagnosis. Special computerized equipments have been made to enable automatic detection of dysfunctional tissues in the body without having to subject the patient under theater operations.\n\nAn example is the computerized X-ray machine that identifies the malfunctioned tissue under the skin, the extent of damage of the tissues, the risk that tissue poses in the health of patient and thereafter makes recommendations of possible treatment or medication.\n\nIn addition to diagnosis, computers have also been used in optical clinics to examine eye problems in patients. Adequate information if fed to databases kept by the opticians, which enables the computers to make appropriate decisions according to the results of examination, and consequently recommend aid spectacles or treatment when necessary.\n\nMost of the tasks that were initially performed by the opticians have been delegated to computers (Cimino & Shortliffe 2006).\n\nIn collaboration with the information technology, doctors have set up online clinics where patients are able to contact them and receive diagnosis or prescription. The patient is relieved the burden of travelling and incurring unnecessary expenses.\n\nIn this field, the patients provide information about their wellbeing to the doctor, which the doctor uses to diagnose the illness. He or she then prescribes appropriate medicine for the patients, which can be obtained from the local pharmacies. Payments are made through money transfer services and therefore the doctor does not come into direct contact with the patients.\n\nIncorporation of computers in the medical field has brought both positive and negative effects with it. First, it produces accurate and unbiased information about the diagnosis. It is therefore more reliable and gives the best treatment prescription for the patient. Computers are also faster, and therefore, speeds up most of the operations thus increasing overall efficiency in service delivery.\n\nOn the other hands, computers are subject to breakdowns and power interruptions. In case the machine fails or there is a power supply breakdown, patients are exposed to the risk of not being attended to.\n\nLack of immediate attendance to patients can lead to severe complications or even death. Computerization of most of these processes in the field of medicine therefore can have an adverse effect in the heath wellbeing of the society at large (Rosenberg, 2004).\n\n  * Education\n\nComputers have also been intensively employed in education sector. Presentation applications such as Ms PowerPoint have been used by lectures to facilitate learning through prepared slides. This is a more efficient way of sharing limited resources with the learners and allows for use of audio-visual aids.\n\nComputers provide connection to the internet. Researchers and other scholars are therefore able to access online information and hold online collaborative discussions. The internet provides a wider range of freely available and current books, journals and articles useful for conducting research.\n\nRecently, online learning portals have also been created which enables students to pursue their education without having to converge into a class. Reading materials are provided in these portals and therefore students can access and use them.\n\nExaminations and assignments are also submitted online, marked online and certificates awarded online. In addition, teleconferencing has enabled learners to directly interact with their lecturers. Coordinated discussions, presentations and questions are also processed real time (Kizza, 2011).\n\nLearners have been able to access and use softcopy notes from lectures, books and other reference materials to facilitate learning. Students have shifted from use of physical exercise books, and are rather typing their notes in computer Word Processors for future reference. Recorded lecture films are also available and accessible to learners in the Internet.\n\nJust like in the case of medicine, use of computers in education has its benefits as well as threats. The machines are subject to breakdowns and also require power. Unexpected breakdowns can therefore interrupt the learning process.\n\nIn case an examination was being taken, power failures can lead to leakages due to extension of the examination period. Availability of study materials in the Internet also encourages learners to become lazy since they are able to copy-paste other peoples\u2019 work rather than doing their own researches.\n\nThe students end up becoming lazy since they can simply copy-paste other researchers\u2019 published works and present them as theirs\u2019. (Herna\u0301ndez & Goodson, 2004).\n\nIn modern days, these new methods of providing education are replacing the traditional classrooms, and they will soon be facing them out. The modern generations are turning more into computer based learning, which implies a further dependency on computers.\n\n  * Administration\n\nComputers have also been widely used in administration. In developed countries, electronic voting systems have been introduced. Voters are provided with electronic voting cards bearing voter\u2019s details. The casted votes are centrally managed from database, and tallying, analysis and presentation of election results is computerized (Herna\u0301ndez & Goodson, 2004). The electronic voting process relies on computers, and so does the voters.\n\nConclusion\n\nComputers have played a very integral part and have therefore become indispensible tools in the modern world. They are essential in performing nearly all operations: banking, communication, education, medicine, administration etcetera. Dependency on computers is therefore inevitable, if a person has to accomplish any task.\n\nWhether a person uses the computer to provide a service, or depends on a service provided with the aid of a computer, the fact is, we all depend on computers. Despite the fact that computers can not be entirely relied on due to their occasional breakdown and dependency on power, they have become quite essential in delivery of quality services, which are necessary to our lives (Kizza, 2011).\n\nReferences\n\nCimino, J. J., & Shortliffe, E. H. (2006). Biomedical informatics: Computer applications in health care and biomedicine . New York, NY: Springer.\n\nHerna\u0301ndez, F., & Goodson, I. (2004). Social geographies of educational change . Dordrecht: Kluwer Academic Publishers.\n\nKizza, J. M. (2011). Computer network security and cyber ethics . Jefferson, N.C: McFarland.\n\nRosenberg, R. S. (2004). The social impact of computers . Boston: Academic Press.\n",
        "label": "human"
    },
    {
        "input": "Comparison of three tablet computers: Ipad2, Motorola Xoom and Samsung Galaxy Report\n\nExecutive Summary\n\nCustomers across the globe aspire to shop for the most cost-effective Tablet Pc in the market. In fact, several Tablet Pcs are offered at inexpensive prices and have appealing specs to different classes of users.\n\nOn an average, a Tablet Pc in the current market costs $200. These tablets embrace the latest android technology. These third generation Pcs are simple products with limited multi tasking features.\n\nThe most common tablet Pcs in the market includes Samsung Galaxy, Motorola Xoom, and Ipad2. These computers have advanced specs and are designed to meet demands of the technologically advanced clients.\n\nThese clients are in constant search for a portable, reliable, affordable, light, and efficient communication device. Fortunately, the above computers meet these criteria with each having a unique feature.\n\nThus, this reflective treatise attempts to explicitly analyze features of Samsung Galaxy, Motorola Xoom, and Ipad2 in order to draw comparison in terms of weight, size, cost, and operating system.\n\nBesides, the paper identifies the most efficient and cost effective among the three communication devises.\n\nIntroduction\n\nAs technology advances, the specs of portable personal computers increase. At present, the most advanced pcs have more than a million active applications.\n\nComponents of these pcs consist of a processor, memory ram, media and graphic accelerator, physical storage, operating system, and different screen resolutions (Jones, 10). They come in different size, shape, weight, and color.\n\nComparison in terms of the Operating System and Resolution\n\nMotorola Xoom\n\nMotorola Xoom has the following configurations; a processor of 366 MH and uses a HD video processor or media and graphic accelerator, memory RAM of 256 MB LP-DDR2, secondary memory of 2 GB internal flash, physical storage (HDD) that is flexible to 32 GB, \u201coperating system of 2.2 Google Android with seven-inch touch resistive screen 480 \u00d7 800 resolutions, a 2 USB port, and 3-5mm Headphone jack\u201d (\u201c Motorola XOOM CDMA specs\u201d, par. 3).\n\nBesides, this Pc utilizes 2100mAh Polymer-Lithium battery with a back up that may operate for three hours. Furthermore, the tablet operates with GPRS and Wi-Fi EEE 802.11a/b/g SIM Cellular Modem for its internet access.\n\nThe device has 350 gram Mass and has various formats such as MPEG2, DOC, MP3, XLS, DOCX, JPG AAC, PPT, AVI, XLS, ODP, PPTX, AC3, MPEG2, PPT, WAV, XLSX, FLV BMP, and AVI. In addition, it uses web standard of JavaScript 1.8 compliant and Xhtml. 1.1 compliant.\n\nBesides, it functions as a cell phone when Headphone and SIM card are inserted. These unique features enable Wi-FI and GPRS to work better in Motorola Xoom. Moreover, the device can also function effectively as 3G Tab with a support of Dongle.\n\nSamsung Galaxy\n\nSame as Motorola Xoom, Samsung Galaxy tab uses the android operating system technology. This devise is \u201cmeant for those consumers who desires to expect features in their Smartphone but want to keep the price tag in the range of affordability\u201d (\u201cAndroid advices: Samsung Galaxy Ace plus Specs & Features\u201d, par. 6).\n\nSamsung Galaxy tab has a \u201c3.65 inches touch screen display, 480 x 320 pixels resolution\u201d (\u201cAndroid advices: Samsung Galaxy Ace plus Specs & Features\u201d, par. 8).\n\nThis makes the tablet have a more precise display technology which is of better quality as compared to Motorola tab. Besides, Samsung Galaxy has \u201c1GHz Qualcomm Sanadragon S1 processor, Adreno 220 GPU, 5 mega pixel camera, 2592 x 1944 pixels resolution, and Android 2.3 Gingerbread OS\u201d(\u201cAndroid advices: Samsung Galaxy Ace plus Specs & Features\u201d, par. 12).\n\nIpad2\n\nInterestingly, Ipad2 possess the most advanced technology. This smart phone and computer comes in two models: Wi-Fi, and Wi-Fi+ Cellular. The computer has a maximum storage of 64GB, a retina display, and 2048*1536 resolution pixel functioning at \u201c264 pixels per inch\u201d ( \u201c Ipad2: Technical Specification\u201d, par. 9).\n\nBesides, it comes with a superior \u201cDual-core Apple A5X custom-designed, high-performance, and low-power system-on-a-chip with quad-core graphics\u201d ( \u201c Ipad2: Technical Specification\u201d, par. 5). Same as Samsung Galaxy and Motorola Xoom, Ipad2 has 4.0 inbuilt Bluetooth technology.\n\nIn addition, this devise has a custom designed facial detection device which is inbuilt in its operating system. This technology allows for video and photo \u2018geo-tagging\u2019.\n\nThis operating system supports \u201cHE-AAC (V1 and V2), AAC (8 to 320 Kbps), Protected AAC (from iTunes Store), MP3 (8 to 320 Kbps), MP3 VBR, Audible (formats 2, 3, and 4, Audible Enhanced Audio, AAX, and AAX+), Apple Lossless, AIFF, and WAV\u201d ( \u201c Ipad2: Technical Specification\u201d, par. 11) formats.\n\nMoreover, it has an installed third generation apple TV application. Unlike the other tablets, which only have android technology, Ipad2 can operate with \u201cWindows 7, Windows Vista, or Windows XP Home or Professional with Service Pack 3\u201d ( \u201c Ipad2: Technical Specification\u201d, par. 13).\n\nThus, the operating system and camera resolution of Ipad2 is more advanced than that of Motorola Xoom and Samsung Galaxy. Reflectively, Motorola has a weak resolution among the three devises. Generally, these pcs have adopted Web compliance standards and offers You Tube services/ videos.\n\nComparison in terms of Cost and Size\n\nIpad2\n\nIpad2 is the slimmest of the three devises. This large font, full screen magnification xoom gadget has a height of 241.2 mm, width of 185.7mm, depth of 9.4mm, and weight of 652 g ( \u201c Ipad2: Technical Specification\u201d, par. 7).\n\nReflectively, Ipad2 has a sleek cover made of fiber material. In comparison, Ipad2, Motorola Xoom, and Samsung Galaxy share a splendid aluminum body. The Ipad2 costs $350 dollars in the market.\n\nSamsung Galaxy\n\nSamsung Galaxy measures 62.5 x 114.7 x 11.2 mm, and weighs 114 grams. Reflectively, as compared to Ipad2, the Samsung Galaxy smart phone looks trendier, though made of plastic and aluminum body.\n\nSamsung Galaxy cost $300 on average in the market (\u201cAndroid advices: Samsung Galaxy Ace plus Specs & Features\u201d, par. 1).\n\nMotorola Xoom\n\nMotorola Xoom is trendier, sleeker, but relatively heavier as compared to the other gadgets. This third generation computer weights 700 grams and share aluminum body.\n\nHowever, it is bulkier than the other two gadgets. On average, Motorola Xoom costs $250 in the market (\u201c Motorola XOOM CDMA specs\u201d, par. 3).\n\nFrom these data, it apparent that Ipad2 has the highest price tag. However, the variance in price tag can be linked to performance, and longer battery life. Since Ipad2 has the highest number of applications, and relatively longer battery life, the high price is justified.\n\nComparison in Terms Performance and Application\n\nIpad2\n\nIpad2 smart phone and personal computer boast of autofocus face detection, video and photo geo-tagging, high performance sound, and connectivity system.\n\nBesides, it has a \u201cbuilt-in 42.5-watt-hour rechargeable lithium-polymer battery\u201d ( \u201c Ipad2: Technical Specification\u201d, par. 9) which can support up to ten hours of web surfing, watching video, and playing music.\n\nBesides, it can be charged via a USB port or a power adapter. Its accelerometer, gyroscope, and ambient sensor make this gadget the best performer among the three.\n\nSamsung Galaxy\n\nReflectively, \u201cGalaxy Ace Plus comes with both 2G and 3G internet access along with Wi Fi wireless internet access as well\u201d (\u201cAndroid advices: Samsung Galaxy Ace plus Specs & Features\u201d, par. 3).\n\nThis environmentally friendly gadget has an internal memory of 4 GB and a micro SD which can support up to 32 GB of data. These features make Samsung Galaxy a reliable gadget.\n\nMotorola Xoom\n\nMotorola Xoom has a 1300 mAh battery \u201cwhich provides sufficient battery life to your phone and as it has Android 4.0 which consumes lesser battery than Android\u2019s earlier versions\u201d (\u201c Motorola XOOM CDMA specs\u201d, par. 7).\n\nWith this modification, it provides up to 7 hours of talk time with a battery life of 420 hours.\n\nConclusion\n\nAfter examining features and specification of these smart phones cum computers, it is apparent that Ipad2 boast of the most advanced technological applications. Besides, it is the slimmest, sleekest, lightest, and most attractive.\n\nGenerally, these smart phone computers are designed as alternative computer devices with affordable price tag and efficient performance compared to existing models.\n\nDefinitely, they are perfect digital devices for \u2018any place and any time\u2019 because of their Android application and sports, web accessibility, multimedia content, high camera resolution, and thousands of active applications.\n\nThese products have gone a long way to promote digital and flexible applications that have direct impacts on lives customers due to their affordability and very effective computing devices of the third generation androids.\n\nWorks Cited\n\nAndroid advices: Samsung Galaxy Ace plus Specs & Features . Web.< http://androidadvices.com/htc-samsung-galaxy-ace-specs-features-comparison/#.UBN5LcWr7IU >\n\nIpad2: Technical Specification . Web. < https://www.apple.com/ipad/ >\n\nJones, Robert. \u201cE-Books and the Tablet PC.\u201d Language, Learning & Technology , 7.1 (2003): 9-10. Print.\n\nMotorola XOOM CDMA specs . Web.\n",
        "label": "human"
    },
    {
        "input": "Apple Computer Inc. Marketing Report\n\nIndustry background\n\nEssentially, international trade is often a characteristic of the computer industry, rather than a mere contributing factor (Bardhan, Jaffee & Kroll, 2004). Currently, even the smallest businesses have become active in marketing their products or services internationally. Personal computing has emerged to be the most important contributing factor to overseas labor employment due to production costs.\n\nThe continued increase in productivity within the computing industry is steered by the continued advancements in technology. Efficiency and other achievements result from the ever changing technology for the better.\n\nAmong the major product lines in the electronic industry are the computers which also form the core products of the electronic companies. However, Apple\u2019s computers have lost significant demand due to the limitations the company experiences from complementary products.\n\nHewlett-Packard and Dell have continued to dominate the computer market with their products having a higher demand than other competitors. Compared to Apple, firms like Dell and HP are substantially bigger and provide substitute products which attract many consumers due to their compatibility. Sony, Samsung and LG have focused more on consumer electronics such as televisions, digital cameras, mobile phones, gaming consoles, DVD players and digital speakers.\n\nCompany background\n\nPreviously known as Apple Computer Inc, Apple store is a global company that produces computers, software and consumer electronics. Its major products are personal computers, iPhone, iPod, iPad and iTune Store. The company was established in 1976 by two electronic engineers, Wozniak and Jobs, who released the first computer called Apple 1.\n\nSince then, Apple store has dominated the personal computer market through innovative products such as the Power Mac and Macintosh, but encountered marketing problems in the 90s. Through Steve Jobs, the firm seeded innovative technologies that led to the introduction of new products such as the iPhone, iPad and iTune music store.\n\nThe company became the new market leader in consumer electronics with the latest success coming from a range of products including the iPod Touch, iTunes, the iPhone, and the iPad (Apple Inc., 2011). Presently, the company is the global leader in technology with revenue of $60 billion after overtaking Microsoft in terms of market capitalization in 2010.\n\nSony was founded as a telecommunication engineering company by Akio, Masaru and Tamon in 1946 (Dogruer et al, 2001, p.4). The firm immediately adopted transistor technology to produce radios, home video recorders and microphones. Due to pressures from competition, Sony diversified beyond consumer electronics and started producing computer chips and computer peripherals. Later, the company entered the gaming machines after Nintendo became very successful in the industry.\n\nDue to rapid growth through mergers, Sony expanded to other major markets for consumer electronics such as Europe and America. So far, the company has focused on consumer electronics and in particular televisions, digital cameras and game machines. The company is also a major producer of personal computers and computer microchips.\n\nProduct overview\n\nThere are four major product segments that defines Sony\u2019s portfolio. They include electronics, games, computers and pictures (Tang, Misra & Shanholt, 2012, pp.16-18). The electronic product line consists of products like audio electronics, digital cameras, televisions and speakers.\n\nIn this line, advanced technologies like Blu-ray and LCD technologies are the key differentiating factors. The games product line consists solely of the Play stations which are differentiated through enhanced technology that allow for superior graphics and increased computation power. The pictures category involves movies while the computer segment includes PCs, computer peripherals and computer microchips.\n\nApple\u2019s product and services segments involve a wide portfolio of Smartphone, personal computers, music store, book leaders and other applications. The key products and services include iPhone, iPad, iPod, Mac, software applications, televisions, iCloud, iOS and Mac OS (operating systems) as well as a variety of services, accessories and support services (USSEC, 2011, p.1).\n\nIn addition, Apple tenders moderator products for instance iPhones, iPads, iPods, Mac and attuned merchandises as well as PC appliances, imprinter, amplifiers, storage devises and supercomputer peripherals. The firm also offers digital content and applications via the App Store, iBook Store, iTune Store and Mac App Store.\n\nStrategic marketing\n\nStrategic marketing is a process that allows a business to focus its limited resources on the greatest opportunities in order to achieve sustainable competitive advantages and thus increase sales (Baker, 2008). The focus of Sony strategy is to ensure that its products are of high quality and develop profitable and long-term loyalty with customers.\n\nThe corporate marketing strategy of the firm is to build awareness before launching a product. International market success depends on standardization/adaptation to local markets. Sonny standardized its products by manufacturing Japanese electronics and adapted local promotions campaigns to build awareness.\n\nApple uses Apple brand as the competitive strength in the highly competitive market such as the PC industry with its Macintosh computers and associated software. The product strategy is to develop innovative products in line with a \u2018digital hub\u2019 strategy such that the firm\u2019s products work as digital hub for other devices (Apple Inc.). The branding approach puts emphasis on sentiments such that the brand is about thoughts, routine, novelty, enthusiasm, dreams, autonomy and desires.\n\nMacro-environmental analysis\n\nRivalry between competitors\n\nWhile an industry characterized by few large manufacturers and very expensive products hunts a niche audience, the evolution of personal computers has caused an explosion in the industry which currently includes dozens of companies pursuing millions of consumers across the globe.\n\nDue to the use of electronic equipments in day to day lives, the equipments are becoming more of commodities (Sony Corporation, 2010). For most consumers, economy is more important than performance specifications. This has forced many manufacturers to pursue best-cost and low-cost provider strategies.\n\nThe companies in the middle range compete for consumers by offering various options at varying prices. The similarity of electronic products because of industry standard setting leads to competition in prices that squeeze margins and drive prices down. Moreover, technological development presents increased competition on the industry.\n\nElectronic products are becoming more and more sophisticated, efficient with less energy consumption. To hasten the situation, industry leaders like Apple, Sony, Microsoft and Nintendo have always kept to this pace.\n\nSupplier power\n\nSuppliers have significant influence over the manufacturers of electronic equipments. While many standards in the industry are open and made by collaborative or independent bodies like IEE, manufacturers also depend on standards owned privately. Often, the technology requires entrance or must be licensed to a paid-membership group. Additionally, the virtual concentration of suppliers puts some pressure on competitors.\n\nFor personal computers, Sony outsources central processing units from Intel. Thus, incompatibility in design does not allow the company to switch to other suppliers. However, microchip manufacturers are not present in computer manufacturing business and hence rely on computer manufacturers for their business. As a result discounts are placed on exclusivity agreements and bulk purchasing thus lessening some pressure from suppliers.\n\nBuyer power\n\nAs in all businesses, customers have the last say and apply considerable leverage over manufacturers. The various buyers with many but differing needs impact manufacturers of electronics directly since they must react and adapt by offering extensive product-lines.\n\nThe notable characteristic between power user looking for the \u2018greatest and latest\u2019 and average user needing a product which is \u2018just good enough\u2019 presents an obstacle to manufacturers focusing on tapping the many customers possible. Manufacturers benefit from the wide use and acceptance of electronic products causing high demand for the products. Brand loyalty and proprietary systems are used to maintain customers.\n\nThreat of potential entrants\n\nMany avenues through which new entrants can enter the electronic market exist; however several hindrances may prevent the entrance. The current market is characterized by well entrenched players who have significant brand loyalty and recognition including Sony, Microsoft, Apple, Samsung and LG among others (Sony Corporation, 2010).\n\nThus developing a successful brand in the presence of such players is difficult. They also keep costs down and dominate with economies of scale that cannot be achieved easily by a new entrant unless a substantial capital is invested. Despite these obstacles, new entrants may still come from newly instituted lean organizations which depend on third party OEM (original equipment manufacturers) for production. This trend is increasing in popularity bringing companies such as Acer Inc. into competition with industry giants.\n\nThreat of substitutes\n\nThe technology sector has substantially grown with the introduction of alternatives such as televisions, game consoles, Smartphone, virtual music stores and personal computers. There are now feature-rich smart phones and PDAs in an average living room.\n\nSuch developments have resulted in consumers focusing their \u201ctechnology money\u201d away from computers, televisions, radios and game consoles. However, despite the focus it is not likely that smart phones will diminish the importance of other products significantly. The only thing that the substitutes have influenced is elimination of monopoly in the electronic market.\n\nMicro-environmental analysis\n\nApple and Sony are leaders in the electronic industry but market their products in the same markets. The firms are probably influenced by the same micro-environmental factors. They are expected to respond to the dynamics of both individualistic cultures of the west and collective cultures of the east.\n\nThis is probably achieved by creating organizational cultures that support both team work and individual efforts in the respective areas of operations. As the firms become increasingly global, Du Plessis and Beaver (2008) insist on the need to develop an effective International Human Resource Management (IHRM) in order to respond to labor sourcing and expatriation issues.\n\nConsumers are also becoming more sensitive to prices and quality hence compelling Apple and Sony to be more responsive through effective quality control systems. Involvement in Corporate Social Responsibility (CSR) is also a strategy that Apple and Sony are using to improve their relationship with consumers.\n\nSWOT analysis\n\nSony SWOT analysis\n\nStrength                       Weaknesses             \nMature value chain             Weak financials        \nStrong brand name              Lack of focus          \nIntellectual property holding  Conservative management\nOpportunities                  Threats                \nNew Chief Executive Officer    Stiff competition      \nEconomic recovery              Macro-economic factors \nIndustry integration           Partnership            \n\n\nApple SWOT analysis\n\nStrengths              Weaknesses                   \nGlobal presence        Low returns                  \nStrong brand image     Labor issues                 \nSynergistic portfolio  Dependency for key components\nStrong media content                                \nOpportunities          Threats                      \nWireless product       Strong competition           \nNew digital platform   Slow Euro-zone economy       \nMP3 player market      Lawsuits                     \n\n\nMarket segmentation\n\nSony Corporation segments its market in three divisions namely the consumer, professional and devices segment, networked products and services segment and financial services segment. Televisions lead in the first segments while game and PCs lead in the second segment.\n\nSony Corporation has also segmented its markets according to geographic regions. These regions include Europe, America, Asian-Pacific, Africa and Latin America. For the Asian-Pacific, European and American markets, the firm markets its major product categories including household electronics, computer hardware and software, computer microchips, mobile phones and gaming consoles. For the rest of the markets, the firm markets only its household electronic goods and computer hardware.\n\nFor several years, Apple has identified four major segments within its customer profile including business, education, creative professionals and high-end consumers. At the turn of the millennium, the company\u2019s core product strengths were in the product segments of Computer Aided Design (CAD) and Desktop Publishing (DTP) (Ashcroft, 2011, p.5).\n\nBy the end of 2010, the situation turned upside down. The core product strengths were now the Personal computers, Smartphone, book readers, iTune store and applications. Market segments widened to include households, celebrities and sports fanatics. All these segments are continuously pursuing whatever information technology offers to their careers and lifestyles.\n\nTarget markets\n\nApple store sell its products to a wider range of markets and across many parts of the world. The company uses retail stores and online sites, to offer its goods and third party goods to the consumer and business segments. Apple has invested in programs that promote reseller sales such as the ASCP which focuses on certain reseller locations and APRP which provide expertise in products and customer service (USSEC, 2009).\n\nIn addition, the company offers its computer products in the government, business and creative markets within its locations. The markets are particularly crucial to third party developers of computer applications and hardware that are compatible with Apple\u2019s computers.\n\nThe superior expansion and computing performance of Apple products are the major attractions for consumers within this market. Moreover, Apple offers its computer products to customers in scientific market as well as information technology markets.\n\nPositioning strategy\n\nThrough product differentiation, Sony is positioned as the most convenient, quality-focused, value-added and technological producer of consumer electronics goods. The focus of Sony strategy is to ensure that its products are of high quality and develop profitable and long-term loyalty with customers.\n\nIndeed, this positioning strategy has enabled the company to sell its products at premium prices even in primitive markets in Africa. The corporate marketing strategy of Sony is to build awareness before launching a product. International market success depends on adaptation to local markets. Sonny standardized its products by manufacturing Japanese electronics and adapted local promotions campaigns to build awareness.\n\nThe success of Apple Company has been credited to its effort in innovating new products. In fact, while many companies experienced decreased revenues during the depression period, Apple continued to increase its revenues throughout. Up to date, the company has invested generously in innovative development of its products.\n\nApple has continually put innovation ahead of all its plans and strategies. This can be witnessed in the current wave of touch screen technology in most of its devices. They have come up with not only iPhones, iPods and iPads with touch screens, but laptops with no keyboards (Apple Inc.). These laptops use a very unique technology in which a user can complete computer tasks like printing or typing with just a few spins of a disc.\n\nHowever, this is not the only new innovation; just recently the company released the world thinnest laptop challenging all of its competitors. The Macbook Air is so thin that it can fit inside an envelope. For the last several years, the company has surprised the market with new innovations starting from its iPhone, iPad, ipod, laptops, desktops, iTunes, and the recent Macbook. This is a clear indication that innovation is the core positioning strategy for Apple.\n\nRecommendation\n\nThe changing consumer trends and market shifts are requiring many companies to institute strategies that will respond to these dynamics and ensure that the firms achieve competitive advantages. Among the trickiest industry is the electronic industry which is characterized by big firms that are swift in responding to the marketing changes.\n\nIn this industry, Apple Inc. and Sony Corporation can only adopt the best-cost strategy to increase their competitive advantages since consumers are increasingly becoming price sensitive and many competitors are pursuing low-cost strategies. This strategy demands the integration of developmental and marketing plans which focus on the best value while reducing the prices of products.\n\nWithin this strategy, the firms should retool their products to attract a wider range of consumer segments while maintaining low prices for the majority low income earners. Avoiding unexpected risks such as those from natural calamities is important. Therefore, the firms should include stakeholders such as OEM partners to distribute the impacts caused by such calamities. But the best way for the companies to position themselves better in the marketplace is to stay ahead in terms of price and quality.\n\nReference List\n\nApple Inc, The new, faster MacBook Air . Web.\n\nAshcroft, J 2011, Apple Inc. The case study 2000-2010 . Web.\n\nBaker, M 2008, The Strategic marketing plan . Cambridge Strategic Publications Limited, London.\n\nBardhan, A, Jaffee, D & Kroll C 2004, Globalization and a high-tech economy: California, the United States and beyond . Springer, New York.\n\nDogruer, B, Ferzly, M, Nguyen, H, Roach, D & Ward, R 2001, Report on Sony Corporation . Web.\n\nDu Plessis, A J & Beaver, B 2008, \u201cThe changing role of human resource managers for international assignments.\u201d International Review of Business Research Papers , vol.4 no.5, pp.166-181.\n\nSony Corporation 2010, Annual Report 2009 . Web.\n\nTang, H, Misra, R & Shanholt, E 2012, Sony Corporation . Web.\n\nUSSEC 2009, Form 10-K: Apple Inc . Web.\n\nUSSEC 2011, Form 10-K . Web.\n",
        "label": "human"
    },
    {
        "input": "The Future of Human Computer Interface and Interactions Essay\n\nTable of Contents\n 1. Introduction\n 2. The Future of Human Computer Interface and Interactions\n 3. List of References\n\nIntroduction\n\nThe society today has completely changed due to technology. Technology is changing at a very rapid rate and with the changes come the need to adapt to them. Computer has changed the way human being does his activities (Beaureau 2008, p.36). Unlike before when most of the activities were done manually, computer has enabled automation of most of activities, especially in large companies.\n\nIt is now possible for a manager to monitor activities taking place in a different company\u2019s branch from where he or she is by use of computerized gadgets. Management and other duties have been redefined by introduction of the modern day gadgets that are computer controlled.\n\nThe Future of Human Computer Interface and Interactions\n\nHuman Computer Interface refers to the interactivity between computer and people (Sutherland, Robertson and John 2009, p.49). Unlike other gadgets that do not communicate with the user, computer is the only tool that will have a direct communication with the user. This interactivity is made possible by both the software and the hardware.\n\nThe user passes the communication by use hardware like mouse and keyboard and receives the communication through characters displayed on monitor, or through sound. However, this method of communication is only reliable to people who are not handicapped. People without hands and those that are mentally handicapped may not be in a position to operate the traditional computers properly (Rodgers and Streluk 2002, p.98).\n\nHowever, this may be changing very soon. Dr. Eric Leuthardt and a group of other scientist have developed a new computer interface that would accommodate the physically handicapped.\n\nThis interface allows one to control the computer using the brain. The computer is programmed to read the mind and respond to the demands of that mind. By using the power of their thought, these physically handicapped individuals are able to control the cursor to issue commands to the computer. This interface will also benefit individuals with spinal code injuries or paralysis\n\nThis invention is so sophisticated as in makes it possible for anyone to use this gadget regardless of the physical challenges that one may be having. Moreover, it comes with speed as the commands will be issued as soon as they come to the mind.\n\nWhen iPad was launched, everyone was asking what the next invention will be. A group of Australian scientist has come with a new invention that is very similar to that of iPad. It has the ability to read anything placed on it. This makes it very appropriate in places that require high level of security like in the airports or the five star hotels. It can also be of good use in places like supermarkets in the billing section.\n\nThe future of human computer interface and interactivity is already here. Life is becoming easier with every technological invention. This has a positive impact both in the short and long run, especially in the field of entertainment and digital divide. Human being will also be able to delegate much of his duties to computers.\n\nA keener look into this phenomenon will reveal that in as much as these inventions are necessary and that they have positive impact on mankind, it is also true that they come at a cost. The effect of these sophisticated machines on the environment is adverse, especially when poorly disposed (Abbot 2001, p.79). These inventions will also impact negatively on culture, as life becomes what one wants, regardless of age. It is therefore necessary to take care as we embrace this technology.\n\nList of References\n\nAbbot, C 2001, ICT: Changing Education, New York, Routledge.\n\nBeaureau, B 2008, Information and Communication Technology: The Industrial Revolution That Wasn\u2019t, New York, Lulu.\n\nRodgers, A, and Streluk, A 2002, ICT Key Stage 1, London, Nelson Thornes Ltd.\n\nSutherland, R, Robertson, S and John, P 2009, Improving Classroom Learning with ICT, New York, Routledge.\n",
        "label": "human"
    },
    {
        "input": "Access to Information Communications Technology in the World Essay\n\nTable of Contents\n 1. Introduction\n 2. Attitudes\n 3. Ethics\n 4. Works Cited\n 5. Links\n\nIntroduction\n\nThe growth of Information Communications Technology in the last decade has ensured many people access to computers. In the developed world especially North America and Western Europe, access to computers by households is almost 100% whereas access to computers in the third world especially Africa is low.\n\nDifferent races therefore have varying rates of access to computers by virtue of residing in those places. It is safe therefore to say that black people of Africa have the lowest access rate to ICT in the entire world.\n\nIn the developed world for instance the US where the population comprises many people of different races, the access rates to computers are not similar. In the US, African Americans and Latinos have the lowest access rate to computers compared to Caucasian Americans.\n\nThe disparity that exists in scenarios such as the above is more or less influenced by the economic status of the communities. Their attitudes therefore are shaped by the prevalent situations as far as access to Information Technology is concerned.\n\nAttitudes and views on computer technology by different races will form the subject matter of this paper. Besides, the paper will tackle the effect of race on how people may evaluate the ethical impacts of computer technology.\n\nIt is safe to argue that the term computer ethics is somehow limited in its use in the era of ICT. Cyber ethics has been fronted by scholars as the right term to use. However, for purposes of definition, this paper will stick to the use of computer ethics. Though the ethics are universal, people of different races expose them to diverse versions.\n\nThat interpretation and how it affects the ethics of computer technology will form the second part of this paper.\n\nAttitudes\n\nThere is a significant difference in the access and ownership of personal computers and access to computers between white people and black people. The differences no doubt come from influence the impact that race has on computing.\n\nAccording to Donna, L. Hoffman and Thomas, P. Novak of Vanderbilt University, white people, 27.2% are more likely to own a home computer and a personal computer compared to black people 16.7%.\n\nOn the other hand, black people are more likely to states that they will acquire a home computer in the near future (3). The same trend is seen in internet access where more whites are likely to have used the web compared to African Americans (6).\n\nAccording to Mossberger et al, income, education and occupation do affect the attitudes adopted by different races in regard to Computers and ICT (6).\n\nHowever, differences in motivation and cultural perceptions did play a big role in the differences exhibited on the access of computers between blacks, whites and Latinos.\n\nThe feeling here is that besides the economical restriction imposed on African Americans and Latinos, a significant number in their populations don\u2019t deem as important the use of computers and access to the net.\n\nHowever, there is another interesting twist to the effect of race on the digital divide and the resulting attitudes. According to Karen Mossberger et al quoting Mossberger et al, \u201cAfrican Americans and Latinos have a more positive attitude towards computer technology compared to Caucasians of similar situations\u201d (2).\n\nThis despite the former two authors being at the lower end of the digital divide in the US. They further add that the positive attitudes are distributed across multiple topics especially among African Americans.\n\nThe attitude among Americans, according to Mossberger et al, is that computers are necessary in order to keep up with time. Latinos and African Americans are likely to agree with statements such as the above than whites of the same situation.\n\nMossberger et al further say that African Americans contend that one needs computer skills to progress. They are also more likely to learn new computer skills than white people.\n\nIn another setting, Mossberger reveals that African Americans are more likely to use an online service in job search compared to whites. In another development, African Americans are likely to defer the use of the internet and are likely not to use it presently but plan to use it \u201csome other day\u201d.\n\nEconomic status can be used to explain the view on computer technology by the above races. Data collected by the National Census Bureau suggest that African Americans and Latinos rank lower than whites, economically.\n\nThe low ranked races therefore express overwhelming desire to learn about computer technology compared to whites who already have it. The low levels of penetration among Blacks and Latinos have generated a sense of urgency among these communities.\n\nOn the other hand, there seems to be complacence on the part of whites. The complacent attitude is either due to the availability of the technology and services to the white population or high levels of contentment with what they have.\n\nEthics\n\nComputer ethics are basically the moral rules governing computer technology and profession. The need for computer ethics arises because there is a vacuum created by the absence of laws governing computer technology (Stamatellos 2).\n\nAccording to Bynum et al, the inequalities that exist between different races may result in the misuse of information technology (182). Ethics require that all people in spite of their culture, gender and race have the same opportunity to share and benefit from the use of ICT resources.\n\nIn most cases however, the above does not happen. The aggrieved races therefore tend to have a skewed view on ethics and in some cases they may disregard them.\n\nThe impact of race on computer ethics is somehow subtle. Every participant in the computer technology world expects equal treatment and access to the resources.\n\nThe positive sentiments expressed by different races about computer technology can be interpreted to mean an acknowledgement of the importance of these ethics by these communities. The positive sentiments endorse the view that these communities who have a history of segregation may be warming up to computer technology as a tool to express and emancipate themselves (Webster 96).\n\nGiven that there is increased anonymity associated with the internet; people of all races have embraced it as a tool for expression and an important tool in communicating with authorities.\n\nMisuse of computer technology presents a major challenge in the upholding of the ethics that govern this industry. The internet for instance is laden with thousands of hate messages targeting different races.\n\nThe anonymity associated with the internet is particularly the chief contributing factor. There is no data however to suggest that a particular race has a negative view of computers and the technology associated with it due to misrepresentation of facts about their respective races. It is safe therefore to conclude that most people regardless of race uphold cyber ethics on consideration of the broader benefits.\n\nWorks Cited\n\nBynum, Ward et al. Computer ethics and professional responsibility. Oxford: Blackwell Publishing, 2004.\n\nHoffman, Donna and Novak, Thomas. Bridging the Digital Divide : The Impact of Race on Computer Access and Internet Use. 1998. Hoffman and Novak co-direct Project 2000. Web.\n\nMossberger, Karen et al. Race, Place, and Information Technology. 2006: Urban Affairs Review, Vol. XX, No. X, Month 2006 1-38 DOI: 10.1177/1078087405283511, Sage Publications.\n\nStamatellos, Giannis. Computer ethics: A global perspective. Jones & Bartlett Learning. N.d. Webster, Frank. Culture and politics in the information age: a new politics? New York: Routledge-Taylor& Francis Group, 2001.\n\nLinks\n\n 1. Computer Ethics and Professional Responsibility .\n 2. Bridging the Digital Divide .\n 3. Computer Ethics: A Global Perspective .\n 4. Culture and Politics in the Information Age: A New Politics?",
        "label": "human"
    },
    {
        "input": "Computerized Physician Order Entry System Research Paper\n\nProper medical care is a basic need for all humankind. The distribution of quality medical care is dependent on many factors. Well-trained health personnel, health administrators and an active support staff are a necessity. However, in spite of their professionalism, errors still arise (Bates, 2002).\n\nThis necessitates adoption of IT to help in tracking patients\u2019 medical history and related literature. While this shared resources addressed many problems, it failed to address other relevant sources of errors.\n\nMisapprehension of handwritten instructions, dosage errors due to illegible decimal places, unnoticed drug interaction and dissimilarities in medical practice were common with traditional methods (Williams, 2002). CPOES is a computer system that addresses these problems. This paper explicates its meaning, how to develop and implement it, and its use in hospitals and other medical institutions. It also addresses such issues as legislation governing the use of CPOES and its potential success.\n\nDefinition of a CPOES\n\nCPOES is a computer based application program. It allows doctors and other lawful users\u2019 direct entry into a computer, analytical tests, medications, patient care, and referrals (Bates, 2002). It forms part of a broad medical database called health information system (HIS). HIS is a special catalog that contains all the information regarding patients in a given region.\n\nDepending on the kind of information structure desired by the authorities, HIS can be specific to a given health institution only, a database serving a state or an entire country. In this regard, CPOES performs the function of an access medium to the centralized database.\n\nGenerally, CPOES is an application program installed in computers used in hospitals and ambulances. Usually, there is a constant need of recording medical data. According to Dixon et al. (2009), CPOE is software that ensures orders are perfectly captured and implemented. It provides a guarded way of making orders and subscription.\n\nLike any computer application program, it requires the support of other programs to accomplish its functions. In hospitals, highly robust software called \u201cclinical decision support system\u201d exists with CPOES. Clinical decision support system (CDSS) is robust data crunching software that helps in analyzing the orders from CPOES to ensure correct input from the Doctor.\n\nCPOES prompts doctors for inputs through a window and displays the required output. CDSS logically analyze theses orders in the background and forwards the desired output to CPOES for display. This help Physicians reduce improper drug prescription and other faults while at the same time lowering costs.\n\nTechnology used and System development\n\nThe development of a CPOES is a complex procedure. Like any other application program, it undergoes the application development life cycle. Firstly, it starts with problem definition. This entails comprehensive statements describing CPOES. The second stage is the program design. This involves outlaying stepwise Structural description of the CPOES. The next stages involve coding the program, debugging it, testing, and documentation. Finally, maintenance and offering extension redesign occur.\n\nThe whole process necessitates specialists with vast knowledge in the fields of IT and medicine. The physician involved at the design stage is responsible for the required medical inputs. He helps in identifying the needed medical procedures for coding. In Williams (2002), the physician-driven CPOES design is preferable.\n\nIn this design, the three core activities that occupy a physician\u2019s daily life are considerable. Initially, the provision of patient care, which involves activities like clinical procedures, filling in patient information, and communication to patients occur.\n\nSecondly, physicians need to ensure that they comply with the requirements of their professional associations. Lastly, they need to manage their time properly, balancing between administrative and patient care activities. These activities provide the primary data and information required in designing the CPOES.\n\nHospitals already have a HIS. This is a database containing the patients\u2019 health records. In most nations, there exists a national database containing the health records of all citizens. In such a case, the national database provides the backbone on which the CPOES exists. In this manner, the CPOES is crucial in updating, editing, and displaying the records in the database.\n\nAccording to Ferranti (2011), a typical CPOES comprehensively manages orders that encompass vast medical disciplines. Such a system should provide an interactive view that enables robust data entry and manipulation. The standard java-based desktop client is usable to query the HIS server.\n\nThe HIS server is able to run on an oracle 10 database. Oracle DB 10 is a relational database. Charts and tables containing patient information need storage in the database. Controlled access to the system ensures that only accredited professionals manipulate its contents.\n\nSystem use\n\nThere are enormous uses of the CPOES in a hospital setup. To begin with, they enable physicians to make orders directly into a computer instead of writing on a paper. Secondly, CPOES have an embedded clinical decision support system that helps in verifying orders. In this way, medical errors resulting from orders are stoppable.\n\nThe clinical order support system ensures adherence to the stipulated standards. This uplifts the service quality offered by the facility. Centralized databases are a common resource accessed by all departments. This augments interdepartmental communication (Kuperman, 2003). The data captured by the CPOES is particularly vital in research, excellence evaluation, and judgments by the management.\n\nThe system Users\n\nIn Bates (2002) report, CPOES provides services to many professionals. Firstly, Physicians finds it an indispensable tool for making and evaluating orders. Hospital administrators use it to extract information vital for decision-making. It offers information regarding quality of service. The representation of patients\u2019 data in tabular form makes it easy to comprehend and sketch conclusions.\n\nLegislation Surrounding its Implementation\n\nAccording to Williams\u2019 (2002), the use of CPOES must follow the regulations concerning the use of IT in health facilities. It helps in respecting the patients\u2019 privacy. Additionally, doctors should hold to their certified code of conduct constantly.\n\nThe potential success of technology in the future of HIS management\n\nIn the past two decades, the implementation of health information systems has met several barriers. These barriers incorporate inadequate financing, unclear implementation road maps and the existence of incompatible technologies. Presently, there exist health statistics systems in hospital (Bates, 2002).\n\nHowever, the adoption of the CPOES is facing similar hitches encountered the past decades. Commercial CPOES have their own undoing. Majority are sluggish. Secondly, the need for customized applications by different institutions has proved hugely expensive. Off the shelf CPEOS, still require a lot of modification to suit hospital needs.\n\nDespite that, the enthusiasm of enhanced quality, easier ordering and access to information that shapes decisions has swayed a few hospitals to adopt it. According to Bates (2002), to enhance the acceptance and adoption of PCOES a different direction should is necessary.\n\nThe negative views by some doctors are changeable through enhanced research on the use of CPOES. Improvement of the technology is also required to make it faster. This will make using CPOES more enjoyable and enhances its value. Lastly, appropriate legislation is required to entrench CPOES in the health provisions (Bates, 2002). In Dixon\u2019s findings, the training of health experts on CPOES is extraordinarily important.\n\nHe argues that health workers would readily accept a technology they recognize. Secondly, he noticed that some institutions were understaffed. The problem of under staffing meant that few workers who can afford the time to learn about CPOES.\n\nA systematically designed workflow would make the adoption of CPOES easier. Other factors considered comprise systematic resource allocation within the institution and working with vendors to uplift the performance of CPOES. Future technical follow-ups are vital for the application to provide a long time of service.\n\nSystem Implementation Plan\n\nTo implement a CPOES in a physician office setting, there should be the consideration of certain factors. An analysis of the current system is vital. This analysis aims at establishing the compatibility of this current system. The results would help establish whether a complete system overhaul might is compulsory.\n\nThe second stage is to reorganize the old system. This involves aligning it to the new system. In addition, converting data and file formats to the new format (Bates, 2002). The next phase involves hardware and programs installation. Train staff on the new application and its operations. Show the difference between the old system and the new one. Offer advice on new specialized staff if need be.\n\nThese are Staff with an excellent understanding of medical and information technology procedures. Personnel involved in the procedure from the onset of execution would have additional advantage. The workflow as defined by the CPOES is truly vital in its acceptance. CPOES changes among entail, authorization procedures, review and execution of orders.\n\nThe physician\u2019s office is a decidedly busy place. Thus, it is imperative to allocate time allowing other operations within the premise. Maintaining of contact with vendors and suppliers is vital. The customer should possess a user manual of the system allowing internal resolution of the hitches that emerge.\n\nReferences\n\nBates, W. & Doolan, D. (2002). Computerized Physician Order Entry Systems in Hospitals: Mandates and Incentives . Web.\n\nDixon, E. & Zafar, A. (2009). Inpatient Computerized Provider Order Entry (CPOE) Findings from the AHRQ Health IT Portfolio . Web.\n\nFerranti, M., Horvath, M., Jansen, J., Schellenberger, P., Brown, T., DeRienzo, M. & Ahmed, (2011). Using a computerized provider order entry system to meet the unique prescribing needs of children: description of an advanced dosing model . Web.\n\nKuperman, J. ( 2003). Computer Physician Order Entry: Benefits, Costs, and Issues. Annals of Internal Medicine, Vol. 139.\n\nWilliams, B. (2002). Successful Computerized Physician Order Entry System Implementation: Tools to Support Physician-Driven Design and Adoption. HealthCare Leadership&ManagementReport . Vol. 10. Is. 10.\n",
        "label": "human"
    },
    {
        "input": "Ethics and Computer Security Essay\n\nAbstract\n\nThe world we live in today is very different compared to the one our fore fathers lived many years ago. It is clear that tremendous change has taken place over the last ten decades, and whose impacts has been felt in all the facets of human existence such as social, political and economic.\n\nOver time, man has been trying to employee various means in order to improve his living standards through innovation and inventions of new machines and equipments with the aim of enhancing production in the society.\n\nThe involvement of machines in the process of production has made it possible for many nations in the globe to produce enough food, treat disease, eradicate the menace of poverty and facilitate capacity building of the people through education, to mention but a few.\n\nThe introduction of computers and the internet in the 1970s marked the end and the beginning of a new era where human labour was no longer required in the production of goods and services.\n\nWorkers were replaced with machines and in the process rendered jobless. But today computers are used to perform a large volume of workforce in organizations, such as storage and processing of data, coordination and planning of duties, controlling and regulating of operations and retrieval of information if need arises.\n\nAt this point in time we can despite the fact that information revolution has altered to a larger extends various components of human existence such as employment, education, production, religion, human relations, family structure and democracy. In order to counteract the impacts of information communication technology (ICT) in the society, researchers developed computer and information ethics body that is essentially an applied scientific body (Weckert & Adeney 1997).\n\nIt can be defined as the scientific approach that is mandated with the obligation of laying down the frame work of operations of professionals using technology at the work place. The involved state holders have devised moral codes and regulations that affirm how professionals should safeguard the welfare of the organization using computers by explicating good practices within the workplace.\n\nMilestone in the establishment of computer and information ethics\n\nIt was during the beginning of 1940s when efforts geared towards innovations and advancement in science and technology kicked-off. More resources in terms of finance and human resource were mobilized towards the creation of new machines and computers that were going to be of great importance in alleviating the living conditions of the people.\n\nWith regard to this, there was the need to establish a new body that constituted the knowledge in science and philosophy that aimed at impacting good moral practices of professionals at the workplace.\n\nNorbert Weiner is regarded as the founder of the new philosophical field in the 21 st century. He was an American scholar, as well as a professor specialized in mathematics and engineering at Liz University. His effort was realized during the World War II when he made electronic computers and other new technologies that were of great help during this moment in time.\n\nTogether with his colleges from other parts of the world, such as Great Britain and Europe, Weiner brought to board a new scientific body referred to as \u2018\u2019cybernetic\u2019\u2019 that was going to address the impacts of technology after the World War.\n\nIn his opinion, he predicted that the world was going to undergo a second revolution that was to be initiated as a result of advancement in science and technology. According to him, creation of cybernetic body was the only effective way to minimize the challenges obscured opportunities that laid a head after the World War II.\n\nAfter critically evaluating the social economic impacts that emerged after the World War, he decided to publish books and other literature materials that were going to inculcate knowledge and skills on the people regarding the concept of information ethics.\n\nHowever, Weiner in his early publications never coined the term computer ethics because he felt that it was not a new vocabulary in the world of technology. It was until a decade later when the term information ethics and cybernetic was coined in the subsequent literature materials and become popular among the scholars.\n\nIn laying down the foundation for this new body of knowledge, Weiner used analytical methods to illustrate the relationship between human nature and society at large and how the two interact simultaneously. He was in a position to highlight various human principles that are useful in shading more light on topic of ethics and computer security.\n\nThey include: Principle of freedom that accords that each individual should fully explore the potential embodied in him or herself; Principle of equality that ascertains that equal chances to be given to two or more people without biasness and principle of benevolence that states that good will should exists between man and man beyond the venture of humanity (Wiener, 1948).\n\nCriticism of ethical relativism\n\nSociety is heterogeneous in nature in the sense that it consists of many people with varied cultures, traditions, norms and values. As a result of this, there are always confrontations and conflicts among the people on which principle of justice should be followed by all the communities.\n\nAccording to Weiner, he affirms that ethical relativism can still exists in the society despite the variance in peoples\u2019 culture, traditions, norms, values and religion, if only they can adhere to the principle of justice that act as the supreme law.\n\nThe bottom line of Weiner\u2019s argument is the fact that principle of justice should not restrict people from fully exploring their potentials in terms of choice and decision making process that makes people to be responsible for their own actions.\n\nTheories of information and computer ethics\n\nMany researchers have placed forth different approaches in addressing the concept of ethics and computer security. This has been a contentious issue since 1985, after John Moore published an article \u201cWhat is computer ethics\u201d in 1988.\n\nMoore was more critical because he was in opposition to describe and elaborate the reasons why computer raised a heated a debate among the people compared to any other kind of technology.\n\nIn his account, he argues that due to the existence of logical malleability in computers, there are many options that a computer user employs to execute different tasks. The availability of these options creates what is termed as \u2018\u2019policy vacuum\u2019\u2019 that provides avenues for manipulation of security codes through hacking and installation of back door programmes.\n\nIn addition, the existence of policy vacuums also predisposes more challenges to professionals in the sense that it creates conceptual muddle about the reliability and validity of the computers. In trying to shade more light on this concept, Moore related the core values of the society such as knowledge, health, life, opportunities and resources with the concept of privacy and security in computers.\n\nIf these fundamental elements cease to exist in a developing society, there would be disintegration, conflicts and instability of the community that, in turn, causes wars and havoc among the concerned parties (Moore, 1999). He affirms that the same approach is applicable to computers and other forms of technologies if not well handled.\n\nEthics\n\nComputer at the workplace\n\nThe introduction of computers in organizations has elicited varied reactions from the public and the international community\u2019s. This is according to a recent report released by international journal of cybernetic ethics on 4 th September, 2008. The main issue of controversy is to determine the extent to which technology should be involved in the process of production and what impacts it has on the welfare of the workers (Beycioglu, 2011).\n\nMost organizations have employed computers to perform almost all the operations in a firm with a minimal assistance from few workers. This, in turn, has replaced many of the workers rendering them unemployed. Furthermore, due to the monotonous effect of using computers, workers have been reduced to machine like creatures in the sense that they cannot exploit their potentials fully. One is required to press a button or initiate a logical command to the computer and the rest of the work is done by the machine.\n\nManagers should take into account the impacts of technology on the welfare of their employees, since it interferes with the normal operations of the organization (Margaret & Henry, 1996). Health and safety matters of the workers should be taken into account because they may influence the rate of production.\n\nPrivacy and Computer crime\n\nComputer crime is one of the challenges facing the entire world today. There are many forms of computer crimes, but in most cases, involves the use of back door programmes to access database of an organization and dissemination of virus through the internet (Himma, 2003).\n\nFirms have devised mechanisms that are effective in protecting their database from being accessed by unauthorized persons through the use of passwords and security codes. However, in some instance cases of violation of information ethics prevail because other workers fail to honour the obligation of adhering to rules and regulations that govern moral contact by reliving security codes to outsiders.\n\nConclusion\n\nThe topic of ethics and computer security is one of the current challenges that the world is facing today. Violations of information ethics among professionals have led to embezzlement of public funds, manipulation of important data for personal interest and incurrence of huge expenses in repairing the system, to mention but a few. Use of back door programmes, hacking and access of passwords are the most commonly methods used by hackers to access and retrieve data without the consent of the owner.\n\nReference List\n\nBeycioglu, K. (2011). International Journal of Cyber Ethics in Education (IJCEE) . doi:10.4018/IJCEE.\n\nHimma, K. (2003). The relationship between the uniqueness of computer ethics and its Independence as a discipline in applied ethics. Ethics and Information Technology , 5(4), 225-237.\n\nMargaret, A, & Henry, J. W. (1996). Computer ethics: The role of personal, informal, and formal codes. Journal of business ethics , 15(4), 425-437. DOI: 10.1007/BF00380363.\n\nMoore, J. H. (1999). Just Consequentialism and Computing. Ethics and Information Technology , 1(1), 65-69. DOI: 10.1023/A:1010078828842.\n\nWeckert, J. & Adeney, D. (1997). Computer and information ethics . Westport, CT: Greenwood Press.\n\nWiener, N. (1948). Cybernetics: or control and communication in the animal and the machine . New York: Technology Press/John Wiley & Sons.\n",
        "label": "human"
    },
    {
        "input": "Human-Computer Interaction in Health Care Essay\n\nMedical errors are increasing in hospitals and normally considered as a result of human fault. Poor interface design which is because of poor human factor engineering (HFE) is the main factor which causes medical errors. Lack of usability testing of medical instruments is another factor which results in medical mistakes.\n\nPoor human factor engineering arises problems, such as poorly designed indicator lights inside ambulances and siren speakers on ambulances. Most indicator lights in ambulances are not properly designed to maintain a proper operation process during the day and night and may cause unnecessary considerations affecting driver\u2019s visibility.\n\nOn the other hand, siren speakers may produce a lot of noise which will interfere emergency workers inside the ambulance to perform their work, and this may cause errors (Carl, Aaron & Elefterios, 2011)\n\nPoor HFE leads to designing of machines which do not alert the user in time when they are in various modes. In addition, similar button, which controls systems on medical instruments, may confuse users, and this may result in medical errors.\n\nGadgets like infusion pumps used in hospitals to automate drip rates are prone to make mistake. Drip chambers and IV tubing for preparation of medication may be reused leading to overdose of patients. On the other hand, lack of usability testing may cause difficulties during operation of various machines causing errors (Fairbanks & Caplan, 2004).\n\nThe following interventions may be employed to reduce medical errors. First, indicator lights in ambulances should be properly designed for various environments. Secondly, sound produced by siren speakers on ambulance should not interfere with communication of emergency works.\n\nSimilar control buttons should be placed on machines to avoid confusion. Control buttons should be grouped in terms of usage frequency and relatedness. Labelling control buttons and varying their sizes can eliminate confusion and consequently reduce occurrence of medical errors.\n\nMedical equipments should be designed to effectively alert the user in time when they are in various modes. They should have a standard operating procedure attached to them and users should be trained on how to use new medical equipments.\n\nEquipping machines with audible alerts is very essential and eliminates the chances of using a machine in wrong mode. New and standardized machines should be used as they reduce the chances of medical practitioners from making errors during their practice (Kaye & Crowley, 2000).\n\nOn the other hand, IV tubing and drip chambers which are considered human machine interfaces used in the preparation of medications are common sources of errors and should be color coded to indicate their current states of use.\n\nIn a nutshell, human factor engineers should be incorporated during the process of designing medical equipments and machines to ensure that user friendly, safe and efficient machines are available in the market.\n\nMachines should be evaluated by usability testing so as to know their effectiveness, their work output and to protect them from human mistakes. Indeed, information obtained during the testing process may be used to improve machines\u2019 performance or even lead to development of more effective machines as argued by Fairbanks & Caplan (2004).\n\nDo you think medical error can be totally eliminated when human factor engineers are incorporated in the machine designing process and usability testing is employed on newly designed machines?\n\nReply to Martha\n\nIt is true that human factor engineering is the main factor responsible for the high noise produced by the siren speaker and the extreme light produced in the ambulance. With proper designing, the siren should have been located in a place where it could not affect communication of emergency workers in the ambulance and bring about hearing problems in that critical time of saving a patient\u2019s life.\n\nThe indicator light should have been dim at night but since it was not designed for night operation it caused unnecessary reflection which effected driver\u2019s visibility. The defibrillator should have been properly labelled to avoid confusion and designed properly so as to give appropriate response in time, hence poor HFE led to the mistake (Fairbanks, Caplan, Bishop, Marks & Shah, 2007).\n\nReply to Gary\n\nAccording to Gary, poor human factor engineering and human factors were responsible for most of the adverse events during the medical operation. On the other hand, the indicator light should have been dim and noise exposure on the paramedics should have been minimized not to interfere with communication between the workers.\n\nCarl, Aaron & Elefterios (2011) argue that besides causing hearing , siren noise may bother other drivers on the road which may lead to accidents worsening situations. The defibrillator was not effective enough to elicit immediate response, and this could only be attributed to poor HFE.\n\nJoseph & Laurie (2003) argue that human factors like medical negligence may lead to errors during medical operations. It is known that paramedics ignore minor details which may result in making mistakes in rush hours while trying to save the lives of patients.\n\nThese minor mistakes made by humans are preventable when one is careful. For example, calculating volumes accurately, avoiding reuse of IV bags and remixing IV bags or other materials frequently used in drug preparation may reduce human errors.\n\nQuestion for the class\n\nDo you think medical error reporting systems are inadequate or do not exist in most hospitals?\n\nReferences\n\nCarl, Q.H., Aaron, J.M., & Elefterios, P.P. (2011). Acoustic characteristics for effective ambulance sirens school of mechanical engineering , Acoustics Australia , 39(2), 43-53. Retrieved from: https://www.acoustics.asn.au/joomla/\n\nFairbanks, R.J., & Caplan, S. (2004). Poor Interface Design and Lack of Usability Testing Facilitate Medical Error, Joint Commission Journal on Quality and Safety , 30(10), 579-584. Retrieved from: https://www.usabilityassociates.com/\n\nFairbanks, R.J., Caplan, S.H., Bishop, P.A., Marks, A.M., & Shah, M.N. (2007). Usability Study of Two Common Defibrillators Reveals Hazards. Annals of Emergency Medicine , epubahead of print, DOI: 1016/j.annemergmed2007.03.029 (in press).\n\nJoseph, L.H., & Laurie, S. (2003). Medical errors and Patient Safety: A curriculum guide for teaching medical students and family practice residents . Web.\n\nKaye, R., & Crowley, J. (2000). Medical Device Use-Safety: Incorporating Human Factors Engineering into Risk Management Identifying, Understanding, and Addressing Use-Related Hazards. Web.\n",
        "label": "human"
    },
    {
        "input": "Industrialization and computerization in entrepreneurship Report\n\nEntrepreneurship has taken a new turn with the onset of industrialization and computerization. Long before the introduction of these two, entrepreneurship was a less common word that was limited to the business-oriented individuals. The introduction of social sites has seen a major breakthrough in the field of entrepreneurship (Abrahamson 1997).\n\nSocial sites have facilitated easy advertisement and access to millions of prospective customers across the globe. One such social site that has contributed to the tremendous growth of entrepreneurship over the last few years is Twitter.\n\nThis report will look at the advantages and limitations of Twitter as far as its technological configuration of providing an open source arena and limiting messages to one hundred and forty characters is concerned.\n\nCountless entrepreneurs have found marketing their products and services via Twitter as a viable means of advertisement. Indeed, hundreds of opportunities are advertised daily on twitter for both buyers and sellers.\n\nTwitter has enabled entrepreneurs and other business organizations to acquire loyal customers (Amit and Zott 2001). In the same manner, buyers have been able to find sellers of their choice through the social site. Therefore, twitter has been of great significance to not only the entrepreneurs, but to the buyers as well.\n\nThe use of Twitter has been favoured by the entrepreneurs for various reasons. First and foremost, Twitter\u2019s \u201cRe-tweet\u201d feature helps to spread the information faster as compared to other social networks (Scott 1991). This feature enables entrepreneurs to reach millions of prospective clients in a few minutes.\n\nThis is especially possible where the entrepreneurs have huge numbers of followers (Leinhardt 1977). The original tweet by the entrepreneurs can be re-tweeted by their followers and their followers\u2019 followers and the chain continues.\n\nEventually, the original information released by the entrepreneurs will be spread to almost every part of the globe (Baker 1990). This fast conveyance of information is a great advantage to the entrepreneurs.\n\nAnother advantage of twitter is attributed to its cheap cost of advertising compared to other methods of advertisement. Because many entrepreneurs advertise their goods and services using Twitter, this has made the cost of advertisement to be lower hence making it more affordable to the entrepreneurs.\n\nTwitter is also the cheapest means of advertisement compared to television advertisement and other forms of advertisement. This affordability of the costs of advertising on twitter is an advantage to the entrepreneurs who obviously seek a cost effective method of advertisement (Fiskel 1980).\n\nTwitter has many users from virtually each nation on earth. With an approximate of over three hundred and sixty two million users, Twitter is certainly the best place for entrepreneurs to advertise their products (Tapscott 2000).\n\nEntrepreneurs make use of a variety of business models to access this large number of users and turn them into customers (Stokes, Wilson and Mador 2010). One such model is the Bricks and Clicks Business Model. Under this model, entrepreneurs allow Twitter users to order for products via Twitter.\n\nThe users however have to go to the entrepreneurs\u2019 stores to pick the ordered products. This model is effective especially where the users are located in the entrepreneurs\u2019 locality.\n\nThe Online Auction Business Model is also extensively used by entrepreneurs on Twitter in their transactions. In this model, entrepreneurs advertise a service or product on sale on Twitter and then provide room for Twitter users interested in it to bid.\n\nThe highest bidder gets the chance to buy the product or service. This model is very advantageous to the entrepreneurs because enables them to sell their products and services at the highest price suggested by the buyers (Timmers 1998).\n\nAs much as Twitter is a suitable social site for the advertisement of products and services by entrepreneurs, it also has some limitations that make it difficult for the entrepreneurs to get the desired effect (Peay 1976). One of these challenges is the limited number of characters that entrepreneurs can type in their messages to clients or other entrepreneurs.\n\nTwitter only allows users to use a maximum of one hundred and forty characters while writing messages. This limitation hinders entrepreneurs from communicating with their clients, prospective customers and other entrepreneurs in an effective manner.\n\nAs a result, the entrepreneurs may experience delays in response from the recipients of the messages due to difficulty in understanding the message (Mandel 1983). If a message is briefly written, there is a high probability of the recipient misunderstanding it.\n\nFrom the foregoing discussion, it is evident that Twitter has played a big role in the spread of entrepreneurship and the facilitation of the activities of entrepreneurs. It has done this by easing communication between entrepreneurs and other entrepreneurs or customers.\n\nIt has also provided a cheaper means for entrepreneurs to use for advertising their products and services. In addition to that, it is also clear from the foregoing discussion that entrepreneurs access their customers using a variety of business models, some of which have been discussed above.\n\nFinally, Twitter has a number of disadvantages that pose a challenge to entrepreneurs using it as a means of communication and advertisement.\n\nReference List\n\nAbrahamson, E (1997) Social network effects on the extent of innovation diffusion: A computer simulation. Organization Science , 8(3), 289.\n\nAmit, R., and Zott, C (2001) Value creation in e-business. Strategic Management Journal , 22(2), 493-520.\n\nBaker, E (1990) Market Networks and Corporate Behaviour. American Journal of Sociology , 96(12), 589-625.\n\nFiskel, J (1980) Dynamic evolution in societal networks. Journal of Mathematical Sociology , 7(9), 27-46.\n\nLeinhardt, S (1977) Social networks: A developing paradigm . New York, Academic Press.\n\nMandel, J (1983) Local roles and social networks. American Sociological Review , 48(32), 376-386.\n\nPeay, R (1976) A note concerning the connectivity of social networks. Journal of Mathematical Sociology , 4(12), 319-321.\n\nScott, J (1991) Social Network Analysis: A Handbook . Newbury Park, Sage Publications.\n\nStokes, D, Wilson, N and Mador, M (2010) Entrepreneurship . United Kingdom, Cengage Learning.\n\nTapscott, D (2000) Growing Up Digital: The Rise of the Net Generation . New York, McGraw-Hill.\n\nTimmers, P (1998) Business models for electronic markets. Journal on Electronic Markets , 8(2), 3-8.\n",
        "label": "human"
    },
    {
        "input": "Key Issues Concerning Computer Security, Ethics, and Privacy Essay\n\nTable of Contents\n 1. Introduction\n 2. Key Computer Security Issues\n 3. Key Computer Ethical Issues\n 4. Key Computer Privacy Issues\n 5. Conclusion\n 6. References\n\nIntroduction\n\nComputer networks that enhance communication crossways the globe have created a world that never existed before. It is tricky to describe the experience since it merely exists in the minds of users referring to them as cyberspace.\n\nComputers have also made daily tasks uncomplicated as most information is exchanged from one region to the next. Around the globe, people contend with fabricating, saving, and handling crucial data on computers (Shelly, Vermaat, & Quansey et al., 2008).\n\nThe issues facing computer use such as defense, ethics, and privacy continue to rise with the advent of extra ways of information exchange.\n\nMore importantly, insecurity cases of laptop use have in-depth outcomes on corporate, governments, and individuals. The safety risks of computer use arise from availability of the internet.\n\nThis enables attacks via networks and intranets, processor hardware pilfering, unlawful contact and abuse of computer, software thievery, scholarly property theft, and systems malfunction (Shelly et al., 2008).\n\nThe above incidences call for rigorous PC maintenance applications capable of safeguarding users against despair from the consequences of risks.\n\nEthical issues also crop up when people use computers. Morley and Parker note that some moral issues of computer use as utilizing copyrighted materials unethically obtained, principled uses of collaterals and intelligence materials, immoral utilization of digital controls and moral corporate activities and judgments (Morley and Parker, 2009).\n\nFurthermore, computer use raises questions on the right of users to conceal their details from other people. Baldauf and Stair observe the ability of individuals to create several classified and unrestricted databases showing facts about their existence anecdotes including daily behaviors, private hobbies, present locations, and movements.\n\nThe accessibility of individuals\u2019 particulars and the occurrence of monitoring technology-utilizing computers raise real concerns regarding individual\u2019s secrecy (Baldauf and Stair, 2010).\n\nKey Computer Security Issues\n\nIssues pertaining to computer security receive great mention in several literatures due to its effects on users including governments, local, global organizations, as well as corporate, and citizens of various countries.\n\nAccording to Salomon, computer security is a broad knowledge area including PC protection methods against exposure to physical damage elements (Salomon, 2010). In a later literature, other authors observe that computer safety exists since they are quick, truthful, and superior in performing tasks yet lacks acumen (Lehtinen, Russell, & Gangemi, 2006).\n\nThey note that this predisposes processors to individuals\u2019 operation in undertaking transgressions. Human beings as well are clever but cannot spot computer-based plots created to defraud them.\n\nComputers are used to propagate identity theft, where persons use identity of others for monetary achievement purporting to be the real individuals. There are also issues of intellectual property pilferage advanced by using computers.\n\nAnother issue is organizational information safeguard. Baldauf and Stair note that valuable organization data and documents can be compromise just by hackers thereby resulting into serious losses. They go ahead to cite hacking of Twitter as an example (Baldauf and Stair, 2010).\n\nThere is computer crime where information transfer through mysterious electronic way leads to committing fraud. Another security issue is computer crackers or hackers; this is where people intrude by surfing net to find valuable information or program, yet not for fiscal of opinionated rewards (Lehtinen et al., 2006).\n\nOrganizations\u2019 outsiders contribute to computer security issues whenever they gain entr\u00e9e to IT rooms unlawfully to seize valuable information. Physical threats of CPU use such as rush in electrical power; thus ensuing from lightning could spoil processor components. Computer viruses, which conceal itself in an executable file in the computers cause security concerns.\n\nFurthermore, viruses can damage computers, software, alter data, and impede user from effecting tasks. Disclosure threats also take place whenever secretive data hoarded or transferred via a network spread to unofficial receiver.\n\nEmail communication also face security problems when unauthorized persons gain access to passwords, spam messages forced people. In addition, hacking social networking account details is more problematic (Belousov).\n\nOther people also advance cyber terrorism where they use computers to bully government or organizations for political or social advantages via the processors, networks, or internet.\n\nTo respond to computer security issues users put in place effective defensive capabilities able to protect valuable information. Using passwords, creating backups to protect data, legal expertise and instituting bylaws and policies are applied.\n\nIt is certain that computer applications rip off people of their privacy. Computers publicize information of users, which is a colossal attack on confidentiality (Lehtinen et al., 2006).\n\nKey Computer Ethical Issues\n\nMoral values guide every aspect of human life and computer ethics tends to spell out the moral expectations of users. Hacking is one of the ethical issues. Hacking refers to using individual\u2019s computer knowhow to gain access to unauthorized data for malicious reasons (Pollach, 2011). Hacking undermines the efforts of maintaining privacy.\n\nThree aspects of hacking include cyber trespass, vandalism, and piracy. They all qualify as forms of computer crimes. Computer users face ethical dilemmas while performing tasks.\n\nFor example, a user may be contemplating whether to copy a program or not. Moral issues also arise on using computer to enhance supervision, observation, recording relating to database exploration, which can be done decently (UCAN, 2011).\n\nAnother ethical concern is about privacy, where people wonder whether it is moral for databases to register personal details of people. People also question the morality in using computers to advance criminal activities.\n\nEthical questions raised on the use of group network sites to put personal details public as it has contributed to private information into the public domain.\n\nKey Computer Privacy Issues\n\nInvasion of people\u2019s privacy through computer use is on the rise and it is upsetting every person. Other people such as hackers, employers, and marketing corporate, could access personal and confidential information (Debatin, Lovejoy, Horn et al., 2009).\n\nSending email messages is the key way through which assault of people\u2019s privacy flourish. Databases including those of institutions, marketing agencies require users to key in their details including where they live, contacts, names, occupation, and credit card facts. Failure to protect such data amounts to breach of privacy (Morley & Parker, 2009).\n\nThe installation of public monitoring techniques aided by computers has improved security a great deal. However, using the technology including video surveillance, CCTV cameras, and convex lenses also contributes to invasion of privacy.\n\nConclusion\n\nAddressing privacy concerns would be tenable when using a screening technology, which protect users from websites that do not maintain higher standards of privacy.\n\nUsers need to be very critical on their credit card details while purchasing items from the websites. It is a warning to computer users against posting excess information about their personal details on the social sites. In addition, users to beware of websites demanding their personal information in exchange of free bees.\n\nBusinesses should endear to protect clients\u2019 privacy through establishing precautionary measures for avoiding employees\u2019 accidental or intentional breach of privacy. Finally, implementation of policy legislations on privacy protection, security, and moral concerns on the use of computers needs to take place.\n\nReferences\n\nBaldauf, K. & Stair, R. (2010). Succeeding with Technology , 4 th ed. Massachusetts, MA: Cengage Learning.\n\nBelousov, A. (ND). Definition of Computer Piracy, Carrying Out Expert Examination . Computer Research Crime Center. Retrieved from http://www.crime-research.org/articles/Belousov0604/\n\nDebatin, B., Lovejoy, P., Horn, A., & Hughes, B. (2009). Facebook and Online Privacy: Attitudes, Behaviors, and Unintended Consequences. Journal of Computer Mediated Communication, Vol. 15: 83\u2013108.\n\nLehtinen, R., Russell, D. & Gangemi, G. (2006). Computer security basics , 2 nd Ed. California, CA: O\u2019Reilly Media, Inc.\n\nMorley. D. & Parker, C. (2009). Understanding Computers: Today and Tomorrow, Comprehensive, 12 th ed. Massachusetts, MA: Cengage Learning.\n\nPollach, I. (2011). Online privacy as a corporate social responsibility: an empirical study. Business Ethics: A European Review . Vol. 20, Is.1: 88\u2013102.\n\nSalomon, D. (2010). Element of Computer Security . London: Springer.\n\nShelly, G., Vermaat, M., Quansey, J., Sebok, S., and Webb, J. (2008). Discovering Computers 2009: Complete. Massachusetts, MA: Cengage Learning.\n\nUCAN. (2011). Fact Sheet 7: Workplace Privacy and Employee Monitoring . Retrieved from https://www.privacyrights.org/consumer-guides/workplace-privacy-and-employee-monitoring\n",
        "label": "human"
    },
    {
        "input": "Third Age Living and Computer Technologies in Old Age Learning Essay\n\nIntroduction\n\nThis essay gives an analysis of factors which have contributed to the successful achievement of the Third Age by certain countries as a life phase for their populations. The second segment of the essay synthesizes computer technologies that can be helpful to older adults\u2019 learning.\n\nSuccessful countries in achieving the Third Age\n\nUnder normal circumstances, it is believed that life is divided into phases which people have to through from the time they are born. The occurrence of these stages appears to differ from one community to another based on a wide spectrum of factors.\n\nAs a result, the onset of the higher phases in life i.e. third and fourth varies from country to country. For instance, it is believed that the Third Age of life begins at the onset of retirement among Britons.\n\nWhile this is true in some cases, there has to be a clear distinction between retirement age and the onset of this age in an individual\u2019s life (Laslett, 1987). It therefore follows that different countries around the world have been successful in achieving the Third Age as a life phase for their populations.\n\nOne of these countries is the Great Britain which has arguably been considered successful in achieving this phase of life for its people. This has been attributed to countless factors going beyond demographic and economic factors.\n\nGood health and appositive attitude towards ones lifespan is equally essential in allowing people to see and experience the Third Age of life. Although the Third Age appeared during the 1950s in Britain, its establishment began in 1980s, a trend that was witnessed in other Western countries like Italy, Japan, USA, Iceland, Australia and Scandinavia and other developed countries (Laslett, 1987).\n\nWhat are the major factors that have allowed this to happen?\n\nThese countries have been successful in achieving the Third Age as a life phase for their populations as enhanced by several factors. Some of these factors include good nutrition, equitable distribution of resources, good social services and safe working conditions (Laslett, 1987).\n\nHigh life expectancy has therefore contributed to most developed countries to successfully achieve this age for their population. It is important to acknowledge the fact that some historical events have also contributed to the success of these countries.\n\nA good example is industrialization which played a pivotal role in promoting the living standards of different populations. A country like England experienced modernization long before the onset of industrialization. Such trends have played a role in allowing inhabitants to achieve the Third Age.\n\nHigh national wealth has also contributed with these countries being able to have sufficient resources for aged and coming generations (Laslett, 1987). From a general perspective, Third Age Living has been achieved in the above mentioned countries as a result of intertwined factors ranging from social, political and economical.\n\nDescribe four computer technologies that can be helpful to older adults\u2019 learning\n\nAdvancement in computer technology has significantly transformed the lives of both young and older generations. Of importance is the manner in which information is shared with a lot of ease from one person to the other.\n\nAt the click of the mouse, one is able to access a wide range of information from any destination around the world. Businesses, relationships and education have all benefited from this technology which continues to transform countless lives around the world (Kim, 2008). These technologies have found momentous applications in learning institutions especially for the elderly.\n\nFor instance, the internet is one the computer technology that has tremendously enhanced learning among elderly people. It allows constant connection and flow of information through emails and social networks as witnessed today.\n\nOld people also find it necessary for shopping and search for important information by using Google and Yahoo among other search engines (Kim, 2008). Screen magnification software is also important in learning institutions for elderly people and those with impaired eye sight.\n\nIt allows increasing of font sizes to allow learners to read without difficulty. Additionally, Synthetic Voice Output helps people with hearing problem. The last technology is the use of Braille Translators and Embossers. These allow blind people to learn computers and use them in their daily lives.\n\nExplain briefly how each one of these could be used to enhance learning for older adults\n\nThese computer technologies have a major role in enhancing learning among ageing people. As mentioned above, the internet has arguably become the most used communication tool around the world with both young and old people appreciating its significance.\n\nFrom shopping to entertainment, old people have come to value the internet with most of them finding it paramount in their daily lives (Kim, 2008). Since internet augments communication, it can equally enhance learning among old people.\n\nThrough the internet, instructors can share learning materials through emails and monitor their performance while at home. The internet also allows learners to contact their instructors as long as they are both connected.\n\nOn the other hand, Screen Magnification Software allows the display of information on monitors in various font sizes and colors to alleviate vision loss.\n\nThis allows learners with limited vision to see and interpret information displayed on the screen. Synthetic Voice Output is a screen reading program which interprets text into audio speech for people with low vision (Kim, 2008).\n\nIt can be helpful among aged computer learners experiencing eye problems. Lastly, Braille Translators and Embossers enable blind learners and tutors to use computers with the help of fingertips. It can be essential among elderly computer learners that are Braille proficient.\n\nConclusion\n\nIn general there are several factors which have enhanced the achievement of the Third Age in certain countries around the world.\n\nThese factors are intertwined and include social, political and economical. On the other hand, computer technology plays a mega role in learning programs among ageing people. Through the integration of certain technologies, old people find it easier to learn computer skills.\n\nReferences\n\nKim, Y. (2008). Reviewing and critiquing computer learning and usage among older adults. Educational Gerontology, 34 , 709\u2013735.\n\nLaslett, P. (1987). The emergence of the third age. Ageing and Society, 7 , 133-160.\n",
        "label": "human"
    },
    {
        "input": "Computer Safety: Types and Technologies Essay\n\nTable of Contents\n 1. Background\n 2. Pervading Home, Office, and Public Spaces\n 3. Personal Experience\n 4. Impact to human existence\n 5. Conclusion\n 6. Works Cited\n\nComputers have pervaded human society in a way never seen before. This was not always the case. In fact, there was a time when computers were a figment of man\u2019s imagination. Starting in the modern age people wanted to build a machine that can handle immense mathematical calculations but for centuries the idea remained a dream.\n\nBut in the modern age scientists and engineers began to have access to technology that allowed them to develop the first computer and the rest was history. In the 21 st century computers continue to pervade everyday life. It is no longer limited to office work because computers also pervaded the home, supermarkets, transportation hubs and other public places. However, it must be made clear if someday computers no longer provide assistance and becomes a bane to human existence.\n\nBackground\n\nIn the past scientists and mathematicians had to be contented with pen and paper in performing their calculations. Needless to say this was a laborious process. This crude method of recording and analyzing data was time consuming and it places considerable limitations on their achievements.\n\nHowever, different types of technologies were developed and machines were invented that at first glance are not complimentary but looking at the greater scheme of things was a build-up to the creation of the personal computer.\n\nIt can be argued that with the invention of the printing press also comes the explosion of knowledge as information can be shared much faster and disseminated over a broader area as opposed to hand-written communication. In the past handwritten notes and the manual creation of books is the only way to spread knowledge in addition to oral history and other verbal forms of communication.\n\nAnother consequence of the printing press is the ability to mass produce information and made available at a shorter rate of time. The indirect by-product of the printing press is the typewriter.\n\nThe printing press may have been an important contribution to the development of computers but without the ability to harness and control electrical power it is impossible to even begin to imagine the possibility of creating a compact and fast-computing machine.\n\nElectricity paved the way for the electronics industry. The electronics industry made it possible for the creation of computer monitors, electric typewriters and eventually the keyboard. All of these made possible the creation of a powerful computing device that can be operated in the home. When somebody found a way to connect computers globally the ability to interact and to do business was forever transformed in the most radical way (Waters, p.5).\n\nThe discovery of a particular knowledge that leads to invention and later on to innovation can take a very long time depending on the needs of people and depending on the availability of technology. But one thing is certain, scientists, engineers and inventors did not start from scratch (Gladwell, p.50).\n\nEach inventor and innovator owes a certain degree of gratitude to those that came before them. Thomas Edison may have perfected the light bulb but he too was standing on the shoulders of giants. This phenomenon was described in detail by sociologists who wrote: \u201cnew inventions can be adequately explained by reference to the state of prior knowledge, because every new invention grows out of existing knowledge, whether that knowledge be scientific, technological, or otherwise\u201d (Schmookler, p.195). In the case of personal computers the evolution took a long time to complete because aside from what can be seen from the outside there is an invisible component called the software.\n\nA computer is simply a box of wires without the presence of effective and efficient software. The most important is the core software which is also known as an operating system or OS. The OS of a computer is a set of instructions communicating directly to the hardware of the computer and this enable it to process information given to it.\n\nThe most common source of information is the data that is encoded by typing into the keyboard. In the past this is a slow process and computers had a basic system that only a computer scientist or a programmer can understand the processes that was going on.\n\nAn untrained person can only see a series of numbers and complex mathematical formula in the monitor and nothing more. But today\u2019s computers had evolved into something that uses a user interface. In simple words there is mechanism or a software that is the conduit between user and machine.\n\nAn average person need not know the complex mathematical process that is required for the computer to draw a line on the screen. The only thing that he needed to know is that by using the \u201cmouse\u201d and the keyboard he or she can draw that line on the monitor screen.\n\nA basic computer set-up requires the combine work of two types of software. The first is the aforementioned Operating System or OS and the second one is application software such as computer games, drawing, and office software such as Microsoft Word for word processing, Mozilla Firefox for web browsing. In the case of the OS there are at least three major groups of operating systems available in today and this includes: a) Apple OS X; b) Linux); and c) Microsoft Windows. The last one is well known all over the world because of its accessibility and user friendly features. In other words, it does not require a computer scientist to use it.\n\nOver the years operating systems are constantly being upgraded to keep up with the increasing demand for more computing power. In the same way computer hardware has to be upgraded as well. As a result more sophisticated software applications are made available in the market. These software applications are redefining what computers mean to mankind. Is it still a blessing or rapidly becoming a curse?\n\nPervading Home, Office, and Public Spaces\n\nNot a long time ago man was resigned to the fact that computers are here to stay. It is already an accepted fact of life that computers are indispensable tools when it comes to the office and various industries. A short while later computer invaded the homes (Anderson, Felici & Littlewood, p.10). But innovation did not end with products that can be used in both home and office. There are innovative products being developed and deployed in public places.\n\nA good example is digital technology that can be installed in transportation hubs to deliver advertising content to the general public. If this is not enough a new generation of computers is flooding the market and this type is characterised by the combination of sophisticated hardware and software that can observe and record human activity.\n\nIn the academic community people are asking if computers can actually help in man\u2019s pursuit for happiness or if this technology is driving man to a deeper level of isolation, stress, and confusion. There seems to be no middle ground. Computers of different forms and functionality has invaded every aspect of human life and there comes a time when dependability on computers can severely affect the beauty and spontaneity of human interaction.\n\nPersonal Experience\n\nI can still remember the old typewriter that I saw in the attic. It was gathering dust in a corner but my parents refuse to throw it away. I immediately understand the sentimental value of the said equipment. The first time I saw it I was curious as to the purpose of the keyboard-like device that instead of a monitor to display the words it uses slender metallic arms to produce words. My curiosity led me to badger my mother until finally she could not take it anymore and began to provide a short explanation of the significance of typewriters. And I could not believe what I was hearing.\n\nIt is amusing and at the same time fascinating to know the crude technology used in typewriters. I can compare it to striking rocks to produce fire. This time around the pounding action is between metal and carbon paper.\n\nAs the force of the metal type-face forces itself into the soft carbon, the dark material from the carbon is imprinted into the paper underneath. As the typist pounds into the typewriter more words are produced, and finally sentences are made. It is without a doubt a tiresome process.\n\nI have a difficult time imagining how the older generation contended with this labor-intensive way of producing documents. I wonder what happens if they make a mistake in typing. My pinkie finger instinctively reaches out to the delete button as I envision the error appearing before.\n\nBut it seems that there is no other way to solve that problem. Obviously a typewriter does not have this feature and so I imagine that they use some kind of an eraser for that purpose.\n\nIt is truly a wonderful thing that computers were invented. It is easier to type away in a laptop whatever documentation is required. Office and school work becomes a less daunting experience. The thought process of writing a novel or a business letter is as difficult as it was in the pre-computer era but at least the time required to delete or to retype is much shorter.\n\nIn addition, there is sometimes the need to rearrange the information contained in the said document because of some errors in the writing process. Nevertheless, this type of problems has become the thing of the past.\n\nHowever, it must be made clear that the evolution of computers especially software applications was a slow process. Microsoft did not become a commercial success overnight.\n\nThere were a series of software applications that had to be tested in the market. I remember some of the software applications that were made available in the past and some were just cumbersome to use. There were a host of different bugs or errors in the programming. In comparison to a typewriter there is a need to know a basic set of skills and then after years of using a typewriter that typist becomes so adept with it that both hands and the equipment seems to be transformed into one seamless machine.\n\nThe same thing cannot be said of computers and software applications. Surely a basic set of skills of needed for it to work but the problem is in the rapid changes in the software and even to some extent the hardware. There was a time when computer users had to contend with the need to buy manuals every time an upgrade was released. It was a frustrating to be always pressured to learn something new.\n\nImpact to human existence\n\nNevertheless, people began to appreciate the importance of computers. As the old generation of typewriter users were replaced with keyboard thumping young men and women. A whole new group of people emerged that validated the significance of this new technology. It can be argued that the change could not be contained with the mere ability to produce documents.\n\nThe next big thing came as an indirect result of computer development is in the area of telecommunication and the networking of computers. These two innovations changed the world in a way that could never have been anticipated by even the most astute analyst of the past century.\n\nThis is in accordance to a theory advanced by analysts who argued that, \u201cThe technology existing at a given point in time sets limits on how much can be produced with a given amount of inputs\u201d (Mansfield, p.12). There is therefore greater impetus from researchers and scientists to push back the limitations given by present technology. This can be achieved through timely scientific breakthroughs.\n\nThe rapid changes in computer technology paved the way for the creation of mobile phones. At the heart of a mobile phone is a miniaturized computer that can receive and make calls. But the added innovation is the creation of a network of communication towers that allowed a caller to be mobile and yet continue to have an interrupted connection with the person on the other line.\n\nThis technology used to be limited to government officials and to the military. But when it was made available to the general public human-to-human interaction changed. I can still remember how mobile phones changed the way people interact but more importantly how businesses were revolutionized. The speed and ease of communication allowed businessmen to expand not only to the next state but around the world.\n\nIt is difficult to understand the emergence of a global network of businesses without the capability to call anytime and anywhere. In the past the caller has to go to a designated phone booth and in some cases had to ask the assistance of an operator to be able to call overseas. Those days belong to history.\n\nHowever, it must also be pointed out that without the creation of the Internet globalization is limited to a few international companies that can afford to move products and services all over the planet. The Internet created a level playing field for everyone.\n\nIt highlighted what theorists call the knowledge-based economy (Carter, p.61). Information about a product is no longer difficult to disseminate all over the world. With the click of a mouse button or the creation of a website a small company in the middle of nowhere can announce its presence through the World-Wide-Web and amazingly people as far as Russia and New Zealand can place orders.\n\nIt has been predicted that computing power and processing speed within computers is about to be increased in a level that seems mind-boggling. Consider the implications of this kind of improvement. An innovation is just around the corner, the kind of innovation that can bring this invention into another level (Gelb, p.7). There will be new products and services that can even transform the way people do business, access entertainment services, and communicate to each other.\n\nThere will come a time when computers are no longer limited to the office, home, and public spaces. There will come a time when computers are about to invade the human body. There is talk about microprocessors and nanotechnology that can further miniaturize technology to the point that it can be embedded to a human body without adverse effects.\n\nIn other words technology is not only worn but technology can become a part of the human body. This can enhance life or become so obtrusive to the point that it violates the principles of privacy. It can probably increase life expectancy but at what cost?\n\nThe need for speedier transactions does not really save time because so much work was generated by the use of this technology. A system that provides a quick response does not increase the time that each person is entitled to everyday; what it does is that it forces people to do more. The company is expected to do more because communication is at a speed that has gone beyond expectations.\n\nThe rapid acceleration in the innovation of computers has left many in awe. The problem right now is to find a way to measure its impact especially when it comes to children. Without a doubt computers is an integral tool in the search for knowledge and it can help people enhance their capabilities.\n\nThink for instance of healthcare and the absence of computers in this field of human endeavour is something that is undesirable. Doctors and health workers will react in horror if they are not given the chance to be able to use computers.\n\nOn the other hand there are those who contend that computers must be regulated because it can easily contribute harm rather than good. The availability of PC games is one contentious issue. When it comes to violence in video games there are parents and government officials are saying that this must be banned. Others disagree and the debate rages on.\n\nAside from issue of violence computers are also considered a problem area when it comes to the emergence of cybercrimes. Consider the capability of an individual to steal money without having to risk exposure by going to the bank.\n\nThis new way of robbing people blind can also be considered as an innovation. But this is the negative form of innovation. It is therefore easy to understand why there are those who are not fully convinced that computers are indeed a blessing to society.\n\nHowever, it is hypocritical for those people to say that they did not benefit from the rapid evolution and diffusion of computers. The ease in banking and other business transactions is a good example of the benefits of computers. The ease in communication is another aspect of the radical change in technology that started in the modern age. The ability to communicate to someone halfway around the world is something that must be viewed with gratitude and not with cynicism.\n\nConclusion\n\nThe radical change in technology must be considered as both a blessing and a problem that is difficult to understand. The difference in perspective is the reason why there are those who continually oppose the deployment of technology that they believed can harm their children and the people that they love.\n\nBut looking at the innovations made in the past centuries should change the opinion of many in favor of those who believe that technology especially computer technology is a gift to mankind. Consider the typewriter and the computer. No one in their right mind will say that they prefer typewriters to personal computers.\n\nThe old technology is cumbersome to use and it places a great degree of limitations on the user. Computer technology on the other hand speeds up the work process and as a result more work can be accomplished. This does not mean that policy makers should not continue to take a closer look at the development of new technology. Everyone should have an open mind.\n\nWorks Cited\n\nAnderson, Stuart, Masssimo Felici and Beverley Littlewood. Computer Safety, Reliability, and Security. London: Springer, 2002.\n\nCarter, A. 1996. \u201cMeasuring the Performance of the Knowledge-Based Economy.\u201d Employment and Growth in the Knowledge-Based Economy. Ed. D. Foray and B.A. Lundvall. Paris: OECD, 1996. 61-68. Print.\n\nGelb, Michael. Innovate Like Edison. New York: Penguin Group, 2007.\n\nMansfield, Edwin. Technological Change . New York: Norton Publishing, 1971.\n\nSchmookler, Jacob. \u201cChanges in Industry and in the State of Knowledge as Determinants of Industrial Invention.\u201d The Rate and Direction of Activity. Ed. R. Nelson. New Jersey: Princeton University Press, 1961. 195-216. Print.\n\nWaters, Malcolm. Globalization . New York: Routledge, 2001.\n",
        "label": "human"
    },
    {
        "input": "Computer Technology and Networked Organizations Essay\n\nTable of Contents\n 1. Introduction\n 2. Oticon\n 3. Role of IT in organizations\n 4. Conclusion\n 5. References\n\nIntroduction\n\nA lot of suggestions have been given by people and organization on ways of transforming organizations. The main target of this strategy is to redesign business processes and business operations so as to give a firm a new look or achieve efficiency and profitability. I shall take a closer look at Oticon Company and the role of technology in transforming its businesses activities.\n\nThe current wave of changes in organization is a result of advances in technology which has been necessitated to cope with emerging demands. Fusion of business processes with technology is undertaken after careful research and taking into account the mission and strategies of the organization and whether the changes will impact positively on productivity and efficiency of business processes. Competition has become intense and conditions under which businesses operate have become complicated mainly as a result of easy exchange of information.\n\nOticon\n\nOticon is an exemplary organization that has successfully implemented information technology in its organizational structure. Griscom (2009) describes it as \u201ca large Danish manufacturing company with a specialty in manufacture of hearing aids.\u201d\n\nIt is ranked top five in the world in manufacture of these devices and has approximately one thousand three hundred employees with an annual turnover of close to ninety million dollars. It has an international presence because it exports products to over one hundred countries in the world.\n\nInformation technology has been integrated in allocation of projects. Top management appoints a leader to oversee the progress of the project. The project is publicized in electronic medium of the company which employees can access through their work stations. Employees are then allowed to sign up for projects they feel they can carry on effectively.\n\nOnes they have subscribed for a project, they can only pull out with approval of the team leader. It works in a manner that if an individual comes up with an idea and has been approved by management, they become project leaders and people are recruited to work on it immediately. This approach helped Oticon improve greatly in responding to customer needs.\n\nOticon further embraced the idea that employees can work on multiple tasks that require different expertise. This can be embraced while participating in emerging projects. It will give a chance to employees to give their input in ways they are satisfied with.\n\nThrough this approach, the company is to benefit from a variety of skills of employees (Colette, 2000). This approach allows employees to advance their skills by working on a variety of projects. Kolind, the CEO dismissed the notion of sticking to one job.\n\nIt was noted that some employees from departments like accounting would contribute great ideas to the marketing department. Management believed that if employees chose to work on specific projects of their choice by signing up on them, they would do them better than when they were assigned.\n\nAnother technological strategy at Oticon was paperless offices. Workstations were introduced and any employee could access office applications at any desk because all documents could be accessed electronically.\n\nOnce an employee enters access code in the system, he or she can access the necessary information including organizers, public folders and tasks. All information is stored in a central processing unit which enables individuals to get access from any computer at work.\n\nBy using this approach, employees can group themselves to perform a particular task with much ease without carrying belongings. This is another indication of how information technology can make work efficient. I would propose to organizations to adopt technology as an enabling factor to their processes because it allows use of multiple options.\n\nRole of IT in organizations\n\nUse of information technology systems in organizations has grown over the years and has become an essential commodity. Information systems include different types of software applications designed to carry out major processes of organizations.\n\nInformation technology is a broad field that comprises mediums of communication and tools that link people with them. Recent studies found out that IT plays a major role in information exchange within the organization and outside.\n\nBy using information technology applications, an organization improves the quality and reduces the time with they perform tasks and decision making. Among the major benefits that come with use of information technology in an organization is efficiency and increased innovation.\n\nEfficiency that comes with adoption of IT practices is reduction in operating costs and faster execution of tasks. In addition, IT enables employees collaborate with ease, improves the ability to code information of the organization and enables coordination (Lagace, 2003).\n\nTechnology has had a great impact on financial sector. Use of technology has brought about efficiency and speed with which business transactions are carried out in the financial market. For instance, introduction of the manner in which orders were taken by New Yolk Stock Exchange by telegraph gave them an advantage over Philadelphia Stock Exchange.\n\nWere it not for the introduction of telegraph, the two competitors would be on the same level. Installation of the telegraph gave one an advantage over the other. This elaborates how important IT is to organization performance.\n\nConclusion\n\nI would enjoy working in an organization that fully utilizes Information technology practices. From the discussion, I found out that this type of organizations executes at speed, encourages innovation and addresses the needs of customers.\n\nReferences\n\nColette, W. (2000). Complexity of New Office Designs: Thinking Through Your Future Workplace. Searcher 8(10) http://www.infotoday.com/searcher/nov00/wallace.htm\n\nGriscom, J. (2009). How Telecommunications Is Changing Work for Nonprofits. Retrieved from the World Wide Web: https://www.techsoup.org/support/articles-and-how-tos/how-telecommunications-is-changing-work\n\nLagace, M. (2003). Stuck in Gear: Why Managers Don\u2019t Act. Harvard Business School \u2013 Working Knowledge For Business leaders \u2013 Research & Ideas.\n",
        "label": "human"
    },
    {
        "input": "Reflections and Evaluations on Key Issues Concerning Computer Essay\n\nSecurity, Ethics, and Privacy\n\nIntroduction\n\nComputer networks enable people to communicate and this has created some places that never existed before as this is just a place created in the mind. It is popularly called cyberspace (Mather et al., 2001, p. 55). Ethics is a very philosophical subject where people can determine right or wrong actions. It defines moral codes of certain field of work besides; computer security and privacy have been the main issues that are addressed under this subject.\n\nSecurity\n\nEven thought the terminology \u2018computer security\u2019 is often used, the content in a computer is vulnerable to very few risks except when an individual is connected to a wider network. Since the use of computer has grown over the past few years and more networks are being developed every day, the use of computers and networks has posed greater risks to computer users. This has made the use of the term very common (Mather et al., 2001, p. 55).\n\nThe main technical areas that are addressed in computer security concerns are availability, integrity, confidentiality and authenticity (Pfleeger, 2006, p. 700). Confidentiality \u2013 this is a factor that addresses privacy or secrecy of information that one has on the computer or computer networks. This concept demands that information should not be accessed by unauthorized person. It is illegal to breach confidentiality since the consequences could be detrimental.\n\nIntegrity is a concept that requires information to be safeguarded from unauthorized changes which the official user cannot detect. Hacking is one of the computer crimes that compromise integrity of computer and computer networks (Foxman & Kilcoyne, 1993, p. 106). Authentication is a concept of computer security and ethics in which the user is identified to be exactly who he/she says he/she is.\n\nThis concept is very important because sometimes unauthorized users can access the information by stealing identity (Foxman & Kilcoyne, 1993, p. 106). Availability on the other hand is the concept that requires the free unrestricted access to the information by the authorized user (Molie, 2009, para. 2).\n\nBreach of this is often denial of services. Serious issues that are often addressed under this include non repudiation and access control. Access control is a situation whereby legal users are not only denied access to some resources, but also some vital services they are legitimately entitled to. Non repudiation is when a person cannot deny having sent information when he/she actually sent it or vice versa.\n\nBesides the technical aspects, the concept of computer security is very wide and it is greatly embedded in disciplines like privacy and ethics. Under these disciplines, computer crimes are described in terms of things that prevent, detect and remedy attacks and anonymity and identity in computer world.\n\nCyberspace is a very important aspect of life and many people depend on it for school work, professional works and communication (Molie, 2009, para. 3). For computer users, authenticity, integrity and confidentiality are the commonest problems they have to deal with, while internet users are more prone to issues of privacy invasion and identity problems (Caudill & Murphy, 2000, p. 12).\n\nPeople often store information on internet as they assume that some of the information they are keeping is not very sensitive or that it is safe (Pfleeger, 2006, p. 701). It is important to note that on the internet, most of the information is easily shared among companies and small pieces of information can be put together to connect the dots and then form something bigger about an individual (Molie, 2009, para. 3). This requires good control over information in that who, how and when to use it is restricted.\n\nEthics and Privacy\n\nIn the current world of technology, information and computer technologies have become central in the fields of industry, healthcare, government and entertainment (Bynum & Rogerson, 2004, p. 63). There are numerous social and economic benefits that have come with these technologies.\n\nHowever, unlike other types of technologies, computer technology has a number of problems that are unique to it, some which could be very serious and negatively affect individuals and the society at large (Stallings, 2008, p. 83). This creates and poses ethical concerns and there are basically three issues that are the core of this subject.\n\n 1. Personal privacy when working on computers\n 2. Harmful activities of the computer\n 3. Access rights\n\nPersonal Privacy: this allows exchange of information on a large scale among a number of different people, from different places and any time (Deguzman, 2010, para. 2). This situation causes increased possibility of disclosing person information or accessing other people\u2019s private information therefore causing potential of violating privacy (Bynum & Rogerson, 2004, p. 63).\n\nIt is a challenge for the common users of the computers or cyberspace to maintain great level of privacy and integrity of information concerning individuals (Mather et al., 2001, p. 55). This means that users should take precautions to make sure that the information is always accurate and also protected from unauthorized access, or disclosure (whether accidental or intentional) to unsuitable users. Concerning ethical usage in computer systems, the access right is highly sensitive issue.\n\nThe cyberspace has become very popular in commerce, entertainment, school and government as already stated and this causes \u2018access right\u2019 issues to be of great concern. In fact it is a top priority among companies and government departments. The issue is further heightened by computer break-ins in high security places like NASA and US security systems.\n\nThere are several reports of attempted illegal access to the US government agencies and military security systems. Hackers pose a major threat to security, identity theft and other cyber crimes (Caudill & Murphy, 2000, p. 12). Harmful activities on the computer system refer to negative impact or injurious consequences like property damage, loss of data, loss of property and other unwanted consequences (Stallings, 2008, p. 83).\n\nThis principle therefore prohibits use of computer systems in ways that leads to harm to other people or the government. Some of the unethical actions include modification of other people\u2019s information or programs leading to loss of data and unwarranted expanses like time and money, destruction of people\u2019s information on the computer or network systems and introducing computer virus to systems (Deguzman, 2010, para. 2)\n\nConclusion\n\nA computer security threat includes any action that might lead to loss of information, data, and damage to the computer hardware, hamper the processes, or cause incompatibility. Many of such actions are usually premeditated and the international infringement of the computer security is a computer crime punishable by law and it should not be confused with cybercrime.\n\nCybercrime is mainly perpetrated through the internet and of course a computer is mostly used. These crimes are top priorities for law enforcers because of the importance of information in the modern world and also the imminent danger of terrorism. Computer security, ethics and privacy deal mainly with computer crime, how these crimes can be prevented, how to detect them and solution to such unethical attacks.\n\nBesides, privacy also deals with anonymity and identity issues in the cyberspace, and area that has lately made personal information for users vulnerable. For daily internet users, privacy and identity is major concerns however for managers, issues of integrity, authenticity and confidentially are the main concerns and their regulation has to be effective not to compromise any of them.\n\nReference List\n\nBynum, T.W & Rogerson, S. (2004). Computer Ethics and Professional Responsibility , Oxford: Wiley-Blackwell.\n\nCaudill, E.M., & Murphy, P.E., (2000). Consumer Online Privacy: Legal and Ethical Issues, Journal of Public Policy & Marketing Vol. 19, No. 1, pp. 7-19.\n\nDeguzman, V., (2010) Computer Security Ethics and Privacy. Web.\n\nFoxman, E.R., & Kilcoyne, P., (1993). Information Technology, Marketing Practice, and Consumer Privacy: Ethical Issues, Journal of Public Policy & Marketing Vol. 12, No. 1 (Spring, 1993), pp. 106-119.\n\nMather, T., Kumaraswamy, S., & Latif, S., (2009). Cloud Security and Privacy, an Enterprise Perspective on Risks and Compliance, O\u2019Reilly Media.\n\nMollie, C., (2009) Computer Security \u2013 What Exactly Is It? Web.\n\nPfleeger, CP. (2008) Security in Computing, Fourth Edition, Safari Books Online.\n\nStallings, W., (2008). Computer Security: Principles And Practice , Sydney: Pearson Education.\n",
        "label": "human"
    },
    {
        "input": "Security of Your Computer and Ways of Protecting Essay\n\nIntroduction\n\nA computer is an electronic machine that is vulnerable to many risks. The computer ought to be protected from risks such as viruses that may affect their normal functioning. In dealing with computer security, many scientists have come up with different ways of protecting a computer from unauthorized parties.\n\nThere are many technical areas of computer security but the main ones are initialized CIA. This initials stand for confidentiality, integrity and authentication. Confidentiality means that other unauthorized persons cannot access what is in the computer.\n\nIntegrity is used to mean that the information in the computer cannot be hacked or changed by unauthorized persons. Authentication is used to mean that the information in the computer is only available and accessible to the authorized parties (Seong 24). The main reason for enhancing computer security is protecting it from unauthorized persons who cause destructions to the computer system hence interfering with confidential information.\n\nAnother issue with computer security is privacy that is related to people who use the internet everyday and they are supposed to ensure that they protect their personal information from the websites they deal with. To ensure that these security systems are in work, some fault-tolerance methods are necessary as discussed below (Seong 83).\n\nDiscussion\n\nMost of the software fault tolerance methods are advancements of the old hardware fault tolerance methods that were less effective in performance. Three software fault-tolerance methods are in use today as discussed (Bishop 45).\n\nRecovery blocks\n\nRandell developed the method in which an adjudicator is used to confirm the effects of a similar algorithm. The system using this method is broken down into small recoverable blocks that build up the entire computer security system.\n\nEach of the small blocks is connected alongside primary, secondary and tertiary case codes, which are just next to the adjudicator. According to Seong, \u201cthe adjudicator is used to show the effectiveness of the various blocks and incase the primary block fails, then it rolls backs the state of the system and tries to fix the secondary block\u201d (76). In case of failure of a block, then it reveals that the block is not worthy for use.\n\nN-version software\n\nThis method works parallel to the traditional N-version hardware. In this method, there are different models that are made up of N different implementations. Each of the variants/ models returns its results after performing any action.\n\nIt is from these results that the effectiveness of the modules is determined and then the correct ones can be known. This method is more effective as it can include hardware using multiple versions of software and the results are correct (Bishop 27).\n\nSelf-checking software\n\nThis method is rarely used compared to the previous methods. It includes extra checks that are set at some checking points. Some rollback recovery methods are also installed in the computer security system. In self-checking, the correct codes are obtained and then used. This method is however not effective because it lacks rigor (Seong 98).\n\nConclusion\n\nThe methods used to create fault computer systems have never been 100 percent effective. They are faced with some failures and most of them are 60-90 percent effective.\n\nThis means that more research needs to be done to come up with very effective and reliable methods. In addition, the methods are very expensive to execute hence the next generation must come up with cost-effective methods.\n\nWorks Cited\n\nBishop, Matt . Computer security: art and science . New York: Addison-Wesley Professional, 2003.\n\nSeong. H. Poong . Reliability and risk issues in large-scale safety-critical digital control systems . Washington, DC: Springer, 2009. Print.\n",
        "label": "human"
    },
    {
        "input": "ClubIT Computer Company: Information and Technology Solutions Research Paper\n\nTable of Contents\n 1. Problem of ClubIT\n 2. ClubIT Resources, customers and supply chain\n 3. Incorporating enterprise resource planning (ERP) in ClubIT\n 4. Supply chain management system\n 5. Customer relationship management (CRM)\n 6. Departments to be affected\n 7. Conclusion\n 8. References\n\nClubIT Computer Company has been in operation for the last nine years. Robert Fraser established the company in 2001. The company offers information and technology solutions to small and medium scale business.\n\nThe company classifies its services and products into five categories: managing IT services, installation of hardware and software, web design or developing of in-house programs and finally it offers data recovery services (ClubIT corporate website, 2010). This paper evaluates strategic management styles in the company and gives recommendations on how to improve the services offered.\n\nProblem of ClubIT\n\nThe company has highly experienced management team, who are not only trained in information technology but have basic management skills. However, the company is faced with the following problems:\n\n  * The company\u2019s target market is small-scale businesses of between 2-200 employees; the company does not have a plan of advancing their technologies to cater for increased need of their target market as they develop to large-scale companies.\n  * Being a private owned company, the company lacks a strong succession plan\n  * The company lacks knowledge management strategy. Information and technology is changing drastically, maintaining employees are important in the sector but ClubIT does not have such a plan.\n\nClubIT Resources, customers and supply chain\n\nThe company main resources are in the experience and innovativeness of its employees. The company employs highly trained and talented IT experts who offer solutions to their target customer needs. When developing software, the company benefits from the experience of Robert Fraser, who has over 28 years of experiences.\n\nIt also taps intellectual assets in their young and energetic employees. To ensure that the company has quality hardware both for internal use and to supply to customers, the company has established strategic partnership with world major suppliers like Dell, HP, Lenovo and Microsoft. The company is a member of AJAX-PICKERING.\n\nThe company target market are small-scale traders, it aims at offering information and technology solutions to organisation of employees more than two but not more than 200. It offers various services that include, data recovery, development of websites and in house programs, supply of hardware and software and Information software and hardware maintenance services.\n\nThe company have an integrated supply chain management that deals with inwards, outwards and reverse logistics. The procurement department is active in ensuring that the company gets materials at the right time and the supplies are of an appropriate quality and quantity.\n\nThe company have good relationship with some of its suppliers and maintains a partnership kind of relationship. Some of its suppliers are Dell, HP Computer Company, Lenovo Computer Company, and Microsoft Software Company (ClubIT corporate website, 2010).\n\nIncorporating enterprise resource planning (ERP) in ClubIT\n\nEnterprise resource planning (ERP) systems are strategic management tools that ensure that both internal and external processes in a business are efficient. It uses computer-based applications to manage different section of a company. It incorporates transactional backbone, advanced application and management dashboard (Alexis, 2007). Lets discus two ERP systems to be implemented in ClubIT:\n\nSupply chain management system\n\nAdopting an integrated supply chain system in the company will ensure that there is constant supply of goods and services. The system should look into forward, reverse and inwards logistics. The system will ensure there is an appropriate management of stocks that there is no one particular time that the company will have more stock than it requires or have a deficit of stock.\n\nThe purchasing department has the role of ensuring that at reorder level, there are appropriate measures taken to have supplies in time. An integrated supply chain management will offer information on the reorder level of various products (Sarika, 2004).\n\nWhen the company adopts and integrated supply chain management, it will have a just in time stock management system. This system ensures there are minimal stocks in the warehouses thus reducing warehouse costs.\n\nAn integrated supply chain management assists in maintaining good supplier company relations. This is where the company and its suppliers maintain good partnership. This partnership is build when supplies are paid in time, delivery made in time and supplier makes quality supplies.\n\nWith an integrated supply chain, then the company will be able to monitor its creditors, have a ensure that quality goods are delivered when needed (Joel, Keah-Choon and Keong, 2008)\n\nCustomer relationship management (CRM)\n\nCustomers are the backbone of a company, when they buy from a business; they enable the firm get profits. Winning customers and maintaining leads to business continuity.\n\nTo win customers, the company should make products that address the needs of the target market. ClubIT target customers are small-scale traders, their needs in information technology varies and thus the company should ensure it has quality packages to address the varying needs. The cost of products sold by the company should be favourable to the customers and in this way, it will be able to sell a wide variety of products.\n\nHaving a computerised customer relationship management (CRM) system will assist ensuring that the customer needs are recognised and products are improved accordingly. For example by collecting and analysing customer feedbacks, the company will be able to know the kind of software that customers want and evaluate the level it is able to satisfy customers with the current products. This will go a long way in developing good customer relations.\n\nWhen a company has good customer relation, it is likely to develop customer loyalty that will boost its sales in short and long term (Peelen, 2006).\n\nDepartments to be affected\n\nDeveloping an integrated supply chain management will affect the following departments:\n\n  * Procurement and supplies department, they will use the system to make strategic decisions on which companies to procure from , the quantity to procure and the time to procure\n  * Stock management department, the department will be assisted in maintaining the appropriate level of stock, know when they expect additional stocks and what amount\n  * Warehouse department, the department will know the size of warehouse to maintain at any one point and when they expect to get goods\n\nCustomer relationship management (CRM) will affect the following department:\n\n  * Customer care service department, they will use the system to collect feedbacks from customers\n  * Research and development department, the department will be using information from the system in advising the company on the kind of products to make\n\nConclusion\n\nClubIT is an information and technology solutions company; it targets small-scale traders. The company boosts of its highly experienced and talented employees who can develop programs that meets the company customers\u2019 needs. To improve its business processes, the company should implement enterprise resource planning (ERP) systems to manage internal and external processes.\n\nReferences\n\nAlexis, L.(2007). Enterprise Resource Planning . New Delhi: Tata McGraw-Hill.\n\nClubIT corporate website. (2010). ClubIT. Web.\n\nJoel, D., Keah-Choon, T., Keong L. (2008). Principles of Supply Chain Management . New Jersey: Cengage Learning.\n\nPeelen, E.(2006). Customer Relationship Management . Amsterdam: Pearson Education.\n\nSarika, K. (2004). Supply chain management: creating linkages for faster business turnaround . New Delhi: Tata McGraw-Hill.\n",
        "label": "human"
    },
    {
        "input": "HP Computer Marketing Concept Essay\n\nTable of Contents\n 1. Introduction\n 2. HP marketing Concept\n 3. Case study\n 4. Crisis management\n 5. HP crisis\n 6. Solution Matrix\n 7. Conclusion\n 8. References\n\nIntroduction\n\nThe marketing concept is the criteria that firms and organizations use to meet the needs of their clients in the most conducive manner. A successful marketing concept is based on the philosophy that is tailored in a manner to satisfy the needs of the customer as prerequisite of their profits gains.\n\nMany organizations are more interested in acquiring huge profits at the expense of customer satisfaction. However, HP Company has resorted to ensuring that it meets the needs of their clients through production of products aimed at the customers\u2019 satisfactions.\n\nApart from merely producing customers\u2019 specification, the organization has hi-tech systems, which ensure the ordered computers are produced promptly to avoid delays. In addition, the organization has also streamlined its delivery mechanism to ensure clients do not wait for their products for long before delivery. This report evaluates the HP marketing concept in terms of how it has helped the organization to wedge out its fierce competitors.\n\nHP marketing Concept\n\nThe Hewlett Packard Company (HP) has often strategically reevaluated its marketing concept to provide the best customer satisfaction compared to its competitors such as Dell and Toshiba. Barton (2007) illustrates that the organization\u2019s resolve to focus its operations on the customers is guided by believe in the power of strong brands.\n\nThe strong brand is build by developing strong customer relations. The initiative has paid off positively as the organization has reduced its cost by over ten per cent. Additionally, the organization has endeavored to use consumers as a channel to reach small business projects. The existence of a strong customer linkage has helped the organization only to produce what is required by its customers, hence reducing piling up cash through over productions.\n\nHP\u2019s marketing concept has been based on setting realistic goals, which are attainable within the specified period. On the other hand, the entire marketing concept is supported by a strong system that conveys timely information and feedback from both the customers and the organization.\n\nThe HP strategy to engage clients in all its operations is a clear demonstration of an effective marketing concept. According to McDaniel and Gates (1998, p. 3) an efficient system is the one that is founded on three foundations, which are consumer orientation, goal orientation and system orientation. Similar sentiments were echoed by Kotler, et al (2001, p. 488), who argued that a company\u2019s products are worthless without consumers to purchase them.\n\nBased by the above analysis, it is evident that HP as an organization has designed a marketing approach that meets all the essentials of a good marketing concept. On its part, HP as an organization has earnestly striven to meet the demands of their clients in the best way possible. In addition, the organization has repeatedly carried out intensive research operations aimed at ensuring equipping the firm with up to date information about customer needs.\n\nCase study\n\nAfter an intensive research, the Research and Development (R&D) department found out that customers required portable, light and sufficient Notebook PCs (Ken, 2005). The existence of a functional online feedback system has helped to meet the distant clients whose observations and recommendations were integrated in the development of the system. Therefore, the R&D then resolved to develop PC Compaq Evo Notebook N1015v. This mini laptop best suits the specifications that were identified from the customer survey process.\n\nApart from the cost of this model being cheap, it is fixed with a powerful battery backup that can last up to four hours before recharging. The portability of this computer was so high since it would fit comfortably in people\u2019s handbags as well as occupying a small space in men\u2019s briefcases.\n\nIn addition, the system has also been fixed with portable speakers to help the customers who suffer from hearing-impairment. This product was received well, leading to massive production that has ensured the organization fulfills the needs of its customers in the best very possible (Hewlett Packard, 2011).\n\nThe merger between the HP Compaq and the HP in 2002 has helped the firm to improve its operations and develop a solid customer relationship plan. HP has developed two main initiatives; \u201cachieve more with less\u201d and \u201coperation one voice\u201d (Hewlett Packard, 2011). The two projects have strengthened the firms system to reduce redundancy and hence continued production.\n\nThese two initiatives have greatly helped the organization to connect to its customers as well as enlarging customers\u2019 linkages with the firm. The overall outcome has been intertwined mutual relationship between the firm and its customers.\n\nCrisis management\n\nCrisis management is the process through which organizations and firms deal with key events and situations that intimidate or are challenging to the firm, its shareholders, or the public fraternity. For any condition to qualify to be referred to as crisis, it should have three main characteristics. Firstly, it should pose some threats to the organization. Secondly, it should have an aspect of surprise. Finally, it should provide a brief decision making time (Barton, 2007, p.112).\n\nHowever, a section of scholars describe crisis as the situation that arises when an existing system can no longer be sustained. Therefore, crisis management is a multifaceted deliberated action aimed at reinstating the initial situation. Due to its demanding nature, the management is expected to deploy all its resources to contain the errant situation to avoid further losses. Conversely, some managers opt to use preventive measures to mitigate the extent of the loss.\n\nThere are several kinds of crises, which are generated by diverse causes. The main causes of crisis include natural disasters, technical challenges, management misconduct, deception, confrontations, malevolence and skewed management virtues.\n\nManagement needs first to understand the cause of the problem before making any action. Just as common knowledge demands, the management should deal with the root cause of the problem to avoid future recurrences. For any organization to deal promptly with a crisis, it needs to have adequate risk warning systems to alert the management of impending threats to avoid sudden surprises.\n\nHP crisis\n\nThe giant technological Organization has been suffering from management crisis ever since merging with HP Compaq In 2002. Although HP Company had a stable management for the last one decade, the situation has been so strained since the merger in 2002. Carly Fiorina\u2019s strategic leadership blueprint that was developed to provide a cohesive leadership during the merger has proved unreliable.\n\nAnalysis of the leadership strategy has proved that mergers require a more complex multi-layered decision making processes rather than an operational standpoint. Although Mark Hurd, the then HP CEO is a brilliant leader, his management has fallen short of the requirements of leading a merging initiative (Chu, 2010).\n\nAfter a flourishing merger that resulted into hefty profits, the HP Company\u2019s revenues have been on the to the extent that it had to be declared a crisis. The crisis grew in magnitude and consequently compelled the CEO to resign. The management crisis was exacerbated by the rapid expansion of the organization that has led to opening of numerous branches worldwide.\n\nThe management has reached the point where they feel much stretched that they require injecting new blood into the system in order to bring harmony and stability in its operations. Although the firm has been doing well, its steady revenue decline has reached its worst ever. Hence, it was paramount for Mark Hurd to come up with new strategies or just step aside and give room for a total overhaul of management with new strategies and solutions that are more creative.\n\nSolution Matrix\n\nHP\u2019s board of directors proposed several remedial measures to reinstate the organization to its expected performance levels. These measures are aimed at bringing a new management board that will have several decision-making organs that report to the board of directors.\n\nSuch a management system will not only provide a wide range of changes but also new approach to issues. The management overhaul is also expected to enhance teamwork thus ensure management is not practiced single handedly. By fostering delegation as a leadership system, the management will ensure that any errant decision by a single manager is detected and corrected before it is too late.\n\nThe board suggested that the organization ought to redesign its mission to accommodate the future needs adequately. This was made by ensuring that the Research and Development department is more vibrant in developing the next generation\u2019s computers.\n\nThe same department is also expected to generate creative solutions to the problems that are facing the organization currently. However, this suggestion proved to be an expensive venture, which required that adequate financial resources be allocated for such a noble investment (Chu, 2010).\n\nThe other major solution consisted in the appointment of the new CEO MR. L\u00e9o Apotheker, on September 30, 2010. The New CEO is expected to redeem the blurred glory of the giant computer firm.\n\nApart from dealing with the current performance slump, the CEO is expected to spearhead and steer the organization\u2019s strategic plan. The immediate impact of the new CEO has been felt in terms of the revival of the organization\u2019s revenues. This is a good omen and it suggests that the organization has been able to turn around its worst economical performance.\n\nConclusion\n\nHP Computer\u2019s exemplary performance over time has been due to its well-tailored marketing concept. The focus of marketing in HP is on developing strong customer relations to meet customer needs in the best possible way. Its marketing plan has helped the organization to develop customer specified computers, which in the end have helped the organization in reducing costs.\n\nDespite its initial phenomenal performance, the organization has been faced with management crisis that forced the CEO to resign even after helping HP to merge with the Compaq Company. Through a managerial overhaul, which consisted in the board of directors appointing a new manager, the firm seems to have overcome the crisis and it is reclaiming lost glory. This is indicative of a good future for the organization after having had a crisis.\n\nReferences\n\nBarton, L., 2007. Crisis Leadership Now: A Real-World Guide to Preparing for Threats, Disaster, Sabotage, and Scandal. McGraw-Hill: New York.\n\nChu, D., L., 2010. HP\u2019s Hurd Crisis Creates a Value Play . Web.\n\nHewlett Packard ., 2011. The HP Corporate Information . Web.\n\nKen, J., 2005. CMA Helps Hewlett Packard Implement Marketing Strategy Pollution in the Government . Computer Associate Press: Vienna.\n\nKotler, P., Cunningham, M., H., & Turner, R., E., 2001. Marketing Management . Pearson Education: Montr\u00e9al.\n\nMcDaniel, S., D., & Gates, C., R., 1998. Marketing Research Essentials . West Publishing Company: Ohio.\n",
        "label": "human"
    },
    {
        "input": "Tablet PCs Popularity and Application Report\n\nTable of Contents\n 1. Introduction\n 2. Types of Tablet PCs\n 3. Current and promising Future types of Tablet PCs\n 4. Popularity of Tablet PCs\n 5. Future Predictions for Tablet PCs\n 6. Sales Charts of Tablets PCs\n 7. The Motorola Xoom Vs iPad\n 8. HP multi-touch Tablet PC\n 9. Conclusion\n10. Works Cited\n\nIntroduction\n\nArguably, the tablet personal computer emerged from the concept of laptop PC but evolved to incorporate a touch screen controlled by a stylus light/digital pen and hand calibration for finger sensitivity, thus making the mouse and keyboard feature of a laptop obsolete.\n\nThe main reason why people prefer the tablet PCs is the ease of mobility and operability compared to the laptops and the desktops. The tablet PCs are in various shapes styles an types but the most common types are convertible tablets, slate PCs, booklet tablet PCs and the hybrid tablet PCs.\n\nTypes of Tablet PCs\n\nThe stale tablet PCs get its name and resemblance from the slate writing pads, which do not have any keyboard attachment or features. The data input occurs through hand writing recognition using the touch-pad pen/stylus or the finger touch. These tablet PCs are also user friendly and allow connection of various external peripherals such as keyboards in case the user needs them. The external devises use the latest wireless or USB connection features.\n\nToday, firms or industries such as the education sectors, health-care industries or manufacturers, who depend on mobility and constant communication with employees or clients, utilize the tablet PCs technology for full-time internet connections. The slate tablets are also more resistant to harsh climatic conditions such as high humidity, dust, heat and accidental impacts such as dropping (Jacko, 34).\n\nOn the other hand, compatible tablet PCs have a similar appearance to the laptops with touch mouse pads and keyboarding features but have incorporated the option of 180 o screen rotation. These PCs forms the bridge between the laptops and full tablet PCs since they offer both the keyboard usage options and the digital pen input options.\n\nIn contrast, the booklet tablet PCs get their \u201cbook\u201d name from the reasoning that pages of a book are written on both sides and this PCs have dual screen on either of their sides. They can therefore be opened like a book. They allow the touch detection mechanism as well as the digital-pen usage. The PCs are often preferred due to their support of internet TV, internet browsing, and electronic interpretations (Jacko, 34).\n\nLastly, the Hybrid tablet PCs has high technological functions that both the slate tablet and convertible PCs have. It is referred to as the \u2018Hybrid\u2019 due to the incorporation of various features of earlier designs. The convertible features are evident when a keyboard is incorporated, while it acts as a slate tablet PC when the keyboarding features are removed (Jacko, 34).\n\nCurrent and promising Future types of Tablet PCs\n\nAmong the tablet personal computers was the iPad launched in mid 2010. Various launches competed with the iPad and there was need to have a clear definition or identity of simple tablet computer. A tablet computer either has a touch screen feature or permits use of a virtual keyboard or writing recognition.\n\nThe term \u2018Virtual computer\u2019 first emerged from Microsoft and it is arguable that the first operating system incorporated on tablet PCs was from Microsoft. Today the PCs use various operating systems from different companies. The e-readers have fewer functions compared to those of a tablet PC, thus causing a bump up over their categorization as tablet PCs. Tablet PCs such as iPads have the e-reader features, therefore an iPad is an e-reader (Gookin, 20).\n\nThe low-powered iPads lack various common feature found in normal laptops and desktops especially some publication demanding features. They are thus easy to carry and handle but limit usage and thus cannot complete the tasks of a laptop or desktop effectively. The origin of tablet computers emerged due to the need for full-time access to various features.\n\nThe need included communication via e-mail, browsing, downloading, uploading, or data storage. Today there is a wide range of the tablet computers with different functions for different needs. According to Gookin (20), rating pricing as well as comparisons between tablet PCs depends on analysis of size (for portability), convenience, features and power.\n\nPopularity of Tablet PCs\n\nThe tablet PCs are more popular among students and personnel in commerce industries due to the evident portability advantages over other earlier types of PCs. The tablet PCs are slightly bigger than the smart phones but slightly smaller than notebooks. The reduction of microprocessor size and establishment of better battery life span are the two main properties that determine improvement of a tablet PC.\n\nCurrent tablet PCs have longer battery life, support handwriting features, wireless technology such as server access, enlarged memory and better screen resolution. The aim of future tablet producers such as HP Company is to establish a multi-tasking gadget that is able to support wider range of features or options for expanding usage.\n\nIn line with Books Llc (91), HP established a tablet PC to counter the high costs of iPad tablet PCs. The company today has one of the global best-selling tablets PC with equally favourable features such as flash players, a high-resolution touch screen, effective hard drive space, built-in stereo audio system, effective RAM, microphones and support of external memory such as support of Micro-SD or USB connectors for drives.\n\nThere is a clear indication that the tablet PCs are becoming better day-by-day because of their support of normal feature found in normal PCs such as laptops or desktops. A good example is the touchpad by HP that incorporates a built-in WI-FI feature to support faster access of internet files and compatible play-in features that support various video and audio files.\n\nThe tablet PCs are also competing for the ability to offer features that will win client\u2019s confidence such as pleasurable and easy to use touch screens. The assured convenience is a sure marketing strategy since it stimulates acquisition and usage.\n\nFor instance, would a client prefer a tablet PC that has a hand touch screen only or one with both the touch application and an allowance for digital stylus pen? Most tablets have slower hand writing speed than the typing speed and thus future PCs would be more appropriate if they allow additional external keyboards and mouse connections to facilitate user needs.\n\nFuture Predictions for Tablet PCs\n\nThe future of tablet PCs depends on ability to produce gadgets with more inbuilt features, more storage space and better portability. This will require a state of the art technology that will enhance ease of usage to combat existing computability and complexity involved in usage.\n\nProbably the technological changes of chips will see the emergence of paper-size or virtual gadgets. We are getting to a stage where users will not need to worry about the struggles of keypad entry of data. The monitors will have higher resolutions that allow easier and faster digital manipulations. The easiness of usage and user-friendliness of the features will also be main determinants of sale and advancement of tablet PCs.\n\nThe battery life is another feature that will determine the generational differences of tablet PCs. The future gadgets will have longer spans and therefore users will have little to worry about the battery life during usage. The future technology will also allow adjustments to cater for customized handwritings. The users will be in a position to have input in form of personal handwritten types, as an alternative to the user defined keyboarding input requirements.\n\nThe time required to put on the gadgets will also decrease meaning that the delay time will be reduced. The future tablet PCs will be paper-size gargets an indication of reduced overall weight for better portability. Like the smart phones majority of the current tablet PCs support different screen orientations, for instance the ability to adjust the orientation automatically between portrait to landscape setting and vice-versa for easier handling and comfortable operability.\n\nSales Charts of Tablets PCs\n\nThe year 2010 was non-contentious year for iPad from Apple since the company had a clear-cut sales record of approximately 3.3 million units within three months of its release, compared to other competing tablet PCs. It is still the reading seller with a rate of approximately 4.5 million units sold every quarter.\n\nHP is also set to release its three tablet PCs and Motorola XOOM is causing blizzard and heated comparison debates. Other companies include LG and Samsung who are promising to avail irresistible offers on future Tablet PCs (Vermaat, 226).\n\nThe Motorola Xoom Vs iPad\n\nApple Company introduced its iPad 2 tablet personal computer in 2 nd March 2010 in a launch that appeared like a war against Motorola XOOM, which had hit the market slightly earlier in later February. The iPad 2 tablet PC was launched in March 11 and Apple was therefore categorical to indicate their eagerness to have the best Tablet PC in the market (Vermaat, 226).\n\nThe users are today seeing various marketing adverts from both companies and it is a clear indication that future gadgets will depend on what extra feature they offer their clients. Is the extra feature important or just a marketing gimmick?\n\nThere are some critical features a user may consider when comparing what is on offer today and what will be in future. First are the specifications of the hardware. The hardware mainly concerns the physical representation of the PC. Secondly, the user must determine the usability in terms of what the new gadget offers, mainly the applications\u2019 and software offers for the user. Marketers are keen to interpret what users want on their future tablet PCs and how they interpret the current hardware as well as software applications.\n\nAccording to Vermaat (226), the differences of the operating systems are also properties that future developments will have to consider. Currently, iPad uses the Apple\u2019s Operating System (iSO) as used in various smart iPhones. On the other hand, the most recent Motorola XOOM a competitor of iPad uses the Android 3.0 software also referred to as the \u2018Honeycomb\u2019, which is customized for the tablet versions of PCs and other iphones.\n\nThe Android 3.0 software is arguably more complex and rigid for PCs since there is need for users to have flexibility during applications and therefore Apple\u2019s iPad is more applauded due to simplicity tricky and instinctive operation system software, over Motorola\u2019s XOOM. This means that software sells hardware and future manufacturers will need to consider the type of software to implement.\n\nConsidering the current XOOM and iPad as examples of the latest tablet PCs, one would easily settle for the Motorola XOOM due to its specifications such as higher screen resolutions, better and wider screen displays as opposed to the iPad\u2019s displays. IPad screen views have black bars at the top and bottom of the screen during wide-screen viewing. The future tablet PCs will need to increase the processor speed from the current dual core to core-two dual processors, like the current desktops and laptops.\n\nThe Motorola XOOM tablet has various extra features such as the inbuilt stereo system, HDMI high definition video output without requirement for extra connections and provision for USB port connections. The cameras of the Motorola XOOM are also of higher resolution count compared to those of iPad 2. These features make the Motorola more superior and a possible future preference compared to other similar tablets from potential competitors.\n\nThe future Tablet PCs must be able to offer 4G inbuilt cards and options of upgrading the software to latest technologies. The compatibility with other common software such as printer software is equally essential. Although the appearance of iPad is more appealing such as better and thinner look compared to the potential competitors such as Motorola, the application features are thinner in terms of applications, than those in competitor\u2019s products are.\n\nThe trade name is a clear marketing feature for the Tablet PC products and Apple has better advantage over competitors due to well established trademark in the PC industry; nevertheless, the offers the definitely below future expectations.\n\nHaving application is important but having support of the developers is an even greater future advantage. The iPad has been in market for over a year and the developers have had a chance to incorporate various applications, majority of which are in use on various iPhones. Competitors of iPads such as the Motorola XOOM and HP touchpad are at the verge of forming a strong establishment in the industry.\n\nAlthough Apple\u2019s selection of software for its Tablet PCs dwarfs the competitor\u2019s efforts to incorporate similar or better software, Apple has not been able to form a better future establishment due to failure to incorporate vital software like the flash. XOOM uses flash while iPad does not.\n\nConsidering the current multimedia requirements, majority of the web features will need to run on flash applications. Future user-requirement will make Tablets with features such as those of an XOOM more superior than the competitors will. Long-term considerations therefore indicate that expandability will overrule current simplicity functions.\n\nThe XOOM definitely have better long-term offers compared to iPad for instance offer of higher specifications like the 4G technology and flash features. The future demands will strongly be bases on the ability to incorporate a wider variety of software (O\u2019Reilly, 29). This means that considering the above status, XOOM is a more preferable future Tablet PC.\n\nCurrent iPad tablet PCs lack features to enable future upgrade options such as change of 3G to 4G technology or failure to incorporate the USB connectors for easier compatibility of various devises such as external hard drives/storage devices. The need to cater for future expansion is important because such allowances cater for the future other than the present needs.\n\nXOOM users are currently surfing the web at 4G speeds while the iPad 2 users will have to wait for the establishment of an iPad 3 that will offer better surfing speeds and probably expansion abilities to cater for extra external storage. A good technological device for the future incorporates some of the future requirements other than forcing users into new purchases whenever they require an upgrade.\n\nThe Tablet PCs are meant for gaming, audio and visual plays. Any of the future devices should therefore incorporate the stereo options even on external speakers. The current market price of 32 GB 4G XOOM is $71 higher than the same 32 GB 3G Ipad 2.\n\nThis means that it is wiser to purchase an XOOM due to the extra applications and features now, than wait to by another iPad 3 that will run at 4G speed in future at approximately $500. The appearance might be stylish and the company name might be big due to earlier establishments in the industries or the reputable history, but this may be deceiving for future prospectors.\n\nHP multi-touch Tablet PC\n\nThe new-looking HP multi-touch Tablet PC that will be launched soon has features meant for the near future. The support of multiple touch signals ability to accelerate the visual displays through the use of an accelerometer, detection of user position to adjust its orientation to automatically suite usage and the use of the attest Microsoft operating system; window 7, makes the devise more superior over potential competitors (McNamara, 19).\n\nOther current establishments are the incorporation of notebooks and tablet PCs features, such as the hybrid Ideapad U1 that has a detachable touch screen acting like a tablet on its own. The notebook also runs on a core-2-duo processor based on a storage space of 128 GB and provides keyboard and mouse features as well. The standalone Tablet PC however runs on Linux platform powers by an inbuilt 16 GB memory and 1GHZ processor. The base of the tablet also functions as a 3G wireless hub to ensure constant internet connection.\n\nMost of these future smart personal computers will be able to support multi-tasking such as audio playback as well as net surfing. Although these devices are presenting a spackle in the technology industries, most users are still wondering when they will get a device that incorporates all the required needs. The devices offer fairly new and unique features different from each other due to need for competitions, but there is lack of a tablet PC that will offer all these features without fluctuations.\n\nThere is a misunderstanding over preferences and standard requirements. People will prefer to have full and speedy internet connections wherever whenever, but they are forgetting that once in a while they also need, quality entertainment, editing, typing, formatting and printing a letter or other formal documents. Although the tablet PCs are called computers, none of the current offers has all this features. Future requirement will need a tablet PC that offers all requirements for less.\n\nConclusion\n\nToday Tablet PCs, the PDAs, smart phones and notebooks/palmtops are constantly replacing the laptops. This evolution is due to people\u2019s demands for something more portable.\n\nPortability means thin, stylish, caters for touch facilities and is friendly especially for the social sites users who need to be fulltime browsers, regardless of place and time constrains. Future demands are however very specific. There is need for better portability than what laptops offer, better and dominant features than what notebooks offer and stronger user-comfort than what smart phones offer. The tablets PCs are having all this demands such as e-reader requirements and more versatile and user-friendly features.\n\nThe success of well-recognized brands like iPad 2 prompts immediate pessimism over sales of tablet PCs. The gadgets are mainly meant for entertainment since users are not ready or used to pointing at the screen all day long. It s also not possible to hold a chat while still enjoy a movie, thus some of the reasons that tablet PCs will not completely replace the notebooks and laptops.\n\nMost users are also adapted to the mouse and keyboarding techniques. Although tablets may have stylish designs and designer looks, lack of ability to perform might be some of the reasons why the sales of iPads are far worse than Iphones. The performance of iPad in the market therefore depends on the cost and its stylish designs. The future economic performance of the tablet PCs will depend on changes that manufacturers will implement. These are technological developments, whose life span can only be judged by time.\n\nWorks Cited\n\nBooks Llc. Tablet Pc: Ipad, Joojoo, Comparison of Tablet Pcs, Microsoft Courier, Hp Touchsmart, Itablet, Adam Tablet, Hp Compaq Tc1100 . New York, NY: General Books LLC, 2010. Press.\n\nGookin, Dan. Laptops for Dummies . Indiana, IN: Wiley Publishing, Inc. 2010. Press.\n\nO\u2019Reilly, Tim. Web 2.0: A Strategy Guide . California, CA: O\u2019Reilly media Inc. 2008. Press.\n\nJacko, Julie. Human-Computer Interaction. Interacting in Various Applications Domains . Germany: Springer-Verlag Berlin Heidelberg. 2009. Press.\n\nMcNamara, Joel. Netbooks for Dummies . Indiana, IN: Wiley Publishing, Inc. 2009. Press.\n\nVermaat, Misty. E. Discovering Computers 2011-Introductory: Living in a Digital World. Boston, MA: Cengage Learning, 2010. Press.\n",
        "label": "human"
    },
    {
        "input": "The Impact of Computers Essay\n\nTable of Contents\n 1. Introduction\n 2. The Singularity is near\n 3. Conclusion\n 4. Works Cited\n\nIntroduction\n\nAdvancement in technology has various benefits as it makes it easier for people to perform tasks. Computers have for instance revolutionalised the way tasks are performed in levels no human mind can fathom. However, with more rapid improvement in computer technology one can clearly underscore its impact on human beings and society in general. One could equally speculate that the technology has an impact on a wide range of areas such as human relations, ethics, politics, religion etc.\n\nThe Singularity is near\n\nScholars and even philosophers have toyed with the idea that the computer could in the long run replace the human capacity to do tasks. This will lead to a situation where all human beings will resort to computers. Morris (98) argues that \u2018\u2026 the ever-accelerating progress of technology gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.\n\nKurzweil sees singularity as a period during which the pace of technology change will be so rapid, its impact so deep, that human life will be irreversibly transformed.\u2019\n\nKurzweil talks of epochs which represent how humans\u2019 intelligence has exponentially grown. In the early epochs, he shows how intelligence power has evolved from biology, through information in DNA; technology, through information in neural patterns etc (Morris 105).\n\nIt is in the fifth epoch that there is an intersection where technology masters the methods of biology. Here intelligence of humans thus falls. He finally talks of a highly functioning human intelligence spreading throughout the universe. This intelligence is non-biological.\n\nTechnology could affect several aspects of social life. Kurzweil predicts a situation where technology would lead to immortality. Look at the impact this situation could impact on the resources; many people will clamber for the scarce resources. This could actually lead to unrest between nations and wars resulting from such demand would be the order of the day.\n\nIt is therefore worth noting the relationship of humans to computers over the next 100 years. I feel that human beings will over-rely on computers over the next 100 years to such an extent that human capacities will be overlooked (Morris 125). One can for example already see how the internet is used to do virtually everything from serving as a platform on which business transactions are carried to dating.\n\nIn Why the West Rules For now: The pattern of History and what they Reveal about the Future , Morris has clearly given a chronology of how civilizations have developed and failed through reasons beyond their control (Morris 114). He has, in addition, given examples of how rapid technological growth made Britain become the leader among economic and maritime giants; a record by which one can see how technology may lead to shifting power, basing on who advances faster.\n\nTechnological \u2018war\u2019 could lead to tension reminiscent of the Cold War. It is therefore worth noting how technological change could affect the international relations. Technological advancement may lead to redundancy in human labor. One can only guess how millions could immigrate in search of labor overseas and the effect this would cause nations.\n\nTechnological advancement has revolutionized the way we do things; it has cut the distance thereby making the world a global village (Morris 150). Communication is now faster therefore making it easy to do business on the virtual platform. Since people working on a globalised platform may need some agreeable standardization, I feel that this technological advancement may lead to a situation where human beings may develop global institutions of governance since technology has broken the geographical borders.\n\nConclusion\n\nIn conclusion, one can observe that there seems to be a pattern of events that can predict how events will be in future. That is, we could be headed to a form of singularity.\n\nWorks Cited\n\nMorris, Ian. Why the West Rules \u2013 For Now: The Patterns of History, and What They Reveal About the Future . Ontario: McClelland & Stewart, 2010\n",
        "label": "human"
    },
    {
        "input": "Strategic Marketing: Dell and ASUSTeK Computer Inc Report\n\nExecutive Summary\n\nDell Company is an international enterprise dealing with computers and its accessories. It is based in Round Rock, USA and employs over 96,000 employees in its worldwide branches. The name Dell was derived from its founder Michael Dell.\n\nGrowth of Dell Company is attributed to mergers and acquisitions including the take over of Alienware in 2006 and Perot Systems in 2009. Some of the products manufactured and sold by dell include: Personal computers, servers, software among other IT products. Marketing of these products depends entirely on the description of the market segment.\n\nManufacturing criteria of Dell also considers proximity to its customers such that just-in-time approach is of essence. In 2004, Dell Company limited took up an initiative to recycle products which lead to its subsequent award for promoting responsibility on the part of the producer.\n\nNotwithstanding, the technical support offered to customers, Dell company utilizes extensive marketing strategy with an aim of reaching many customers. Use of advertisement and dell kiosks has increased product penetration in the market. Dell kiosks were oriented to give personalized service to customers i.e. by way of shopping, or telephone services.\n\nASUSTeK Computer Inc. participates in manufacturing and sale of ICT equipment and software programs. Some of its major products comprise of: laptops, personal computers, broadband products, mobile phones and computer accessories. Its location is in Taiwan.\n\nIntroduction\n\nThis paper presents a marketing report of two internationally based companies namely: Dell and ASUSTeK Computer Inc. Dell are based in Round Rock USA while ASUSTeK is located in Taiwan. The ASUS manufacture ICT equipments to be sold in international as well as local market whilst Dell assembles components. It is important to investigate whether the company\u2019s management participates in strategic marketing and planning.\n\nIf so, then we shall explore the strategic Hierarchy of the business investment. Firms thrive under certain micro and macro environment. 2008 recession is one of the macro environments which affected negatively the productivity of both companies whilst the micro environment mainly captures marketing criteria used to outwit its competitors i.e. Apple. These environments will be investigated in-depth by this business report.\n\nDell and ASUSTeK Computer Inc have registered a substantial level of success from its business dealings due to maximization of efficient marketing strategies. The strategies which will be analyzed in the report comprise of: segmentation, target markets, positioning strategy, and ultimately SWOT analysis. Before concluding the paper, recommendation to the respective industries will be outlined.\n\nIndustry Background\n\nNearly all sectors in an economy depend entirely on information communication technology. ICT industry is therefore the backbone of both manufacturing and service sectors. This industry has registered a fast pace of progress in terms of technology. In the past, desktop computers with low speed were utilized.\n\nThis is not true in the recent times since Dell and ASUSTeK Computer Inc have managed to supply in the market mini computers with high level of speed. Dell\u2019s brands such as Vostro, OptiPlex have the features of reliability, long life, and serviceability (Koehn, 2001). ASUSTeK Computer Inc is also manufacturing pocket PCs, broad bands, and graphics cards with a target of meeting individualized needs of their customers. This is a clear indication that ICT industry is dynamic in nature.\n\nApple industry which is a major competitor released iPad tablet computer which was quickly embraced by the business world. Use of ipad in Japan had a great impact in the business world simply because of its resolutions and advanced features. Another factor contributing to the success of iPad is the use of stylish, supreme marketing and excellent branding of the products.\n\nFrom this brief analysis, we can conclude that, ICT industry emphasizes on introducing sophisticated product into the market in order to capture the attention of the target group and out-beat competitors. It must always be the objective of the company to satisfy the needs of the customers by producing quality goods and services.\n\nCompany background\n\nIn 1984, Michael Dell formed PCs limited to sell IBM PC-compatible computers from components of stock. These computers were sold directly to customers.\n\nThe aim of PCs limited was to understand the needs of the customers so as to be able to supply customers on a direct basis. Due to the success of the business, Michael dell dropped out of school to channel more energy on the new business. This was after he received a support of 300,000 Dollars from his immediate family. In 1985, Dell Company managed to market \u2018Turbo PC\u2019 on its own.\n\nThis computer was sold for 95 US Dollars. At that time PCs limited utilized the national computer magazines as the advertisement channel. Orders placed by the customers were organized in such a manner that it fell on a selection criteria. In its first year of trading, the company recorded gross earnings of 73milion dollars. In the year 1985, the company assumed another name, \u2018Dell computer Corporation\u2019 and began its international business expansion by entering Ireland.\n\nIn 1996, Dell started selling computers through its website and in 2002 its line of business stretched to include television and other computer accessories. This necessitated rebranding of the company name as \u2018Dell Inc\u2019. CEO Kevin Rollins took over from Michael Dell in 2004 and served till 2007.\n\nDuring this time, Dell acquired Alienware as a subsidiary company which introduced AMD microprocessors. Due to its dismal performance, Michael Dell took over as CEO again and in 2008 and acquired EqualLogistic with an objective of gaining a grip in iSCSI Storage Company.\n\nAcquisition of this company reduced manufacturing costs to its lowest level. Dell announced in 2009 a motive of purchasing Perot Systems to become a subsidiary company. Perot Systems specialized in strategic consulting, development of application and system integration (Koehn, 2001). It also dealt with outsourcing in addition to running call centers and handling claims.\n\nASUSTeK Computer Inc deals with computers and its application in information communication. Its products consist of consumer electronics such as desktop personal computers and other hardware devices. Some of the subsidiary organizations are: AMA Corporation, ASUS computer international, ASUSTeK holdings and other several well developed companies.\n\nASUS and Germin have engaged themselves in developing mobile phones located in some specific places. The product developed will be called Garmin-Asus nuvifone series. Furthermore, the company entered into partnership with DeviceVM LTD to integrate new technology of Splashtop which allows a user to boot computers and access internet within the shortest time possible.\n\nThere is also a possibility of forming a joint venture with Giga Byte technology Co. This means that Giga Bytes would transfer its assets and operation into the joint venture. Partnership between ASUS and a Chinese company, Huan Hsin Holdings, Ltd, would lead to manufacture of notebook casings. Another form of joint venture was with Disney Consumer Product inclined to development of Disney Netbook which is very efficient to use.\n\nStrategic marketing and planning\n\nStrategic marketing and planning describes how a business conducts its activities with an objective of realizing maximum returns from investment (Baker, 2008).\n\nIt captures overall activity of the business starting with production, establishment of market segments, interests of the customers, financial base of the business and the human resources. For a business venture to meet its wide goals and specific objectives, the management is under strict obligation to shape the company\u2019s products in a manner that will lead to realization of market needs. Strategic planning therefore relates to the overall direction of the business.\n\nTop management of a business venture ought to conduct a comprehensive review of the market and its opportunities continuously which enable them make a long term strategic arrangement. Strategic marketing plan integrates both the goals of a business with the needs of the customer.\n\nIn order to realize strategic marketing plan, planners need to segment the market with reference to such factors as Geography, Demography, Psychographic and behavioral factors (Baker, 2008). This stage is preceded by profiling the market segments while taking note of income potential, market share potential, and productivity potentials. Finally, a market segment marketing strategy is developed. This will require a choice between a market leader or product line, mass marketing or target marketing, direct or indirect sales.\n\nStrategy used by Dell has its basis on the following key elements: direct sales, market segmentation, sharing of data and information with supplies, customer care services and bunch customization. It is the hope of the company that the strategy will lead to realization of Dell\u2019s goal of becoming \u2018virtually integrated\u2019 business (Koehn, 2001).\n\nThis means that the customers, suppliers and the business are pooled together such that activities are conducted in real time basis. The aspect of selling directly to customers enables Dell to gain knowledge of customer\u2019s preference and needs. Feedback after a direct sale acts a source of information about design problems consequently leading to improvement of company\u2019s products.\n\nASUSTeK Computer Company participates in improving its products so as to meet individualized needs of the customers. This is evident with advent of new multimedia notebook and other products. Infiltration into foreign markets like China in addition to joint ventures with international business is a move towards realization of goals through strategic marketing and planning. As stated earlier, ASUS entered into joint with several businesses including Huan Hsin Holdings and Giga Bytes Company limited.\n\nMacro-environmental forces\n\nThese forces have its sources from outside the company. They comprise of factors that affect the running of an organization but cannot be controlled directly (Pride, 2009). The recent globalization is one of the macro-environmental forces affecting the running of business investment.\n\nGlobalization means that competition is stiff as more differentiated products enter the market. A company must make use of innovative marketing techniques in order to move with social, technological, political and economic changes. Dell and ASUSTeK Computer Company have a technical team mandated to research on the possibilities of introducing new products to match market requirements. Invention of multimedia devices and programs is actually a move towards countering macro-environmental forces.\n\nMicro-environmental forces\n\nThese are factors which influence directly company\u2019s strategies and they include: customers, employees, suppliers, shareholders, competitors and other internal factors (Pride, 2009). A firm can use these elements to formulate a workable marketing strategy after sourcing sufficient market information.\n\nMarketing mix can be used by a firm to arrive at the best combination which will ensure that the customers are satisfied. In terms of positioning its sophisticated products, Dell utilizes numerous distribution channels available worldwide. Advertisement through print and visual media is part of the promotional strategy employed by Dell. The slogan \u2018 Yours is here\u2019 which was formulated in 2007 aimed at creating awareness that Dell manufactures and sell customized computers thus satisfying individual needs.\n\nDell kiosks available in many parts of United States are places where customers get a glimpse of various computer products and they can subsequently place an order. Joint venture entered between ASUS and DeviceVM LTD was aimed at capitalizing on new technology of Splashtop which allows a user to boot computers and access internet within the shortest time possible. This is a clear distinction of micro-environment formulated by ASUS to remain competitive.\n\nSWOT analysis\n\nThis is the analysis of individual competencies by measuring, strengths, weakness, opportunities and threats (Kotler, 2009). It is the responsibility of the management to maximize on the strengths, reduce weakness, make advantage of the opportunities that comes along, and counter the impediments of threats.\n\nStrengths\n\nDuring their financial year ended July 2005, Dell registered more than 1 Billion Dollar profits translating to 28% growth. This growth is purely a result of its renowned brands. Direct sales to customers, use of information communication technology and maximum use of customer relation management assist Dell to capture on trustworthy customers. Another specific strength for the company is relatively cheap and readily available labor.\n\nASUS\u2019S strong financial base is derived from manufacture and sale of computer related components and equipments. During the financial year ended 2009, its revenues amounted to TWD 667.7million. The strength of the company also lies on the many subsidiaries located across the globe. This company attracts many customers due to its standardized products which meets immediate needs of the customers.\n\nWeakness\n\nExistence of large product and component supply can cause failure in operation of Dell business investment. In the year 2004, several computers were recalled due to the likelihood that they could explode or cause some technical problems. Inability to change suppliers reduces the choices available to the company.\n\nUsually Dell contracts with specific few suppliers which then increase the risk in case supplier defaults to supply or delivers substandard components. ASUS faces a weakness of penetrating markets where Dell has established itself. This is contributed by the fact that Dell has strong market defense mechanisms.\n\nOpportunities\n\nA change made in the position of a chief executive was an opportunity for the company to make more profits. Although Kevin Rollins came in as CEO, Michael Dell remained the chair person in the organization. Another opportunity for dell is the introduction of new products in the market as a diversification strategy.\n\nTelevision set, printers and phone accessories are different categories of products at the exposure of Dell. Selling low price and rebranded PCs in US market is a move towards formulating new market segments. ASUS has an opportunity to enter rich nations located in the Middle East. This will lead to more revenue generation.\n\nThreats\n\nCompetition is one of the biggest challenges facing dell in the global market. Apple and other new entrants contribute to stiff competition in the market. Even though Dell ventured into low cost markets i.e. Middle East, competitors still infiltrate such market segments. Fluctuation in the world currencies is another milestone pulling down the prospects of Dell. Recent recession saw frequent changes in the exchange rates. This is not a conducive environment for business as well for ASUS which depends on the global market.\n\nMarket segmentation\n\nTo ensure that all categories of customers were attended to, Dell made its market more homogeneous (Rainer, 2009). In the year 1998, most of the products were sold to businesses or government associations. Major part of the customers comprised of corporate sector that purchased approximately 1 million PCs annually.\n\nThis corporate lot bought PCs at the highest price level in the industry thus contributing to supernormal profits. Example of these corporate sectors include: Ford Motors, Toyota, Shell oil and Michelin. Sales to individuals and small businesses were conducted by phone or internet.\n\nDell customers utilized toll free line where they could call, place an order or get information about a product through fax. Payment was also effected through credit card.\n\nIn the international arena, call center with tall free lines were based in Europe and Asia. These centers were very efficient such that calls were rerouted to specific countries where customers could be attended effectively. An example is a call from Kenya which is rerouted automatically to US for an English speaking sales representative. Dell also conducts its sale activity via internet.\n\nASUS Computer Company managed to sell its products to different customers after deriving different market segments. This company serves both individual and corporate sectors with a bigger proportion of sales contributed by firms. Pricing strategy helps to differentiate the markets for ASUS products.\n\nNormally, corporate sector purchase product at a higher price level compared to small scale retailers and direct consumers. Supply of new netbook is part of the segmentation strategy where individuals are differentiated from the business world. Netbook was a very effective invention for the business fraternity since fast internet access enabled spontaneous placing of orders.\n\nTarget markets\n\nBoth print and visual media has been used by Dell to reach specific class of customers. Graphics used in advertisement takes into consideration the audience. To capture the attention of college students, Dell gives detailed specifications about the product which in real sense meets the needs of the student i.e. high internet interactivity. Online marketing by targeting specific corporate world and monitoring the results of an advertisement is an inclination towards reaching variety of market segments.\n\nASUS Computer Company is considering setting up markets in the sub-Saharan countries to capture the fast pace of socio-economic changes. Products manufactured by ASUS regard highly both younger and older population. PCs meant for the older class are simplified to ease its usage. The company also identifies the nature of young college students who value games and high mobility thus manufactures PCs and laptops to satisfy their needs.\n\nPositioning strategy\n\nThis strategy allows a company to avail its products to a place where there is a proximity to customers (Dann, 2004). Advertisement, packaging, and distribution are critical factors to attain wider market coverage. Dell opened up Dell kiosks where customers could catch a view of various products before placing an order.\n\nThis is a good description of positioning strategy. Sales agents in various countries play the role of availing dell\u2019s products to its customers. Display of beautiful models is a marketing approach utilized by ASUSTeK Computer Company.\n\nRecommendations\n\nWhilst e-commerce is the present approach to trade, Dell must consider advertising its products in an innovative manner. It is prudent to review marketing strategies in a continuous basis so as grasp socio-political and economic changes. Due to globalization effects, Dell and ASUS should establish more customer experience centers in African countries which are still in the verge of achieving its development goals.\n\nConclusion\n\nDell and ASUSTeK Computer Company are among the leading dealers in computers and its accessories. The success of these two organizations is credited to strategic marketing and planning initiated by the management. This is evident from analysis of segmentation, target markets, positioning strategy, and SWOT analysis of Dell and ASUSTek organizations. A recommendation made is for the companies to infiltrate African countries apart from Europe and Asia. This will expand market share of both organization.\n\nReferences\n\nBaker, M. (2008). The Strategic Marketing Plan Audit . Cambridge: Cambridge Strategy Publications\n\nDann, S., & Dan, S. (2004). Introduction to Marketing . Milton: John Wiley &Sons.\n\nKoehn, N. (2001). Michael Dell,\u201d in Brand New: How Entrepreneurs Earned Consumers\u2019 Trust from Wedgwood to Dell . Boston: Harvard Business School Press.\n\nKotler, P., et al. (2009). Principles of Marketing . Sydney: Prentice Hall.\n\nPride, W., et al. (2009). Marketing: Core concepts and applications . Milton: John Wiley &Sons.\n\nRainer, K., &Turban, E. (2009). Information systems and the modern organization. In Introduction to information systems. New Delhi: Wiley.\n",
        "label": "human"
    },
    {
        "input": "Computer Addiction in Modern Society Essay\n\nTable of Contents\n 1. Introduction\n 2. What Is It?\n 3. Conclusion\n 4. Reference List\n\nIntroduction\n\nIn psychology, addiction put in simple terms is dependency on substance or behavior. Dependency here is taken to imply an individual will do something to the extent of interfering with normal life functions; moreover, no matter how he/she tries to quit this action or behavior, he/she encounters difficulties.\n\nHistorically, addiction was taken to imply dependency on psychoactive substances; however, in contemporary times psychologists consider including things like pornography, work, computers, and gambling among others in the definition of addiction.\n\nMaressa Hecht Orzack comes in at this point where she introduces computer addiction by claiming it is addiction just like any other addiction; for instance, alcohol addiction. Maressa\u2019s definition that, computer addiction is an accurate description of what goes on when people spend large amount of time working on computers or online is true, timely, and \u2018accurate\u2019 and the writer of this paper strongly agrees with these claims.\n\nWhat Is It?\n\nIn her paper, Drug Addiction, What Is It? Maressa brings in a deep analysis of what she perceives as a new trend of addiction, \u2018computer addiction.\u2019 Psychologists agree that addiction is a \u2018hiding place\u2019 where people resort when under pressure or when running from something in their lives say, a duty or one\u2019s past.\n\nSimilarly, those caught in this newfound addiction have boldly confessed to be running from something in their lives. For instance, Maressa opens her paper by confessing she almost fell into this addiction; reason, she was running from her anger and inability to understand a new computer program manual, something that led her to escape to solitaire.\n\nThe habit of finding reprieve in something more interesting and pleasing in trying times is common with human nature. People tend to evade the reality, choosing to live their own lives as they find convenient. Unfortunately, fantasy does not mimic reality; sooner, these people find themselves at the point where they started with unmet obligations. Probably, Maressa had to face the reality of learning the computer programs with patience and persistence.\n\nTaking a closer look into the events of addicts according to classical definition of addiction, one realizes a common factor, excessive use, and dependence on a substance. Maressa draws from this observation when she notes, \u201cI concluded that this inappropriate and excessive use of the computer might be a distinct disorder\u201d (Orzack, 1998).\n\nNote the words she uses; \u2018inappropriate,\u2019 and \u2018excessive\u2019 and this fits well in the classical definition of \u2018addiction\u2019 as aforementioned. Moreover, in spite of knowing the debilitating effects of this behavior, the affected people compulsively continue practicing it, implying they have no control over themselves; hence dependency. In another case presented, by Maressa, Patient D resorted to computer games running away from her car wreckage.\n\nThis goes back to the initial argument that addicts are ever running from something in their lives. Maressa notes that people are hooked to computers because they derive some form of satisfaction and pleasure from it. This satisfaction and pleasure leads one to spend unusually more time on computer hence addiction. Nevertheless, there has been controversy concerning this issue; fortunately, Maressa offers a way out.\n\nMaressa notes categorically that there is thin line between productive use of computer and addiction. However, she is quick to point out even productive practices may turn addictive. For instance, she cites a group of people working on developing new computer hardware.\n\nThese people excluded themselves from normal lives, spending most of their time in laboratory, something that affected their social lives with regard to family and friends. Even though this practice was productive, it led to addiction and this qualifies Maressa\u2019s definition of addiction. Unfortunately, computer usage is becoming inevitable with the revolution that information technology is causing in all life practices. What is the way out of this quagmire? Maressa offers insight to this issue by comparing it with eating disorder.\n\nFeeding is inevitable and there is no way individuals can abstain from eating even those addicted. Nevertheless, these individuals can adopt healthy feeding habits; similarly, computer addicts can adopt healthy computer usage practices with help of professionals to overcome their addiction. Therefore, Maressa\u2019s observation on computer addiction is correct given that, it shares most factors with other known forms of addiction.\n\nConclusion\n\nComputer addiction as Maressa defines it is an accurate description of what goes on when people spend large amount of time working on computers or online. Based on knowledge from earlier studies on addiction, this definition fits in clearly. Classical definition of addiction considered addiction as dependency on substance use characterized by continued and compulsive use of drugs.\n\nBased on this observation, the key words here are \u2018continued\u2019 and \u2018compulsive\u2019 and when applied in computer usage, these terms qualify such a practice abusive. Maressa gives a detailed account of how people in the past have used computer to hide from life realities thus being hooked including her experience in 1995 when she was trying to run from studying new computer programs. Therefore, Maressa\u2019s definition of \u2018computer addiction\u2019 is true and precise.\n\nReference List\n\nOrzack, M. H. (1998). Computer Addiction. What Is It? Psychiatric Times. Retrieved from < https://www.psychiatrictimes.com/internet-addiction/computer-addiction-what-it >.\n",
        "label": "human"
    },
    {
        "input": "The Alliance for Childhood and Computers in Education Essay\n\nTable of Contents\n 1. Microsoft Word and Writing Poetries\n 2. Presentation for the Parents\u2019 Open House Evening\n 3. Computers Are Needed in Classes\n 4. Reference List\n\nThe Alliance for Childhood is one of the most famous organizations, which aim at promoting children healthy development, their enjoyment of this life, and love to education. Nowadays, the vast majority of children cannot imagine their lives without computers: they may communicate being far from each other, play various games, listen to music, find more friends online, and get their education.\n\nHowever, as The Alliance for Childhood states, people\u2019s faith in technology should not be higher than children\u2019s needs. The representatives of this organization admit that \u201cthe renewal of education requires personal attention to students from good teachers and active parents, strongly supported by their communication.\u201d ( The Alliance for Childhood )\n\nLots of parents think that the earlier their child will learn how to use computer, the better job will be waiting for him/her in future. The Alliance for Childhood cannot agree with such a point, neither do I. Parents are afraid that their children may be left behind, this is why they are eager to use all chances, which are presented to them. \u201cWiring and computerizing America\u2019s schools is an urgent priority \u2013 not for children, but for high-tech companies that need to constantly expand their markets.\u201d ( The Alliance for Childhood )\n\nThis is why it is crucially important to pay attention to ethical and social technological implications. The Alliance for Childhood focuses on the importance of physical, emotional, and social development of children.\n\nComputers pose certain hazards to child\u2019s life, and the mission of parents and teachers is to protect future generation from destructions and health problems, caused by computers. Of course, there are certain benefits of computers and the abilities children may get, however, it is necessary to remember about the limits and pay enough attention to active life, healthy food, and real communication \u2013 and all this is impossible to get sitting in front of the computer.\n\nMicrosoft Word and Writing Poetries\n\nThe Microsoft Word program turns out to be rather helpful for completing different kinds of assignment. Students get much more opportunities to demonstrate their creativeness, present quite interesting ideas in order to explain the topic under consideration, and represent their personal vision of the issue. The Microsoft Word program may be also used as a helpful tool to teach students of upper elementary schools on how to write poetry.\n\nWithout any doubts, students may get lots of benefits using the Microsoft Word program. Microsoft Word may correct automatically students\u2019 spelling mistakes and underline the sentences, which may cause some doubts. With the help of this program, students can easily find out several synonyms to make the work more sound.\n\nIt is quite possible to add a captivating picture or any other illustrative material in order to attract reader\u2019s attention. However, it is necessary to remember that this program deprive students from the opportunity to correct all the mistakes and train him/herself to pay attention to each detail in the work. Students can just rely on the corrections, their computers should make.\n\nBut still, Microsoft Word cannot be sure about the proper use of this or that word. When a student creates poetry, it is necessary to use soul and imagination. Of course, this program helps to represent the ideas in the best ways; but the works created by means of some computer programs may be usually deprived of soul and personal emotions, and such deprivation is not inherent to real pieces of art.\n\nPoetry is something that should come from the heart; the Microsoft Word program can help to present the ideas in a really attractive way, but still, students should not rely on this program only in order not to forget about emotions, which are not inherent to computers.\n\nPresentation for the Parents\u2019 Open House Evening\n\nParents\u2019 open house evenings play a very important role in the life of school and each student. Students have a wonderful chance to demonstrate their abilities and underline the issues, which are crucially important for students. This is why it is very important to present captivating presentation and use of the reliable material to prove or disprove some fact.\n\nWithin 15 minutes, it is quite possible to present the topic and explain its strong and weak points. However, certain steps have to be taken. First, it is necessary to choose the appropriate media for the presentation and for the display. Nowadays, a lot of things can be made with the help of computers.\n\nIf yesterday, people spend much time to paint something or create the necessary exhibits, nowadays, the same people have a good chance to make a presentation within several minutes \u2013 all the necessary example can be created on computers and presented to the smart auditory.\n\nOf course, one of the most reliable and frequently used media is CD-R or even CD-RW disks in compatible format. It is crucially important to close the session, otherwise, the information on the disk will be impossible to read on the PCs. Second, it is better to make several copies of the disk and check whether the information on it is available.\n\nTo make the presentation really captivating, it is better to use big and not quite bright letters. This way, parents will not spend their time and efforts to view the information. It is also better not to use too complicated terminology, because not each parent can be aware about it. This is why it is better to find the way to speak to the point and use simple language. Interesting photos and real life facts \u2013 this is also what can make the presentation better.\n\nComputers Are Needed in Classes\n\nWithout any doubts, the use of computers makes our lives easier. Students have a wonderful opportunity to prepare interesting tasks and do not pay much attention to their spelling, as they truly believe that certain computer programs will make all the necessary corrections.\n\nTeachers may spend less time to check each work, because computer programs may be rather helpful. Is it correct that computers turn out to be so crucially important in our lives? There are actually so many doubts according to this very answer. But, this time, we have another question to consider: whether computers are so important in schools.\n\nIn fact, computers in education may be rather helpful. The Internet and emails may easily stimulate the student to work \u201cbecause they notice that they have audience.\u201d (Ortega and Bravo, 2000) This is why it is better to use any means to attract student\u2019s attention to study.\n\nHowever, it is also important to analyze what classrooms should be equipped with computers and students of what age should use them. To my mind, libraries and labs should have several computers in order to make the work of students easier and provide them with more time for researching. With the help of computers, students find the necessary material quicker and can concentrate on the assignment itself.\n\nAt such classrooms where the lessons on literature, language, and history take place, computers are not necessary. However, where certain experiments take place or when students learn to programming or design, the computers should be obviously present.\n\nThe students of 10-12 years do not pass such classes as programming or science; this is why the students of such age and early may study without computer support. As for the other students, to get proper education, they should have an access to computers and be involved into the work.\n\nReference List\n\nAlliance for Childhood. (2000). Computers and Children: A Call for Action . Web.\n\nCordes, C., and Miller, E. (Eds.) (2000). Fools Gold: A Critical Look at Computers in Childhood. Alliance for Childhood . Web.\n\nOrtega, M. and Bravo, J. (2000). Computers and Education in the 21 st Century . Springer.\n",
        "label": "human"
    },
    {
        "input": "Advancement of the Computer: Microchips and Semiconductors Report (Assessment)\n\nThe first property that determines how materials are universally classified is the atoms that make up the material. These atoms are neutrons, protons, and electrons. Most materials in their elementary form are neutral since the number of protons is equal to the number of electrons. At the same time, those that are in a compound form are the ones that have a combination of two or more elements.\n\nThe second property is how different atoms making up the material are arranged in it; since various materials have a different arrangement of atoms. Thus, some are more compact than others. This is evident in the three states of matter that is: liquid, gas, and solid.\n\nThe third property is how atoms are bonded within the material itself. There are two types of bonding, in particular, covalent and ionic bonding. The first type involves sharing of the valence electrons in the material, while ionic bonding is the process where acceptance and donation of valence electrons between the atoms are present (Nils 2010).\n\nA semiconductor is a material that is able to exhibit properties of both metals and non-metals such that they act as both insulators and, at the same time, have the ability to conduct electrons. The use of silicon based semiconductors revolutionized the field of computing due to their ability to change their level of conductivity by a process known as doping. It refers to the addition of impurities in the silicon semiconductor that alters its conductivity.\n\nThe application of semiconductors led to the invention of different miniature electrical components such as transistors and the integrated circuits. These devices possessed the ability to amplify signals, for example, the operational amplifier which is an integrated circuit. They also had the ability to switch and convert signals from one form to another.\n\nThese aforementioned properties of the silicon based semiconductor impacted on the advancement of the computer which became smaller, faster, and more efficient. Hence, the computers we use today are very different and far much better than the initial variants. The earlier computers used to consume a lot of heat. This feature has been rectified with the new generation of computers that consume less power have the ability to cool when heated. All these features are possible thanks to the invention of the silicon based semiconductor.\n\nNowadays, the field of computing is complex and more efficient. For example, cell phones, laptops, tablets, and notebooks were created due to the revolution experienced as a result of the silicon based semiconductor. The robot systems are being used today in almost all industries and have led to reduced workload as well as fast and efficient production of goods.\n\nMicrochips are miniature sets of electrical components that are usually etched on small semiconductor materials. The semiconductor materials preferred are germanium or silicon. It should be mentioned that a microchip forms a specific electrical circuit that achieves a specific purpose although the only difference with normal circuits is that they are very small in size. Microchips cannot be assembled by humans without the help of machines as they are so small and the soldering of the electrical components requires very specialized machinery (Turley 2002).\n\nMicrochips are related to integrated circuits in the way that they are just another form of integrated circuits. The components in the microchips are a combination of different integrated circuits that have been reduced in size. Among these components one could name transistors, operational amplifiers, power regulators, relays, and microcontrollers. These devices are combined on a single semiconductor to form a microchip. Thus, the basic relation between the microchip and integrated circuits is that the microchip itself contains integrated circuits as its integral components (Cardona 2004).\n\nA human brain does not process information on a given time span since the speed of processing varies. At the same time, the central processing unit of a computer processes information depending on the computer\u2019s clock speed. Hence, computers with a higher clock speed tend to process information faster while computers with a lower clock speed tend to take more time in the processing of data (Winston 2006).\n\nOne should note that artificial intelligence refers to the ability of computers and machines to reason and make decisions based on their preprogrammed logics. This includes robots working in industries or even intelligent cars that can navigate themselves on the road. These systems usually require minimal human aid as their level of success is very high and accurately efficient (Nils 2010).\n\nThus, the central processing units make decisions based on preprogrammed logic. This means that it cannot make decisions contrary to the programmed logic and hence it lacks the ability to reason rationally. On the other hand, human brain makes decisions based on logic as well as the rationale and experience. Therefore, it is common for humans to come to decisions that do not make sense and this is the main difference between the human brains and the central processing units of computers (Winston 2006).\n\nWorks Cited\n\nCardona, Manuel. Fundamentals of Semiconductors : Physics and Materials Properties . London: Springer, 2004. Print.\n\nNils, Nilsson. The Quest for Artificial Intelligence: A History of Ideas and Achievements , New York: Cambridge University Press, 2010. Print.\n\nTurley, Jim (2002). The Essential Guide to Semiconductors . New York: Prentice Hall, 2002. Print.\n\nWinston, Patrick. Artificial Intelligence . Reading, Massachusetts: Addison-Wesley, 2006. Print.\n",
        "label": "human"
    },
    {
        "input": "Computers in the Classroom: Pros and Cons Essay (Article)\n\nFor parents right now, the classroom may already seem fairly alien. When an elementary school student shares their frustration with their Prezi presentation about apartheid, or laments the lameness of their PowerPoint animated transitions, parents know that the world has changed.\n\nWhen a high school student makes a video accompanied by music and special effects, demonstrating how Ovid\u2019s word choices reveal character, the change is undeniable. However, having computers in the classroom is just the beginning. The future school is likely to be almost unrecognizable, in both good ways, and in ways that will need careful management in order to be an unmixed blessing.\n\nComputers in the classroom! It sounds new. It scares some teachers, delights some students, and baffles some parents. Bring Your Own Device , as a policy, can be intensely stressful when first implemented. It was so scary and anxiety producing this September that it caused a major domestic disturbance in this parent\u2019s household.\n\nIn fact, however, that particular innovation has been under development since the 1970s. One person who undertook to figure out how to help teachers teach and students learn with technology, starting in the mid-70s was Dustin Heuston. His ideas seemed \u2018out there\u2019 back in the 70s and 80s, but now seem standard. He recognized that just putting a computer in a school would not make kids better learners.\n\nTo achieve his goal of providing a nearly one-to-one educational experience to each child, the whole approach to presenting information and helping students use it constructively would need adjustment. This, in turn, would largely re-define what teaching and learning mean.\n\nThe equipment that will be available to students may soon be nearly unrecognizable. While laptops and tablets are in some classrooms now, in the future, each child could well have a wrist-mounted computer.\n\nMore conveniently, students might have a computer chip implanted somewhere on their person that could communicate with whatever serves as the equivalent of the internet in their decade. In such an environment, teachers will need to figure out how to do more than merely direct students to websites for facts and then ask them to regurgitate them.\n\nTeachers will need to be able to teach kids how to think critically and apply the information that is available to them in solving problems. With any luck, and the right preparation, the kids of the future will go on to solve world problems such as hunger, environmental degradation, and geopolitical conflict.\n\nOne immediate challenge, and one that teachers (and some committed, serious students) are trying to resolve right now, is the problem of distraction. For the teacher, having kids involved in multiple social interactions online while sitting at their seats may seem like a class control nightmare. For many students, the activities of their classmates may prove fatal to their focus on the task at hand and on the instructor.\n\nThere are, indeed, tools available at present, such as Insight by Faronics, to control and manage the problem of kids visiting websites off-topic, playing games, Facebooking, or simply continuing legitimate work but doing so when the teacher has called for attention. They are even beginning to seem less creepy and intrusive. Thank goodness, because in the future, the teacher may confront student web activity that may be nearly invisible to the eye.\n\nThe solution of some teachers at the moment, for example, some faculty at the Science Leadership Academy in Philadelphia, is to permit most online activity, but to offer no leniency if the student has missed a key instruction, or course content, and fails an assessment. Thus, in the words of one teacher, there may be a great deal of what looks like goofing off during class, but one can distinguish the kids who are fooling around on their devices too much by their grades.\n\nThe advantages may outweigh such problems. Imagine being able to teach several classrooms at once, in widely dispersed regions of the world. This is already happening to some extent via MOOC courses such as Coursera.\n\nA webcam, a decent microphone system, and a solid internet connection are all that are needed to reach literally tens of thousands of students globally. At least one instructor, Al Filreis, at the University of Pennsylvania, reports that it was \u201ca blast\u201d. Of course, in the case of Coursera, there are plenty of teaching assistants and a good bit of peer reviewing by other students to help with the volume of work.\n\nThis model for making quality instruction available to the entire world is likely to expand, and the potential impact is hard to fathom. The limitations of affordability, cultural prohibitions (for example, against women in the classroom), and geography, will become simply irrelevant.\n\nImagine students being able to do collaborate in real time, with no barriers of messaging or signal loss. This is happening to some extent now using Skype or Facebook or one of the Google tools. However, there are sometimes delays in these otherwise rich media channels, due either to dropped signals, or slow internet connections.\n\nA future of untrammeled, nearly effortless, and reliable communication will be accompanied by other challenges. How can teachers design assignments and assessments to require rather than punish the inevitable cooperation that will occur? This is going to require imagination and creativity, but good teachers are endowed with these anyway.\n\nMany classrooms are already filled with equipment and software that would have seemed like science fiction just a few years back. Many students have access to a breadth of information that seems to rival Dr. Who. With any luck, these kids will go on to use all these resources to solve tough global problems like hunger, environmental degradation, and conflict.\n",
        "label": "human"
    },
    {
        "input": "Modeling, Prototyping and CASE Tools: The Inventions to Support the Computer Engineering Essay\n\nDespite the fact that the engineering is constantly being enriched with the new notions and techniques which add the new ways and methods to make the process of engineering quicker and flawless, the science still has its basics, which are modeling, prototyping and CASE tools. Whenever there is a computer problem to solve, there is always the necessity to take the abovementioned notions into account. The quality of the program working depends on these issues to the highest degree.\n\nNow I would like to give definition for these notions.\n\nModeling includes creating the scheme, the outline for the future project. It enables the engineer to help to see the problem at once and to understand the ways to solve the puzzle. As Shelly points it out, there are three kinds of modeling that can improve the process:\n\nThe system analysis phase includes four main activities: requirements modeling, data and process modeling, object modeling and consideration of development strategies. (138)\n\nAs it can be seen, modeling is basically creating a platform which will hold the program atop.\n\nThe next element, prototyping, means that a certain piece of work is produced quickly as a draft: \u201cprototyping produces an early, rapidly constructed working version of the proposed information system called a prototype\u201d (Shelly 314).\n\nHowever, the prototype version can have certain mixed effects on the system operation:\n\nSome found prototyping to be positively related to higher system performance, but others found that prototyping might create less robust, less functional systems, with potentially less coherent designs. (Chiang 51)\n\nThus, it is clear that the idea of prototyping concerns a draft of the future program that highlights all the details of the program yet represents just an outline, not being a complete work yet.\n\nThe last issue of the computer engineering processes to speak about is the one called CASE tools. Following the deciphering that Lyytinen provides (1), CASE is computer aided software engineering. Thus, the basics of their work can be defined in the following way:\n\nThe field of CASE is, in principle, very broad and ranges from simple tools such as linkers, loaders or compilers up to complicated integrated environments. In more recent thinking, however, CASE has been largely adopted to denote a more narrow range of technologies. (1)\n\nCombined together, the three issues of computer engineering can actually work. These are the ground for the computer programs to be created. Taking into consideration that these stages include the use of the tools that are constantly developing, the computer engineering keeps going on without stopping, which makes it one of the lead sciences in the 21 st century.\n\nThe stages described above make the initial draft for the future computer program to stem from and remain the cornerstone for programming processes.\n\nWorks Cited\n\nChiang, Roger, Keng Siau, Bill C. Object-Oriented Systems Analysis and Design Using UML. London: McGraw-Hill International. 2002. Print.\n\nLyytinen, Kalle. Next Generation CASE Tools . New York, NY: IOS Press, 2009. Print.\n\nShelly Gary B., Harry J. Rosenblatt. System Analysis and Design . New York, NY: Sengage Learning. 2009. Print.\n",
        "label": "human"
    },
    {
        "input": "Experts Respond To Questions Better Than Computers Essay\n\nIt is important to think about effects of computers on roles played by experts in responding to queries on various fields of study. Robots might replace human beings in future because of advancements in technology. Critics argue that computers are more effective, fast, and thorough in providing information on certain areas of study. However, experts give better responses because they possess ability to synthesize, evaluate, and criticize information in order to provide best answers to queries.\n\nFirst, skills of an expert are needed to feed a computer with information that it generates when presented with a query. Therefore, an expert gives better responses to queries than a computer. Information obtained from computers is secondhand because it originates from experts (Baldauf and Stair 34).\n\nHowever, critics argue that computers give better responses because humans are prone to errors. This argument is erroneous because even computers make errors depending on quality of programs installed. Moreover, an expert can correct errors but a computer cannot.\n\nSecond, robots only respond to queries that contain certain search phrases. A robot cannot give responses to unclear queries if a certain phrase is not provided (Helander and Prabhu 42). In contrast, experts understand queries better than robots and offer the best answers after evaluating and analyzing information (Baldauf and Stair 37). A Computer may give a wrong response because a search phrase might be connected to a different answer in the computer\u2019s program.\n\nCritics argue that even experts occasionally fail to understand queries and thus give wrong answers. However, interaction between people facilitates better understanding because more information that improves comprehension can be provided (Helander and Prabhu 44). A computer cannot respond to additional information because it responds only to certain phrases. If the scope of a phrase is not inclusive of all aspects of the query, then a robot provides an insufficient answer.\n\nThird, responses from computers are limited in scope because there is no interpersonal discourse between a user and the computer (Markoff 73). In contrast, a human expert can provide additional information on a certain query by interacting with the other person. An expert evaluates a query from different perspectives and as such, gives the most appropriate answer.\n\nIf computers give better responses to queries than humans do, they would be used to instruct students in institutions of learning. However, they are not used because they have several limitations such as inability to interact with students. Some critics argue that computers are not used because policies to allow there are nonexistent. However, there is no evidence of computer use in teaching except video conferencing between learners and instructors.\n\nComputers cannot instruct learners because of two main reasons. First, they lack ability to learn and their functioning depends on humans. Second, they lack creativity and respond to queries based on their programming (Markoff 75). If used for instruction, they cannot answer questions from learners due to inability to interact with students. Therefore, humans are better at responding to queries than robots.\n\nIn conclusion, human experts are better at responding to queries compared to robots. Robots provide responses based on their programming that may be limited. Therefore, they depend on human intelligence and secondhand information. In contrast, humans provide responses after careful evaluation and analysis of information, functions that robots cannot perform. In addition, humans can correct errors in responses while robots cannot. Therefore, experts provide better responses to queries than robots do.\n\nWorks Cited\n\nBaldauf, Kenneth, and Stair Ralph. Succeeding with Technology . New York: Cengage Learning.\n\nHelander, Michael, and Prabhu, Peter. Handbook of Human \u2013Computer Interaction . New York: Elsevier Press.\n\nMarkoff, John. A Fight to Win the Future: Computers vs. Humans . 14 Feb. 2011. Web. < https://www.nytimes.com/2011/02/15/science/15essay.html?pagewanted=all&_r=0 >.\n",
        "label": "human"
    },
    {
        "input": "Computer Based Training Verses Instructor Lead Training Term Paper\n\nTable of Contents\n 1. Introduction\n 2. The Similarities between CBT and ILT\n 3. The Differences between CBT and ILT\n 4. The Advantages and Disadvantages of CBT and ILT\n 5. Conclusion\n 6. References\n\nIntroduction\n\nAdvancements in information and communication technologies (ICT) have led to the shift from instructor lead training (ILT) to computer based training (CBT). Generally, computer based training involves providing instructions and learning materials with the aid of computers. In the contemporary world, most institutions combine the use of computers and internet technologies to improve the effectiveness of CBT. At least 6.8 million students were registered in academic programs that use CBT in 2011 (Little, 2001, pp. 203-207).\n\nResearch indicates that the use of CBT, especially, in higher education and organizational training is increasing at the rate of 13% annually. Approximately, 70% of the heads of leading institutions of higher learning believe that CBT will improve the competiveness of their organizations (Little, 2001, pp. 203-207). This paper will discuss the similarities between CBT and ILT; the differences between CBT and ILT; and the advantages and disadvantages of CBT and ILT.\n\nThe Similarities between CBT and ILT\n\nComputer based training is similar to instructor lead training in the following ways. First, the learning objectives in both training methods are usually the same (Clark, 2010, p. 56). Most institutions, especially, colleges and universities normally offer the same course in two different classes.\n\nOne of the classes is normally taught through instructor lead training, whereas the other is taught through online CBT. Thus, instructors have to formulate the same learning objectives for the two classes in order to achieve standard learning outcomes. Generally, every training method must have learning objectives. Thus, both CBT and ILT have specific learning outcomes, which they must achieve.\n\nSecond, both CBT and ILT classes must have a syllabus. The instructor who is in charge of the course normally prepares the syllabus, which is used to teach all students regardless of the training method. The rationale of using this strategy is that it enables instructors to deliver the same content to students, who are pursuing the same degree, but are enrolled in different training programs. The content may include reading materials, illustrative diagrams, tests, and homework.\n\nThird, both computer based training and instructor lead training have a structure that guides the instructors and the learners (Clark, 2010, p. 89). In the conventional instructor lead training, the course content is often delivered in a systematic manner in order to enhance understanding.\n\nConcisely, the instructors normally start with an introduction, followed by topics that will enhance learners\u2019 understanding of more complex concepts. Computer based training programs also use the systematic approach to teaching to deliver content. For example, students who are learning to use a computer software using the CBT method will first acquire background information about the software so that they can understand the instructions concerning its application.\n\nFinally, the students\u2019 ability and commitment determines the extent to which learning outcomes can be achieved in both CBT and ILT programs. Research indicates that students\u2019 attitudes towards learning specific subjects do not change significantly when the training method is changed from ILT to CBT and vice versa (Clark, 2010, p. 97). This means that the students\u2019 ability to learn and their interest in a particular course will determine their success, regardless of the training method that is applied.\n\nThe Differences between CBT and ILT\n\nTo begin with, CBT and ILT usually take place in different learning environments (Ellis & Persad, 2004, pp. 26-44). Instructor lead training normally takes place in a designated venue such as classroom where all learners congregate. The learners interact with their teacher on a face-to-face basis.\n\nThe course content is delivered through lectures, group discussions, as well as, questions and answers sessions. CBT, on the other hand, takes place at any location that is convenient to the learner. Most CBT programs are currently using the internet to deliver course content to students. Concisely, students interact with their instructors and access learning materials through websites, e-libraries, email, and social media among others. Thus, the same training can take place at the same time at different locations.\n\nThe second difference between CBT and ILT is that the later is scheduled while the former is not. Instructor lead training depends on a timetable that enables learners and instructors to manage their time. The timetable must be used because the training has to take place in a classroom, which might not be available for use throughout the day.\n\nFurthermore, most instructors and learners have different classes to attend at different times of the day. Thus, they need a timetable to schedule their learning activities. Computer based training, on the other hand, can occur at any time of the day since the instructor and the student do not have to meet physically (Ellis & Persad, 2004, pp. 26-44). Consequently, the learner can receive the training at the most convenient time, thereby eliminating the need to schedule learning activities.\n\nFinally, computer based training and instructor lead training differ in terms of the pace of learning. In instructor lead training, learning occurs concurrently. This is because the students are taught in the same venue and at the same time. Thus, they progress with the training at the same pace.\n\nHowever, CBT is associated with little or no concurrence because students learn in different venues at different times (Ellis & Persad, 2004, pp. 26-44). Consequently, each student progresses with the learning at his or her pace rather than that determined by the instructor.\n\nThe Advantages and Disadvantages of CBT and ILT\n\nThe advantages of computer-based training include the following. First, the students can receive the training at the time that is most convenient to them (Myre, 2000, p. 25). This creates a stimulating environment in which the students can learn at their pace. In this regard, slow learners will have an opportunity to understand the course content without delaying the rest of the class.\n\nSecond, CBT gives students adequate time to learn since they have access to instructions and course materials throughout the day. Thus, students can utilize their time to reflect on course content and to seek online assistance from their instructors. Third, CBT programs can be tailor-made for specific industries (Myre, 2000, p. 26). In this case, an industry or a company can develop a special computer software that can be used by employees to receive training.\n\nSimilarly, CBT can be custom-designed to meet the training needs of physically challenged individuals. For instance, it can eliminate writing among those who cannot use their hands. Fourth, CBT is a cost-effective method of training employees and students. In particular, it eliminates the cost of hiring fulltime trainers, as well as, the cost of travelling to the training venue. Finally, CBT enables students to acquire advanced ICT skills since it involves the use of computers and the internet.\n\nThe disadvantages of CBT include the following. To begin with, the initial cost of acquiring the learning equipment such as computers, as well as, internet connection can be very expensive (Myre, 2000, p. 31). Additionally, the learner might incur extra costs in order to train on using computers and the internet. CBT limits students\u2019 ability to obtain immediate feedback from their instructors.\n\nThis is because, requests sent through emails or websites can only be answered if the instructor is online. Similarly, the students have to forego the benefit of face-to-face interactions with their instructors and colleagues. Consequently, they might not be able to seek clarification on ambiguous concepts. Finally, CBT discourages close supervision, thereby allowing some students to lag behind in their studies.\n\nILT has the following advantages. First, it facilitates face-to-face interactions between the learners and the instructor (Myre, 2000, p. 45). This enables students to obtain instant answers to their questions. Moreover, the instructor gets the opportunity to validate the students\u2019 learning immediately after each lesson.\n\nSecond, the instructor can adapt his or her teaching style to the students\u2019 learning needs, thereby improving learning outcomes. Third, training within a classroom enables instructors to give individual attention to students. Individualized training is important because it can help instructors to improve their students\u2019 understanding and competence. Fourth, ILT programs enable students to make mistakes in a controlled learning environment.\n\nConcisely, the instructor can easily identify students\u2019 mistakes and help them to correct such mistakes. Fifth, ILT facilitates interactions among students, thereby enhancing their understanding of various concepts (Myre, 2000, p. 47). Finally, ILT programs facilitate acquisition of kinesthetic skills. These are skills, which can only be learnt by engaging in the actual activity. Thus, they cannot be learnt through the simulations that are used in CBT programs.\n\nDespite its advantages, ILT programs have the following disadvantages. Providing personalized instructions can be difficult if the class consist of a very large number of students. Maintaining the same pace of learning in one class is often very difficult (Myre, 2000, p. 78). This is because slow learners might not be able to keep up with the pace of learning that has been set by the instructor. Quick learners, on the other hand, might get bored as the instructor slows down the pace in order to accommodate the slow learners.\n\nConclusion\n\nThe main similarity between CBT and ILT is that all of them use a syllabus to deliver the course content. Additionally, both of them have learning objectives, which they aim to achieve. The difference between them is that ILT takes place in a classroom, whereas CBT is delivered through online or computer-based interactions from different locations. CBT is cost-effective and convenient. However, it does not allow learners to interact face-to-face with their colleagues and instructors (Sloman, 2005, pp. 8-15).\n\nILT facilitates face-to-face interactions, personalized instructions, and immediate correction of mistakes. However, it might not be convenient because it is scheduled. This paper discussed the similarities between CBT and ILT; the differences between CBT and ILT; and the advantages and disadvantages of CBT and ILT. Based on the discussion, the selection of the right training method should depend on the learners\u2019 needs and resource availability.\n\nReferences\n\nClark, R. (2010). Evidence-Based Training Methods. New York: McGraw-Hill.\n\nEllis, R., & Persad, P. (2004). Design and Implementation of Computer-Based Training. Asian Journal on Quality, 5(2) , 26-44.\n\nLittle, B. (2001). Achieving High Performance Through E-learning. Industrial and Commercial Training, 33(3) , 2003-207.\n\nMyre, R. (2000). Comparing the Effectiveness of Instructor-led Training to Stand-alone Web-based Training. New Jersey: New Jersey Institute of Technology.\n\nSloman, M. (2005). Learning in Knowledge-intensive Organizations: Moving from Training to Learning. Development and Learning in Organizations, 19(6) , 8-15.\n",
        "label": "human"
    },
    {
        "input": "The Influence of Computer on the Living Standards of People All Over the World Essay\n\nA computer is an electronic device applied in general and arithmetic functions. A computer comprises a processing unit and a memory store. The processing unit performs arithmetic and logic operations based on the information stored.\n\nAll the processes in a computer rely on both the hardware and software. Hardware comprise of tangible elements like a mouse, keyboard, monitor and the central processing unit. Software comprise of various programs and languages used by a computer. People use computers in most activities and it would be unimaginable how life would be if there were no computers.\n\nIn the past, people considered computers to be a reserve for scientist, engineers, the army and the government. However, that has changed already as computers are now in several workstations, schools and homes. The efficiency, accuracy and ability to accomplish numerous tasks have endeared computers to people and slowly becoming a need rather than a want. Ability to access internet through computers has enhanced research and innovation.\n\nComputers touch on every aspect of modern living. The influence of computer on the living standards of people all over the world is unmatched. Banks use computers for purposes of storing information and arithmetic functions. They also use computers for speed, expediency and safety measures.\n\nCommunication has been escalating since the innovation and introduction of computers. Through emails and virtual worlds in the internet, people are now communicating effectively and in a speedy manner. Computers can qualify as business tools.\n\nComputers are integral in the corporate world for they ensure easy transaction of business activities, record keeping and arithmetic functions like preparing a balance sheet. Today computers have enhanced business through online trading, settling of outstanding bills and the stock market.\n\nUse of computers in medical science exhibits their abilities and effectiveness. Diagnosis of diseases improves with the use of computers. Computers have enhanced development of the best cure for various diseases through research. Computer technology applies in various medical practices such as surgery and scanning of body organs. The role of computers in education lacks an equal among other assistive technologies.\n\nLearning today revolves around computers as teachers use them for instruction and preparing teaching materials. Students use computers to carry out research and writing their assignments. Introduction of e learning in various learning centers around the world requires continued use of computers by the instructors and students. Knowledge of computers is mandatory for young people aiming to meet both their education and career goals.\n\nMedia is a field that has demonstrated the quality and value of computers. Computer software and technologies apply in media to generate both audio and visual compositions. The technology applied in producing animation and three-dimensional visuals relies heavily on computers.\n\nSome of the effects executed in films and movies develop from computer technology. All these activities and actions demonstrate the value of computers in the entertainment industry all over the world. Other fields that have exhibited the importance of computers through their use include the transport industry, metrological departments and the sports industry. Computers apply in plane and train reservations in countries with developed economies.\n\nWeather predictions apply distinctive types of computers that analyze data and make appropriate predictions. Computers have enhanced the study of body development through sports by using specially created programs. These programs monitor the behavior of various body organs and muscles during an active session with the body. Sport technologies have influenced the development and steady growth of sport science and sport nutrition.\n\nIn almost every aspect of life, we operate an electronic device generated and developed using computer technology. I believe that computers are playing a crucial role in human wellbeing. We are at a stage in human development that we cannot afford to imagine life without computers.\n\nIts is hard to imagine having to do things like washing clothes, arithmetic work as well as storing information into files and books manually. That would is stressful and may result in slow human development. Computers have added efficiency, speed, effectiveness and creativity into human activities. I believe that it is essential to get training on computers because they dominate nearly every element of modern life.\n\nFor example, a student who conducts manual research and writing of term papers is at a disadvantage in various aspects compared to a student who uses a computer to perform all this tasks. The aspects that come into focus in this scenario are time, quality, amount of workload and effectiveness of the overall process. A student who uses a computer spends little time on assignments, gets quality work with few mistakes and does little work compared to a student who applies manual means.\n\nTherefore, it is right to conclude that human beings are dependent on computers today. People will do exceedingly little if someone takes computers away from them. Despite computers coming with challenges such as high cost of purchasing and power consumption, people have done their best to utilize technology to improve their wellbeing.\n\nI believe that the invention and development of computers was a stage in the evolution of humankind into a complete creature. It is easy to imagine a virtually crippled humankind if all the computers and technologies developed using computers stop functioning today.\n",
        "label": "human"
    },
    {
        "input": "Leasing Computers at Persistent Learning Essay\n\nTable of Contents\n 1. Accounting for \u201cfair-market-value\u201d and \u201cone-dollar-purchase\u201d\n 2. Classification of leases\n 3. Appropriate lease alternative for Persistent Learning\n 4. Principal arguments for and against capital leases\n\nThe current case study entails leasing computers at Persistent Learning, an educational software company. Leasing Computers is in the midst of making a financial decision on the most appropriate way of acquiring assets (computers and related hardware) for purposes of expansion. Owing to its financial position, the company has two options of financing its expansion objective. The options are to undertake a \u201cfair-market-value\u201d lease and the \u201cone-dollar-purchase\u201d.\n\nUnder the fair-market-value\u201d lease the company would be under rental agreement while under the \u201cone-dollar-purchase\u201d the company would provide financing for the computers purchase. This case study analysis is an attempt to answer different questions relating to the \u201cfair -market-value\u201d less and \u201cone-dollar-purchase\u201d less options. In addition, it provides the most effective and appropriate choice between the two leasing options.\n\nBased on the case study, Persistent Learning competitors own their computers which give them a competitive advantage over rivals. Competitors own their computers because they have a higher cash-flow compared to Persistent Learning. In addition the companies have an outstanding capital base that enables them to purchase computers. Companies with their own competitors treat the computers as fixed assets which is a long term investment in an organization.\n\nAlso, the companies with computers are flexible as they do not have to negotiate with third party capital equipment providers. This saves time and a lot of paperwork often incurred during a leasing process. By owning capital equipment such as computers, such an ownership enables the companies to enjoy economic life of the capital. Furthermore, the companies can easily sell old computers or upgrade them at a much cheap prices.\n\nAccounting for \u201cfair-market-value\u201d and \u201cone-dollar-purchase\u201d\n\nWithin a period of three years, fair market value lease would be accounted for as an operating expense or lease payments as opposed to an asset to Persistent learning. On the other hand, one dollar purchase would be accounted for as an asset since the company would get to own the computers after a period of three years.\n\nUnder the fair market value the equipments are accounted for as operating expenses because the company would be paying for the equipments for which they lack ownership obligations. The fair market value as a leasing expense is accounted for on the company\u2019s income statement.\n\nOne dollar purchase would be accounted for as both a liability and an asset and recorded on the balance sheet. It is accounted for as an asset as it reflects the equipments\u2019 value to the organizations. On the other hand, it is accounted for as a liability because Persistent Learning has an obligation to pay the lease expenses to the lesser.\n\nAs an asset, the equipment acquired through the one dollar purchase would depreciate during the three year period. As a result, it would be accounted for as a depreciation expense in both cash flow and income statements. Since Persistent Learning does not own the equipment, there is no depreciation expenses realized under the fair market value. In addition, no liability is accorded to the leasing company thus it cannot be accounted as a liability.\n\nClassification of leases\n\nFair market value lease would be classified as an operating lease while the one dollar purchase would be classified as capital lease since under the fair market lease, capital equipment is only leased for the operating period. On the other hand, the one dollar purchase ensures ownership of the property or capital equipment after the agreed lease period. Therefore, under the former a firm does not own the equipment while at the later the equipment is treated as owned.\n\nIt is important to note that both capital and operating leases either have no effects or different effects on income statement, cash flow statement, or the balance sheet. Although an operating lease may not have an effect on the balance sheet, it tends to affect the income statement.\n\nOn the other hand, capital lease has an effect on the balance sheet and income statement. For instance, depreciation of funds under the capital lease lowers their value on the balance sheet, while at the same time recording a depreciation of expenses, which is seen in the income statement.\n\nOperating lease is reflected on the income statement as expenses incurred during the accounting period. Therefore, operating expenses act as lease payment which is an expense that reduces net income. Total cash flow statements are not affected by capital and operating leases since the actual cash flow remains constant even if a lease is accounted for as an operating lease of a capital lease.\n\nWith regard to income statement, cash flow under the income statement tends to be comparatively higher in comparison with cash flow under an operating lease. The reason behind this is that under capital lease, a lease is treated as an asset which depreciates as cash flow inclines during the lease period.\n\nAppropriate lease alternative for Persistent Learning\n\nThe best leasing alternative for Persistent Learning based on the case study is the \u201cFair-market-value\u201d lease. In this alternative, the company would engage in a rental agreement. It is the cheapest option under the circumstances and the most appropriate type of lease as it is flexible and Persistent Learning can still purchase the equipment after a period of three years at the prevailing market fair value.\n\nThe company is certain that the capital equipment would be needed for a period of three years. Because of market uncertainty and obsolescence of equipments, Persistent Learning would choose the \u201cFair-market-value\u201d lease. In addition, the company would not require the computers after three years. So there is no need to own computers at the end of three years and sell them at lower prices. Having to dispose or sell the computers is a time consuming process which is also costly.\n\nBased on the case study, the company plans to use the computers in three years and the economic life of the computers is 5 years. Therefore, the lease period is defined to be less than 75% of the capital equipment\u2019s expected economic life which makes Persistent Learning qualify for fair market value lease.\n\nOn the other hand, the present value for the lease payments is less than 90 percent of the equipment. Persistent Learning would have to pay for the equipment usage for three years and supposing that it still needs the equipment for another year, the company has the choice to renew the lease.\n\nAlternatively, Persistent Learning can choose to purchase the computers at the fair market value. In this kind of purchase, Persistent Learning would buy the computers after calculating the equipments depreciation in a period of three years. Lastly, the monthly lease rates are lower in fair market value lease compared to the one dollar purchase. Therefore, I would prefer the fair market value lease to the one dollar purchase.\n\nPrincipal arguments for and against capital leases\n\nInitially, capital and operating leases were recorded differently but the reevaluation by the FASB would now ensure that all leases are treated as capital leases. In other words, all leases would be recorded as liabilities and assets. Capital leases have the capacity to recognize expenses (liabilities and depreciation expenses) compared to operating leases. As a result, leases would be recognized as liability and as an asset when accounted for in a balance sheet.\n\nConsequently, firms (lessee) would be in a position to claim for depreciation of the leased capital equipment annually. In addition, the lessee would be allowed to deduct the interest expense in each financial year. Operational leases are not shown on a firm\u2019s balance sheet like it is the case for capital leases. Therefore, treating all leases as capital leases would ensure that leases are recorded on the balance sheet thus showing the financial position of a firm to creditors.\n\nOne of the major arguments against this move is that debt level reported would increase. This is because classification of operating leases as capital leases would require the levels of reported debts to be exposed to financiers which have effect on individual credibility. In addition, this would have an impact on debt covenant compliance. Another argument against the change is an anticipated increase in lease expenses.\n\nUnder operating leases, the lease expenses remained constant for the agreed period. However, treating leases as capital leases would increase lease costs. By reclassifying the leases, the level of debt indicated in the balance sheet would be increased by firms operating large operating leases. This may have negative effect on the financial position of an organization. Lastly, the commitments by lessee to not assume ownership of capital equipment after the lapse of lease period should not be treated as a debt but rather as a rational choice.\n",
        "label": "human"
    },
    {
        "input": "Online Video and Computer Games Expository Essay\n\nVideo and computer games emerged around the same time as role playing games during the 1970s, and there has always been a certain overlap between video and computer games and larger fantasy and sci-fi communities (King and Borland 2003). Many early games were solitary endeavors, but the past decade has witnessed a massive explosion in the popularity of networked games, with titles such as Doom and Quakes standing out as early exemplars.\n\nIn the mid 1990s, online video and computer games (OVCGs), which could be played through servers that connect hundreds of thousands of computers together, began to appear across the US, Europe and Asia.\n\nOnline video and computer games (OVCGs) do not require that players be physically co-present; rather, players gather in virtual game worlds where they play avatars- computer-mediated fantasy characters. For online video and computer games (OVCGs) to function it needs a computer as well as the software, instead of other tools used by several other games.\n\nIn addition, many online games, such as EverQuest (Sony) and World of Warcraft (Blizzard), have a monthly subscription fee. Similar to Role-playing games (RPGs) and collectible Strategy Games (CSGs), but different from many non-networked video and computer games, Online video and computer games (OVCGs) are specifically designed to offer hundreds of hours of highly interactive gameplay and for the development of characters\u2019 identities (Schubert 333).\n\nOnline video and computer games have settings and systems, just like Role-playing games (RPGs), but the computer controls both, which simplifies the games in some important ways.\n\nOnline video and computer games involve multiplayer game worlds and allow for instant peer-to-peer communication, either through typed conversation or through voice-over chat. As players cooperate with one another on multiple occasions and for multiple purposes, they develop the same sense of shared community, and become known to one another through their specific style of play and their characters\u2019 names.\n\nPerhaps the biggest draw of on-line gaming, however, is the visual effects, which can be highly detailed with a sense of three dimensional spaces. The ability to play at any time one desires, for as long or as short a time as desired, is also an advantage since a player can find others online at any hour of the day or night.\n\nMany online games today allow for a high degree of character customization and allow various paths towards success, so it is possible to create an interesting, original character that is not only present in one\u2019s head, or on a piece of paper, but which is walking, jumping, fighting, or flying on the screen in front of the player. As Online video and computer games become more akin to \u201cmovies that I can control\u201d their attractiveness increases.\n\nUnfortunately, OVCGs requires a substantial monetary investment and a degree of technical proficiency. Additionally, Online video and computer games are still less portable RPGs and CSGs. Despite these problems, Online video and computer games are the fastest growing segment of the fantasy game industry and have gained more widespread acceptability than either table top RPGs or CSGs (Schubert 338)\n\nTo continue our fictional example of Academia: The Overeducated, the online version immerses players in a virtual university and the opening shot on the computer screen is that of the Registrar\u2019s office. There, the player selects his/her character\u2019s name, age, sex, height, weight, race, and other physical characteristics, and also signs up for first-year classes.\n\nThis would generate a graphical representation of the player\u2019s character that one could then watch, control, and manipulate through virtual world. The character would be given a campus map and directions to a dorm room, as well as a key to the room. From that point on, the player would navigate through the halls of the Administration Building, on his or her way to the dorm, realizing that every person walking through the hall is also a player; a real person, somewhere in the world, sitting in front of a computer.\n\nThe player could have the character to stop and ask people about classes, inquire if a teacher was strict with attendance or not, and learn about useful bits of information that would help the character succeed in whatever tasks were encountered during game play.\n\nMany role playing and collectible strategy gamers are also computer gamers, and those that are not are typically at least familiar with inline computer gaming and likely know some of the more popular game titles.\n\nIn the gaming industry, several of the larger RPG and CSG publishers have produced computer games based on their projects, or else have licensed their products to computer game manufacturers. While each type of game has its distinct advantages and disadvantages, an increasingly internet connected world and today\u2019s fast paced life make Online video and computer games very attractive to gamers generally.\n\nSome current online, multiplayer video and computer games include: Hallo 2 (produced by Sony Online Entertainment), and Dark Age of Camelot (Produced by Mythic Entertainment).\n\nWorks Cited\n\nKing, Brad, and Borland John. Dungeons and Dreamers: The Rise of Computer Game Culture from Geek to Chic. New York: McGraw-Hill, 2003. Print.\n\nSchubert, Damion. Online Games: An Insider\u2019s Guide . Boston: New Riders, 2003. Print.\n",
        "label": "human"
    },
    {
        "input": "Social Implications of Computer Technology: Cybercrimes Essay\n\nCybercrime, returning to a definition provided by Toby Finnie, Tom Petee, and John Jarvis, refers to any crime that involves a computer and a network, where a computer may or may not have played an instrumental part in the commission of crime (7).\n\nThe term cyber crime or cyber related crime would be used to refer to criminal act like that of identity theft, fraud, security breach, child pornography (Finnie, Petee, and Jarvis 7). Many of the techniques involve the use of a computer or a network, but many more techniques have nothing to do with computers other than information stored in text files on a computer\u2019s hard drive.\n\nTo address cyber crime and cyber related crimes further, a distinction is made between computers as targets of crime and computer-facilitated crime.\n\nWhile the former refers to crimes targeting computers or other electronic channels as such and include acts like unauthorized entry into computer systems, vandalism, virus attacks, or warfare offensives, so called computer-facilitated crimes are in fact \u201ctraditional crimes that can be or have been committed by using other means of perpetration which are now carried out through an Internet based computer-related venue (e.g. email, newsgroups, other networks) or other technological computing advancement\u201d; or, to put in other words, crimes that use the computer as a medium to commit crimes (Finnie, Petee, and Jarvis 8).\n\nComputer-facilitated crimes can be more systematically classified under three main traditional categories of crime: against persons, against property, and against public order and public interest.\n\nIn reading the discussion above it becomes clear that the term cybercrime actually refers to computer-related crime; however, some consider computer crime to be a subdivision of cybercrime that warrants its own definition and understanding.\n\nThe term \u2018cyberspace\u2019 became popular descriptor of the mentally constructed virtual environment within which networked computer activity takes place. \u2018Cybercrime\u2019 broadly describes the crimes that take place within that space and the term has come to symbolize insecurity and risk online.\n\nBy itself, cybercrime is fairly meaningless because it tends to be used metaphorically and emotively rather than scientifically or legally, usually to signify the occurrence of harmful behavior that is somehow related to the misuse of a networked computer system (Finnie, Petee, and Jarvis 10).\n\nLargely an invention of the media, \u2018cybercrime\u2019 originally had no specific reference point in law and the offending that did become associated with the term was a rather narrow legal construction based upon concerns about hacking. In fact, many of the so-called cybercrimes that have caused concern over the past decade are not ethically crimes in criminal law.\n\nHowever, regardless of its merits and demerits, the term \u2018cybercrime\u2019 has entered the public parlance and we are stuck with it. It is argued that the term has a greater meaning if we construct it in terms of the transformation of criminal or harmful behavior by networked technology, rather than simply the behavior itself.\n\nThis is an interesting happenstance within the context of transformation thesis, because although the contemporary meaning of \u2018cyber\u2019 is firmly linked to technological innovation, its origins lie in the Greek Kubernetes, which is also the root of the word \u2018govern\u2019.\n\nMore by happenstance than plan, the word cyber and crime interrelates linguistically. This linkage becomes more significant if we understand cybercrimes as the crimes which are mediated (governed) by networked technology and not just computer.\n\nWorks Cited\n\nFinnie, Toby, Petee Tom, and Jarvis John. The Future Challenges of Cybercrime: Volume 5 Proceedings of the Futures Working Group . Quantico, Virginia 2010. Print.\n",
        "label": "human"
    },
    {
        "input": "Through a Computer Display and What People See There: Communication Technologies and the Quality of Social Interactions Opinion Essay\n\nIntroduction: Communication Technologies Come to the Rescue\n\nIt is hard to deny that communication technologies play a great role in the lives of billions of people all over the world. With the rise of technology and the surge of innovations that have opened new possibilities for communication between the people in different parts of the world, ordinary live conversation seems to have faded into the background, making more room for new means of communication.\n\nBecause of the current fascination with technological advances and the active use of the latter, people are rapidly losing the skills of live communication, which is likely to lead to a number of difficulties in socializing, both concerning people\u2019s personal life and business affairs.\n\nReaching for the Most Remote Corners of the World: The Positive Aspects\n\nOne of the most obvious advantages of communication technologies is the fact that with the help of the latter, one can keep in touch with the people who live miles away, which would be completely impossible without the recent innovations.\n\nTherefore, it can be concluded that with the help of the current developments, the distance between the subjects of communication is no longer a problem, not to mention the fact that new forms of media have offered the methods for distanced learning (Atkin 71). However, there are still a lot of issues about communication technologies that need to be revisited.\n\nGetting Lost in the Virtual Reality: Where Technologies Fail\n\nLosing touch with the real world is the hazard that everyone has talked about; ironically enough, even the new forms of media that the given problem concerns have been used to discuss it. Indeed, with the growing impact of social network, people seem to be losing the skills of live communication, which is likely to lead to considerable problems in future. Moreover, frequent use of technology as a communication tool can lead to negative social effects, such as the need to stay anonymous (Levi 262).\n\nHence, the threat of deindividualization becomes much more tangible: \u201cThe members of virtual groups are more anonymous. This leads to what psychologists call deindividualization, which is the loss of self-awareness and evaluation apprehension caused by feeling anonymous\u201d (Levi 262). It is worth mentioning, though, that the latter concerns rather online chatting than the use of mobile phone or social networking.\n\nHumans vs. Machines: Concerning the Probable Prospects\n\nHence, it seems that technologies will definitely be of great help in providing high-quality means of keeping in touch. However, it still must be admitted that technologies will never be able to replace personal contact, while they should not.\n\nAccording to Partee, live communication is essential for people: \u201cWe humans need personal contact to communicate values\u201d (Partee ix). It is clear that once the current communication technology takes over the world, the tradition of personal conversation will disintegrate into a mess, since people will most likely lose the necessary skills required for a live conversation.\n\nAnother Means to Raise the Stakes: Technologies Have a Chance\n\nOne must give credit to the developers of the modern communication technologies, though; it is clear that, when used reasonably, these technological advances can help greatly. Therefore, it can be assumed that, when used with due care, communication technology will not make people less sociable, but will help them keep in touch despite the distances between them (Shih and Hung, 67).\n\nAbout Use and Abuse: A Reasonable Solution to the Problem\n\nTo balance out the use of live and virtual communication, it is necessary to realize what the current technological advances offer (Konijn, Utz, Tanis and Barnes 19).\n\nTherefore, it is necessary to consider such innovations as mobile phones, smartphones, computers, etc. not a substitute for live communication, but an additional opportunity to get in touch with the one who is currently not available in person. With such an approach, it can be expected that technological advances will be used as a complementary communicational tool, and not as the only possible one.\n\nConclusion: Looking for a Compromise\n\nIt is hard to ignore the fact that communication technology in many aspects is very helpful, since it provides chances for people to arrange something concerning not only their personal life, but also their business issues, which means that communication technology development boosts business processes.\n\nWith all due respect for the engineers who have provided the humankind with the existing technologies for even more efficient communication, one still has to admit that in many ways, the given technologies kill the tradition of live conversation slowly, but steadily.\n\nSince technologies have become so fully integrated into people\u2019s lives that rejecting thee technologies means losing a huge chunk of their lives, at the given stage, a compromise between virtual and live communication must be provided. Therefore, technologies must be used only when the alternative method of communication is not available.\n\nWorks Cited\n\nAtkin, David J. \u201cCommunication Technology and Social Change: Theory And Implications\u201d. New York, NY: Routledge, 2007. Print.\n\nKonijn, Elly, Sonya Utz, Martin Tanis and Susan Barnes. \u201cMediated Interpersonal Communication\u201d. New York, NY: Routledge, 2008. Print.\n\nLevi, Daniel. \u201cGroup Dynamics for Teams\u201d. Thousand Oaks, CA: SAGE, 2011. Print.\n\nPartee, Morris H. \u201cCyberteaching: Instructional Technology on the Modern Campus\u201d. Lanham, MD: University Press of America, 2002. Print.\n\nShih, Timothy K and Jason C. Hung. \u201cFuture Directions in Distance Learning and Communications Technologies\u201d. Hershey, PA: Idea Group. 2007. Print.\n",
        "label": "human"
    },
    {
        "input": "Basic Operations of Computer Forensic Laboratories Essay\n\nTable of Contents\n 1. National standards\n 2. Laboratory components\n 3. Working conditions\n 4. Standard laboratory equipment\n 5. Tools\n 6. References\n\nNational standards\n\nNational standards provide a platform upon which computer forensic laboratories operate in the US (Nelson, Phillips & Steuart, 2010; Easttom, 2014). They are aimed to achieve practical and realistic computer forensic laboratory goals. All computer forensic laboratories in the US have to adhere to the national standards before they could be certified (Easttom, 2014).\n\nThe standard 1.3.3.1 provides essential information that is crucial for developing technical skills for personnel. The standard 1.4.2.6 outlines emerging technical procedures that should be fulfilled by computer forensic laboratories. The standard 1.4.2.8 provides a framework within which samples are handled in a computer forensic laboratory.\n\nThe standard emphasizes documentation that is aimed to maintain a high degree of the validity of the laboratory procedures. The standard 1.4.2.11 offers approaches that should be adopted to certify laboratory equipment and instruments. The standard also aims to ensure that adequate instruments are utilized to carry out laboratory procedures. The standard 1.4.2.12 offers guidelines that should be adhered to when maintaining computer forensic laboratory equipment and/or instruments.\n\nAll instruments and/ equipment should be maintained in a way that promotes safe and valid analysis. All testing laboratories should be certified to operate upon meeting the requirements of the standard 1.4.2.13 that offers the framework for calibration of equipment and/instruments. Finally, the standard 2.11.4 aims to ensure that all technical personnel of a computer forensic laboratory pass a mandatory competency test before a laboratory could be certified (Easttom, 2014).\n\nLaboratory components\n\nThere are 5 main categories of components that are used in computer forensic laboratories (Nelson et al., 2010; Easttom, 2014). First, computer forensic laboratories should have specific facilities that are utilized to ensure secure working environments.\n\nThe environments could be achieved by adopting controls that prevent unauthorized access to digital information stored in computer systems. Second, laboratory configuration is an essential component of computer forensic laboratories that aim to put in place the required furniture and furnishing.\n\nThe following examples of configuration components are common in many computer forensic laboratories: desktops, bookcases, evidence safe or locker, LAN and server stations, storage shelves, and forensic software. Third, the equipment used in a computer forensics laboratory may depend on the type of operating systems, storage capacities of computer hard disks, tape media, and the type of forensic investigation mainly conducted in a laboratory.\n\nFourth, the software components used in the laboratory could be designed locally or purchased from commercial software developers. These are crucial programs that are used in data capture and analysis, among other uses. Fifth, reference materials offer excellent resources that professionals refer to when in need. The resources provide relevant answers to questions with regard to digital evidence and procedures (Easttom, 2014).\n\nWorking conditions\n\nWorking conditions of personnel in computer forensic laboratories differ from one facility to another. Technicians are involved in collecting and analyzing digital evidence. They could either investigate crime in the field or in the laboratory (Nelson et al., 2010). In most cases, forensic science technicians spend a considerable amount of time writing reports in the laboratory.\n\nAlthough computer forensic experts work during normal business hours, they could be called upon to investigate urgent crimes within their areas of jurisdiction outside normal working hours. Crime scene investigators and experts in computer forensics appear in court as expert witnesses who provide detailed and specialized evidence regarding computer-related crimes (Easttom, 2014).\n\nStandard laboratory equipment\n\nStandard computer forensic equipment is used to support standard procedures and conditions in the laboratories. The equipment makes it possible for many computers used within the context of digital evidence investigations to be used along similar methods on each occasion (Nelson et al., 2010).\n\nA mobile forensic workstation is used to collect digital evidence in the field. The equipment is also utilized to analyze suspected computer data. The rapid imaging device is an essential device used to copy suspect hard drives found in computers used to commit crimes. The equipment copy and retain the integrity of the data found in the hard disks. Interceptor equipment supports wireless networks that support airborne communications.\n\nThe equipment captures crucial contents of airborne communications in static and mobile locations. This is important because computer forensic experts have adopted the use of wireless networks to gather, analyze and store computer evidence (Taylor, Haggerty, Gresty & Lamb, 2011). In addition, forensic workstations could be used in the laboratory for the analysis of data obtained from the laboratory (Nelson et al., 2010; Easttom, 2014).\n\nTools\n\nComputer forensic investigations involve the use of specific tools used in the analysis of computer memory (Easttom, 2014). The analysis is important because it identifies digital evidence hidden in computer memory devices like hard disks. MemGator interrogates files in a computer in order to isolate crucial evidence.\n\nIt gives a report to an investigator who decides the value of the information obtained. Memoryze is used to obtain memory from Microsoft Windows-based computers. In addition, the tool analyzes live memory in a running computer. Computer forensic investigators use PTFinder to search a memory of a computer that uses a Windows operating system. It identifies important threads and processes that can be placed into a file for further analysis.\n\nReferences\n\nEasttom, C. (2014). System forensics, investigations, and response (2 nd ed.). Burlington, MA; Jones and Bartlett Learning.\n\nNelson, B., Phillips, A., & Steuart, C. (2010). Guide to computer forensics and investigations . Stamford, CT: CengageBrain. com.\n\nTaylor, M., Haggerty, J., Gresty, D., & Lamb, D. (2011). Forensic investigation of cloud computing systems. Network Security, 2011 (3), 4-10.\n",
        "label": "human"
    },
    {
        "input": "Computer Aided Software Tools (CASE) Report (Assessment)\n\nTable of Contents\n 1. Introduction\n 2. Comparison\n 3. Repository\n 4. Forward Engineering features\n 5. Reverse Engineering\n 6. Modelling tools\n 7. Preferred method for a company\n 8. Conclusion\n 9. Reference List\n\nIntroduction\n\nIn software development, Computer Aided Software Engineering tools (CASE tools) are indispensable as they cut down the cost and time of software development while at the same time improving their efficiency and quality. They also make it easy for information to be presented in a logical manner thus making communication easy. They are often used as supports for traditional methods and object-oriented methodologies.\n\nMany researches have focused on the use of CASE tools in the workplace. However, very few of them have compared the existing options of CASE tools. Therefore, this essay provides a comparison of two of the existing CASE tools based on parameters like repository, forward engineering features, reverse engineering features, and modelling tools. It also provides a suggestion of the better CASE tool.\n\nComparison\n\nFor the sake of comparison of the CASE tools, two tools, visual analyst and the IBM rational software, were selected. The comparison involved focusing on the repository techniques used, the forward and reverse engineering characteristics, as well as the modelling tools used. These are looked at in details leading to an appropriate conclusion on the preferred CASE tool.\n\nRepository\n\nRepository is necessarily a base of reverse engineering and the standard starting point for the same. It is also the main approach and preliminary point as well as a template in the modernisation of COBOL. Developers of any application need a repository to ease their work besides cutting cost to improve efficiency during the development and upgrading of IT systems. Another key function of repository is binding of the tools used in Visible Analyst (VA).\n\nImportation of information in the form of diagrams and objects into repository is a precursor of VA modelling tools, which store them before use. A repository therefore forms the foundation in the development of models besides acting as the obligatory initial point irrespective of the method used. The quality of a repository also has an overall effect on the success of the development.\n\nThe use of the repository is common to both the visual analyst and IBM rational software with varying differences evident on the utilization of services (Roger, 2001, p.23). The use of repository in CASE tools is noteworthy because it facilitates information storage. The efficiency of a repository system in a CASE tool determines the success of the tool and hence the popularity among clients (O\u2019Brien, 1995, p.34).\n\nThe content of a repository includes data, process, models, and rules or constraints. The IBM rational software uses a secure repository to manage contents besides providing a centralized store of data on businesses and other related information. A viewer only gets read-only rights, as a case that contributes to the security of the system and the information stored in it.\n\nOrganizations can upload employees\u2019 information here, convey information to the organizational members, and post changes in policy and other reports such as goals and objectives (O\u2019Brien, 1995, p.36). An online database that requires a server is necessary for the IBM repository to be effective with the possibility of many users accessing the service. For the visual Analyst, the same technique is applied with only a difference in the databases used.\n\nThe cloud is the choice of technique used with information being stored in a centralized large server. The server is secured with passwords and other protective tools with the only available access being coded for administrators only (Roger, 2001, p.25). A series of supercomputers keep the server functioning with the date moving to-and-fro at a large speed. The visual analyst therefore provides a secure method of data storage for a cheaper cost compared to IBM rational software.\n\nForward Engineering features\n\nAs a definition, forward engineering is the science involved in the development of new software from the already existing ones or as a change to existing software. Forward engineering uses the existing information on repositories to develop systems with greater functionality, stability, efficiency, and according to user needs.\n\nThere are advantages to the use of forward engineering as compared to starting from the scratch, as less time is spent. Developers depend on the already proven systems. Currently, it surprises to realise that every system developer considers the use of forward engineering as an efficient method of getting around system development.\n\nForward engineering is also currently used to convert diagrams stored in repositories into codes to be executed in any platform as indicated by their type. Forward engineering does not use pre-existing codes in the development of new models, as it is the case with reverse engineering. It is therefore employed in the development of non-existent models.\n\nIn information technology, forward engineering involves the formulation of skeleton codes from pre-existing models. In both IBM rational software and visual analyst CASE tools, forward engineering is a common feature, as it is to most or all the other existing tools.\n\nIn IBM rational software, forward engineering consists of a systematic process. The model of the software is pertinent here. The components have to be identified first. The tasks involved are classes of software identification, checking the syntax, and class path. Code generation follows with a backup of the source suggested.\n\nThe last step involves viewing the generated source. Visual analyst CASE tool utilizes forward engineering in a simpler way compared to IBM rational software. The complex procedure like that followed in IBM is not followed here. The process ends up being simpler and faster. However, this compromises security and efficiency in the final product with low productivity.\n\nFor small enterprises willing to use the CASE tools, visual analyst would offer a better alternative with similar objectives. In forward engineering, more codes are developed in visual analyst tools compared to IBM. The argument holds because visual analyst offers a better and more flexible environment for model manipulation. Therefore, it edges out IBM based on this consideration despite the lower performance rating.\n\nReverse Engineering\n\nCompared to forward engineering, reverse engineering entails the development of models from pre-existing codes (Roger, 2001, p.27). Through manipulation of existing codes, a software developer can develop new systems especially those that are currently not produced anymore to develop them.\n\nThis entails a careful study of the existing codes, their manipulation, and use to develop the new models. This step is important in the vital software tools that are hard to reproduce. Different IT companies have different thoughts on the efficiency of reverse engineering in the development of newer models with some preferring to use forward engineering at the expense of reverse engineering.\n\nAs the opposite of forward engineering, reverse engineering attempts to recreate models from codes that are pre-existing (Roger, 2001, p.27). The initial step involves meticulous scanning of the existing codes leading to the generation of new models. These are distinct from previous versions, as they carry a unique character that differentiates them from other versions.\n\nThe existing requirements dictate the type of code that is used, and the more diverse the code, the greater the flexibility of the model. Reverse engineering is critical in generating models for systems that have since run out of market, or are no longer produced since they are out-dated. It also finds the use in the analysis of existing software to establish designs and their specifications.\n\nReverse engineering is well utilised in both tools as required in the competitive market. In IBM reverse engineering procedure, a derby database or a DB2 oracle database is used. The process guides the user in a series of steps with each step requiring a confirmation to proceed to the next. The first step involves the creation of an empty project referred to as \u2018my application\u2019.\n\nThe whole process of reverse engineering in IBM rational software involves about seventeen processes that are comprehensive and easy to follow. However, these require expertise and patience as they involve close follow-up. Visual Analyst CASE tool on the other hand \u201cbuilds and reverses the engineer database schemas for Oracle and SQL\u201d (O\u2019Brien, 1995, p.39). The code is automatically generated with fewer steps involved. This means that it is less involving besides its delivery of greater results compared to IBM.\n\nReverse engineering, as it happens with visual analyst, involves a somewhat similar process to that of the IBM CASE tool with only few differences. Pre-existing codes are used to generate newer models with the initial step of scanning being similar both processes. The creation of newer models to replace old and out-dated models is more successful when using the visual analysis CASE tool compared to the IBM rational software.\n\nModelling tools\n\nModelling tools are very important in the development of CASE tools. They are used in the development of newer software after an appropriate method is decided on. A good example is the UML (Unified Modelling Language) modelling tools, which are favoured by the IBM.\n\nOnce a developer settles on the appropriate method of software engineering and development, he/she needs an appropriate tool to use. The tool used is referred as a modelling tool. There are different tools that can be employed in the development of models. However, this depends on the convenience of the developer and the software involved.\n\nModelling tools are vital components of CASE tools. IBM rational software and visual analyst are no exceptions. In IBM rational software, modelling tools are valuable in information storage as they enable the writing of information on the repository. The tools used include the rational software modeller, rational software architect, and rational systems developer (Roger, 2001, p.28). Both CASE tools support the Unified Model Language (UML), which is the most current (Roger, 2001, p.28).\n\nTherefore, this means that, in comparing the modelling tools used by both CASE tools, one will see the little significant difference that exists between them. This similarity excludes the use of modelling tools used by the two to differentiate between them. However, the number of tools varies with the visual analyst using a greater number of tools and improving on diversity and flexibility (O\u2019Brien, 1995, p.39). This stands out as an added advantage over the IBM rational software.\n\n                     Visible Company                             IBM                                                   \nRepository           Single type of repository is used             * Uses three types of repositories                  \nForward Engineering    * Model: driven architecture is not used    * The architecture is model driven                  \n                                                                   * There is unification of the Modeling Language used\nModeling Tools         * Use UML and other tools                   * Mainly UML is used                                \nConstruction Tools     * Shared Modeling tools                     * IBM software modeler                              \n\n\nPreferred method for a company\n\nFrom the above discussion, there are evident differences between the IBM rational software and the visual analyst CASE tools. These exist in the repository techniques used, the available forward and reverse engineering features of the tools, and the modelling tools.\n\nGiven the choice of both tools for an organization or a company, visual analyst would emerge as a preferred personal choice for Tawazun Training Company, which was established in 2009. It needed to enhance its information system. The reasons behind the preference include the differences above, as well as the additional reasons stated below.\n\nOf the two tools, visible analyst is known to support both structures and other different types of designs. As O\u2019Brien (1995, p.40), confirms, \u201cVisible analyst also supports UML or object-oriented design\u201d. This means that it is more accommodating to many designs. Despite having poor java integration, it is useful in \u201creverse engineering databases for Oracle and SQL\u201d (O\u2019Brien, 1995, p.39), as pointed earlier on.\n\nAnother advantage is that it recognizes most design diagrams besides having the capacity to export and print them, as opposed to the others. When it comes to the flexibility of the tool, it is more flexible and consistent compared to the IBM tool even though this is user-prompted. Since there is a tutorial on the use of visible analyst program, it is easy for armatures and first time users to learn how to use it. This eliminates the challenge of training users.\n\nAt the same time, it simplifies the program for the general population in addition to cutting down costs and time spent or wasted in informing the users about it. These are some of the considerations in setting up a CASE tool for a company. As such, the visual analyst satisfies most, if not all of them. The features in the visual analyst are similar to those in IBMs CASE tool with both having the same capability and near performance.\n\nThe common features include the CASE repository in which data on businesses and clients is stored for reference. This is linked to the workstation, which in turn consists of a planning toolset, an analysis toolset, and a design toolset. Other key components include the automatic code generator, the report generator, user interface generator, and the toolset used for automatic code generation.\n\nThese are important in the development of the software involved. The pricing of the two software tools varies based on their use, the type of the software, and their functionality. With all these factors constant, the visual analyst CASE tool is cheaper in installation, purchasing, and maintenance compared to IBM rational software.\n\nTherefore, this makes it a preferred choice in my choice of organization since it would cut on operational costs. However, with the increased number of software changes that are necessary, as well as the constant reviews needed, the visual analyst would also end up increasing the cost of organization\u2019s running. However, this drawback is countered by the fact that there is an increased functionality with each software change leading to a more efficient working environment.\n\nIn the overall look and feel of the CASE tools, other tools have about the same value with the only difference being the physical appearance and the general look (Roger, 2001, p.29). Both tools also have similar outlooks. This is confusing to most beginners of information technology. These similar looks and feels are of little significance in the making of a choice between the two.\n\nThe claim holds because the selection of a tool is not guided by the look and feel but rather by the performance index attributed to it. Based on these criteria, visual analysis CASE tool would emerge as the most preferred by people who need to improve the information systems of their organizations. Another determining factor would be the ease of access and usage.\n\nVisual analysis is easier to operate, understand, and maintain for a medium sized company where importance is given to the operating cost rather than investment in CASE tools. This principle has led to the tool being popular among small businesses and companies. As a result, it continues to be seen throughout different parts of the world. With the above considerations, I would find visual analysis a preference for the organization.\n\nConclusion\n\nIn conclusion, there are various sites and companies offering CASE tools, which include visual analyst, IBM rational software, oracle designer, and visual-paradigm. Of the above methods, each has its own strengths and weaknesses over the other. A comparison between IBM Rational Software and Visible Analyst reveals this, as done in the paper.\n\nThe substantial differences exist in the uses of repository, forward and reverse engineering features available, and the modelling tools among others. The visual analyst tool emerges stronger compared to IBMs tool, as discussed as a personal preference in the paper. The differences in functionality, ease of use, and look and feel characteristics lead to the conclusion that visual analyst is better that IBM rational software.\n\nReference List\n\nO\u2019Brien A. (1995). Introduction to Information Systems, An End user/Enterprise Perspective . London: McGraw Hill Edition.\n\nRoger, S. (2001). Software Engineering \u2013 A Practitioner\u2019s Approach . London: McGraw-Hill International Edition\n",
        "label": "human"
    },
    {
        "input": "Computer Forensics and Investigations Essay\n\nPrinciples of computer forensics\n\nPrinciples of computer forensics are standard rules that govern how digital evidence is handled to make it admissible in court (Nelson, Phillips & Steuart, 2010; Taylor, Haggerty, Gresty & Lamb, 2011; Easttom, 2014). Many countries and states had their principles of computer forensics.\n\nHowever, efforts have been made to align several principles in order to have internationally accepted principles that can be applied across the world (Taylor et al., 2011). The standardization efforts have resulted in the adoption of four key principles. First, digital evidence should be collected in a manner that does not allow alteration of crucial data. This principle attempts to uphold the integrity of evidence (Taylor et al., 2011).\n\nSecond, the processes of collecting, storing and analyzing digital data should be fully documented, and reasons should be given for any manipulation done. This principle aims to make professionals handling digital evidence responsible for their actions. Third, digital evidence should only be accessed by forensically competent persons. This principle ensures that non-competent persons do not interfere with digital evidence (Nelson et al., 2010; Taylor et al., 2011).\n\nFourth, it should be ensured that the right procedures are followed during computer forensic investigations. If the law and principles of computer forensics are followed, then digital evidence would be admissible in court. Admissible evidence is crucial in promoting justice and fairness in criminal proceedings (Nelson et al., 2010; Taylor et al., 2011; Easttom, 2014).\n\nThe role of computer forensics as it relates to other IT disciplines\n\nComputer forensics is the integration of computer science and law. It is crucial in the investigation of crimes that are related to the manipulation of computer systems (Easttom, 2014). All IT applications rely on the use of data that are analyzed, stored and retrieved for particular uses (Nelson et al., 2010).\n\nComputer forensics could be used in legal matters to solve criminal issues in all other IT applications. Therefore, forensic science with regard to computer systems plays crucial legal roles in relation to other IT application (Taylor et al., 2011; Easttom, 2014).\n\nHistory of computer forensics\n\nCrimes related to the use of computers came to the limelight in 1978 in Florida after legislation was adopted to prohibit unauthorized changes of data preserved in computers. Federal laws recognized crimes related to the use of computers in the 1980s. History of computer forensics can be categorized into three distinct phases (Easttom, 2014). First, the ad-hoc stage was marked by lack of clear frameworks for dealing with computer crimes.\n\nThe phase was also characterized by many legal issues that revolved around the applications of computer systems to handle digital evidence used in court. Second, the structured phase involved the adoption of specific tools and procedures in digital crime investigations and prosecutions. Third, the enterprise phase (the current phase) involves fast collection of digital evidence, creation of sophisticated tools and many companies offering forensic services (Nelson et al., 2010; Easttom, 2014).\n\nHow to use computer forensics in criminal investigations\n\nFor digital evidence to be admissible in court, investigations should be conducted in a manner that adopts the principles of computer forensics (Easttom, 2014). The following steps are involved in computer forensics investigations:\n\n 1. A computer system containing crucial evidence is secured to ensure that data are safe.\n 2. All files in a computer system that are not encrypted are copied.\n 3. Deleted information is retrieved.\n 4. Contents of hidden files are revealed using specific software to identify hidden data.\n 5. Protected files are decrypted and accessed.\n 6. Inaccessible parts of computer disks are analyzed to locate files that could contain crucial data.\n 7. All steps of the procedure are documented.\n\nConstitutional protections and laws covering investigations\n\nComputer forensic investigations are protected by the US constitution and various federal and state laws. Therefore, computer forensic investigators need to conduct investigations within confines of the law. Federal computer crime laws protect various aspects of investigations.\n\nSome of the federal computer crime laws include Health Insurance Portability and Accountability Act, USA Patriot Act, Child Pornography Protection Act, and Communications Decency Act 1986, among others. Case laws are based on verdict given by judges in computer crimes, and they are adopted as legislation that protects computer forensic investigations (Nelson et al., 2010).\n\nEthics\n\nThe code of ethics requires computer forensic investigations to be conducted using accepted ethics (Taylor et al., 2011). Some computer forensic issues include privacy, impact on society and intellectual property rights (Nelson et al., 2010; Easttom, 2014). Computer forensic professionals should protect the secrecy and privacy of clients\u2019 information (Easttom, 2014). A high degree of secrecy and privacy could be achieved when personnel adhere to ethical standards.\n\nComputer forensic professionals should follow standard ethical procedures when conducting investigations. If the standards are followed, then the evidence would have a high degree of accuracy and authenticity. Adherence to standard ethical procedures also goes a long way in preventing alteration of crucial forensic evidence that would be admissible in court.\n\nReferences\n\nEasttom, C. (2014). System forensics, investigations, and response (2 nd ed.). Burlington, MA; Jones and Bartlett Learning.\n\nNelson, B., Phillips, A., & Steuart, C. (2010). Guide to computer forensics and investigations . Stamford, CT: CengageBrain. com.\n\nTaylor, M., Haggerty, J., Gresty, D., & Lamb, D. (2011). Forensic investigation of cloud computing systems. Network Security, 2011 (3), 4-10.\n",
        "label": "human"
    },
    {
        "input": "Ethics in Computer Hacking Essay\n\nAccording to hackers, hacking is the art of checking anomalies within a network or website or computer system for points of entry, to exploit it to the hacker\u2019s advantage. Ethics is the moral conduct of an individual, the conscience to judge between right and wrong. Hacking by no means follows ethics; the infiltration is to the benefit of hacker and loss of users of computer system, network or website. (Erickson 1-3)\n\nIan Murphy was the first hacker convict. In 1981, Ian Murphy under the pseudonym Captain Zap, hacked the AT&T computers to alter call billing rates. This he did by changing the internal clocks for billing, transferring late night calling discount to midday. It was the first and among the most prolific hacking cases of all time (Delio n.p.).\n\nThe purpose of technology is to ease the lives of people. The computer has made things which were previously hard to be done, such as going to space or creating precision in production. The potential to achieve extraordinary things at the hands of computers over the decades has grown with the improvement of computer technology.\n\nThe result has been the rise of computer processing power, storage and network capabilities. The software segment of computing has not been left behind, such as the Android system for smart phones that brought great change in the lives of mobile phone users.\n\nHacking, on the other hand, achieves the opposite of this by destroying the computer networks to benefit a few. In the case of AT&T versus Captain Zap, it was a prank, which resulted in financial losses. It further was a motivation to hacking which caused losses of hundreds of millions of dollars to firms and individuals worldwide. The industry is now multi-billion dollar industry, which countries join to use cyber crime to fight other countries. Iran, in August 2012, had a reduction in nuclear plant activity due to a computer worm.\n\nHackers have three main targets: networks, websites and computer systems. A computer network is the interlinking of one computer with another to create a system for exchange of files and information between users of those computers. Forms of networks include the internet, wide area network and local area network. A local area network connects computers to each other within an area.\n\nA wide area network connects computers of one organization, which location is in multiple areas such as different towns or cities. The internet is a worldwide connection for transference of files. Infiltration of a computer system can result in system downtime, compromise of network security and further hacking the user(s) computer system. Use of a computer antivirus, which will detect potential threats to the system such as ESET antivirus, will be a great help (Erickson 197-220).\n\nThe second main target is websites. A website is an online representation of the information on a static or dynamic web page. Bringing down the website is breaching of a display of information on the website distorting emails for a website\u2019s newsletters and affecting internal website information systems. The first line of defence against hacking lies in software development of the website, focusing on proactive threat protection. Having an online website protection service such as McAfee boosts online website security.\n\nThe computer system is the last main target of hackers. It is the ultimate target for hackers crippling websites and networks. The computer system consists of software and hardware. Main forms of hardware include, display hardware, processing hardware and storage hardware. To avert hacking those, an antivirus and use of a complex computer system that guards against infiltration, will be the key.\n\nIn recent times, there has been the rise of ethical hacking. Ethical hacking goal is to hack websites or computers and network systems. The system owner corrects these areas at the guidance of the hacker. It is setting a thief to catch another thief. However, the owner of the computer system opens it up to a hacker to improve it, which may create leeway for him/ her to exploit (Olson n.p.).\n\nWorks Cited\n\nDelio, Michelle. \u201cThe Greatest Hacks of All Time.\u201d Wired Magazine . Web Jun. 2001. < https://www.wired.com/2001/02/the-greatest-hacks-of-all-time/?currentPage=all >.\n\nErickson, Jon. Hacking: The Art of Exploitation . San Francisco: No Starch Press, 2008. Print.\n\nOlson, Parmy. \u201cExploding the Myth of the \u2018Ethical Hacker.\u2019\u201d Forbes. Web. < https://www.forbes.com/sites/parmyolson/2012/07/31/exploding-the-myth-of-the-ethical-hacker/#2fd62ceb33ea >\n",
        "label": "human"
    },
    {
        "input": "Ethics in Computer Technology: Cybercrimes Term Paper\n\nTable of Contents\n 1. Introduction\n 2. Cybercrimes and cyber-related crimes\n 3. Instances of cybercrime\n 4. Ways of preventing cybercrimes\n 5. Conclusion\n 6. Works Cited\n\nIntroduction\n\nEthics are moral philosophies that guide every society. These philosophies differ from one society to another such that what is regarded as good in one society may be regarded as bad in another (Mizzoni 8). The world has experienced technological revolution over the last couple of decades. Technology has advanced rapidly during this period compared to any other time in the past. Technology has made life easier for most people by helping raise living standards. Problem solving has also been made easier by technology.\n\nWhile technology has generally caused positive impacts on the lives of human beings, it has also caused some negative impacts. There are some people who have taken advantage of technology to commit crimes. This has led to the emergence of the term \u201ccybercrime\u201d. This essay discusses the impacts that technology has on human beings, cybercrimes and the crimes that are committed against mankind using modern technology.\n\nCybercrimes and cyber-related crimes\n\nCybercrime is a term used in technology ethics to refer to the wrong doings that are facilitated by technology. These are crimes that are aimed at harming an individual or a group, and they are executed using modern technology (Kshetri 4). Cybercrime is a kind of crime that has been growing fast, causing a lot of concern among law makers. A wide range of crimes can be committed using technology.\n\nComputers are the gadgets that are mostly used to commit cyber crimes targeted on individuals. However, computers can also be targets. Cybercrimes may have devastating effects on the national economic health or to the national security of a country (Kshetri 56).\n\nAs a result, issues of cybercrime and cyber related crimes have come to the spotlight over the last couple of years. Governments have now started recognizing cybercrimes as crimes that are punishable under the law. In the past, cybercrimes were not recognized as serious crimes like they are today. In addition, cybercrime has of late become an important topic in the study of technology ethics owing to its rapidly increasing significance.\n\nAmong the common cybercrimes are financial theft and espionage. These are crimes that are committed by both governmental agents and non- governmental agents, internationally or nationally. Other unethical behaviors that are associated with cybercrimes include copyright infringement, pornography, as well as child grooming among others.\n\nThese are crimes that have the ability to cause serious effects on affected individuals. It is for this reason that security personnel all over the world have taken the responsibility of curbing the vice. Laws regarding the same have been made stricter (Kshetri 60).\n\nIt is imperative to note that there are three main types of cyber-related crime that are currently recognized by the law in the United States of America. The first one is the category of crimes that are executed using a computer as a weapon. In such crimes, the perpetrator uses the computer in order to commit a crime on another subject, usually an individual or a group of individuals. The second type of crime is the one that uses a computer as an accessory to the crime.\n\nHere, the perpetrator gets access to the crime he or she intends to commit with the help of a computer. An example of this is a case where a criminal uses a computer to access data of a company, which he then uses to his benefit. The third type of cyber-related crime is the one that targets the computer itself; the harm is directed to the computer itself (Kshetri 56). Examples of such crimes include theft of a computer and installation of malware that destroys the computer among other crimes.\n\nCyber and cyber-related crimes have ethical consequences in any society. The crimes influence the behavior of individuals, as well as their actions. For instance, pornography is a crime that can be associated with cyber technology. This is an issue that is likely to have a negative effect on the society.\n\nIt is, specifically, likely to have more impact on the young children who get access to pornography. Further, children grooming is also negative on the society. It leads to intimidation of children and threatening them to participate in vices such as sexual activities and child prostitution among others.\n\nIt also encourages child trafficking, an issue that has concerned the globe for a long time. Incidents of copyright infringement have increased with advancement in technology (Brenner 25). Many people are now able to reproduce the works of others and sell the works as theirs with the help of computers and the aid of computer programs. This is an ethical problem that has affected musicians and film actors, making them lose a lot of revenue.\n\nInstances of cybercrime\n\nThere are a number of cybercrimes instances that have been identified before. The major instance among them is fraud of finances from another person\u2019s account. There are some people who pose data in computer networks so that they can gain from the data. For instance, an employee who wants to get money from the company illegally or wants to get money for another employee may present false information into the database so that the money is directed to his or her account.\n\nThere are other people who steal money from customers. Such people access bank accounts of the customers and defraud them. This is a crime that is very common nowadays. It has actually increased as methods of money transfer advance to online transfer. People get credit card information, access the accounts and defraud the accounts (Michel 40).\n\nHacking is the other common cybercrime. This is situation where one is able to identify the weaknesses of a system, get access to it and use the data for his own benefit. This is common in companies, where a company\u2019s website is hacked by competitors to get information that they take advantage of to overcome the competition.\n\nMalware software programs may be used to facilitate hacking. Hacking of business organization websites is the most common type of hacking that is experienced today. Criminals try to break down codes that are used to protect information to access the information and use it to their advantage. However, hackers may not always be criminals. There are ethical hackers who are legal and are hired by a company for purposes of protecting its data. Only illegal hackers are referred to as unethical hackers (Brenner 33).\n\nWays of preventing cybercrimes\n\nCybercrimes are important and their impact cannot be ignored. These crimes cause a lot of harm to the affected party. It is, therefore, important to find ways of preventing cybercrimes. One of the ways that one can use to evade cybercrimes is installing computer anti- virus software.\n\nThis prevents the computer from being attacked by the malwares used by hackers to break down websites. Other security software programs such as spam blockers and encryptions can also be used. These software are important for preventing data from being accessed illegally. Firewalls, as well as intrusion detection systems can also be of great significance in a bid to protect hacking of data. Crimes such as fraud and hacking are reduced immensely once data is protected from being accessed illegally (Brenner 67).\n\nLaws that address cybercrimes should also be enforced. People who are caught trying to commit cybercrimes should be dealt with accordingly. This is important since it will deter cyber criminals from executing crimes out of fear of the legal consequences for their unlawful actions.\n\nConclusion\n\nTechnological advancements have been on the increase since the last century, resulting in a wide range of benefits to the society. For instance, technology has helped improve the living standards of people, improve communication, and increase business efficiency among other benefits. The cost of communication has also decreased significantly.\n\nHowever, technology is regarded as a two sided sword, whereby it has led to both positive and negative effects. Some of the demerits that are associated with technology include the increased cases of cybercrimes over the last couple of decades. However, the crimes can be prevented by taking protective measures such as installing anti-virus programs and firewalls. Successful prevention of cybercrimes will be a huge stride towards making technology a good experience for everyone.\n\nWorks Cited\n\nBrenner, Susan W. Cybercrime: Criminal Threats from Cyberspace . Santa Barbara, CA: Praeger, 2010. Print.\n\nKshetri, Nir. The Global Cybercrime Industry: Economic, Institutional and Strategic Perspectives . Heidelberg: Springer, 2010. Print.\n\nMichel, Dion. \u201cCorruption, Fraud and Cybercrime as Dehumanizing Phenomena.\u201d International Journal of Social Economics 38.5 (2011): 466 \u2013 476. Print.\n\nMizzoni, John. Ethics: The Basics . West Sussex, U.K: Wiley-Blackwell, 2010. Print.\n",
        "label": "human"
    },
    {
        "input": "Preparing a Computer Forensics Investigation Plan Essay\n\nHow to prepare a windows-based computer for a forensic investigation\n\nForensic investigators use specific hardware and software to examine computer systems. The increased adoption of Windows operating systems has made computer forensic investigators use Windows-based platforms as sources of digital evidence. The first step involves taking the image of the computer suspected to have crucial digital data.\n\nIf crucial evidence is suspected to be held in volatile storage, then a live analysis is conducted, but a dead analysis is performed when the evidence is thought to be contained in permanent storage disk locations. A Windows-based computer would require retrieval of information before shutting down the computer. However, if the information is thought to be contained in the permanent storage, then a computer has to be shut down before transporting it to a laboratory for forensic analysis.\n\nA computer forensics expert should be careful not to change data held in non-volatile storage when powering down the computer. When using a Microsoft Windows system, the information stored in non-volatile storage could be prevented from interference by removing the power cord from the socket (Nelson, Phillips & Steuart, 2010; Easttom, 2014).\n\nThe first step in the laboratory examination would involve analysis of the status and setup of the computer. The computer should be booted and BIOS setup selected. Caution should be taken so that the Windows-based computer does not use internal digital devices to boot.\n\nAlternatively, internal drives should be disconnected so that they would not interfere with the intended booting procedure (Nelson et al., 2010; Taylor, Haggerty, Gresty & Lamb, 2011; Easttom, 2014). At this point, information could be retrieved from the computer for forensic analysis.\n\nHow to handle digital evidence\n\nDigital data could be changed easily, and this could interfere with the integrity of digital information. Also, alteration of digital data could make it difficult to differentiate original data from copied data. There are four principles that are followed when handling digital evidence (Easttom, 2014). First, digital evidence should be collected in a manner that does not cause changes in the form of data. If the data are changed, then the integrity of the data could be compromised.\n\nSecondly, only trained persons should be allowed to handle digital evidence. Persons who are trained could handle digital evidence professionally and be responsible for breaching ethical, legal and professional standards (Nelson et al., 2010). Also, digital evidence that is professionally handled by trained personnel could have higher chances of being admissible in court than digital evidence handled by untrained persons.\n\nThird, all processes used to analyze digital evidence should be well documented and stored for reviews in the future. There should be clear reasons for any changes that are done on the digital evidence. This helps to hold professionals responsible for their actions. Fourth, computer forensic experts should examine copies of original files suspected to contain evidence (Easttom, 2014). In other words, original files should not be examined or manipulated.\n\nGathering data\n\nThe quality of evidence gathered in computer forensics greatly depends on the law enforcement and procedures used when gathering the evidence (Nelson et al., 2010). The law is clear about specific legal guidelines that should be followed when handling forensic evidence. For example, the Health Insurance Portability and Accountability Act prohibits professionals from disclosing clients\u2019 information without their permission (Easttom, 2014).\n\nTherefore, it would be illegal for a computer forensics professional to disclose private information about a person who is being investigated without his or her permission. Gathering data in computer forensics is also expected to follow standard procedures that aim to promote quality of the evidence. Standard evidence gathering procedure requires forensic experts to use tested and accepted tools for data collection.\n\nSome of the tools may include boot software, computer forensic software, analysis software and intelligence analysis software, among others. General practices and procedures also require that all personnel involved in gathering evidence should be aware of the best procedures and practices. This helps to maintain the integrity and authenticity of forensic evidence (Nelson et al., 2010; Easttom, 2014).\n\nPrivacy issues\n\nPrivacy issues are common in the field of computer forensics. Legal and ethical standards require that computer forensic experts should uphold the privacy of client organizations. In some cases, leakage of a client\u2019s information may result in media attention that could negatively impact a business organization.\n\nCode of ethics prohibits persons from disclosing assets of an individual when conducting forensic investigations. It is also against the code of conduct to disclose an individual\u2019s information on the internet during forensic investigations (Nelson et al., 2010; Taylor et al., 2011).\n\nHow to use data as evidence in a criminal proceeding\n\nThe data collected from the computer system would act as evidence in a criminal proceeding only if it meets the standard requirements (Taylor et al., 2011; Easttom, 2014). First, there must be proper documentation to show that the data was collected using standard legal and ethical procedures.\n\nSecond, it should be shown in a court that the data being presented as evidence have not been altered to affect their integrity. Third, it must be shown that the persons handling the data at various stages are trained for that purpose. Once the three conditions are met, the data would be used as standard evidence in a criminal proceeding.\n\nReferences\n\nEasttom, C. (2014). System forensics, investigations, and response (2 nd ed.). Burlington, MA; Jones and Bartlett Learning.\n\nNelson, B., Phillips, A., & Steuart, C. (2010). Guide to computer forensics and investigations . Stamford, CT: CengageBrain. com.\n\nTaylor, M., Haggerty, J., Gresty, D., & Lamb, D. (2011). Forensic investigation of cloud computing systems. Network Security, 2011 (3), 4-10.\n",
        "label": "human"
    },
    {
        "input": "Tablet Computer Technology Essay\n\nIntroduction\n\nTablets refer to mobile portable computers that embrace the most modern technology of touch screens. Such computers use the screen as the primary input mode though some have digital pens for data input. The screens are sensitive to touch and gravity.\n\nTablet computer technology consists in the fact that large computer components are shortened ensuring that their size is considerably reduced compared to the laptops or desktops; their efficiency is enhanced as well.\n\nHybrids are an improvement of the laptops since they have keyboards that can be detached from the main central processing unit. Slates, on the other hand, have an inbuilt keyboard engrafted in the system. Instead, slates wholesomely rely on the on-screen keyboard for the text input (Computers. n.d.).\n\nComparison of selected tablets\n\nIpad 2\n\nIt runs on a CPU with A 5 microchip dual core together with a display of 9.7\u2011inch LED-backlit Multi-Touch display with IPS technology, and a resolution of 1024\u00d7768, 132 pixels per inch. It weighs 613g and runs on a MAC X v10.6.8 operating system with a storage capacity of 64 GB. This gadget currently retails for $499 (Tabletpccomparison, 2012)\n\nSamsung galaxy note 10.1\n\nThis tablet runs on the latest android operation system (4.0 Ice cream sandwich). It has an internal memory of 16GB together with a quad core processor and a micro SD card slot for expanded storage of up to 50GB. It weighs less than 500g and operates on the technology of AMOLED display with a resolution of WVGA 800\u00d7480 and a detachable input pen. It retails for a price of $450 (Computers. n.d.).\n\nLG G-Slate V909\n\nIts price is $366. It is run by a dual core processor with a speed of 1 GHZ dual core (NVIDIA Tegra 2 T250). Its internal capacity is 32 GB with a RAM of 2 GB and an approximated weight of 1.45 pounds. The machine is operated by an android operating system.\n\nIts screen resolution is 1,280\u00d7768 pixels. The gadget has a screen size of 8.9 inches and a display technology of thin film transistor liquid crystal display otherwise known as TFT technology (Tabletpccomparison, 2012)\n\nToshiba Thrive 10 Tablet\n\nThe gadget costs about $400. It runs on the latest Android 3.2 honeycomb Operating system with a NVIDIA\u00ae Tegra\u2122 2 Dual-Core Processor (1GHZ dual core). It has a 16 GB memory capacity with a display of 10.1 high-resolution widescreen with (1280\u00d7800) pixels, LED Backlit, Multi-touch. Its weight is 10b.\n\nSony Tablet S\n\nIt runs on an android honeycomb operating system with a screen display of LED technology and 16:9 widescreen. The machine has a resolution of 1280\u00d7800 pixels together with a processor type: NVIDIA Tegra2 and approximated weight of 12b.The processor speed is 1GB. It has an internal storage capacity of 1 GB is sold at the price of $366.\n\nAll the tablets discussed above are very powerful gadgets. As a student though, my preference is the Samsung galaxy phone. This gadget offers a unique range of qualities that corresponds to my needs and interests. The four chips provide very high speed along with the powerful display and the pen input system. As a student interested in graphic designs, it gives me a gadget with an unprecedented capability to develop my designing skills. The machine has a humongous number of applications, and is also relatively affordable.\n\nReferences\n\nTabletpccomparison. (2012). Retrieved from https://www.tabletpccomparison.net/\n\nComputers. eHow. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Testing: Beneficial or Detrimental? Research Paper\n\nWith the increasing popularity of computers, more college instructors have converted from paper-and-pencil to computer-based tests for benefits such as reduced grading effort and the ability to test more frequently (Etrurk, et al. 2004). Computer based assessment has increased in the recent times. Different organizations are using it for different purposes.\n\nAccording to Etrurk and others (2004), several organizations are currently using computer based assessment for drivers\u2019 license exams, job interviews, certification of exams and entrance exams for post-secondary education. It has been argued that the mode of testing administration affect scores (Clariana & Wallace, 2002). So far, there is evidence that identical paper-based and computer based tests cannot obtain same results. However, study results have been inconsistent (Kingston, 2009,).\n\nEven when Computer-based and paper-based testing with have identical items, they may not produce equivalent measures of student learning. Some research also found that the format of the exam and the subject being tested on also matters. In this essay, I will analyze the current body of research and attempt to determine whether administration via computer yields equivalent scores as paper-and-pencil administration, and further explore the validity of computer-based testing.\n\nMost of the studies so far conducted shows that computer-based tests are more superior to paper-based test. Some instructional quota insists that the two tests methods should give same scores. Practically these methods do not give same scores (Mason et al. 2001). Other studies have concluded that computer-based and paper-based tests are nearly identical (Erturk et al. 2004).\n\nThere are several variations of Computer-based and paper-based testing. Some of these differences highlight advantages of one over the other method. In a synthesis of 81 recent studies, Kingston (2009) concludes that the mode of administration does not affect student achievement across a grade level.\n\nHowever, recent studies by Butters and Walstad (2011) in which multiple-choices answers were used for testing shows that students who completed the test on computers outperformed a paper-based test group and that computer-based testing has the potential to reduce item guessing.\n\nVariations of score have been caused by some discrepancies. Researchers who support the test mode effect argue that previous experiments which showed no differences in achievement and arrived at a different result did so due to differences in distribution of assessment and sample size (Maguire et al. (2010).\n\nWide range of students gives consistence results than skewed population. They further found out that most students were more comfortable interfacing with computers than using a pencil and pen during their tests; this mitigate students\u2019 test anxiety. Clariana and Wallace (2002) found out that scores variations were caused by settings of the system in computer-based and level of strictness of examiners in paper-based.\n\nThe difference in computer-based and paper-based tests can be affected by learner\u2019s characteristics. Learners with poor knowledge of computers are likely to fail an online exam. On the other hand, computer literate student with higher academic attainment taking an online exam is likely to do well (Clariana and Wallace, 2002).\n\nClariana and Wallace (2002) further examined previous studies which showed that computer based tests on verbal quantitative and analytic sections of all tests were all greater than complementary paper-based test scores. They also found out that computer familiarity is the most fundamental factor in test mode effect for unfamiliar content and low attaining examinees.\n\nThey also found out that computer familiarity is the most fundamental factor in test mode effect for unfamiliar content or for low attaining examinees. This is mainly for students with limited access to computer. Higher attaining students will easily adapt to any assessment method and develop test-taking strategies for any new approach. Computer familiarity should become a less factor when all students have become computer literate (Butters et al. 2004).\n\nAlthough some differences are common, testing methods sometimes have multiple choice questions. Together with multiple choices testing, subjects should be put into consideration before a method is used. Kingston (2009) concludes that English, arts and social sciences had an advantage on computer-based test.\n\nOn the other hand, Mathematics and science test favored paper-based test (Kingston, 2009). I have been exposed to computers all my life and been using internet for over ten years. This makes me feel more comfortable in computerized testing environment. I have learned that writing through a computer give better results.\n\nThere are several befits associated with computer based testing. This test method saves time for students under it. Choices are mostly made by a simple click of a mouse rather than writing. This makes it easy and fast for students who are familiar with computers and high attaining. Furthermore, examinees do not have to go to examination centers, instead they just can have an internet connection (Noyes and Garlandb 2008).\n\nAnother benefit of computer-based test is immediate scoring and reporting of results. The test can be easily monitored since the system records start and end time, amount of time spent on a particular item and integrated survey responses. Tests are recorded and scored electronically with minimum challenges. Results of tests can be accessed by third parties easily in spreadsheets, word and other application packages. The massive data generated makes analysis, access, retrieval and reporting of results easy (Meissner, 2007).\n\nFlexible test scheduling is one benefit of computer-based tests (Meissner, 2007). There is availability of single-day tests administration window to continuous ones. An approach chosen by a student is based on his or her access, publishing frequency and exposure.\n\nExam administrators have an opportunity to ensure test security, standard setting and forms assembly. Candidates are flexible to take exam at their convenience since it is offered all year round. After administrators create an exam, it becomes hard for content to be jeopardized or items to be compromised.\n\nItem statistics available for tests sponsor are monitored as they are generated. The flexibility of technology provides the testing program with experience by virtue of unique testing experience from various candidates. Various forms of test navigations are used (Noyes and Garlandb 2008)\n\nInstructors and administrators have an opportunity to include innovative item formats that are made possible by use of technology. Computer-based tests engage navigation and presentation of items in several ways. Test authors can pre-define sequence of sections or test-lets through linear by considering performance of previous tests. This testing system affords several unique content presentation styles. Various elements of tests may be presented on one page per item or clustered together according to the preferred method.\n\nThere are reduced costs of test production, administration and scoring. Tests are not published as hard copies before they are done by candidates. During tests, no invigilators are required to supervise candidates. Finally, no teachers are required to mark tests before results are released. This reduces the cost of preparing, administering and processing of results.\n\nAccording to Meissner (2007), use of computer based tests enhances security of the tests and results. The system reduces cumulative impact of item exposure on subsequent administrators. Although the scheduling is flexible, items are exposed and security of tests jeopardized.\n\nThis situation can be mitigated by having by several forms at a time. It can also be dealt with by having a smaller number of forms but refreshed them frequently. Security can be enhanced by encrypting the processed data before transmission. To access the data, owners can be subjected to authentication procedure which may include figure prints and digital photograph.\n\nComputer-based tests are consistent and reliable as compared to paper-based tests. Computer-based testing applies high degree of standardization. Policies and procedures are easily adhered to. These structures ensure consistency and reliability of the system. This can be seen in timing which is strictly and rigidly controlled. Furthermore, referencing materials can be incorporated as attachments (Meissner, 2007).\n\nApart from the several benefits highlighted above, computer-based test has a number of detrimental to students. When computer-based test is made mandatory for all students, those with poor computer skills are affected negatively. They end up scoring lower than what they would have passed had paper-based test been used (Russell et al. 2003).\n\nComputer-based test does not provide examinees with the opportunity to revise their work after marking. This affects students\u2019 future performance negatively. On the other hand, Vispoel et al. (1992) concluded that such tests do not always yield equivalent results. They added that the tests should be equated to ensure use of fair test scores. The amount of control in computer-based tests does not allow review of items by examinee.\n\nComputer based tests do not allow examinee to change their answers as they would have done in a paper-based test (Russell et al. 2003). Previous research has shown that most answers change by student become correct. Therefore, failure by students to change answers when they think they have entered a wrong one, effects negatively on their performance during computer-based test (Russell et al. 2003).\n\nAccording to Russell et al. (2003), item layout and presentation have some effects on examinee. Tests requiring multi-screens, graphical or complex display have negative effects on examinee performance. Sometimes graphical display issues may be detrimental to performance of examinee. Such issues include screen size, font size and resolutions of graphics of some items. Such situation affects performance of examinee negatively (Russell et al. 2003).\n\nMalfunctioning of computer software or hardware during a test can be detrimental to examinee. When a computer part is changed or computer system is restarted, a student is forced to restart again leading to double submission (Noyes & Garlandb, 2008).\n\nThere are ethical issues concerning computer-based tests. Confidentiality of examinee\u2019s work is not guaranteed in online examination. Respondents also tend to create a particular impression of their results (Noyes & Garlandb, 2008).\n\nTo conclude, benefits of computer based exams outweigh its detrimental. Benefits are on both sides of the exams. Students or examiners have their benefits, for instance, score and flexibility of exams. Sponsors spend less money to offer such tests because there is no supervision, printing and physical marking. Administrators too have their work simplified as the system requires little supervision.\n\nDetrimental of this testing system are few. However, almost all are on the examinees\u2019 side. As time goes by, more people become computer literate and access computers, most of the tests would be conducted on computer in most places. Negative effects will be less because majority of examinees would be computer literate.\n\nTo ensure that it does not disadvantage certain student, computer-based testing needs close cooperation between exam administrators and examinees. Preparing online questions needs a lot of efforts from technical team. Questions should be standardized with student\u2019s level.\n\nSome of the instructional designers are sometimes not equipped with skills to develop the best examination procedure. Sponsors need to ensure that all instructional designers and administrators are qualified to reduce detrimental. If all these issues are attended to, advantages of computer-based testing outweigh detrimental.\n\nThe method will be beneficial to instructors and students if well administered. More studies need to be conducted to ascertain some of the variation in scores of computer based and pencil-based tests. Methods and procedures which ensure variations are significantly low should be developed through extensive research.\n\nAs long as that the test developers ensure that the composition mode of these test is not putting certain students at a disadvantage and detrimental are reduced to minimum, Computer-based test will provide higher reliability, I believe it is beneficial for both students and teachers to adopt this testing method.\n\nWorks Cited\n\nButters, Roger B., and Walstad, William. \u201cComputer VersusPaper Testing in Precollege Economics.\u201d Journal of Economic Education 42.4 (2011): 366\u2013374. Print.\n\nClariana, Roy and Patricia Wallace. \u201cPaper-based Versus Computer-based Assessment: Key Factors Associated with the Test Mode Effect.\u201d British Journal of Educational Technology 33.5 (2002): 593-602. Print.\n\nErturk, Ismail, Yasar Ozden, M. and Refik Sanli. \u201cStudents\u2019 Perceptions of Online Assessment: A Case Study.\u201d Journal of Distance Education 19 (2004): 77-93. Print.\n\nKingston, Neal M. \u201cComparability of Computer- and Paper-administered Multiple Choice Test for K\u201312 Populations: A synthesis.\u201d Applied Measurement in Education 22.23 (2009): 31-32. Print.\n\nMaguire, Karen A, Daniel Smith, Sara Brallier, and Linda J. Palm. \u201cComputer-based Testing: A Comparison of Computer-based and Paper-and-pencil Assessment.\u201d Academy of Educational Leadership Journal 14.4 (2010): 117-125. Print.\n\nMason, Jean B, Patry Marc, and Berstein J. Daniel. \u201cAn Examination of the Equivalence between Non-Adaptive Computer Based and Traditional Testing.\u201d Journal of Educational Computing Research 24.1 (2001): 29-39. Print.\n\nMeissner, Dennis. A Successful Conversion: The Benefits and Best Practices of Computer-Based Testing . 2007. Web.\n\nNoyes, Jan Michael, and Kate Garlandb. Computer- vs. paper-based tasks: Are they equivalent? Ergonomics 51.9 (2008): 1352\u20131375. Print.\n\nRussell, Michael, Goldberg Amie and O\u2019Connor Kathleen. Computer-Based Testing and Validity: A Look Back and Into the Futur e. 2003. Web.\n",
        "label": "human"
    },
    {
        "input": "Information Technology: Computer Software Essay (Article)\n\nInformation technology has contributed much to the modern appearance of the world. This is because, many people from diverse areas can effectively communicate instantly with the use of advanced information technology. Information technology constitutes various areas such as computer hardware, information systems, programming languages and computer software.\n\nComputer software differs from the hardware components of a computer because it cannot be touched. In a firm which deals with computer hardware there are more people on software programming than those in hardware design. Smaller hardware necessitates the introduction of more software applications that are required to help connect the components of the hardware.\n\nSoftware was initially linked to the hardware by the Original Equipment Manufacturers. This was to enable customers to buy the smallest computer in the market, and have the Original Equipment Manufacturers\u2019 engineers install the required software.\n\nComputer software is a set of computer programs that instructs the computer on what to do and how to do it. It uses various programming languages to come up with these instructions. These instructions are in a form of a code that automates some hardware activities. By using these codes, one is able to perform several operations and reach so many people within a short period of time. These codes are executed when a user inputs something into the computer, and the result is thereof obtained (Hally, 2005).\n\nComputer software has gained wide application from the rapidly changing technology. It has many areas and sectors using it. For instance, in the business sector, it has contributed so much. It has increased business profitability by reducing costs through the use of software in processing data, invoicing and payroll.\n\nAlso it is profitable in that, it has improved distribution through online stores and thus creating new market opportunities. By the use of software spreadsheet, employees are able to concentrate on value added tasks rather than the routine tasks (Blais, 2011). Computer software has hence gained popularity in the business sector.\n\nComputer software has also contributed to effective learning as various people who are not able to travel to certain regions can access lessons that are offered online. Also, students from diverse areas are able to communicate effectively with each other, thus sharing ideas and helping each other in project development.\n\nThis is achieved through configuration of computers in a certain geographical area so as to share resources like databases and programs. Trainers can also offer training programs online and hence reach so many trainees without necessarily moving to where they are. This has greatly reduced costs such transport and accommodation costs.\n\nBusiness men are able to reach so many customers in diverse locations. They can advertise their products and also collect views from customers concerning their products. Some software enables business men to create a forum for the customers to freely express themselves as they can access the businessmen by a single execution of a command (Longley & Shain, 2012).\n\nInformation technology has truly contributed to the advancement of the computer software, which has so many applications in the current generation. It helps in promoting the globalization of the world to be virtually a small area that can be reached easily. Also various programming languages which have been used to automate multiple works have led to simplification of work that could have been difficult to do manually (Webster & Robins1986).\n\nReferences\n\nBlais, S. (2011 ). Business Analysis- Best Practices for Succes s. New Jersey: John Wiley and Sons, Inc.\n\nHally, M. (2005). Electronic brains/Stories from the dawn of the computer age . London: Granta Books.\n\nLongley, D. & Shain, M. (2012). Dictionary of Information Technology 2 ed. New York: Macmillan Press.\n\nWebster, F & Robins, K. (1986 ). Information Technology- A Luddite Analysis (Communication and Information Science). Norwood NJ: Ablex Publishing.\n",
        "label": "human"
    },
    {
        "input": "Project Management and Computer Charting Problem Solution Essay\n\nTable of Contents\n 1. Abstract\n 2. Introduction\n 3. Project management process\n 4. Conclusion\n 5. Reference List\n\nAbstract\n\nSince modern business management requires efficient information management, healthcare providers require enhancing the management of patient information. Thus the concept of computer charting emerges. This paper identifies Charting by Exception, popularly referred to as CBE, as one of the most useful modern computer charting systems.\n\nImplementation of CBE is a complicated process, but can be effective undertaken through a simple project management model. As such, the paper describes CBE project implementation against the 4 D project management model. This model is preferred for its simplicity as well as ability to track project progress.\n\nIntroduction\n\nModern business management requires efficient management of information. Within the healthcare industry, the management of patient information is vital as it determines the success of healthcare provision. The need to enhance the management of information has led to the development of computer charting a concept that incorporates the use of IT based patient information management tools.\n\nComputer charting systems have a broad based primary objective; however, the overarching aim seems to be improving efficiency in documentation of patient information. This is aimed at increasing accuracy in decision making regarding the patient care. As a result, the healthcare industry accrues numerous advantages from computer charting (Keenan, Yakel, Tschannen and Mandeville, n.d.).\n\nThe ever changing trends in healthcare information management require consistent changes to existing information management tools. While current computer charting systems ensure increased efficiency in patient information management, they are also time consuming since healthcare experts, especially nurses, spend a lot of time keying data in computer charting systems. This reduces the amount of time nurses spend with patients (Harrison, 2003; Huff, 2004).\n\nThe need for improved computer charting systems in the healthcare industry is necessitated by the desire to reduce the time spent in documentation. This translates to more time available for nurses to provide patient care. Additionally, other than reducing charting errors and omissions, the need to reduce redundancies in management of patient information necessitates systems change.\n\nThis therefore calls for the transition from the current computer charting to Charting by Exception, CBE (Jaffe, 2011). CBE is an improved version of the traditional computer charting systems already in the market. Compared to other computer charting systems, CBE offers numerous advantages to tertiary healthcare providers.\n\nOther than creating a legally recognized patient data base, CBE eliminates the need for narrative documentation as it only focuses on variances or exceptional patient information. This is attained through recording information that varies from normal charting protocol. CBE uses standard information management tools such as graphs, sheets, among others.\n\nDespite being a rather complex system to implement, CBE is recommended for tertiary healthcare institutions (Jaffe, 2011). Thus, the main objective of CBE is to increase time available for nursing care by reducing documentation time, further enhancing accuracy in decision making for tertiary healthcare providers.\n\nAs mentioned earlier, CBE provides additional advantages in the management of patient information for tertiary healthcare providers as compared to other computer charting systems available. Additionally, CBE is a complicated system. Therefore, its implementation needs a simple project management model.\n\nThere are numerous project management models, but according to Fischer (2005), 4 D models are the most preferable since they are not only simple but also help project managers verify their project plans at both the strategic and tactical level. Additionally, 4 D models enable managers to minimize and optimize risks as well as enabling managers to follow project progress while addressing emerging opportunities for change.\n\nProject management process\n\nThe 4 D model is linear and involves four sequential stages. Most projects fail due to lack of a clear project definition. As such, the 4 D model requires a clear detailed definition for the project at hand. A detailed definition for the project includes clear objectives. Additionally, project managers need to identify the desired outcomes for the projects at the definition stage (Park and Meier, 2007).\n\nAs indicated earlier, the main objective of CBE is to increase time available for nurses to provide care by reducing documentation time, while further enhancing accuracy in decision making for tertiary healthcare providers. In line with this, the desired outcome for CBE is to limit the time used by tertiary healthcare providers during documentation. Reduced documentation time implies that nurses have extra time. Additionally, CBEs main aim is to reduce errors and thus improve decision making.\n\nTo achieve the objective stipulated above requires project managers to set realizable time based goals. As such, since this involves transition to a complicated computer charting system, adequate time is needed to train nurses, manage transition, test and measure its effectiveness and correct changes before delivery. Depending on the size of institution, managers need one to two months to fully implement CBE (Jaffe, 2011).\n\nThe most difficult stage is the designing stage. Designing largely involves planning how the objectives are to be achieved. In this case, a step by step plan seems relevant (Park and Meier, 2007). Appropriate actions which culminate in the implementation of CBE are identified. Such actions mainly focus on testing CBEs viability, evaluation and feedback reporting, as well as incorporating necessary adjustments.\n\nTo enable this, a project management checklist seems relevant. Such a list not only allows the manager to track the project progress but also record events. In the designing stage, a number of assumptions are made. For instance, the project manager assumes that the healthcare institution charting protocols meets State charting requirements. Additionally, the project manager assumes that nurses understand existing charting protocols.\n\nTo avoid any conflict with state authorities, the manager is required to consult attorneys as well as human service representatives within that State. While planning is aligned to the objective set, the project manager needs to also estimate the cost for the entire project. In this case, the cost for training nurses, the cost of the new system, materials needed, contingency plans, the cost incurred while testing and evaluation are factored in.\n\nA project of such magnitude involves a number of risks. It is the role of the manager to identify all the possible management and systems risks. Since CBE offers legally valid patients data, there is the risk of using such data in a lawsuit against the healthcare provider. Additionally, since CBE only records variance or exceptions from the norm, it eliminates the need for narrative data, which poses future liability risks. Moreover, time and money budgeting is done on estimates, thus the risk of cost overruns (Jaffe, 2011).\n\nOf the four stages, the actual implementation takes the longest time. The Doing it stage involves carrying out the activities according to plan (Park and Meier, 2007). This stage involves training the nurses on how to use CBE. After training, nurses gather all the standard CBE forms in a specific location. Using a CBE checklist, nurses read each of the patient notations according to the institutions charting protocol and mark against the checklist where patient data matches the existing charting protocol.\n\nAny variances noted are recorded in narrative form and entered into the system. A resident physician is notified (Jaffe, 2011). Within the process, nurses also record the progress and challenges. Such records provide valuable feedback to the project manager. With the information provided from the feedback report, the manager leads the project management team in making a thorough review on the success of the project.\n\nThe purpose of conducting a thorough review is to identify opportunities for improvement. Adjustments are made to the systems design. This provides the manager with a system fully designed to serve the unique needs of the particular healthcare institution (Park and Meier, 2007). The system is thus ready for delivery.\n\nThe delivery stage is the final phase of the project. This stage mainly involves measuring the actual outcomes against the initial objectives. The usefulness of the products is evaluated against the previous products.\n\nThus, the manager tests whether CBE enables nurses to reduce documentation time, and whether errors and omissions are reduced. The success of the products is determined by evaluating the extent at which CBE improves decision making regarding patients care. Additionally, managers evaluates whether the time saved is spent providing patient care (Park and Meier, 2007; Jaffe, 2011).\n\nConclusion\n\nDespite the fact that computer charting is not a new concept, new trends in information management require healthcare providers to constantly evaluate the usefulness of existing information management tools. Since traditional methods of computer charting fail to address the concept of efficient time management, CBE seems relevant especially for tertiary healthcare providers.\n\nImplementation of CBE is a complicated project, but the 4 D project management model significantly simplifies the process of implementation. Despite the numerous risks associated with CBE, the system accrues numerous benefits to tertiary healthcare providers. Most significantly, the system increases time available for nurses to provide care to patients, minimize errors thus enhancing decision making.\n\nReference List\n\nFischer, M. (2005). 4D Modeling: Applications and benefits . Web.\n\nHarrison, B. (2003). Becoming familiar with computerized charting. Web.\n\nHuff, C. (2004). Off the Chart? Web.\n\nJaffe, S. (2011) . Nursing practice & skill . Web.\n\nKeenan, G., Yakel, E., Tschannen, D. and Mandeville, M. Chapter 49. Documentation and the nurse care planning process. Web.\n\nPark, B. and Meier, R. (2007). Reality-based construction project management: constraint-based 4D simulation environment. Journal of Industrial Technology 23(1)\n",
        "label": "human"
    },
    {
        "input": "The Causes and Effect of the Computer Revolution Cause and Effect Essay\n\nThe innovative technologies dominate in the modern world. More and more people become occupied by the computers and the Internet. It is obvious that the effect of the intrusion of the computers in the everyday life is great.\n\nComparing and contrasting the life of people before and after the spread of the computer technologies, it becomes obvious that more and more people become occupied by the computers and the opportunities they offer and cannot imagine their lives without them.\n\nComputer revolution has a lot of specific advantages which have made life of simple people easier and the professional implementation more effective, however, the computer technologies have brought some particular disadvantages which effect the life of simple people.\n\nTherefore, the main idea 9of this paper is to discuss the positive and negative causes and effect of the computer revolution in the modern world. The computer revolution has brought the Internet to the modern world. Many people cannot imagine their lives without the Internet, the mobile phones, Wi-Fi and other priorities. These and many other aspects are going to be discussed in this paper with the stress on the effect of the development of different things.\n\nStarting the discussion with the positive effect of the issue, it should be stated that the implementation of the computer technologies in the modern world has lead to the fact that most of the processes became automatic. This has relieved many people from performing dangerous tasks. Most of the manufacturing processes are automated and the computer and other innovative technologies are used to monitor the processes and their correct implementation (Card and DiNardo 750).\n\nTherefore, the use of computers at the manufacturing helped people get rid of the dangerous work and become more professionally useful in other spheres. This is one of the most positive effects as the manufacturing of many products became easier and less dangerous, however, there are still professions which include much hazard and the computer revolution has brought nothing particular for them.\n\nThe next positive effect of the computer revolution is the information availability. The appearance of the Internet has created the supportive environment for development of the particular content and spaces for people all over the world.\n\nNow, to get to know something interesting or the latest news people should not go either to the library or wait for the night news. It is possible just to search for the appropriate information online. Moreover, the information delivery has become easier and people are able to find the news and data interesting for them online.\n\nIt has increased the speed of the information sharing and people are able to draw conclusions faster, use the knowledge they get more effective. Therefore, the world has become faster and this is one of the effects of the computer revolution. The information delivery along with the manufacturing processes which are able to be completed faster. This may create an impression that human life is also going faster and rush.\n\nIt has already been mentioned that the information access is one of the main effects of the computer revolution and the Internet development. However, this very aspect may be considered as the negative if to look at the situation from another point off view.\n\nThe easy and fast access to the information creates more possibilities for people, but also makes those consider more and more information which is of no use. People become so overburden with data that they are unable to distinguish important data from useless information. It creates many difficulties as human brain is unable to analyze the information people get every day. that is why many people are depressed, exhausted, and stresses every day.\n\nThis is the negative effect of the computer revolution and we are unable to judge the final effect of it as living in the condition of the constant brain activity, trying to know and remember as much as possible people forget about the effect on their health and unpredicted outcome in the future.\n\nThe notion of informational security has appeared with the computer revolution. Data has become even more important than the human life. This is a great contrast, as being too available, much data in conserved from the unauthorized access. The military, political and economical data along with medical and other spheres are too valued and many people are ready to pay any money to get what they need.\n\nThe hackers and other people who are able to get the data risk their lives to achieve their aims. Thus, the priorities have shifted and no one knows what results will be. The computer revolution helped the medicine to make a great step ahead. The use of innovative technologies helps scientists in the whole world develop and test new medicine that helps people become healthier. Therefore, it may be concluded that the development of the science as a whole is created with the help of the opportunities prepared by the computer revolution.\n\nSocial spaces, charts and other information sharing content along with online computer games has made many people refuse from reality and live in the unreal online world. This has become a real problem for many people as being able to communicate with others, to entertain and to play online, many teenagers forget about the beauties of life and stay in front of computers all day long. The harm from such time spending cannot be even imagined.\n\nFirst of all, people harm their health as sitting the whole day restricts them in movement and makes their eyes strengthen. Thus, the overweight and blindness are the main possible health problems people may have. Additionally, getting used to talk with others online many young people do not have desire to talk to people in the real world. It affects the sociability the future structure of the society.\n\nThe reduction of the number of marriages and the lowering of the birthrate is one of the main outcomes of the spread of the computer technologies. The cyberspace has captured many people, it has become like a drug and people (especially youth) are unable to get rid of it.\n\nIn conclusion, it should be stated that the consideration of the causes and the effects of the computer revolution helps to say that there are both positive and negative effects. Depending on the sphere and the implementation of the computer technologies, the effect may be different.\n\nGenerally, the computer revolution has brought more positive effect to the whole mankind, still, some aspects make it possible to judge positively about the issue. The automation of the manufacturing, the scientific discoveries and the possibility to conduct better research different spheres are the great effects which are to be appreciated.\n\nWorks Cited\n\nCard, David, and John E. DiNardo. \u201cSkill-Biased Technological Change And Rising Wage Inequality: Some Problems And Puzzles.\u201d Journal of Labor Economics 20.4 (2002): 733-783.\n",
        "label": "human"
    },
    {
        "input": "Computer Networks and Security Report\n\nIntroduction\n\nComputer security is a popular term used in modern information age. The term is used persistently to address issues about safety of information systems and networks. As the use of computer networks gains much recognition in the society; there is an urgent need to address the security issues of information stored on such systems.\n\nComputers without any security policies or measures can make a network susceptible to threats, attacks or may even disable network activities within minutes. Similarly, it may take much effort and time to recover a compromised resource.\n\nInternet has provided more opportunities for individuals, organizations and governments. However, its simplicity of access has made it insecure for privacy, retrieval and distribution of information. Several protocols used to guarantee security on the internet have proved ineffective; hence tools to eavesdrop or \u201csniff\u201d passwords on the network are often exploited by malicious hackers. Applications which broadcast unencrypted passwords across the network are equally vulnerable.\n\nClients and server applications depend on client and server software\u2019s to be \u201ctruthful\u201d on issues of user identity using their applications. Other applications depend largely on the client to limit its operations to those connected with what it is obliged to do, with limited or otherwise no enforcement by the server.\n\nTo address the above mentioned issues, Kerberos was viewed as a solution. Kerberos is a unique network security mechanism which uses cryptography. Cryptography mechanism allows a client to verify its identity to the server and vice versa over the unprotected network connection.\n\nKerberos\n\nKerberos is a modern security mechanism which is used to safeguard communications occurring over insecure network. It achieves this through proving the authenticity of nodes communicating to one another. In other words, it is a computer network authentication protocol that works using the basic of \u201ctickets\u201d to use computers communicating over unprotected networks to verify their uniqueness to one another in a protected mode.\n\nKaufman et al (1995) offers another definition of Kerberos. He asserts that Kerberos may designate a set of free software that was published by the Massachusetts Institute of Technology that implemented the Kerberos protocol. The primary objective of designing Kerberos was safeguarding client-server by providing a mutual authentication facility. That is, both the server and the node identify each other\u2019s while communicating.\n\nKerberos messages are secure against replay attacks and eavesdropping. The technology is based on symmetric key cryptography and demands a trusted third-party. Correspondingly, this security mechanism may employ a public key cryptography by using asymmetric key cryptography during the process of authentication (Kaufman et al, 1995).\n\nKerberos uses the symmetric Needham-Schroeder protocol. It also utilizes the trusted party known as the Key Distribution Center (KDC). The KDC contains two logically distinct parts, the Ticket Granting Server (TGS), and the Authentication Server (AS). The KDC keeps a database of private keys and element on the network regardless of whether it is a node or a server.\n\nThe node and the server share the private or secret key known to the KDC and to the nodes and servers. The authenticities of this key act as proof of an element\u2019s identity. According to Neuman and Ts\u2019o (1994), for communications reasons, the KDC releases a session key which the parties communicating employ to encrypt the contents they are broadcasting. The security of the Kerberos protocol depends entirely on temporal assertions of validity referred to as Kerberos Tickets (Neuman and Ts\u2019o, 1994).\n\nElements of Kerberos Protocol Tools\n\nKerberos uses different types of elements of network security to accomplish or manage authentication between the services and users. These elements are important. Without them, the objective of safeguarding network resources would be fruitless. Some of the most important elements are described below.\n\nClient and Servers Tickets\n\nClient and servers are the primary elements of Kerberos. According to Neuman et al (2005), the client designates a system or a person that wants to access the network or web enabled services. The server on the other hand uses the Kerberos authentication service to ascertain clients have valid access to the program or an application they wish.\n\nTickets are useful in the Kerberos authentication service (Neuman and Ts\u2019o, 1994). They contain information that is essential for identifying the client to the server. Kerberos encrypt the information contained in the ticket to ensure the illegal access is not allowed.\n\nPort\n\nA characteristic that Kerberos tools have in all Kerberos system administration is the port. Kerberos have by default port 88.This suggests that when communication tools are developed; the passwords, log on IDs and user IDs have to communicate through this port.\n\nIf accurate combination of passwords and user IDs is verified by the authentication server, then access or log- in is granted through this port. Network administration can therefore give access to files, database, programs and emails, among other services when they link the encryption keys with this port.\n\nEncryption\n\nEncryption is a typical element of all Kerberos enabled tools. Encryption illustrates anything inputted by a keyboard or read on a file and translated into a coded format. The encrypted message makes it unreadable, unrecognized or unutilized by any program or person that is not permitted to do so.\n\nA more significant feature of encryption is that no password is stored in an encrypted format. Each encryption algorithm uses a precise key length that can alter an imputed unencrypted password that is entered in an encrypted one to grant strengthened protection.\n\nActive Directory Trusts and Domains\n\nThey are Kerberos tools available for windows server. It is a graphical interface feature that Microsoft Corporation has made it available for network administrators. It assists network administrators in viewing all trusted and domain relationships so they can enforce protection levels within these domains. The tools can be employed for all trusted and related activities such as varying and viewing trusts.\n\nActive Directory Computers and Users\n\nThey are Microsoft management console tools which contain the administration tool pack. The administration tool pack uses the Kerberos protocol. The characteristic of Active Directory Computers and Users is that they include the ability of controlling, configuring and publishing information from the Active Directory. All administrative roles in domain controllers that need authentication are encrypted.\n\nHow Kerberos Authentication Works\n\nKerberos succeeds its security operations in various ways. Each activity follows one another in succession. The process of authenticating begins when a client computer makes a formal request to access or use the server. The client requests the authentication service to generate a ticket containing client credentials and sends the information to the server together with session information (Kaufman et al, 1995).\n\nThe client and Kerberos use the client information such as the password to validate the information. The Kerberos then broadcasts an encrypted ticket to the server with a password known to the server and Kerberos. Strebe (2002) notes the server validates the information to verify the source and the identity of the user.\n\nIt can also use the timestamp of the ticket to establish the validity of the request. The lifetime of a ticket lasts a few minutes; this is to ensure the process occurs in the shortest time possible for security purposes (Bellovin and Merrit, 1990). In understanding how Kerberos achieves these roles, the writer will list and describes each packet that is involved between client and application server and clients and the KDC during the process of authentication.\n\nStage 1\n\nAuthentication Server Request, AS_REQ\n\nAccording to Neuman et al (2005) this phase is known as the initial authentication request. The client requests the KDC for a Ticket Granting Ticket. The request, in the form of a message is unencrypted.\n\nStage 2\n\nAuthentication Server Reply, AS_REP\n\nWhen a preceding request arrives, the AS checks whether the reply contains TGT, which is encrypted with the TGS private key, and the session key encrypted with the secret key from the requesting user (Neuman et al, 2005). If one of the two keys fail to exist, an error message is posted to the client, otherwise, the AS process the reply. The processing is carried out using the following format.\n\n  * It randomly fixes a session key which acts as a secret key and shares it with the client and the TGS\n  * It also establishes the TGT inserting the requesting user\u2019s principal, which is the service principal.\n  * Lastly, it creates and dispatches the reply that contains the ticket produced earlier and encrypted using the secret key for the service. The timestamp, lifetime, service principal and session keys are encrypted using the secret key labeled for the user invoking the service.\n\nStage 3\n\nTicket Granting Server Reply, TGS_REP\n\nAt this stage, the user who is authentic, and wants to access the service in context but does not have a valid ticket, sends a request as the Ticket Granting Server replies to the Ticket Granting Service (Neuman et al, 2005). The Ticket Granting Service model is as explained below;\n\n  * It fixes an authenticator with the client machine, user principal, and timestamp and encrypts with the session key common with the TGS.\n  * It also establishes a request packet that contains; the Ticket Granting Ticket, which is already encrypted with the key of the Ticket Granting Service; the service principal which the ticket is needed and lifetime unencrypted; and the authenticator.\n\nWhen the previous requests arrive, the TGS\u2019 initial step is to verify the principal of the requested service is present in the KDC database. If present, it opens the TGT by using the key and extracts the session key. It uses the key to decrypt the authenticator. To achieve service ticket, the following criterion has to be met.\n\n  * The TGT should not have expired.\n  * The principal contained in the authenticator should match the one available in the TGT.\n  * The authenticator should be available in the replay cache and should not have been expired.\n\nStage 4\n\nApplication Request, AP_REQ\n\nWhen the client has credentials to access the service, that is the session key and the ticket, it can request the application server for a right of entry to network resources through the Application Request message. It should be noted that unlike other messages where the KDC is involved, the Application Request is not standard. It varies based on the application in context.\n\nStage 5\n\nPre-Authentication\n\nAs represented in Authentication Server Reply, before distributing a ticket, the KDC verifies the principal of the asking service provider and others are available in the database (Strebe, 2002). If the request is from an illegal user, TGT cannot be invoked because of the password, thus it is difficult to release a session key for creating a legitimate authenticator.\n\nKerberos Operation (Author, 2011)\n\nProtecting Application Data\n\nAs explained earlier, Kerberos provides the services of authentication only; an assurance the authenticated element is a dynamic partaker in an exchange. According to Jablon (1996) a derivative of Kerberos authentication protocol is the swap of the session key between the client and the server (Tanenbaum, 2003).\n\nThe session key may consequently be used by a program or an application to provide privacy and integrity of communication. The Kerberos describes two types of messages, private and safe messages to summarize data that need to be protected (Strebe, 2002). However, the application is at liberty to use a suitable method to a particular data being relayed.\n\nSuccess of Kerberos\n\nKerberos has achieved notable success in network security issues. This is in contrast with other similar security tools providing security services.\n\nPassword protection\n\nKerberos does not allow a user\u2019s password to be sent across the network either under encryption or in plain text. Rather, the protocol depends on keys that are transmitted through encryption. Hence the keys cannot be intercepted (Jablon, 1996). For example, when the security of the network is tampered, Kerberos makes it likely for trespassers to construe the content of network transmissions. Similarly, target services and user authentications remain safe.\n\nClient/Server Authentication\n\nKerberos allows the client and server to authenticate to each other. Communication can break or fail if they are not able to authenticate.\n\nClient/Server Ticket Certification\n\nKerberos timestamps the tickets that pass from the server to client and from client to server, it also encompasses lifetime information besides common authentication. Hence the period of authentication is restricted (Bellovin and Merrit, 1990). The time spent by the user during implementation may be altered by design but the limit is less to ensure that replay attacks are not contained. By limiting time, the strategy makes communication secure.\n\nReusability and Durability\n\nKerberos protocol authentication is reusable and durable. When a user authenticates using the Kerberos protocol, the authentication can be reused during the lifetime of the ticket. This is to imply that it is feasible to remain valid through the protocol without re-entering a username and password over the network.\n\nSession Key Generation\n\nThe Kerberos protocol uses a dual key encryption method. The product of the service session key provides a unique link between the client and the service that is safe. This unique \u201csecret\u201d link is used as the encryption key during client/server communication (Bellovin and Merrit, 1990). This adds security to Kerberos communication.\n\nInternet Standard\n\nThe Kerberos protocol depends on the open internet platform. It is not restricted to authentication mechanisms or proprietary codes. This enables developers to depend on open and free reference implementation of a public method. Similarly, inexpensive commercial implementations can be developed separately or bought.\n\nUbiquity\n\nKerberos is widely embraced and trusted by security experts, developers and cryptologists. Hence, new breaches are likely to be noted and resolved instantaneously. This is in contrast with proprietary systems. Kerberos has substantial investments hence its security mechanism is difficult to overcome. Besides, the strength of Kerberos is anchored on numbers.\n\nFailures of Kerberos Protocol\n\nThough Kerberos cancels or guards against severe and common security threats, it may be challenging to implement. This is because of various reasons:\n\nKnowledge in Implementation\n\nAccording to Bellovin and Merrit (1990) Kerberos protocol was developed to be used for a single user client system. In cases where a client acts as a multi user system, the Kerberos authentication mechanism may be compromised to various replay attacks and ticket stealing.\n\nThe general protection of a multi user client system such as memory protection, file system protection among others maybe a restricting reason of Kerberos authentication (Bellovin and Merrit, 1990). No knowledge or cleverness in implementing a Kerberos authentication can substitute system administration practices Kerberos server and client systems.\n\nSupport of Proprietary Software\n\nKerberos utilizes mutual authentication model. It is important for client and servers to be deliberated with Kerberos authentication. Some proprietary applications also support Kerberos. Besides, various legacy systems were deliberated and maintained programs. However, they were not designed with third party authentication tools in mind (Kohl et al, 1994). This means that they need to be rewritten to embrace Kerberos authentication.\n\nServer Attacks\n\nThe Kerberos authentication method is susceptible to severe attacks against ticket granting service and the initial ticketing service, KDC. The whole authentication mechanism depends on the \u201cfaithfulness\u201d of the KDC (Kohl et al, 1994). Hence any person who can compromise authenticating users can compromise the authentication of users of the system depending on the KDC. Thus, this needs efficient system administration practices while managing the Kerberos KDC.\n\nAll Nothing Strategy\n\nKerberos can be an all nothing strategy when utilized in a network. An encrypted password communicated over the network to a non-Kerberos enabled aware the service is in danger. Hence, the networks accrue no benefit from using Kerberos. To make a network secure with Kerberos, a person must use Kerberos enabled version on all clients and server applications that communicates the passwords unencrypted.\n\nConclusion\n\nThe Kerberos protocol provides the needed security to protect communications occurring over a network. Without its strength, computer networks would be at risk against threats and attacks.\n\nPrecisely, without the facts of the identity of an element or individual identity requesting an operation, it is challenging to make a decision whether an operation should be denied or granted. Kerberos achieves network security by using shared key cryptography. This makes both the client and server access the same password or key to positively recognize the user.\n\nPerhaps, this strength of the Kerberos helps to manage passwords for many users by granting services that assist in coordinating passwords between the services and the users they desire to use. The conventional authentication strategies are not a guarantee for use on networks where attackers can easily check network traffic and seize passwords. Kerberos uses strong authentication approach that does not divulge passwords, hence supporting network security.\n\nReferences\n\nBellovin, S., M., & Merritt, M., 1990, Limitations of the Kerberos Authentication system. Computer Communication Review 20(5), pp. 119-132.\n\nJablon, D., 1996, Strong Password-only Authenticated Key Exchange. Computer Communication Review 26(5), pp. 5-26.\n\nKaufman, C., Perlman, R., & Speciner, M., 1995, Network Security: Private Communication in a Public World , Prentice-Hall, New Jersey.\n\nKohl, J., T., Neuman, B., C., & Ts\u2019o, T., 1994, \u201c The Evolution of the Kerberos Authentication System\u201d. Distributed Open Systems . IEEE Computer Society Press. Washington.\n\nNeuman, B., C., & Ts\u2019o, T., 1994. \u201cKerberos: An Authentication Service for Computer Networks\u201d. IEEE Communications, 32(9), pp. 33\u201381.\n\nNeuman, B., C., Yu, T., Hartman, S., & Raeburn, K., 2005, The Kerberos Network Authentication Service (V5) . Available at; https://www.ietf.org/\n\nStrebe, M., 2002, Network Security Jump-Start: Computer and Network Security Basics . John Wiley and Sons, New Jersey.\n\nTanenbaum, A., S., 2003, Computer Networks , Prentice Hall PTR, New Jersey.\n",
        "label": "human"
    },
    {
        "input": "HP: Arguably the Best Computer Brand Today Essay\n\nTable of Contents\n 1. Introduction\n 2. HP is the Leading Computer Brand\n 3. Conclusion\n 4. Works Cited\n\nIntroduction\n\nI have always liked HP computers especially when it comes to dealing with graphics and other its aspects. These machines can deliver different kinds of services and rarely let someone down.\n\nGoing by the fact that HP computers are very common and used by many people, it is prudent to say that many people consider them as highly necessary. This essentially means that they are among the best if not the number one among various computer brands in existence today. The paper is a persuasive essay, which argues that HP computers are far much better than other computer brands.\n\nHP is the Leading Computer Brand\n\nThere are many brands of computers in the market today each claiming to be the best. While there are clearly a number of strong and established brands, there must be a clear leader in the pack. To me, that leader is HP computers. One needs not to look far for evidence of this, HP computers currently take the lion share of computer sales worldwide.\n\nThey are found in offices, institutions and homes all over the globe. This is a manifestation of their superior quality that has enabled consumers to accept them at such a huge scale. Compared to a brand like Sony which has extensive penetration in the electronics business worldwide, HP stands out in terms of personal computers.\n\nIn one of their adverts featuring music star Jay-Z, HP depicts the computer as being personal. That is, it does so many things be it business or social activities. On the other hand, an advert featuring Indian Actress Kareena Kapoor for the laptop VAIO X series depicst the computer as sexy and physically appealing. Comparing the two, HP makes more sense as computers are about performance and not necessarily appearances.\n\nJust to give an example, one particular brand that has earned HP respect is the Pavilion Elite HPE-180t that works best for me. Like other HP computers, it comes with a voluminous storage memory. This gives it an edge in performance of many tasks compared to other computer models.\n\nMemory has been one aspect that has kept HP in the lead as a lot of research has gone into developing HP memory. This means HP computers can hold great amount of data in the safest possible manner and safeguard it from being corrupted. With this knowledge, a HP user can have complete peace of mind and still get the maximum output from their computer (Mishra, Para 1).\n\nApart from computer sales,HP as a company also offers a wide range of support services. The company manufactures many other products that are used together with their computers providing for seamless compatibility. This has the advantage of providing people with a complete answer to their information technology needs. Of course, other companies producing different brands also provide an array of support equipment but none matches the scale at which HP does it.\n\nAnother advantage of HP is that the company gives buyers the choice to buy directly from them and even ship the product free of charge. With this age of imitations, it is easy to get genuine HP computers as a result. While it is true that different brands have more or less the same features, the extra services given to farmers and choice is crucial in determining the best brand.\n\nThe wide range of computer models that HP boasts gives computer users the freedom to choose a model that works best for them and clearly, from the person using the computer for simple personal chores to the professional requiring super machines, HP has the answer.\n\nConclusion\n\nClearly, the computer industry is filled with many strong brands that offer many innovations all the time. While this is commendable, it is apparent that HP has stood out as the greatest computer brand recently.\n\nWorks Cited\n\nMishra, Sachin. Why HP computer memory is one-step ahead . Web.\n",
        "label": "human"
    },
    {
        "input": "Purchasing and Leasing Computer Equipment, Noting the Advantages and Disadvantages of Each Report\n\nSummary\n\nThe research should be carried out by visiting firms that deal with leasing equipment for short and long periods of time. Adequate information should be obtained to ensure that decision makers do not make a wrong decision when choosing the method in which to upgrade their computer equipment.\n\nThe management should make sure that they understand the benefits and weaknesses of every option in order to make a wise decision. First, they should consider how fast the equipment needs to be updated and the cost of purchasing. After considering several factors in the field of computer and their software, I recommend that my organization should consider leasing this equipment. This is because they become technologically out dated very fast, hence, contradicting the essence of purchasing.\n\nIntroduction\n\nThis research was carried out to identify the most appropriate method organizations should consider when there arises a need to upgrade their computer equipment. This means that organizations should consider the most economical way to go in order to save on cost, hence, achieving one of the main goals which is to cut cost and maximize profits.\n\nThe research should be carried out by visiting firms that deal with leasing equipment for short and long periods of time. Adequate information should be obtained to ensure that decision makers do not make a wrong decision when choosing the way in which to upgrade their computer equipment (Nevitt, Fabozzi and Mathew, 2011). In addition, visits to firms that sell new computer equipment should not be ignored.\n\nInvestigations on these issues should be done by people from the organization by conducting interviews from these target firms. This means that the organization should consider sending representatives to enquire about leasing and purchasing from dealers. Furthermore, the organization should consider visiting computer specialist to consult about available options.\n\nThis is extremely important as specialists in the field of computers may give most viable information on leasing and purchasing equipment to upgrade computer networks. All information collected should be analyzed and the most advantageous method be recommended to the decision makers.\n\nDiscussion\n\nComputer equipment needs to be updated very often, hence, raising the cause for thorough consideration of the most viable way of conducting the upgrading process. This means that organizations that have to upgrade their systems often need to consider the cheapest and most economical method in order to cut operational costs (Oz, 2008).\n\nThis means that they should weigh out options and decide between purchasing and leasing to find out which has most advantages. The option with most advantages should be considered and, therefore, adapted by the organization in order to maximize profits and put costs as low as possible.\n\nAdvantages\n\nLeasing computer equipment is economical for the organization because updating computer equipment should be done very often. This is essential for the organization to achieve its organizational and operational goals as advancement in technology goes along with effectiveness in production and other operations. For example, advancement in computer software translates to increase in computer speeds, hence, increase in speed of all operations carried by computer systems within the organization.\n\nLeasing equipment enhances flexibility as organizations may lease the right equipment to handle tasks at hand. This means that equipment can be hired on demand, hence, avoiding instances of organizations having to purchase equipment that are used occasionally.\n\nOrganizations work in a diverse nature and they may require to carry out special tasks demanded by customers (Kendall, 2008). They can lease the equipment to handle those specific tasks and take back once the task is over, hence, saving on the costs that could have been incurred in purchasing new equipment.\n\nLeasing equipment allows organizations to acquire them without incurring huge expenditures when compared to purchasing. In most cases, purchasing equipment may affect organization\u2019s cash flow since down payment may be required.\n\nThis means that when any organization has decided on making a purchase, huge sums of money may be involved, hence, leading the management to incur costs (Gelinas et al, 2004). In leasing, depending with the lease period organizations may save so much money as they have to incur very minimal expenses in acquiring equipment to carry out their operations.\n\nLeasing of equipment is very important because organizations can use lease payments as expenses in order to reduce taxes when filling their tax returns. This means that lease payments should be recorded as organization\u2019s expenses, hence, reduced from the taxable amount.\n\nThis reduces the amount of tax the organization has to pay, hence, translating to reduced operational costs. This advantage is important because an organization that leases all its equipment and records them as organizational expenses end up reducing their taxes by huge sums of cash. This becomes profitable to the organization because at the end of any accounting period, the amount paid to tax collector is very little as compared to what could have been paid if they bought that equipment.\n\nAnother advantage of leasing equipment is that it passes the load of obsolescence to the lessor. This means that since computer equipment gets outdated very fast, the organization will not be faced by hectic time of dealing with the outdated goods (Chandra, 2005). Instead, they should hire out new equipment that matches current technology. In addition, cost associate with disposal of outdated computer equipment is not on the organizational side but on the side of the lessor.\n\nDisadvantages\n\nOn the other hand, leasing can have its disadvantages as well. One disadvantage of leasing equipment is that the overall cost of hiring the equipment is more than the price of a new computer.\n\nThis is because payments made throughout the lease period include a given commission for the lessor. Therefore, purchasing new equipment ends up being cheaper than leasing the equipment for long term. Another disadvantage is that the organization lacks ownership of the leased goods, hence, cannot manipulate or dictate much over the equipment.\n\nIn fact, this becomes hectic when the equipment ceases to be used in the organization before the end of the lease period. The organization cannot dispose it since the owner of the good is the lessor. Finally, leased equipment has to be paid for even if they are not being used by the organization. This means that the organization has to pay for the equipment throughout the lease period regardless of whether they enjoy its services or not.\n\nPurchasing equipment\n\nAdvantages\n\nOne advantage of buying equipment is that the organization acquires ownership once the good has been purchased. This advantage can be enjoyed when dealing with equipment that does not run technologically out of date very fast (Harold, 2010).\n\nOrganizations may also consider tax incentives as an advantage hence considering the option of purchasing equipment. This means that the government may at times wave or reduce taxes for some equipment to encourage people to invest in them. In fact, buying goods can have the organization enjoy tax reductions hence cutting down on operation expenses.\n\nDisadvantages\n\nPurchasing costs are too high, hence, making the entire exercise very expensive for organizations to purchase goods that get technologically out of date very fast. In order for organizations to make such purchases, they might be required to obtain loans from commercial banks which end up becoming difficult (Hosford-Dunn et al, 2008). This is because banks require down payments in order to give loans to be paid on monthly payments.\n\nAnother disadvantage with purchasing these equipment is that they run obsolete very fast, hence, having the owner face a challenge of re investing in new equipment. Equipment that gets technologically faced out tends to have very little resale value, hence, becoming hectic to the organization to dispose the out dated equipment.\n\nConclusions\n\nIt is important for organizations to put into consideration several factors before choosing on the method of upgrading their computer equipment. The management should make sure they understand the benefits and weaknesses of every option in order to make a wise decision.\n\nFirst, they should consider how fast the equipment needs to be updated and the cost of purchasing (Harder, 2004). Obsolescence is another factor to be considered because once equipment becomes obsolete, they become a burden to the organization. After considering all these factors, the decision makers should be able to choose the most appropriate method which align with the organization\u2019s objectives of maximizing profit and minimizing cost.\n\nRecommendations\n\nAfter considering several factors in the field of computer and their software, I recommend that my organization should consider leasing this equipment. This is because they become technologically out dated very fast hence contradicting the essence of purchasing. The element of equipment becoming obsolete also leads to my recommendation because, once these goods cease to be used in the organization, they will end up becoming a bother to the organization.\n\nReferences\n\nChandra, H. (2005). Fundamentals of financial management . New York: Tata McGraw-Hill.\n\nGelinas, U. J., Sutton, S. G., Hunton, J. E. & Hunton, J. (2004 ). Acquiring, developing, and implementing accounting information systems . New Jersey: Thomson/South-Western.\n\nHarder, F. (2004). Fashion for profit: from design concept to apparel manufacturing \u2026 a professional\u2019s complete guide. London: Frances Harder.\n\nHarold, J. R. (2010). An Introduction to Accounting and Managerial Finance: A Merger of Equals. London: World Scientific.\n\nHosford-Dunn, H., Roeser, R. J. & Valente, M. (2008). Audiology practice management. New Zealand: Thieme.\n\nKendall, K. E. & Kendall, J. E. (2008). Systems analysis and design . San Jos: Pearson/Prentice Hall.\n\nNevitt, P. K., Fabozzi, F. J. & Mathew, J. V. (2011). Equipment leasing. Sydney: John Wiley and Sons.\n\nOz, E. (2008). Management Information Systems. Michigan: Cengage Learning.\n",
        "label": "human"
    },
    {
        "input": "Company Analysis: Apple Computer Report\n\nAbstract\n\nThis paper is about the analysis of the Apple Computer Company. The company overview has been given and the SWOT analysis of the company has been carried out. In addition, focus has been put on the application of behavioral sciences and theories in regard to Apple Computer.\n\nCompany Overview\n\nThe main activities that Apple Computer carries out include designing, manufacturing as well as marketing of the personal computers and also the related software. It also provides related services, peripherals as well as the networking solutions. In addition, apple computer engages in the designing, developing and marketing \u201ca line of portable digital players along with related accessories and services including the online distribution of third-party music, audio books, music videos, short films and television shows\u201d (Datamonitor, 2006, p.5).\n\nThe selling of the company\u2019s products is carried out globally and the company does this through the company\u2019s retail stores, \u201cdirect sales force\u201d, online stores, as well as, as pointed out, \u201cthird-party wholesalers and resellers and carries out the management of its businesses basically on a geographic basis\u201d (Datamonitor, 2006, p.5).. The company\u2019s operations are carried out via 5 operating segments. These segments include; \u201cEurope, Japan, Americas, retail and others\u201d (Datamonitor, 2006, p.5).\n\nThe reportable segments which include Japan, Europe and the Americas do not encompass those activities that are linked to the retail segment. Considering the Americas segment, this consists of South as well as North America. More so, the European segment consists of countries in Europe and also those countries in Africa and the Middle East. \u201cEach reportable geographic operating segment provides similar hardware and software products and similar services\u201d (Datamonitor, 2006, p.5).\n\nLooking at the retail segment, this one carries out operations of the retail stores that are under the ownership of the company in the United Kingdom, the United Sates, Japan and Canada. The company\u2019s retail stores that are located in the United States are more than 233in number and there are more than 84 international stores.\n\nIt has also been found out that \u201cthe \u2018others\u2019 operating segment focuses on Asian-Pacific, which includes Australia and Asia except Japan; and the company\u2019s subsidiary, FileMaker\u201d (Datamonitor, 2006, p.5).FileMaker carries out the development, publishing and distribution of \u201cdesktop-based database management application software for Mac Os and Windows-based systems\u201d (Datamonitor, 2006, p.5).\n\nThe company\u2019s headquarters are located in California (in Cupertino). The total number of employees is over fourteen thousand. The company\u2019s revenue that was recorded in the year 2005 (end September) was 13,931 million U.S dollars and this was a 68.3 percent increase as compared to the previous year (2004).\n\nThe increase was attributed to the raise of the iPod sales that went up by 409 percent. The company\u2019s net profit during the fiscal year 2005 was 1335 million dollars and the fiscal year 2004, the net profit was 276 million U.S dollars.\n\nSWOT Analysis\n\nStrengths\n\nOne of the strengths that this company has is the global presence. It is present in the Americas, the Asia Pacific and EMEA. More than half of the income comes from the markets that are set up outside the United States which is its home market. It is reported that \u201ca balanced presence in mature as well as emerging markets has enabled the company to record a steady revenue growth\u201d (Datamonitor, 2006, p. 18). In the course of the last ten years, the company has realized a strong growth.\n\nThe margins of Apple Computer in the period between 2001 and 2005 were greater than the industry average and this were boosted by strong growth in all its markets (Datamonitor, 2006). It has been established that \u201cthe company\u2019s operating performance makes its market position strong and it raises the level of investor confidence as well\u201d Datamonitor, 2006, p.18).\n\nThe other strength of this company is the brand image. The company\u2019s brand image is well regarded among a large number of consumers. It controls the brand image it has in order to ensure differentiating its product offerings and boosts sales.\n\nThe strong brand of Apple Computer \u201cenables it to set up entry barriers in the market and this also makes it possible for the company to command a premium pricing for its products such as iMac and iPod, giving it a competitive edge over regional as well as other global competitors such as IBM and Sony\u201d (Datamonitor, 2006, p.19).\n\nThe third strength of Apple Computer is synergistic portfolio. This portfolio is \u201ca portfolio that is highly synergic in nature\u201d (Datamonitor, 2006, p.19). The broad portfolio brings down the level of the company\u2019s reliance on any single product line and following this, it brings down the level of the company\u2019s business risks. Moreover, it as well boosts the company\u2019s \u201ccross-selling opportunities\u201d (Datamonitor, 2006, p.19).\n\nThe fourth strength of the company is the strong media content. Some of the company\u2019s products such as the iPod and iTunes turned out to be a big success story of the company. For instance, as it is pointed out, in 2006 during the month of February, \u201ciTunes downloads topped one billion songs\u201d (Datamonitor, 2006, p.19).\n\nThe \u201ciTunes Music Store\u201d makes it possible for the users to buy, in a legal manner, and download music as well as videos online. \u201cThe iTunes Store features over 3500 music videos, Pixar and Disney short films, a variety of hit TV shows, 35,000 podcasts, 16,000 audiobooks and more than two million songs from the major music companies and independent record labels\u201d (Datamonitor, 2006, p.19).\n\nWeaknesses\n\nOne of the company\u2019s weaknesses are the low returns. It realized weak returns on assets as well as on investment in course of that few years. For instance, it is reported that in the course of between the year 2001 and 2005, the company recorded average RoA of about 4.7 percent which represents a figure that is lower than the industry average (5.8 percent. In the course of the same period the average RoI stood at 6.7 percent which also below the industry average (13.8 Percent).\n\nA similar trend was followed by the \u201creturn on Equity\u201d (RoE) during the same period and it stood at 7.4 percent and this was below the industry average (18.5 percent). Having a weak RoA as well as a weak RoE can wash away the confidence of the investors and can paralyze the growth plans that the company has put in place.\n\nThe second weakness is the dependency for key components .The company is prone to a supply danger for the key components. In a market in which speed to market is crucial, the Apple Computer Company\u2019s dependency for key components could expose the company to a competitive disadvantage.\n\nOpportunities\n\nThe possibility that there will be an increase in demand for \u201cconnectivity and networking\u201d products in the future offers an opportunity. The company is inclined to capitalize on this opportunity for the reason that it has already been giving out several offerings in the same area. An example is given of the AirPort which is the Apple Computer\u2019s \u201cwireless technology network\u201d which enables the users to set up a computer network and carry out connection to the internet without using cables, networking or extra phone lines (Datamonitor, 2006).\n\nIt is pointed out that, \u201cbased on the IEEE 802.11b wireless standard and Wi-Fi certified, AirPort allows high-speed wireless communications within a radius of approximately 150 feet from an AirPort base station\u201d (Datamonitor, 2006, p.20). Raising the level of \u201cwireless hi-speed internet\u201d taking on would serve to augment the top line development of the corporation.\n\nThe other opportunity is offered by the new digital platform. The Apple Computer Company is putting focus on the transformation of the company\u2019s Mac platform in to a \u201cdigital hub that combines iTunes video content to customers\u2019 television screens and portable devices\u201d( (Datamonitor, 2006, p.20). This stage would boost the company\u2019s \u201cdigital content\u201d business that is on the increase at a higher speed. owth in the long run.\n\nThe third opportunity results from the MP3 player market. In the United States, the MP3 market share of the company went up to 78 percent in the month of December, the year 2006 from 71 percent at the end of the year 2005 (December). Yet, the market share of this company in the international markets stays below the level of market share in the United States. This implies that the penetration of the company outside the United Sates has a great potential for growth.\n\nThreats\n\nOne of the threats that the company faces is the presence of strong competition in the industry in which it carries out its operations. This industry goes on to be characterized by \u201cfast advancement in technology in software and hardware development, which has remarkably raised the level of applications and capabilities of the products, and have resulted in the frequent introduction of new products and significant price, feature, and performance competition\u201d (Datamonitor, 2006, p.21).\n\nIn the course of a number of years that have passed, there has been stiff competition in the industry on the basis of price. The competitors of the Apple Company have been cutting down prices to increase their market share. Unless appropriate measures are taken by the company, this may go on affecting the company negatively in the future.\n\nThe third weakness follows the slow Eurozone economy. Such a slow down took place in the year 2005 and this went hand in hand with increased oil prices as well as higher levels of lack of employment. Europe is among the main markets of the Apple Company and if the slowdown goes on in the Eurozone, it could bring down the level of spending on the It products and negatively affect the company\u2019s revenues.\n\nThe fourth threat is linked to the lawsuits. The company is subject to particular legal proceedings. It is reported that \u201cApple faces several lawsuits related to patent infringement, false advertising, and unfair business practices\u201d (Datamonitor, 2006, p.21). Such issues can have a material on the company\u2019s financial situation on top of the company\u2019s brand image.\n\nApple\u2019s approach towards employee behavior\n\nThe company makes use of personal control as it has expectations of its employees to carry out their activities in an efficient manner, not hiding any information, and sharing experiences. Apple thus targets at influencing as well as shaping the employee behavior through promoting its objectives and values in the daily face to face interaction and also through the great eagerness and motivation skills of the company\u2019s management which is supposed to influence all the employees.\n\nIn addition, the company is also \u201cengaged in terms of behavior control, as the firm aims at generally directing the actions and behavior of its employees through certain rules like the code of business ethics\u201d (Hill & Jones, 2004, p.410). However, it is supposed to be pointed out that this form of strategic control, in general terms, plays a small role and this is for the reason that the Apple Company has preference for innovation and creativity rather than for standardized procedures.\n\nMotivation in Apple Company\n\nFor this company to capitalize on the positive effects that are brought in by employee motivation, it has set up a number of employee benefits such as \u201cemployee stock option grants, employee stock purchase plans, and an employee savings plan\u201d (Hill & Jones, 2004, p.395).\n\nThese financial incentives are in line with the motivation theory that was presented by Taylor in which he stated that the employees are motivated by pay. However, it came to e established that workers are not only motivated by the financial incentives because they are only forced to carry out boring repetitive tasks and they are not offered a chance to be creative to make their work interesting. Apple Company has come to realize this.\n\nFollowing this, in addition to the financial incentives that are offered by the company, the company also has taken an initiative to set up an incentive for its employees to remain in the company by offering them an opportunity for them to be creative and innovative and this enables them to convert the technological ideas they have in to a reality (Hill & Jones, 2004).\n\nThis is in line with the Herzberg\u2019s theory. Herzberg believed that organizations are supposed to motivate their workers by taking up a democratic approach to management by raising the level of the nature as well as the content of the actual job through particular methods and one of these methods is empowerment which implies giving more powers to the employees in order for them to come up with their own decisions in their working life fields.\n\nDimensions of the organizational structure\n\nApple\u2019s management uses a functional structure because the company is organized along \u201cfunctional lines\u201d. Such a structure categorizes people basing on the common experience or expertise they have as well as resources.\n\nIt makes it possible for the functions of the company to learn from each other and to turn out to be productive and also specialized. In addition, such a structure makes it possible to have effective monitoring as well as efficient activities and as a result, this brings down the level of costs and raises flexibility in the operations (Hill & Jones, 2004, p422).\n\nIt is pointed out that, \u201cby decentralizing authority and responsibility and through a relatively flat hierarchical structure, Apple encourages its low-level managers and employees to take the initiative and foster the company\u2019s strengths\u201d (BusinessWeek, 2004, p.1). In addition, by decentralizing, this boosts the organization\u2019s planning as well as decision making processes and this is because of the availability of better information (Morden, 1993, p.228).\n\nPolitical and legal forces\n\nThe political and legal forces are products of the changes that are carried out in regard to the law and regulations. The environment within which the Apple Company carries out its operations can be manipulated by the legal decisions as well the political judgments (Dickel, 1994).\n\nSuch bodies as CARP or RIAA can have a great impact through imposition of new laws that limit the \u201cdigital music\u201d industry in its attempts to expand. One of the negative influences was felt by the Apple Company in the year 2002 when the U.S government became very active in the regulation of the industry through the means of the \u201cMusic online competition act\u201d so that it could \u2018tweak\u2019 several aspects of the \u201cU.S Copyright Act\u201d with updates.\n\nApple ethical position in the market place\n\nIn considering Apple\u2019s ethical position in the marketplace, the company has been criticized by several parties of engaging in business practices that are not ethical. This can be verified in the observation that \u201cApple faces several lawsuits related to patent infringement, false advertising, and unfair business practices\u201d (Datamonitor, 2006, p.21).\n\nCultural behavior within Apple Computer\n\nWithin the Apple Company, \u2018internationalization, adapting to local tastes and preferences, being able to deal with the different infrastructure and altered traditional practices becomes more decisive abroad to acquire customers\u201d (Iliev, Lindinger & Poettler, 2004, p.149).\n\nEven if this company does not vary its product and the marketing message from one country to the other, it has strategies that were developed early enough in order to offer a response to pressures in \u201clocal acceptance by adapting\u201d (Iliev, Lindinger & Poettler, 2004, p.149). By Apple setting up intercultural communications skills when they are carrying out business activities assisted them to avoid problems that are associated with internationalization.\n\nChange and organizational development\n\nOver time, the Apple Company has been known to be innovative. While technology advances, the company has been able to capitalize on this and developed new products in order for them to meet the customer needs in the global context. It has been putting in place the appropriate organizational structure in order to realize this goal.\n\nIncorporation of team building, motivation, leadership and accommodation for personality types in Apple\u2019s infrastructure\n\nThe values as well as the norms that are incorporated in the company\u2019s corporate culture offers support to the company\u2019s organizational structure by promoting, initiative, motivation, leadership, innovation and creativity all through the company which can be taken as being central elements of the company\u2019s strategy.\n\nIn general terms, \u201corganizational structure, control, and culture shape the way people behave, their values and attitudes, and determine the way they will implement an organization\u2019s business model and strategies\u201d (Hill, 2004, p.405).\n\nTherefore, through the steady interaction of the structure of Apple as well as its control and culture and also through the coordination and motivation that follows on of the employees, it is made possible for the company to carry out the implantation of its policies effectively at different strategy levels.\n\nReferences\n\nBusinessWeek, (2004, February 2). Steve Jobs \u2013 It feels good. BusinessWeek , p.1.\n\nDatamonitor, (2006). Apple Computer, Inc. Retrieved from http://www.alacra.com/acm/2009_sample.pdf\n\nDickel, K. E. et al. (1994). Strategic management \u2013 a methodological approach . New York: Addison-Wesley Publishing.\n\nHill, C. W. & Jones, G. R. (2004). Strategic management \u2013 An integrated approach . Boston and New York: Houghton Mifflin Company.\n\nIliev, V., Lindinger, A & Poettler, G. (2004). Apple Computer Inc. strategic audit. Web.\n\nMorden, T. (1993). Business strategy and planning \u2013 Text and cases . London: McGraw-Hill.\n",
        "label": "human"
    },
    {
        "input": "Computer Technician and Labor Market Essay\n\nTable of Contents\n 1. The profession\n 2. Source of demand\n 3. Supply\n 4. Government control and regulations\n 5. Works Cited\n\nIn capitalistic nation like the United States of America, the labor market is controlled by forces of demand and supply. When demand for a certain profession is high, then salaries and wages are expected to be high; on the other side when the demands of a certain profession is low, then wages and demand of the profession will be lower.\n\nWhen a profession is paying higher than the equilibrium wage rate, then the profession is considered to be getting higher return than the expected ones (Goodwin-White, 1119). This paper discusses computer technician profession as one career that earns high returns than the equilibrium wage rate.\n\nThe profession\n\nOne profession that is earning higher than the equilibrium wage rate is computer technician job; the main work of the technicians is to repairs and maintains computers hardware, software, computer accessories, and servers.\n\nOther than the tasks started above, at high level, technicians are involved in the works of configuring new hardware, maintaining system networks, installing and updating software packages; there are different ranks and levels of the career from certificate level to doctorate level. Wages and salaries from the profession depend with ones level of education.\n\nSource of demand\n\nThe main source of demand for computer technicians comes from corporate and individual customers who have adopted current technologies in computers. There is an increased use of computer for various activities; the computers, both hardware and software need to be maintained and managed; this is the role of the technicians.\n\nThe demand and increased use of computers means that there will be an increased demand for technicians and their service; other than the increase in demand, there is a fast upgrading of computer systems calling for people to need the demand of technicians. To tap the increasing market, technicians use different methods; they include formal employment, freelance computer technicians and businesses.\n\nWhen employed, their wages is relatively high compared to wage of other people in different professions. On the other hand, when they are operating personal businesses and as freelancers, they get an increased demand for their services, thus increasing their returns (Goodwin-White, 1119).\n\nSupply\n\nThe demand of technicians is expected to be fulfilled by young people graduating from colleges and universities. However, the arte at which students are taking computer science as their course is not as high as the demand for the course. On the other hand, it is considered as one of the courses that require high college entry marks thus it limits the number of students that engage and take the course this further reduces the supply of professionals.\n\nAnother thing that has affected the supply of the labor is increased movement of trained profession to other nations where they go to get better chances in life; the outflow makes the United State market suffer a deficit (Sobel and Stroup 23).\n\nGovernment control and regulations\n\nAlthough the United States regulates wages and salaries in the economy, it does not set the highest limit that someone should get as his or her salary. On the other hand, there is no central body of technicians that can control the prices they should charge for their services. The lack of control leaves demand and supply to be the only determinant of price in the market; the capitalistic nature of the market works for the good of the profession.\n\nWorks Cited\n\nGoodwin-White, John. \u201cEmerging Contexts of Second-Generation Labour Markets in the United States\u201d. Journal of Ethnic & Migration Studies 35.7 (2009): 1105-112. Print.\n\nSobel, Macpherson, and Stroup Richard. Economics: Private and Public Choice . New Jersey: Wisely,2008. Print.\n",
        "label": "human"
    },
    {
        "input": "Effects of Computer Programming and Technology on Human Behavior Essay\n\nTable of Contents\n 1. Technology and Communication\n 2. Technology and Information/Education\n 3. Technology and lifestyle\n 4. Works Cited\n\nThe continued use of computers in our everyday life is beginning to alter how we as humans behave. For instance, \u201cmultitasking, output and efficiency\u201d (Ullman 2) concepts that ideally just work for machines are slowly defining \u201chuman thought and imagination\u201d (Ullman 2).\n\nComputers have a way of actively engaging an individual or seeking their attention when they are about their businesses by either popping up messages on the desktop about \u201cunused icons on your desktop\u201d (Ullman 1) or aid you in writing a document with software like \u201cClippit\u201d (Ullman 1).\n\nThe concept of multitasking \u201cintroduced in 1960\u2019s, was an engineering strategy for making computers more efficient\u201d (Ullman 1) and it achieved this by switching \u201cits attention to some other task\u201d (Ullman 1) while waiting for the next human input when being used.\n\nIt is only natural for humans to want to adapt this type of efficiency after \u201cyears of working in an environment where efficiency is a god and idleness in any component is intolerable\u201d (Ullman 2) by keeping themselves \u201cas busy as possible\u201d and focusing on different things at the same time (Ullman 2).\n\nFor instance, we can \u201cdrive, eat, talk on the cell phone\u201d simultaneously in an attempt to be efficient; \u201cthe ability to multitask, to switch rapidly among many competing focuses of attention, has become a hallmark of a successful citizen of the 21 st century\u201d (Ullman 1). With its continuous advancements, modern technology will continue having an impact especially in the areas of communication, information and lifestyle.\n\nTechnology and Communication\n\nCommunication has never been faster or even more instant with the day to day use of both phones and computers. Phones transitioned from the basic feature phones people used to own for the sole purpose of calling and texting, to smart phones that have amazing capabilities and have adapted the concepts of computers. In his article, Mobile Telephone History, Tim Farley exhaustively discussed the development of these devices.\n\nThese devices have brought unique changes (Farley 1). Firstly, mobile phones, whether the smart phones or otherwise, provide an avenue for communication among people of different geographic locations, making distance no obstacle. Software like Skype, enable voice or video calls over the internet and this links friends and families in different continents whether using phones or computers. Secondly, communication has also become instant since one is able to get immediate feedback without delays.\n\nEmail services have facilitated efficient communication between employers and employees, students and lecturers, in comparison to letter writing which as compared to previous times would take much longer. Thirdly, all these devices have reduced face to face/personal interaction since communication is hugely done using these avenues. This is definitely a negative effect in as much as communication has been enhanced by modern technology.\n\nTechnology and Information/Education\n\nModern technology has in effect enhanced our accessibility to information, whether it is about general world news or completing our research projects for school work. In terms of education: students are able to have online discussion forums, do extensive researches and currently there are even online courses which are available for those who cannot attend classes (this is a milestone since conventionally, classrooms were the only forums of learning).\n\nThomas highlighted some negativity to this form of education. He claimed that although \u201conline discussion forums can be effective in developing student\u2019s knowledge\u2026they do not allow for social construction between students\u201d (Thomas 1).\n\nAnother commendable result of modern technology is its ability to make the globe small, i.e. when it comes to television; people in various places can all be streaming live news coverage in a totally different location from the viewers. This means that information is widespread and covers a larger span in the shortest time possible.\n\nTechnology has also enabled news and other events watched on television to be streamed live using computers connected to the internet. Other than education and information, in recent times there have been social networks like facebook, tweeter, among others which the young generation has particularly embraced as a form of disbursing information. Much of the rapid spread of news, information and even learning material owe it to technology for providing easy instant access to such.\n\nTechnology and lifestyle\n\nOur lifestyles have been transformed by advancing technology. Life can definitely be said to have become easier in more ways than one. First change that is undeniably notable is in the house environment.\n\nPeople own microwaves, dishwashers, washing machines, driers, vacuum cleaners etc, all in an attempt to ease the work they need to do. Secondly, the transport industry cannot be said to have been left behind in this technology. Buses, planes trains and trams, all characterize how we are networking the different geographic locations in order that efficient mobility is achieved.\n\nLastly, the work environment is also not spared for there are different machines and devices that have been adapted to improve productivity and enhance efficiency. Robots are also gaining popularity in such forums since they can do the extra work humans allocate them. With all this in mind, as much as our lifestyle is improving by reducing the amount of work we do, caution need to be taken that individuals do not resort to laziness with the excuse that machines will do the work for them.\n\nWorks Cited\n\nFarley, Tom. Mobile Telephone History . Cems, 2005. Web.\n\nThomas, Matthew. The Impacts of technology on communication- mapping the limits of online discussion forums. Impact of Technology, 2000. Web.\n\nUllman, Ellen. \u201c The Boss in the Machine .\u201d New York Times, February 19, 2005. Web.\n",
        "label": "human"
    },
    {
        "input": "The Computer Microchip Industry Problem Solution Essay\n\nThe microchip industry is very competitive probably because it is very dynamic. You can rarely find chips that were used, say like five years ago. This means that the microchip companies work round the clock to come up with even much better chips than the previous ones. However, the companies have to deal with culprits who are very good at cloning their newly developed chips.\n\nThe makers of clones have mastered the art of imitation such that the consumers cannot draw a line between a clone chip and the original chip. This paper will focus on the strategies that Smart Chip Company can employ to beat these criminals in their game.\n\nSmart Chip Company has been in the microchip industry for long and has been able to build its reputation in the market. The efforts of this company are only helping few individuals who tend to reap from where they did not sow. In the case of smart chip company it would cost them a lot of money and time if they decided to trace the imitators. The best approach is to outperform such competitor (Robbins & Coutler, 2008).\n\nThe Porter\u2019s Value Chain Management proposes a differentiation strategy in a case like the microchip company. In this kind of competitive strategy, the smart chip company must select one unique attribute and capitalize on it. This is because the clone makers do not know the concepts that are involved during the manufacturing processes. King (2005) states that most people would expect the cost of this company\u2019s products to go up.\n\nThis is considered as one of the ideal approaches of adding value into a product. For instance, the company can decide to focus on durability and by doing so, the company will have a competitive advantage in the market. However, before the company decides to utilize a given attribute, it must first evaluate the needs of the consumers with the purpose of establishing the most desired attribute.\n\nFor differentiation to be effective, the smart chip company must identify ways of cutting back on the costs of production. This suggests that the costs incurred during differentiation should fall within the set limits of production costs (Kotler, 2003). The company should extend the reduction in production costs to all departments.\n\nFor instance, the cost of labor can be trimmed by automating most of the tasks so that there are fewer employees. Similarly, the company can negotiate for reduced prices of raw materials such that the company spends less on acquisition of raw materials.\n\nThe smart chip company can also get rid of competitors by eliminating the distributors. In this approach, the company will be interacting with the consumers directly and will earn more returns that were initially reserved for the distributors. Alternatively, Mitchell and Coles (2003) agree that the smart chip company can purchase or acquire the clonners. This move will eliminate unfair competition because the competitors will be forced to adhere to the policies of the acquiring company.\n\nThe company can also decide to explore other markets that are yet to be exploited. This is because customers tend to be loyal towards brands that have been in the market for long or in other words, pioneers. By the time the clone makers realize what the Smart Chip company is up to it would be too late because the company would have acquired a bigger client base. Alternatively, the company may opt to increase its authorized dealers in the market. This is one of the strategies employed to bring the product closer to the consumers.\n\nMorover, for Smart Chip company to remain competitive, it must encourage its employees to be innovative by allowing them to do things differently while performing their tasks. Most clone makers are people who have worked for reputable companies and left when they felt they were not being appreciated for their efforts.\n\nThe Smart Chip Company must therefore focus on employee retention and succession plans. The company can also have a competitive advantage by issuing product guarantees. This will enhance the firm\u2019s image in the market because customers will feel assured while making the purchase. This is because counterfeits rarely come with guarantees because their manufacturers are not certain about their quality.\n\nAdditionally, the Smart Chip Company can conduct awareness campaigns to sensitize the public on the risks of buying counterfeit goods. This is because most customers are not aware of the risks they are getting into. Once the public is sensitized on how to identify the brand made by this company, the competitors would have lost the battle.\n\nBesides, the company can issue licenses to other related but registered companies to allow them to make chips. This way the clone makers will have been locked out of the market. This is because every company that wishes to use the technology developed by the Smart Chip would have obtain a license (Mullins, 2005).\n\nThe managers of the Smart Chip Company must remain vigilant at all times. This will enable them to notice when new opportunities approach. The company must also foresee threats and address them before they get out of hand. Once a clone maker has been identified, the company should monitor the operations of the competitor to understand their trading patterns.\n\nIn general, adding value to the product is the best approach because this is what attracts customers. Most customers do not care about the price of an item as long as it meets their needs. Therefore, differentiation should be employed with care so that the returns are not jeopardized. If Smart Chip Company is able to deal with customers, suppliers, new entrants to its market, and substitute products and services, the firm can be able to realize competitive advantage.\n\nReferences\n\nKing, J.B. (2005). The Top 10 Reasons Businesses Succeed. Retrieved from https://ezinearticles.com/?The-Top-10-Reasons-Businesses-Succeed&id=12514\n\nKotler, P. (2003). Marketing Management . Millennium Edition. New York: Prentice Hall.\n\nMitchell, D. & Coles, C. (2003). Ultimate Competitive Advantage: Secrets of Continually Developing a More Profitable Business Model . Francisco, CA: Berrett-Koehler Publishers.\n\nMullins, L. (2005). Management and Organisational Behavior (7th ed.). London: Pitman Publishing.\n\nRobbins, S.P. & Coutler, M. (2008). Management (10th ed.). New York: Prentice Hall.\n",
        "label": "human"
    },
    {
        "input": "Computers & Preschool Children: Why they are required in Early Childhood Centers Research Paper\n\nToday, more than ever before, technology is increasingly documented as a fundamental learning tool for advancing the social, emotional, intellectual and linguistic development of young children. Indeed, the question being asked by educators and parents in the 21 st century is no longer about whether and to what scope information and communication technologies (ICTs) should be utilized with young children in early childhood centers, but rather how it should be employed to achieve maximum benefits (Couse & Chen, 2010).\n\nEducators readily acknowledge the fact that keeping up with the ongoing convergence of technology for the classroom presents unique challenges to them as they come to terms with the ever growing potential of ICTs to augment the capacity of young children to learn, solve problems, and express their thoughts. It is against this background that this essay will seek to demonstrate why computers are a necessity in early childhood learning.\n\nA wide body of literature demonstrates that preschool children are not only empowered by the use of computers, but the utilization of the new media in learning environments is central to enhancing their creativity, development of ideas, collaborative play and enculturation into the knowledge society and economy (Stephen & Plowman, 2003).\n\nThis view is reinforced by Long-Breipohl (n.d.), who postulates that computers are great motivators for learning, in part, due to the fact that they encourage interaction among young children. Empowerment, enculturation into the knowledge society and promotion of interaction, in my view, goes a long way towards providing an enabling environment where young children can be able to benefit intellectually, especially in the development and conveyance of ideas.\n\nComputers can be purposively used by preschool children to record their creative work. This affords them the opportunity to interact with the devices \u2013 may they be desktops, laptops, tablets or iPods \u2013 to better understand at an early age how they operate for future use.\n\nLong-Breipohl (n.d.) is of the opinion that \u201c\u2026the earlier a child is introduced to the use of computers, the better will he be prepared for coping with the demands of workplaces later in life and the greater is the likelihood of a successful future\u201d (p. 1). In this perspective, the use of computers for learning in preschool age is positively correlated to a brighter future thereafter.\n\nComputers, according to Breipohl (n.d.), can be used to improve a child\u2019s academic achievement. Indeed, computers and other related technologies are playing a critical role in accessing information, and it is a well documented fact that any form of learning is highly dependent on information.\n\nIn consequence, it can be argued that any form of media that will afford a child the access of relevant educational content will inarguably go a long way to improve his academic achievement. Computers, when used in collaboration with the internet, link the child to a whole new world of learning possibilities. It is against this backdrop that stakeholders in the education sector have discovered the intrinsic value of computers to the learning process.\n\nGrowth and development is rapid in early childhood \u2013 a phase of individual development witnessed from birth all the way to the age of 8 years. Accordingly, researchers and educators have highlighted the significance \u201c\u2026of the children\u2019s active use of technology in making decisions, technology resources in writing and drawing, and logical thinking programs to solve problems and illustrate ideas\u201d (Couse & Chen, 2010, p. 76).\n\nThese are fundamental concepts that must be conveyed to the child at an early age, and computers and other forms of technologies offer the needed platform for the needed impartment to take place. It should be remembered that any form of technology that enhances children\u2019s thinking capacities also affords them prospects for active control and problem solving while granting educators a much needed window into the children\u2019s social and intellectual development (Stephen & Plowman, 2003).\n\nInquiry-based learning forms a critical component of the education process, and the internet is being used by many educators around the world to assist young children to develop their inquiry techniques by showing them how to find content that builds on their learning interests (Hertzog & Klein, 2005). According to Wang et al (2010), \u201c\u2026children naturally explore and learn about their environments through inquiry, and computer technologies offer an accessible vehicle for extending the domain and range of this inquiry\u201d (p. 381).\n\nIn addition to this observation, online inquiry-based learning has the potential to broaden children\u2019s intellectual capacities in ways that traditional techniques of learning may not be able to match due to the nature and scope of information found on the internet, and also due to accessibility of diverse range of information. This observation is advantageous to the very objective of learning for knowledge accumulation.\n\nAway from issues of inquiry-based learning, Hertzog & Klein (2005) asserts that \u201c\u2026computers can serve as catalysts for social interaction and emotional growth\u201d (p.25).\n\nSocial and emotional development forms a critical area for young children by virtue of the fact that they serve as basis for future development and growth, including the capacity to socialize with others, the development of self-esteem and identity, and, by extension, development of their cognitive abilities. Indeed, Hertzog & Klein (2005) opines that the use of computers by preschoolers facilitate both social and intellectual relations \u2013 each to the advantage of the other.\n\nIn addition, the authors suggests that that good software facilitates children in early childhood centers to talk more about their collaborative and creative work, not mentioning the fact that they are afforded the framework to engage in more sophisticated cognitive types of play than traditional techniques can allow. Long-Breipohl (n.d.) is of the opinion that the educational software that is presently available can, to a large extent, be depended upon to assist young children in the learning process.\n\nAnother advantage of using computers in early childhood centers arises from the fact that technology, when employed effectively, generates an active interaction between the child and the content. Indeed, computers should be introduced at an early age because the complexity of the mentioned interactions increases with continuous usage and the level of the child\u2019s talent.\n\nIn line with the above observation, researchers and educators have noted that children who employ technology from an early age for creative and educational purposes engage in much more meaningful learning than those who basically utilizes instruction-based approach (Hertzog & Klein, 2005).\n\nWhat\u2019s more, computers have been found to facilitate the expression and progression of originality in the learning process. Such an attribute, in my view, must be used to benefit young children in the development of their creative and innovative capacities.\n\nLastly, computers should be used in early childhood centers by virtue of the fact that they have the capacity to incorporate instructional techniques in early childhood inquiry education to, among other things, \u201c\u2026enrich and provide structure for problem contexts, facilitate resource utilization, and support cognitive and meta-cognitive processes\u201d (Wang et al., 2010, p.381).\n\nInquiry, according to the authors, has long being advanced as the best basis for children learning, especially in critical subjects such as science, mathematics and the progression of language and literacy.\n\nChildren impulsively inquire, posing questions and investigating, to better understand the world. In this respect, computers, by their very own interactive nature, should be used to nurture and cultivate this attribute because it is beneficial to an individual\u2019s life-long development (Wang et al., 2010).\n\nTo conclude, this essay has laid bare all the advantages that computers bring to the social, emotional, educational and cognitive life of preschoolers. In this day and age, computers must never be viewed as peripherals in educational and developmental needs of young children; rather, they must be used as essential tools that serve to enhance the learning experience of these preschoolers.\n\nOf course computers have their own disadvantages, especially in exposing the minors to undesirable content (Stephen & Plowman, 2003). But when used effectively, the benefits computers offer to preschool children far outweigh their social costs.\n\nReference List\n\nCouse, L., & Chen, D.W. (2010). A tablet computer for young children? Exploring its viability for early childhood education. Journal of Research on Technology in Education, 43 (1), 75-98. Retrieved from MasterFILE Premier Database\n\nHertzog, N., & Klein, M. (2005). Beyond gaming a technology explosion in early childhood classrooms. Gifted Child Today, 28 (3), 24-65. Retrieved from MasterFILE Premier Database\n\nLong-Breipohl, R. (n.d.). Computers in early childhood education: A jumpstart for the future . Web.\n\nStephen, C., & Plowman, L. (2003). Information and communication technologies in pre-school settings: A review of related literature. Journal of Early Years Education, 11 (3), 223-234. Retrieved from Academic Search Premier Database.\n\nWang, F., Kinzie, M., McGuire, P., & Pan, E. (2010). Applying technology to inquiry-based learning in early childhood education. Early Childhood Education Journal, 37 (5), 381-389. Retrieved from Academic Search Premier Database\n",
        "label": "human"
    },
    {
        "input": "Impact on Operations Resources of JIT at Dell Computer Exploratory Essay\n\nIntroduction\n\nJust- in- time (JIT) as a form of inventory processing focus on reducing futile time, costs, and improving the quality of a product during the manufacturing process. The sequence of activities during this process includes; delivery, gathering of orders and shipment to the final consumer.JIT idea was first developed by Henry Ford. But later, it was adopted by Toyoda family. The JIT approach is essential for companies as it minimizes storage costs by eliminating stock levels and aid in utilizing warehouse space.\n\nThe JITphilosophy \u201cinventory is waste and minimizing of inventory releases tied up capital\u201d, ensures a company manufactures what is needed and at the right time hence saving on production costs. JIT inventory system stresses on the amount of time required to produce the correct order; at the right place and the right time.JIT system helps in reducing lead-time (Jaffe et al, 2007). Lead-time is the time taken for a manufactured product to reach the final consumer.\n\nJIT reduces product cycle hastening faster delivery to intendedconsumer. Besides, JIT system minimizes inventory redundancy and obsolescence. This is achieved by JIT facilitating direct disposal of the finished product to the customer. Further, JIT technique enhances product quality(Jaffe et al, 2007). This process is achieved through a total lean performance and quality circles which ensures a product passes the quality standard required.\n\nJIT System at Dell\n\nDell is one of the pioneer companies which have embraced JIT approach in its manufacturing and production activities. The Value added activities such as; a direct model is a business strategy has been possible by JIT. Dell uses the technique in customizing its production process, inventory management besides being a direct relationship with its customers and emphasized by Dell computer(Jaffe et al, 2007).\n\nDell has encompassed the use of JIT in computer hardware and software to provide consumers with hi-tech services, updated technology, hi-tech products, customized computer systems and affordable products. JIT is a fundamental method used by Dell. It helps the company to reduce wastes by manufacturing only what is needed, delivering at the right place and in the right time.\n\nSummary\n\nCompanies which have instituted JIT system in their production and inventory management systems have gained significant benefits which the system brings. Such benefits are direct thus helping the company save cost thus increasing profit margin. Besides, efficiency and reliability is created in the company.\n\nMain Section\n\nDell and Its Products\n\nDell is a worldwide computer company founded in 1984 by Michael S. Dell. It has strategic distributions outlets across the world. Dell is one of the leading companies embracing business strategies, innovation, and research in their business line of operation(Jaffe et al, 2007). Dell deals with a wide range of computer hardware which includes digital cameras, home theaters, servers, desktop computers, laptops, printers, monitors and CPUs. Besides, they provide customers with tailor-made Dell PC varying in prices depending on the model requested by the customer(Jaffe et al, 2007).\n\nOutsourcing services\n\nDell has outsourced its computer support services to other countries such as India.Besides support services, Dell has also outsourced manufacture of computers to various countries such as; Brazil, South Africa among other countries to reduce the costs thus boosting the profit margin. Also, Dell has formed a strong relationship with its competitors such as; HP (Hewlett & Packard) and Acer in outsourcing computer parts.\n\nCustomization\n\nWith the ever fluctuating customer\u2019s need, dell has found it necessary to customize its products to cope with varying markettrends. Dell has continuously embraced customization to create customer loyalty by brand dependability. Some of the products dell has customized includes; laptops and desktop(Song & Zhao, 2009).\n\nDell has come up with different laptop models to counter the fluctuating customer wants. For instance, dell is currently offering new generation laptops which are customized to accomplish specific consumer requirement. This strategy has helped Dell bind the consumers more tightly to their products.\n\nDell has also customized its services such as on site engineering services and online system support which facilitates efficient and faster product usage and availability. Dell has also customized its supply chain operations and information technology tools to ensure that deliveries are convenient which facilitates JIT. Customization has helped Dell eliminate obsolete thus concentrating in satisfying customers\u2019 needs(Jaffe et al, 2007).\n\nDell\u2019s JIT inventory control system and how it affects Customers\n\nJIT revealsdiverse challenges in the management and help to provide solutions for improvement. This has helped Dell to significantly reduce its inventories. This has been important to Dell (Dooley et al, 2009).\n\nThe operations of JIT at Dell is unique, this is in the sense that, the company need not to have custody of raw materials to service an order until the order is confirmed by the customer thusaccomplishing in servicing the order in the shortest time possible. JIT approach has reduced the bulky of inventory management thus eliminating storage costs and obsolete product hence ensuring timely supply to the customer(Dooley et al, 2009).\n\nDell Suppliers\n\nDell has continually strengthened its supplier base by formalizing the supply of materials to ensure that they are environmental friendly to enhance quality of a manufactured product. The suppliers have to ensure that the materials supplied are in compliant with Dell Restricted Material policy(Dooley et al, 2009). Implementing suppliers policy ensures that Dell maintain less direct inventory in its manufacturing plants.\n\nDell Assets\n\nDell has many assets, though most have been spread across the world. Specifically, Dell has a huge chain of computer products ranging from laptops, desktops, home theaters among others (Song& Zhao, 2009). JIT has encompassed order processing facilities, supplier requirements, customization and supply chain. JIT has simplified work roles thus creating specialization. However, Dell has to come up with good strategies to ensure that when more orders emanate from customers, they are able to service without much delay. This will enhance quality and improve customer\u2019s trust(Song & Zhao, 2009).\n\nAdvantage of JIT to Dell\n\nThe advantages of JIT as related to Dell Computer business model includesreducing lead time. This means that the time taken to fulfill a customer\u2019s order has been greatly reduced and facilitated smooth flow of inventories. Also, JIT system has resulted in a major reduction in manpower by eliminating the warehousing phase which leads to a further reduction in cost and manufacturing of goods only when demand arises(Song & Zhao, 2009).\n\nIn addition, JIT in dell computer has facilitated direct market strategies which facilitate the use of the internet and real- time business transactions which have aid to improve supplier relationships. Finally, the use of JIT technique has freed up tied capital by eliminating redundant and obsolete inventories\n\nDisadvantage of JIT as Related to Dell\u2019s Business Model\n\nA JIT system has resulted ininvariance\u2019s in regard to the flow of inventories. This means that at various instances, the supply chain has not been able to meet the demand expected. Also, JIT requires an extensive amount of cash to implement and regain control thus this has been a problem to Dell.\n\nReferences List\n\nDooley, J. K., Yan, T., Mohan, S., Gopalakrishna, M., (2009). Inventory Management and Bullwhip Effect during the 2007 -2009 Recession, Evidence from Manufacturing Sector. Journal of Supply Chain Management. Volume 46 (1) pp. 12-18 DOI: 10.1111/j.1745-493X.2009.03183.x\n\nJaffe, D., Muirhead,T., Tey,T.,&Avutu,R., (2007). Dell Entreprise Technology Centre Series\n\nSong, J.,& Zhao, Y., (2009). The value of Component in a Dynamic Inventory System.With Lead Times . Manufacturing & Service Operations Management. Vol. 11, (3), pp. 493- 508 DOI: 10.1287/msom.1080.0235\n",
        "label": "human"
    },
    {
        "input": "Responsibilities of Computer Professionals to Understanding and Protecting the Privacy Rights Essay\n\nPrivacy ranks as among the very important factors that many computer users consider when making the decisions about how to approach the concept of information technology.\n\nIn the present day, there are many who have built lucrative businesses out of collecting data about computer users and some organizations even finance their activities mainly through selling marketing data or even selling lists with the names and details of potential customers. From an interpersonal perspective, some individuals even utilize internet-based services to spy on personal backgrounds of other individuals.\n\nA common example of this is the use of search engines by employers to look into information about employees and potential employees over the internet. Criminals also use the internet to search for personal details which they then use for identity theft. It is because of such security concerns that computer professionals have special responsibilities in relation to understanding, respecting and protecting the privacy rights of other computer users and this essay shall explore these.\n\nGiven that internet use is an international phenomenon, it is impossible for computer professionals and policy makers to control all the content of the data that is transferred online.\n\nBe that as it may, computer professionals still have some responsibility of protecting the rights of fellow citizens one of which is to come up with mechanisms to protect copyright and intellectual property rights. The main aim of copyright is not only to prevent illegal copying of information, including the information stored in digital format which can be copied, altered and transmitted through electronic networks with relative ease (Lopez, 1998).\n\nThe practical challenges that owners of digital data face are very important for governments trying to apply or extend existing copyright laws to digital means. It is the responsibility of computer professionals to push the authorities to enforce laws that protect intellectual properties. At an international level, it is the responsibility of computer professionals to contribute to the efforts of bodies such as the World intellectual property organization (WIPO) towards the facilitation of the protection of property rights.\n\nAs noted by Welfens (2002), the internet has greatly contributed to expanding individuals\u2019 learning horizons but the ease with which information has become accessible is raising concerns among citizens in relation to the exposure to offensive material. Parents and educators have also raised concerns about the negative influence that the internet is having on young minds.\n\nAs a way of addressing these concerns, computer professionals have contributed to the formation of civil liberty organizations such as the Electronic Frontier Foundation (EFF) and the Computer Professionals for Social Responsibility (CPSR) which supports legal and legislative towards the protection of user\u2019s civil liberties and protects the privacy and civil liberties of online users, respectively (Dhillon, 2003).\n\nComputer professionals can also help computer users protect their privacy while using the internet by recommending a few measures that they (users) could apply while using their computers. One such measure would be to disable cache and history functions in individual computers whenever the can. This will prevent a malicious individual from using this information track the sites that the user have visited.\n\nThe computer professionals could also advise users not to release their personal identification information to anyone else as this information could be used against them. The number of computer users is increasing by the day but not all users have the knowledge of how to protect their privacy while using the internet. It is therefore the responsibility of computer professionals to take all the necessary steps that would help preserve the privacy of computer users, some of which have been mentioned in this essay.\n\nReferences\n\nDhillon, G. (2003). Social Responsibility in the Information Age: Issues and Controversies . Hershey, PA: Idea Group Inc.\n\nLopez, X.R. (1998). The dissemination of spatial data: a North American-European comparative study on the impact of government information policy . Greenwich, CT: Ablex Publishing Corporation.\n\nWelfens, P.J.J. (2002). Internet economics dot net . New York, NY: Springer.\n",
        "label": "human"
    },
    {
        "input": "Computer Survey Analysis: Preferences of the People Essay\n\nIntroduction\n\nThis paper reports about the survey that was conducted about computer ownership. The survey was conducted with the aid of the research questions which included; what type of computer was owned by the respondent and the brand of the primary computer. The main aim of choosing these topics was to find out the preferences of the people when it comes to computer ownership.\n\nThe survey involved responses from 26 various individuals and upon completion of the survey, each individual had responded to the research question. Therefore, the survey was effective since it received 100% responses.\n\nThe research questions were clear and focused since they had direct forward answers and also they did not present a complex situation for the respondents. However, if the survey was to be carried out again I would include options for the research questions including the major brands only.\n\nAlso I would increase the number of the respondents in order to get consistent information. Consequently the scope of this survey was inadequate since it had low number of respondents. I would, as well, add some more research questions in order to make the survey comprehensive. Some of the questions that I would likely to add include; what is the buying price of your primary computer? Is your computer suitable for the work that you are doing? Etc.\n\nResearch questions and analysis\n\nThe first research question was about the type of computer that the respondent owned. This question was carefully worded since each computer has its own type. This question was effective since it prompted the respondents to choose from the available options that include; desktop, laptop and tablet.\n\nIt was also worthy to mention that this question was related to the research question since it helped to establish the type of computer that is widely used. Furthermore the question was specific and narrow which made it easier to obtain the responses. The question was not complex therefore the respondents had an easier time of answering. There was an adequate range of responses since all the types of the computers were included in the survey question.\n\nThe second question was about the brand of the primary computer. This question was not correctly worded. This is because the respondents were faced with difficulty in answering this question because they might interpret it in various ways. However the question is clearly related to the research questions since it prompted the respondents to establish the brand of their computers.\n\nThis research question was also general therefore a divergent of responses was obtained from the survey. As a result of this, there was adequate range of possible responses according to the understanding of the respondents.\n\nThe first research question prompted the respondents to establish the type of their computer. When the survey was conducted it was discovered that most people prefer the use of a laptop other than desktop and tablet types of computer. For instance, the survey revealed that 84.6% of the 26 respondents possess the laptop computers which are 22out of 26 people.\n\nThe rest 4respodents possess the desktop computer which represents 15.4% while there was no respondent possessing tablet computer. These results indicate that most people prefer the laptop computers compared to the desktop and tablet computers. Finally it is revealed that most people do not have the tablet computer.\n\nThe second question of the survey forced the respondents to establish the brand of their computers. There were ranges of responses since the people possessed different brands of computers. For instance the survey revealed that many people possess the HP brand of the computer which had 24.0% representing 6 respondents.\n\nThe second brand that is possessed by many people includes apple and other brand with each one having 16.0% representing 4 respondents. Sony brand then follows closely with 12.0% which represents 3respondents. Dell and brands that were built by oneself followed next with each having 8.0% representing 2 respondents. Toshiba and Compaq brands each had 4.0% representing 1 respondent. Lastly there are brands which are not possessed and included Acer, Alienware, emachine and Gateway.\n\nConclusion\n\nIn conclusion, the survey reveals that most people prefer the laptop computers, and the HP brand. The research questions assisted in the survey result however the research questions were not enough to make a comprehensive survey.\n",
        "label": "human"
    },
    {
        "input": "Are We Too Dependent on Computers? Argumentative Essay\n\nAlthough computers were invented less than a century ago, they have revolutionized the way in which we carry out our day to day activities. In our modern day living, many aspects of our lives are in some ways influenced by computers and computing systems are nowadays commonplace in most areas of our lives including shops, most homes, schools and various work places. This highlights the enthusiasm with which people have embraced this technology.\n\nOwing to the wide application area of computers, there arises the question as to whether we as human beings are too dependent on computers. This paper shall argue that while the widespread usage of computers is undisputable, human beings are not too dependent on computers. To reinforce this assertion, this paper shall consider the various arguments put forward in support of the view that computers are not overused.\n\nWhile computers are used for activities such as online shopping and communication, this is only a matter of convenience and it does not point to overdependence on computers by human beings. Shelly et al. states that computers have over the years proved to be efficient means through which we can get things done (27).\n\nThe author highlights the various benefits that the world has accrued as a result of computerized technology. For example, the internet acts as a platform from which people can easily communicate with each other through social networking websites such as \u201cfacebook\u201d and \u201ctwitter\u201d.\n\nOnline shopping also presents people with a means through which they can perform their shopping activities from the safety of their homes. However, all this is a matter of convenience and even without the usage of computers, we could still have these activities taking place although with significantly more effort.\n\nWhile it is stated that computers have afforded people a chance to obtain education through \u201conline education\u201d, this is not a substitute to the traditional educational system that remains favored by all countries all over the world. Despite assurances by most institutes that the quality of education provided through the internet and computers is the same as that provided in a traditional classroom, most people still opt for the traditional forum and those who take up online education mostly do so as a last resort.\n\nHowever, online education used in conjunction with classrooms can greatly enhance the learning experience of the student (Palloff and Pratt 23). This demonstrates that in the education field, computers only serve as a supplement to the traditional methods of teachings. Claims of overreliance of computers in education are therefore refutable.\n\nHowever, there are areas such as in the business world where there has been an evident overdependence on computers. Most of this has been brought about by the desire of corporations to minimize their production costs and increase their efficiency so as to obtain a competitive edge over other organization.\n\nThis high dependency on computers has led to high unemployment rates especially in developed countries where businesses are opting for capital intensive methods of production than human labor. If this overdependence continues, it will invariable result in increased poverty and as a result, crime, chaos and insecurity will heighten.\n\nThis paper set out to argue that human beings are not too depended on computers. To support this claim, this paper has highlighted instances where computers are used only as a supplement to other traditional means.\n\nThe paper has also pointed out situations where over reliance on computers has been seen to exist. In such cases, it has been seen that the effect may be adverse if not properly controlled. From the arguments put forward in this paper, it is clear that while computers are beneficial to us, we should take care to not over rely on.\n\nWorks Cited\n\nPalloff, Rena, and Pratt, Keith. \u201cBuilding Online Learning Communities: Effective Strategies for the Virtual Classroom.\u201d John Wiley and Sons, 2007.\n\nShelly, Gary, et al. \u201cDiscovering Computers 2009 Complete\u201d. USA: Cengage Learning, 2008. Print\n",
        "label": "human"
    },
    {
        "input": "Computer Communication Network in Medical Schools Essay\n\nTable of Contents\n 1. Literature Review\n 2. Analysis of these Literature Reviews\n 3. Conclusion\n 4. Reference List\n\nComputer usage in medical schools is becoming increasingly inevitable as time goes by. The enormous and drastic technological revolution that started in late 20 th Century and persisted to contemporary times has affected all learning institutions largely, medical schools included.\n\nMajority of researchers agree that computers play major role in medical schools and in future, computer communication network in medical schools might be the centre of interest if the current requirements by medical school curriculum is anything to go by. Most medical schools have made it compulsory for any reporting student to have a computer and this point the place of computer communication network in medical schools now and in the future.\n\nNevertheless, while most researchers agree on numerous issues concerning this area, there remain few disagreements concerning particular elements of the whole process. This paper reviews different literature materials in this area, citing the controversial elements, weaknesses, strengths, and areas subject to improvement in computer communication network in medical schools.\n\nLiterature Review\n\nAs aforementioned, there are numerous peer-reviewed studies concerning this area and researchers agree on several issues. According to Ostbye (2006), \u201ccomputers and computer networks are becoming increasingly recognized as important facilitators of communication among researchers and educators in medicine schools\u201d (p. 43).\n\nThis study reveals that use computer communication network in medicine schools is increasingly gaining popularity in a bid to keep pace with the increasing changing trends in contemporary world which has become a global village.\n\nThis research revealed that, computer networks are used to access directories and important information like journals contained in Medline, Pub Med and other scholarly sites in the interment. Moreover, through computer communication network, students now transfer computer files or data to lecturers thus making learning easy and interesting.\n\nOstbye (2006) notes, because of the increasing need to use computer communication network in medicine schools, education stakeholders are organizing more computer conferences to create awareness across the world. In conclusion, Ostbye agrees that, the place of computer communication network in medicine schools is inevitable noting that most medicine schools have fully embraced the use of this technology in their curriculum.\n\nIn another research on the same issue, the findings are similar to what Ostbye observed. \u201cIntroduction of the new technology has brought significant changes in education especially in medical education which undergone profound changes due to recent technological advancements\u201d (Bulu, 2009). This confirms earlier claims that the recent technological advancements have infiltrated most education systems including medicine schools.\n\nThis research particularly cited the huge investments in information and communication technologies (ICT) in medicals schools especially in developed countries. This investment is meant to ameliorate education standards in medicine schools. Most stakeholders agree that ICT is a powerful tool in addressing most education challenges in medicine schools. The role of ICT in medicine schools is so significant that it was cited as one of the objectives in the Millennium Development Goals (MDGs).\n\nStudy carried in Edinburg University medical school showed, \u201c86 percent of students agreed that computer skills will be beneficial to them in their future career and that 62 percent wanted a structured course in computer use\u201d (Bulu, 2009). This shows how important computer communication network in medicine schools has become. Currently, students do not want to remain in the Stone Age where information flow was restricted to emissaries; no, they want free flow of information accessible at the click of the mouse.\n\nA research carried by Platt, Anderson, and Obenshain in 1999 to determine use of student-centered, computer-mediated communication to enhance the school curriculum indicated increased need and pivotal role played by computer communication in medical schools. \u201cMany medical schools have made computer purchase compulsory for all incoming students\u201d (Platt, Anderson & Obenshain, 1999, p. 757).\n\nThe requirement that every incoming student to have a computer, emphasizes on the pivotal role played by computer communication in medicine school. This implies that every medical student in these institutions has access to computer communication. A report from the Department of Biometry in Louisiana State University Medical Center recommended integration of computer communication network in the curriculum of school of medicine.\n\n\u201cMedical students use computers to; manage information support patient care decisions, select treatments, and develop their abilities as life-long learners\u201d (Platt, Anderson & Obenshain, 1999, p. 757). There is probably no other possible and convenient way of achieving this objective other than incorporating computer communication network in the curriculum of medicine schools.\n\nCommenting on the same, Koschmann had earlier suggested in 1995 that, \u201cstudents should learn about computers, learn through computers, and learn through computers\u201d (Koschmann, 1995, p. 818). This suggestion has happened; students are contemporarily learning with computers as exposited next.\n\nThe place of computer communication in medicine schools has shifted from knowing about computers to learning with computers as aforementioned. A paper written by Seghieri, Dussert, Palmari, Berthois, Martin, and Penel in 1997, shows how far computer usage has gone in medicine schools.\n\nThese authors designed software to \u201csimulate the calcium signal following hormone or growth factor stimulation in epithelial cells\u201d Seghieri, et al, 1997, p. 1). This is a basic illustration of the direction being taken by computer usage in medical schools.\n\nComputers have become useful tools in studies. Even though ten years ago computer usage in research was solely based in research institutions, this phenomenon is gradually changing and computer usage is finding its way into classrooms and lecture theatres. The future of medicine lies in nano-science and bioinformatics through computer world and the humble beginning of computer communication network in medicine schools forms the basis of this exciting future, notes Koschmann (1995. P. 819).\n\nFinally, a high-level committee formed in 2003 to look into health telematics cited the need to integrate a solid computer communication network in medicine schools.\n\nEven though this committee agreed with the improved utilization of computer communication network in different medicine schools, it cited some shortcomings in the full utilization of the same because medicine schools have failed to produce professionals who can fit in the contemporary ever-changing needs in the field of medicine (High Level Committee on Health, 2003, p. 6).\n\nThis study implies that even though the current utilization of computer communication networks in medicine schools is recommendable, there remains a lot to be done to match health needs in society, which underline the reason why medical students are in school.\n\nAnalysis of these Literature Reviews\n\nThere are several common factors in all these research papers reviewed here. The most outstanding factor is that computer communication network has been extensively applied in medicine schools. Authors of these papers note many medicine schools have made it a requirement for every incoming student to have a computer.\n\nThese authors also insinuate that the future of medicine lies in incorporation of computer communication in studies. Currently, computer communication network has made learning fun and easy given the fact that peer-reviewed information is now readily available in the internet. This satiates the urge for more information and provides students with rich source of invaluable information in medicine.\n\nBased on what Koschmann had foreseen in 1995 that students have to learn about, through, and with computers, most researchers agree this has been achieved in contemporary times. More than fifteen years after Koschmann\u2019s observation, students are currently learning not only through computers, but also with computers. Nevertheless, researchers seem to disagree on some issues.\n\nWhile many researchers do not deny the inevitability of integrating computer communication in medicine school, some differ on the mode of implementation of the same. Platt, Anderson, and Obenshain (1999), note that while most medical schools have made it compulsory for incoming to have a computer, there is no formal training offered to prepare students for meaningful use of these computers (759).\n\nOn the other side, other researchers argue that the integration of computer communication in medicine schools is a leap jump towards future medicine with formal training or not. This issue boils down to the objective behind the introduction of such systems in medicine schools.\n\nThe biggest question remains whether the introduction of computer communication network in medicine schools is to prepare the platform for future medicine or the future is now.\n\nThis draws to attention the observation by the High Level Committee on Health, which noted that the present breed of medical students is not fully armed to meet the ever-changing health needs in society tied to technological advancement. On the other hand, there is the question of challenges facing computer communication network in medicine school.\n\nThe biggest problem facing use of computer communication network in medicine schools is the issue of security. \u201cViruses, worms, and other malicious programs coupled with privacy matters pose the greatest use to computers in the 21 st Century\u201d (Boeckeler, 2004). This remains as one of the unresolved challenges in computer communication network in medicine schools and in all other areas of computer use.\n\nSome people have deliberately chosen to harm computer users by spreading malware to interfere with smooth communication over computers. Boeckeler (2004) fears that, \u201csome people are very skilled and patient and it would very difficult to stop them from harming computer and internet users.\u201d The other challenge is the issue of insufficient training to prepare students to use computer communication network in schools.\n\nFinally, research points out that computer prices are still prohibitive and this poses a big challenge to students who come from average families. Nevertheless, there is hope to overcome these challenges with future research set to address some of them like cyber security. Future research has to look into how students could be trained formally to utilize computer communication network to meet the health needs of society.\n\nConclusion\n\nComputer communication network has been applied extensively in most medicine schools. Researches indicate most medicine schools have made it an admission requirement for any incoming student to have a computer. This implies that; most, if not all students have access to computer communication network in medicine schools. This form of communication allows students to exchange and access invaluable contemporary information over the internet via their computers.\n\nOnline information makes learning fun and easy for in most cases students have hunger for information that cannot be satiated by tutors principally. The future of medicine schools lies on efficient use of computer communication to explore areas like nano-science and bioinformatics among others.\n\nNevertheless, researchers fail to agree on the mode of implementation of this computer communication network in medicine schools with some holding the view that the implementation process is vague. Security and privacy issues in computer usage pose great challenge in computer communication usage; however, future research can address that.\n\nReference List\n\nBoeckeler, M. (2004). Overview of Security Issues Facing Computer Users. SANS Institute.\n\nBulu, M. (2009). Use of Information and Communication Technology by Medical Students: A Survey of VSS Medical College, Burla, India. Retrieved From,\n<Http://Www.Allbusiness.Com/Education-Training/Curricula-Medical-Education/12894127-1.Html\n\nHigh Level Committee on Health. (2003). Health Telematics Working Group of the High Level Committee on Health. European Commission; Directorate G \u2013 Public Health.\n\nKoschmann, T. (1995). Medical Education and Computer Literacy: Learning About, Through, And With Computers. Acad Med. 70(1); 818-21\n\nOstbye, T. (2006). Computer Communication for International Collaboration in Education In Public Health. Annals of the New York Academy of Sciences . 670(2); 49-49\n\nPlatt, M., Anderson, W., & Obenshain, S. (1999). Use of Student-Centered, Computer-Mediated Communication To Enhance The Medical School Curriculum. Medical Education. 33(1); 757-761\n\nSeghieri, P., Dussert, C., Palmari, J., Berthois, Y., Martin, P., & Penel, C. (1997). A Minimal Model for Calcium Signal Generated By Tyrosine Kinase and G Protein Linked Receptors; A Stochastic Computer Simulation With CALSIM. International Journal Of Medical Informatics. 46(1); 53-65\n",
        "label": "human"
    },
    {
        "input": "Computer Security: Safeguard Private and Confidential Information Exploratory Essay\n\nComputer security refers to systems effected to safeguard private and confidential information and services from access by unscrupulous persons. The four critical aspects of computer security are confidentiality, integrity and authentication or availability abbreviated CIA. Confidentiality entails privacy, integrity; protection from unauthorized, authentication; verification of the user and availability accessibility to authorized parties. Computer security is essential to both a business and an individual in the day to day operations.\n\nComputer security is critical in securing the strategies of the business from its competitors but availing them to customers through the Internet. The business\u2019 strategies are essential for its survival in the ever competitive world. These strategies have to be easily accessible to the customers but not to the other market players.\n\nAccess codes to the business\u2019 policies that have been posted on the World Wide Web are availed to the targeted clientele. The safeguarding of these strategies within the business gives it a competitive edge in the market. This results in increased sales and subsequent profit margins, in addition to providing an anchorage in the market.\n\nAn operational security system within the business is essential in preventing cases of fraud in the business in the form of security gateways. Effective gateway systems, which are all inclusive of the fore mentioned aspects of computer security, will enable their detection of fraud in the eventuality that it occurs.\n\nThe authenticity of the system provides for transparency and accountability. Any individual gaining access to information is directly accountable in the occurrence of a default during their operations. The blue tooth security technology also inhibits unauthorized persons from accessing information because it requires pass codes (Umar 2004).\n\nAccess of personal information has also been significantly computerized by means of the mobile phone. The tendency to abuse this information is, therefore, relatively high. However, a workable security system like cellar in the mobile phone has helped reduce thus abuse of information. The regulation to access of this information permits limited or no access by unauthorized persons. This has been practical for me in the use of my cellar-installed mobile phone.\n\nThe technology allows me to connect with my friends and associates at the touch of a button without my privacy being compromised. My associates, in turn, enjoy quick and convenient secure communication and transactions with me. The blue tooth security technology also inhibits unauthorized persons from accessing information because it requires pass codes and my consent.\n\nThe Internet data encryption technology has particularly proved to be a critical tool of computer security in both my business and personal life. Business wise, I have kept my strategies strictly private and confidential within my business circles. My customers and suppliers easily access the well guarded information and transact business online (McLoone & McCanny 2003). On the contrary, my competitors can not access the information which has given me a market leader status in the market.\n\nMy personal information and curriculum vitae are also widely available to the world. However, I have undertaken to guard it against access by unauthorized persons through the application of The Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule. This has helped me gain more friends, associates and business partners while safeguarding my privacy from a legal point of view. The result has been a growth in my social, economic and personal spheres of life.\n\nComputer security is, therefore, essential in the business and personal fronts of life. Not only does only does it give a global aspect but also does it in a secure manner to all the parties involved. This result in a win-win case for both the business vis-\u00e0-vis the individual and his associates. The individual stands to make all rounded significant steps in the right direction. More so, the business also stands to operate at optimum levels resulting to maximization in the profit margins.\n\nReferences\n\nMcLoone, M, & McCanny J.V. (2003). System-on-chip architectures and implementations for private-key data encryption. New York: Springer\n\nUmar, A. (2004). Mobile Computing and Wireless Communications . New York: Nge solutions, inc\n",
        "label": "human"
    },
    {
        "input": "Levels of Computer Science and Programming Languages Essay\n\nTable of Contents\n 1. Introduction\n 2. Low-Level Computer Programming Languages\n 3. High-Level Computer Programming Languages\n 4. Conclusion\n 5. Works Cited\n\nIntroduction\n\nSome people, illiterate in computer science, do not understand what computer scientists refer as, computer-programming languages. Verily, computers operate under certain conditions, language being one of them. The language, programmed language rather, create computer programs thus regulating its operation according to specific instructions. Additionally, computer-programming languages help computer users to create or analyze computer algorithms applicable in many areas like accounting, statistics and business management.\n\nThus, these programmed languages help to create highways of human communication through computers. Programming languages express inscription. The two modes include syntax, which is an encryption of form, and semantics, which convey meaning. Computer languages are as old as the computer itself.\n\nIn ancient times, these languages were paramount in the operation of different machines. Today, there are so many programming languages created by computer scientists and still more are yet to come. Interestingly, computers only understand one programming language. In simple terms, the computers execute information through binary coding system.\n\nThus, one way to write a computer program is through binary machine codes. The paper will examine the two levels of computers in computer programming languages. There are two levels of computer programming languages, low-level and high-level languages. Largely, these two levels closely bear a resemblance to binary machine coding technique. (MacLennan, pp.1-8).\n\nLow-Level Computer Programming Languages\n\nA programmed language instructs the computer to comply with specific commands as done by the user. Normally, computer users address their commands into the computer hardware where the instructions, now in machine code form, change those instructions into binary notations using two digits 0 and 1.\n\nThus, low-level languages do not have an effect on computer instructions. Since their effect to abstraction is almost impossible, a computer user can create programs without using compilers or interpreters. Additionally, low-level programming languages fall into two main categories, first and second generation. In the first generation, the machine code-microprocessor is responsible for processing information according to language instructions.\n\nHowever, this system is cumbersome and requires more time backed with memorizations of some numerical codes. Perhaps this is the reason why, modern computer programmers do not create new programs falling in this class of first generation low-level computer programming language.\n\nThe second generation low-level programming language is simply, assembly language programming where, the computer programmer does not need to memorize much information although plain instructions are necessary. Consequently, the programmer assembles programmed data into the machine code. Thus, from the first and the second generations, two examples of low-level programming languages exist. These include machine language and assembly language. (Friedman, Mitchell, Haynes, pp. 2-33).\n\nFor the programmer to create low-level programming languages, computer architecture is very necessary for machine coding in the Central Processing Unit (CPU) of a computer.\n\nSince low-level programming languages interrelate with the Central Processing Unit or computer hardware, it is somehow hard to any computer user to analyze abstraction using this level of computer programming languages. Nevertheless, low-level programming languages do exist especially when creating small programs which do not require massive data processing.\n\nHigh-Level Computer Programming Languages\n\nUnlike low-level programming languages, high-level programming languages have strong abstractions as detailed in the computer architecture. In most cases, programmers find them easier to create or even transfer them from hardware to hardware using natural languages.\n\nIt is for this reason; computer programmers refer high-level programmed language as non-Central Processing Unit languages because, the language does not portray CPU features like scope and computer memory. Nevertheless, the main reason of hiding computer details is to make the whole language be friendly to the user who may be worried of machine code information.\n\nThus, the execution meaning-semantics of the computer hardware is totally different from the program hence, the language created is simple and easy to understand as compared to low-level languages.\n\nSince there is a difference between a computer program and the architecture of a computer, this leads to high abstraction. Additionally, high-level programming languages contend with variables, complex mathematical expressions, usability and objects. Thus, from these examples, it is clear high-level languages do not apply opcodes just like in low-level languages. Instead, instructions feed the machine code unswervingly.\n\nHowever, if a programmer is using a more generic data, it can be impossible to use high-level programming language. In this scenario, the data is written using low-level programming language technique bearing in mind; high-level programming language would simply it. High-level languages need a compiler to change the language into binary digits of 0 and 1used by the computer. Examples of high-level languages include C++, Scheme, Prolog, Python, Prolog, Java, ADA, Fortran 90 and Lisp, Fortran 77.\n\nThese high-level languages offer a high percentage of abstraction compared to that of the Central Processing Unit. In addition to this, high-level languages can execute the same information regardless of the platform a language is running. (Pierce, pp. 330-338).\n\nConclusion\n\nAs technology continues to grow, computer scientists are busy developing new computer programming languages that will execute complex data and provide easier processing. (Pascal, Lapujade, Gilles and Fr\u00e9d\u00e9ric, pp.164-169). These programming computer languages are either depended on the computer hardware directly or indirectly.\n\nThose languages written using the computer hardware- machine code are low-level languages while, those languages created independent of the computer hardware though they need a compiler to change data into digit form which is then interpreted by the compute hardware, fall in the class of high-level computer programming languages.\n\nThe two levels of computer programming languages are beneficial depending on the application of the language. For example, low-level languages create simple programs, while high-level languages create huge but easier to understand programs.\n\nWorks Cited\n\nFriedman, Daniel, Mitchell, Wand, and Haynes, Christopher: Essentials of Programming Languages . Massachusetts: The MIT Press. 2001. Print.\n\nMacLennan, Bruce. Principles of Programming Languages . Oxford: Oxford University Press. 1987. Print.\n\nPascal, Lando, Lapujade, Anne, Gilles, Kassel, and Fr\u00e9d\u00e9ric, F\u00fcrst. Towards a General Ontology of Computer Programs, Institute of Computer Software, 2007, 164-169.\n\nPierce, Benjamin. Types and Programming Languages . Massachusetts: The MIT Press. 2002. Print.\n",
        "label": "human"
    },
    {
        "input": "Writing Argumentative Essay With Computer Aided Formulation\n\nThe course has a principal thesis that caters for origin and description of argumentation. Students need to learn the purpose and style of writing an argumentative essay. With the help of amplification of opinions, a good professional argument demonstrates effectiveness and purpose.\n\nThe school argumentative essay must follow key features of a good writing style. These are emphasis from the collage or school tutoring systems. In comparison to professional argumentation in the work force, Chryssafidou (n.d, 2), indicates that it is challenging and often difficult to write an argumentative essay especially within an applicable area such as the workplace.\n\nOne has to see ideas in a systematic format in support of one position of the argument and disproval of the other. Essential skills taught in school covers components, schemes or patterns that are essential for easy and academically related essays. Professional style of writing proposes the dialectic form of essay, which utilizes computer supportive setting to support the position of the argument.\n\nThis assists in captivating an argument as a procedure to solve differences of opinions by supporting opinions with arguments, while anticipating counterarguments, instead of critical analysis of specific characteristics of an argument Chryssafidou (n.d, 2). One has to find ways of refuting the counter-arguments to strengthen personal opinion.\n\nAccording to Chryssafidou (n.d, 4), computer systems support opinions through provision of illustrative arguments. They also assist in engaging professional in formulation of illustrations and encourage dialogue through exchange of comments. This procedure assists in entangling difficult situations.\n\nThe formal education on essay writing lacks the opportunity to build up experiences as a critical aspect of solving challenges. Students often forget to substantiate their side of the argument at the beginning of the essay. They either lack enough understanding or ignore requirements to have a stand of opinion, to build up a logical debate within the essay.\n\nThe evident difference between professional and academic argumentative essay is that students will engage in an argument to refute a certain point-of-view and reject the opinion without proper justification. In line with Chryssafidou\u2019s article (n.d, 5); proper formalism of an argumentation is imminent in a computer system.\n\nThe system is a procedure that consists of various elements and connectors to construct the argument. The elements are \u201cclaims, data, warrant, backing, issues, position and arguments\u201d (Chryssafidou, n.d, 5). Various suggestive efforts of the software tool relate to formulation of arguments and assists in identification of a relationship between claims and supportive evidence.\n\nThe article assists students on matters concern with formulation of professional arguments. It helps one in choosing an opinion when initiating an argument, formulating clear disagreements through clarified structures, enhancing procedures for planning and writing the essay. The article also offers the option of writing an essay in a virtual context through online tutorials and provides guidance over formulation of collaborative appraisals as opposed to cumulative assessment of a situation.\n\nBy analysis of this text, I have learnt that one should not assume familiarity of argumentation skills and schemes when that knowledge lacks developed experience and substantive support. Strong arguments have well-built support such as the dialectic essays that are computer aided.\n\nThe academic essay may have strong support from various resources but lack the required characteristics to synthesis them into a coherent scene. There is a difference between developing an opinion followed by a reason for the choice, and developing an argument together with high-levelled standards in support of the position.\n\nThis article raises various concerns over significance of argumentative analysis. The author presents a different perspective to the academic guidance of written argumentative essay, which is the use of computer-aided models. The main question is whether the arguer has the ability to determine the critical aspects of an argument and state the presumptions or principals of an argument. These features often face implicit omission in school writing and guidance.\n\nWhich aspects of an argument lead to deeper and consultative disagreements that can provide better understanding of the argument? According to Chryssafidou\u2019s article (n.d, 9), it is possible to present different parts of a large argumentative transcript and merge the results to a unified structure. Is it possible to present basic elements of a contrasting argument? Is it feasible to have an illustration of interdependence between common or otherwise conflicting conclusions?\n\nOther concerns raised by the article points out on existence of errors connected to assumptions that arguments consist of single datum. While the structural representation of arguments in a computer system brings out details clearly, the legitimacy of an opinion depends on the structure of argument.\n\nThe computer-aided system however lacks policies and governing procedures. Does lack of policies compromise viability? The extra support of structuring problems and revealing possible solutions or options of designing and recording decisions is an excellent way of availing proposals to a problem for future reference.\n\nThe main weaknesses connected to the computer-aided designs for projecting a problem lies on details pertaining components of the argument, since there is compromise on overall goals and argumentative text. Other limitations concern reconstruction procedures. Formalism indicates that critic\u2019s assist is reinvention of better arguments (Chryssafidou, n.d, 11).\n\nReference List\n\nChryssafidou, Evi. \u201cDIALECTIC : Enhancing essay writing skills with computer supported formulation of argumentation.\u201d Kodak/Royal Academy Educational Technology Group Journal, School of Elec. & Elec. Engineering, University of Birmingham. (n.d). Web.\n",
        "label": "human"
    },
    {
        "input": "Challenges of Computer Technology Research Paper\n\nIs it possible to imagine this life without computers? In fact, computers play an unbelievably significant role in people\u2019s every day life both for work and entertainment. Each sphere of human life is connected with computers, Internet technologies, and online communication.\n\nThe field of computer technologies is advanced day by day, and the changes are quite noticeable. It is necessary to admit that the development of computer technologies causes the rising of ethical, legal, and even some moral issues; this development has been advanced within a short period of time considerably in order to improve own business, distant communications, and information exchange.\n\nPeople click on their mice, type their keyboards, gaze at their screens, and enlarge their level of knowledge, meet new people, and, of course, earn money. Computer technologies have a certain influence on many spheres of life, and geology is one of them. The science about the Earth and its constituents provides people with a good chance to learn more about the place, where we live and know for sure how it is possible to improve our planet in future.\n\nThe relation between the spheres of computing and the sphere of geology has lots of peculiarities and advantages connected to legal, ethical, social, and moral issues; the development of one sphere causes the changes within the other sphere, this is why any change should be analyzed from both sides and taken into consideration.\n\nDiscussion\n\nComputerization of Society\n\nOne of the first computers appeared in far 1940s in order to achieve positive results in the military sphere. At the beginning of the 1970s, the first personal computers became available for people. And, of course, the year of 1985, when the first Microsoft Windows was introduced, turned out to be crucially important for society and computerization all over the world.\n\nNowadays, people may spend so much time in the Internet, so that \u201cthe relationship of Internet and society is characterized by antagonisms that are an expression of the modern antagonism between cooperation and competition\u201d (Fuchs, 340). If people do not have common affairs, it does not matter that people have nothing to do with computers.\n\nMany people use computers to complete certain tasks, conduct researches, draw tables, and present information in a clear way. So, it is quite possible to find out numerous advantages as well as disadvantages of computerization of society, however, people will never argue that the use of computers may worsen their lives. Nowadays, people focus on such details, connected to computerization: design, size, font, and color turn out to be so important.\n\nFor example, in the article of Steve Lohr, much attention is paid to clouds, which may considerably influence the development of the company, because \u201ceveryone talks about Google-style cloud computing software\u201d (Lohr, 2009). Such points could seem senseless for people from the 19 th century, but a person of the 21 st century can spend days and nights discussing these kinds of issues.\n\nWith the help of such example, it is possible to say that computerization makes people pay attention to the details, which are not crucially important for this life, but still, become more and more important. People start caring less about personal preferences and interests, spend less time with own families in order to create proper conditions for work. Computers help the users in many ways, but it is necessary to remember that wrong treatment with computers may have negative effects on the users.\n\nThese problems may be connected to financial, physical, and emotional side. Possible bankruptcy, visual impairment, emotional surges, etc. Some people do not still have enough experience to control their work with computers, and this is why they may suffer because of own mistakes.\n\nComputer Technologies and Geology\n\nIn fact, computer technologies are closely connected to any sphere of life, and it is not surprisingly that geology has a kind of dependence from the development of computers and innovative programs. Geology is the science that studies our Earth and its constituents. With the help of new programs and other computer applications, the work of geologists, the analysis of information, and data gathering become simpler and simpler.\n\nComputers open more and more opportunities to study the structure of the Earth, evaluate what may spoil it and how it is possible to improve its conditions. Scientists may create illustrative graphs and vectors to explain some significant information and save it for further analysis. The development of computer technologies also promote proper saving of the material during a long period of time: disks, flesh cards, and numerous Internet databases.\n\nHowever, it is necessary to admit that the use of computers in geology may be dangerous: certain undetected errors take place and may significantly influence the results. Computer modeling helps to create a proper image, predict possible development of the events, and even find out several possible outcomes. However, all these calculations are made by computers, not by humans.\n\nAnd the results of these investigations have a great impact on humans, this is why those people, who run the computers during the investigations, become responsible for the results and for all possible mistakes from computers\u2019 side. There are several areas of geology, in which computing may be used: structural geology, mineralogy, mining geology, and hydrogeology.\n\nThese areas of geology turn out to be rather important, this is why geology and computer technologies and their relations should be examined. In hydrogeology, computer technologies help to comprehend groundwater effects; in geophysics, magnetism, gravity, and seismic shifts may be studied. Such kind of examination helps to overcome all those challenges, which may happen, and warn about possible errors within computer technologies and wrong results.\n\nEthical, Legal, and Moral Issues and Computers\u2019 Evolution\n\nSara Baase admits that ethical rules aim at enhancing \u201chuman dignity, peace, happiness, and well-being\u201d (Baase, 30). This is why ethical issues have to be considered in any sphere, where people spend certain time of their lives. It does not take much time to read a couple of ethical rules in order to be a worthwhile part of this world.\n\nSo, with a rapid rising of computer technologies, some ethical issues have been changed. For example, it is not appropriate to say that something is ethically correct or wrong; it is better to divide the ideas into three categories: obligatory, prohibited, and acceptable. In this case, computer users have more rights while using computers; their actions are not limited.\n\nThe major point is that people can define what is ethically acceptable for them and what may be not. Protection of liberties is also considered as ethically essential. There is one more distinction that has to be mentioned \u2013 positive rights, when people are obliged to provide certain services and products for others. These claim rights have to taken into account in order to follow all the ethical norms.\n\nAs for the legal perspective, it is possible to remember about the Fourth Amendment that is all about people\u2019s right to be secure within their houses and papers (Baase, 66). Computer technologies make personal information available to government, even more, with the help of technological innovations, the government has lots of opportunities to search houses without even informing people about it. Such actions turn out to be legal, but not appropriate.\n\nMoral issues and computing have to be properly analyzed, because some computer errors may lead to people\u2019s injuries or even deaths. Lots of people still argue whether it is possible to make a computer morally responsible. If the predictions made with the help of the computer are wrong and bring negative results, computers have to responsible for this or people, who work on this computer. If the computer is responsible, it is necessary to punish it.\n\nBut such punishment as to break the computer down or not to turn it on will be not enough to forget about human death. So, each of the above-mentioned issues has certain impact on further development of computer technologies. This is why in order to behave properly even within a cyberspace, it is important to analyze and be aware of several moral and ethical issues, and follow that any take steps is legally approved.\n\nComputing ethics is not that difficult to comprehend: mind own duties, do not do any harm to people around, follow the settled rules, and be responsible for any of your word and action. In this case, your life with computer technologies will be worthwhile, and no one can blame for some kind of indecent behavior.\n\nConclusion\n\nPeople spend too much time close to their computers: some computer users prefer to be online in order to get fresh news and be aware of all changes in the world as soon as possible; some people just use computers to speed up their work, get the results a bit quicker, and save their work for a long period of time; and some people find computers as good means to entertain, communicate, and find out captivating material to discuss. This is why the functions of computers are great in numbers.\n\nComputer technologies develop rapidly day by day, offering more and more opportunities to people. Because of such a rapid development of computer technologies, certain changes with moral, legal, and ethical issues take place. In this paper, we have analyzed how people changes ethical norms and what actions are acceptable legally.\n\nComputerization of society plays a significant role for human development, and in order to develop properly, people should remember about moral and ethical aspects of every day life.\n\nComputerization touches numerous spheres of our life: education, science, design, art, etc. Lots of scientists make use of computers to achieve good results and improve our future. Geologists are one of those, who use computers to investigate the Earth and its components, and analyze the findings from many perspectives.\n\nIf people do not know how to use computers properly and follow moral and ethical rules, they have a chance to make a mistake and fail their work. This is why it is better to be ready to challenges, caused by rapid development of computer technologies, and use computers taking into consideration both negative and positive consequences.\n\nWorks Cited\n\nBaase, Sara. A Gift of Fire: Social, Legal, and Ethical Issues for Computing and the Internet. Upper Saddle River, NJ: Prentice Hall, 2007.\n\nFuchs, Christian. Internet and Society: Social Theory in the Information Age. New York: Routledge, 2008.\n\nLohr, Steve. \u201cWhen Cloud Computing Doesn\u2019t Make Sense.\u201d The New York Times. 15 Apr. 2009. 15 Jul. 2009. < https://bits.blogs.nytimes.com/2009/04/15/when-cloud-computing-doesnt-make-sense/ >.\n",
        "label": "human"
    },
    {
        "input": "Computer-Based Communication Technology in Business Communication: Instant Messages and Wikis Report\n\nTable of Contents\n 1. Introduction\n 2. Enhanced business communication\n 3. Impending business communication\n 4. Possible risks and solution of problems\n 5. Conclusion\n 6. Reference List\n\nIntroduction\n\nThe development of computer-based communication technologies and the methods of how people prefer to use social networking have raised dramatically (Perkins 2008, p. 44). People are eager to improve their communication in business in order to make fast and effective decisions, develop proper relations, and engage more responsible partners within a short period and be sure of their correctness and success.\n\nThe technologies like instant messages (IM) or wikis are the best successful examples of communication technologies in business; however, as any other achievements in the computer world, they are characterized by many positive and negative aspects. On the one hand, IM and wikis make communication between different people possible; on the other hand, much work and implementations are required.\n\nTo solve the problems within the chosen filed, it is necessary to make people ready to challenges and provide them with the necessary amount of knowledge about IN and wikis\u2019 peculiarities and properly explain the necessity to study these technologies on the high level.\n\nEnhanced business communication\n\nInstant messaging (IM) is considered to be one of the frequently used communication technologies in the world of business. Many corporations have an access to IM and prefer to use this method in order to discuss current problems, share different files, and save their time.\n\n\u201cCorporate and government entities are increasingly using social networking to facilitate communication and collaboration among individuals and groups, both internally and externally\u201d (Perkins 2008, p.44). IM is one of the simplest ways to share information; however, it is not the only advantage of this technology: these technologies also promote the reduction of e-mail traffic (Wilkins 2007, p. 31).\n\nWikis, in their turn, provide their users with a magnificent chance \u201cto contribute to the content by editing the pages\u201d (Fernando 2007, para.2). Many users are deprived of a chance to create an appropriate plan of their actions being far from each other. This is why the development of such communication technology is considered to be beneficial indeed: people are free to create an event by means of wiki services and add necessary changes, improvements, and suggestions at the same time.\n\nThe result of this communication is a clear and properly discussed plan that is suitable for each member of a team. One of the most successful examples of how wiki services influence the business world is the cooperation by Penguin Publishing and the creation of the wiki novel A Million Penguins (Fernando 2007, para. 7). This experiment in corporate writing proves that people have all chances to succeed in the chosen activity and help other people.\n\nImpending business communication\n\nConsidering the pros of the above-mentioned compute-based communication technologies, it is necessary to admit the negative aspects of IM and wikis. Though IM provides people with a possibility to generate new business (Pannunzio & Nelson 2008, pp. 6), the use of this technology requires much time and efforts.\n\nIM is regarded as an informal style of conversations; this is why messages are usually brief and not always serious. They provide people with an opportunity to deflect their attention away from work and decrease the number of achievements possible to the chosen business sphere. People have to focus their attention on work and make use of IM on to achieve particular professional goals.\n\nWikis are the technologies which have to be thoroughly studies by the users. This is why it is necessary to find enough time and grasp the basics of this type of communication. If a person fails to understand how it is necessary to use wiki services, it may negatively influence the success of the communication through wikis.\n\nPossible risks and solution of problems\n\nTo overcome difficulties and challenges in business communication through wikis or IM, people have to organize their activities in the way they can gain the necessary level of knowledge and find practical application to their theoretical knowledge. To provide proper implementation of the computer-based technologies, government should take care of the education process.\n\nIf it is hard to add the necessary subject into the college/university program, companies and organizations which support IM or wikis should think about how their colleagues can use the offered services. It is enough to present several introductory courses, explain the frameworks of communication in order not to be distracted from work, and underline the benefits of such communication.\n\nConclusion\n\nIn general, people get a wonderful chance to enjoy the achievements made in the computing field and business communication. The development of IM and wiki services help to save people\u2019s time, provide them with good chances to share information, and solve problems within short periods. Technologies aim at making human lives easier and more comfortable, and the world of business is one of the first spheres where the use of communication technologies is characterized by benefits.\n\nThought, there are still some challenges people have to face while using the technologies, solution of these problems makes people stronger and more sufficient in the chosen sphere. This is why people should use another chance, grasp the essentials of computer-based communication, and achieve good results in business.\n\nReference List\n\nFernando, A 2007, \u2018Working off the Same Page: Based on the Idea That More Minds Are Better Than One, Wikis Let You Collaborate with Colleagues and Strangers Alike\u2019, Entrepreneur , < https://www.entrepreneur.com/ >.\n\nPannunzio, CO & Nelson, C 2008 \u2018Leverage the Power of Social Media\u2019, ProQuest Central , pp.6 \u2013 10.\n\nPerkins, B 2008, \u2018The Pitfalls of Social Networking\u2019, Computerworld, vol. 42. no. 7, p. 44.\n\nWilkins, J 2007, \u2018RU Ready for IM?\u2019, Information Management Journal , vol. 41, no. 3, pp. 27 \u2013 31.\n",
        "label": "human"
    },
    {
        "input": "Computer Fraud and Contracting Essay\n\nTable of Contents\n 1. The law pertaining fraud\n 2. Fraud through online\n 3. How to interpret the online computer contracting\n 4. Current trends of the computer frauds\n 5. Conclusion\n 6. References\n\nThe law pertaining fraud\n\nThe law does not provide the consumers with measures to enforcing the online contracts because the argument is that, it is impossible to tell the intention and the consent of the consumer when they signed the contract. In most instances, it is easy for the consumer to identify these malicious programs such as viruses or Trojan horses. On the other extreme end, the computer fraud might be easy to detect if one is able to identify the good software especially among the popular freeware applications available from the net.\n\nThe issue may be due to lack of transparency and uncertainty about what the end-user thinks when engaging with such programs. The personal judgement and consent of the user is what determines whether they agree to contract a program especially when the label shows or indicates in an otherwise pernicious manner such as a \u201cspyware.\u201d Some consumers may perceive software labelled \u201cWeatherBug\u201d as a legitimate program while other would take it as destructive software. (PC Hell, 2005)\n\nFraud through online\n\nThe scope of online fraud can be clear through definition of the term \u201cSpyware\u201d. As applied in a wide range of computer applications especially software development, the authoritative definition of the term may be difficult.\n\nAccording to a notice by ABA concerning the Federal of Trade Commission in 2004, (2006)\u201cSpyware is software that aids in gathering information about a person or organization without their knowledge and that may send such information to another entity without the consumer\u2019s consent, or that asserts control over a computer without the consumer\u2019s knowledge.\u201d\n\nEvidence indicates accurate differences between the merchant and the consumer with regard to online contracts. In most cases, the malicious software programmers have little or no intention over the program broadcast as the consumers may point out after the destruction has occurred.\n\nMost consumers would claim that the software was maliciously loaded on their systems without their authorization while considering the intention of the interface designer may as genuine due to various clicks from beginning to end on the contract interfaces may also apply.\n\nNot all malicious software designers and distributors use these contracting interfaces. As much as such contracting interfaces help to manage the relationship between contractors and consumers, wicked programmers will use programs with interfaces, which do not require user identity to install. Such software developers do this intentionally because they are aware that their products contain viruses and other malicious software and in such situations, they commit \u201cidentity theft\u201d.\n\nIf the end-user has not authority of assenting a download or upload then he/she would ascertain theft of individual\u2019s rights. Such software is rare and this reason, this paper addresses the cases where computer fraud occurs through interactive contracting interfaces between the consumer and programmer or the distributor involved in the system.\n\nThe contract law can provide meaningful constraints over computer fraud such as software distribution if a mechanism was invented requiring explicit approval and hence notice by the consumer before ordering or agreeing with the loading mechanism. Such a mechanism would actually have an impact over the distribution volumes. According to DeVries, (2003) such a sensible strategy would probably work well in curbing online crime but it would also have a big impact over the distribution of software.\n\nProbably such measures would have disastrous outcomes especially on the information privacy law but this may call for consideration of other measures for instance, the contract terms instructions where experts formulate means of blocking the use of misleading interfaces. This would probably enhance legitimacy of software distribution by providing significant scrutiny over mechanizations of destructive software.\n\nArguably, promotion of computer fraud occurs through existence of unambiguous software termed as \u201cfreeware\u201d or \u201cshareware\u201d. They provide benefits to the end users and in most cases; they have to assent to the agreement by appending a check on the user interface form. In such situations, the user has consent over functionality of the program. Although uncertain, their level of interest determines if they would grant program access in their system.\n\nThe freeware may be termed as software whose distribution occurs at zero cost and mainly involves the small downloadable applications compatible with majority of the operating systems. On the other hand, the transfer of shareware from one workstation to another as free trial occurs with the aim that the user might purchase the full version later after testing.\n\nMany end-users will comfortably welcome free information or software from the web for instance information concerning products that they intend to buy. They would hate the popular pot-up dialog boxes but will not be in a position to determine the intentions of a particular marketing company. Some advertisements would solicit the user to access free information but in return, they expose their information for possible attacks or future pop-up advertisements and worse still loading of destructive applications at the end-user system.\n\nToday the basis of the problem can only centre on situations where the software merchant has plausible claim over consumer consent over submission of personal information in exchange of free products or services. Infringement of an existing and protective law occurs because these claims can occur from either side.\n\nThe consumer can also have a plausible claim that fraud occurred without the personal consent. Contract and application law is therefore ambiguous because of the uncertain outcomes. Liberalization of the contracts diminishes the chances for formulation of contract law because the law ought to provide a framework that provides chances for more and better explicit consent formation before collection of personal information occurs.\n\nToday there is no basis to anticipate that judicial reverence over online fraud will extend to legitimacy of distribution of clearly malice programs.\n\nHow to interpret the online computer contracting\n\nFrom the consumer point of view, it might be very hard for them to pick out the malicious persons targeting marketing companies whose aim is to collect personal information from the genuine online merchants. People ought to understand that by agreeing to contracting processes through the online interfaces where one only need to click means licensing various application programs.\n\nThe process of downloading an application may involve several other applications without the consumer consent. Spyware programs appends onto genuine programs. It is often to find extra-unwanted programs attached to the main program and the process of download of the required one cannot occur without the full download.\n\nCombination of different programs may also occur in which case people exacerbate the issues of inadvertent or eligible assent because of the fact that the extra programs are difficult to locate and eliminate them or it is inevitable to download them. (Serafin, Manners and Forbes. 2004)\n\nApplication of some principles as a guidance though the contract development and signing may be used. Using some standards requires proper analysis but it presents troubling and unsolvable issues regarding assent. Some of these principals include having a written or spoken manifestation of assent before signing of an online contract.\n\nThe contract must be intentional or reasonable such that any other parties can treat it as assent. If such manifestation of assent is not involved one should treat the contract as void or a fraud. Manifestation of mutual understanding takes the form of a proposal or an offer, which is acceptable or rejected, by the other involved party. The disadvantage in this case is that neither the offer nor the acceptance is identifiable. The moment of the formation is also not determinable.\n\nToday the \u201cbrowser wrap\u201d terms that are usually behind the marked hyperlinks provides the terms and conditions for an agreement. The user clicks the option of either accepting of declining with the terms. There are however considerable disagreements regarding the interfaces with the query of whether they can be able to support contracts without prior determination of assent. An important development ought to support apparent manifestation beside the online validation through an interface.\n\nCurrent trends of the computer frauds\n\nToday, the causes of skipped supervisory re-evaluations by personnel managers are the huge workloads caused by accumulation of information and data or collusion during entry into the databases. The supervisory data reviews may utilize the special codes to hinder computer edits. Bypassing of many important procedural instances is the main aspect that facilitates fraud intentionally or unintentionally.\n\nResearch by Miller, (2008) has established that today\u2019s web crime statistics are difficult to obtain because of the various methods, periods and sources that are in use to determine them. Various Internet Fraud Complain Centres such as The \u201cInternet Complaint Centre (IC3)\u201d established in 2000 or the \u201cNational White Collar Crime Centre (NW3C),\u201d are addressing the rapidly spreading arena of cyber fraud/crime. They receive, build up a case and then refer it to the \u201cFederal Bureau of Investigation (FBI)\u201d as a criminal complaint.\n\nThe issue of cyber crime/fraud and contracting issues are however taking a slow pace to deal with because they belong to various categories. The consumer can present a crime in an alarming manner while it is a minor problem thus misleading the investigation teams. The issue of unsolicited junk mails and \u201cspoofed\u201d websites that imitate the legitimate ones is the root cause of the complexity involving the fraud cases.\n\nComputer fraud cases involve the \u201cauction fraud, non-delivery fraud, credit or debit cards fraud, non fraudulent complaints such as child pornography or spam/Junk mails.\u201d These are all situations that are accessible through federal laws meant to support the dynamic investigations, developmental analysis and aims to reach out for the public awareness efforts.\n\nConclusion\n\nFrom the business experts\u2019 point of view, computer fraud and contracting is a crisis they people have to deal with because electronic commerce is has just begun. People are coming to terms with the online procurement process and this unlocks more chances for fraudsters. Fraud and contracting is a bigger problem than what most people would anticipate.\n\nAs technology advance, it forces the fraudsters to acquire better skills and thus today\u2019s fraud involves experts rather than the imagined idlers. They have mechanisms of manipulating computers to access the intended information. The best mechanism of protecting oneself against fraudsters is by learning more about fraud and advancing technologically.\n\nMost fraudsters will use spyware to find personal information. Some of this software is able to collect information without the consent of the owner. One basic way of avoiding the transfer of personal information to fraudsters is to get rid of temporally internet files and browser history. Users ought to avoid visiting of suspicious websites. (Wells, 2008)\n\nThe issue of identity theft is common today with many victims having to spend huge amounts to clear damaged identities. It is wise to spend on solid protection against fraud other than spend on clearing the mess.\n\nTechnology equips the users with software capable of utilizing multiple options of defence against information theft, attenuation, swindles, and other variety of frauds. Good antifraud software should provide advice against dangerous Web sites, rate safety probability, provide automatic safety tune-up, shield browsers and block malicious programs or pop-ups.\n\nThe consumer should be a few clicks away from security zone so that they can access them any time they need protection or privileges. By installing updated Virus scanning software, one is able to stay ahead of the hackers or fraudsters.\n\nReferences\n\nABA.2006. 2004 \u201c Annual Review of Antitrust Law Developments: Annual Review Of Antitrust Law Developments\u201d. New York NY. American Bar Association, 2006.\n\nDeVries W.T, 2003. Protecting Privacy in the Digital Age , 18 Berkeley Tech. L.J.\n\nMiller, R. L, Gaines, L.K. 2008. Criminal Justice in Action Cengage Learning.\n\nPC Hell. 2005. WeatherBug Removal Instructions and Help . Accessed from http://www.pchell.com/support/weatherbug.shtml\n\nSerafin T, Manners and Forbes. 2004. \u201c The Federal Trade Commission held Workshops on spyware\u201d Web.\n\nWells, J.T. 2008. Computer Fraud Casebook: The Bytes that Bite . New York NY. John Wiley and Sons\n\nWinn J.K. 2005. Law, Commerce and Technology . University of Washington\n\nSchool of Law. Accessed from https://www.law.uw.edu/\n",
        "label": "human"
    },
    {
        "input": "Computer Systems in Hospital Analytical Essay\n\nHospitals require to be installed with appropriate Information Technology (IT) systems in order to facilitate execution of tasks in hospitals. Application of computer systems in hospitals will help in minimizing extensive mobility within the hospitals as hospital workers move around to access physically dispersed people, knowledge and shared resources.\n\nHospitals computer systems should have a central database that should be used for storing all patients\u2019 records. The presence of a central database will help different hospital personnel to easily access patients\u2019 medical records for medication purposes.\n\nFor example, when a patient is requested to go for a lab test, the lab technician should send the patients results to a central database where the physician can easily access it and recommend appropriate medication. Similarly, a central database is essential to nurses as they can use it to confirm doctors\u2019 prescriptions. Moreover, a central database will be of great importance in instances where the nurses realize that there are some inconsistencies in a patient\u2019 record.\n\nThe nurses can request the concerned physician to retrieve the patient\u2019s record and make necessary changes without forcing the nurse to go to the filing cabinet to retrieve the patient\u2019s hand copy and take it to the physician for correction. Nursing assistants will easily record patients\u2019 vitals and synchronous the information with the patients\u2019 record in the central database.\n\nThe central database will be important to the physician as well as pharmacy department as it will be used to keep a record of those medicines that the hospital has stocked. This will be important to the physician during the prescription process as it will enable him/her to be aware of all the substitutes\u2019 drugs that are available in the hospital in order to know which drugs to subscribe to the patient.\n\nProviding a central database will help in managing hospital processes more effectively. For example incomplete orders from the physicians will easily be corrected. Equally, the tracking of lab and radiology orders will be easy to manage since the concerned technicians will send the results directly to the central database where physician can easily access them (Preece, 2002).\n\nIt has been observed that poor communication methods that are employed in hospitals are the main causes for the high interruptions present in hospitals that are blamed for the high instances of inefficiency in work practices. Studies have shown that there is a high association between interruption level and medical errors. Adopting computer based communication facilities will greatly boost the communication efficiency in hospitals.\n\nFor example, the use of emails will help to eliminate those communications that are synchronous in nature such as face-to-face as well as telephone conversations that are associated with high level of interruptions. Therefore, the uses of email as well as instant messaging for urgent cases are more effective in hospital setting. Relevant parties should be advised to check their emails at specified time intervals and respond accordingly (Parker & Coiera, 2000).\n\nHospital wards are mainly distributed work environment as a result of hospitals workers being spatially separated because of their constant mobility as well as the propensity of hospital work being distributed in time. Therefore, the presence of central database as well as use of emails for communications purposes will facilitate passing of important information from one shift to the next to ensure continuity of patients information.\n\nTo avoid interruptions, mobile communications should be used only for emergencies. Finally, the hospital computer system should be secured accordingly in order to secure patients\u2019 information from unauthorized access that can render to maliciously damage or altering of the patients\u2019 records (Waegemann & Tessier, 2002).\n\nReference List\n\nParker, J. & Coiera, E. (2000). Improving Clinical Communication: A review Psychology. Journal of JAMIA . 7, 4, 453-461\n\nPreece, J. (2002). Interaction Design: Beyond Human Computer Interaction . New York: Prentice Hall.\n\nWaegemann, C. P. & Tessier, C. (2002). Documentation Goes Wireless: A Look at Mobile Healthcare Computing Devices. Journal of AHIMA , 73, 8, 36-39.\n",
        "label": "human"
    },
    {
        "input": "Computer Use in Schools: Effects on the Education Field Argumentative Essay\n\nTable of Contents\n 1. Introduction\n 2. Arguments in Favor of Computers\n 3. Arguments against Computers\n 4. Conclusion\n\nIntroduction\n\nWhile computers were only invented a few decades ago, these devices have had a deep impact on almost all parts of human life. Most aspects of modern living today are influenced by these systems and more ways of utilizing computers are being devised. Computers have affected the education field and they are used in schools all over the country. The use of computers in schools has elicited varied reactions among educators.\n\nIn general, there has been disagreement regarding the usefulness of computers in the classroom. While some argue that computers are a potent learning tool, others see computers as overrated machines that may in fact have negative impacts on the learning process. This paper will argue that computers are important assets to the student and therefore, each child in every school should have access to a computer.\n\nArguments in Favor of Computers\n\nResearch efforts by students are greatly enhanced by computers. Using computers, students are able to access the wide variety of resources available on the internet. Students no longer have to rely solely on the physical libraries for relevant research material. Accessing information using computers increases the efficiency with which students operate.\n\nBy using search engines, students can quickly identify the relevant material for their research and use it to achieve educational goals. The computer therefore enhances the quality of education by providing students with a wide array of resources.\n\nComputers increase the collaboration efforts among students. In the traditional classroom setting, collaboration among students from different schools was hard to implement due to the physical separation between them. Using computers that have internet connectivity, students can collaborate with each other in spite of distance and hence enhance their educational experience. By holding discussions on difficult problems in class work, students can exchange ideas and arrive at solutions. These activities, which are facilitated by computers, will increase the academic outcome of individual students.\n\nComputers can increase student interest in class since it is possible to present material in a way that increases enjoyment. Obtaining and retaining the interest of the student is a major goal of educators since high interest positively contributes to understanding. Teachers present some topics in a boring manner and this makes the students to lose interest.\n\nLoss of interest has a negative impact on the achievement level of the students in the particular subject. Computer programs used for educational purposes are often designed to be fun and interesting since the developers want to appeal to the user and hence increase their sales. Computer use therefore helps to increase student interest in school material.\n\nComputers make the carrying out of self-pacing exercises by students possible. The level of understanding varies from student to student with some being able to grasp information faster than others. In the traditional classroom, the teacher is unable to meet the unique needs of all students since he/she has to teach at a uniform pace. Computers make it possible for students to access information at their own pace because most applications are user driven.\n\nThe learning efficiency of the student is significantly increased by the use of computers since the student is able to make use of the learning model most suited to him/her. The personalized learning environment provided by computers also helps to build student self-confidence since the student can set his/her own pace and level of challenge.\n\nComputers encourage exploratory learning among students who would otherwise rely primarily on the teacher for all their learning. Computers provide students with greater levels of freedom as they are able to access many educational resources both on their local machine and on the internet. In most cases, the student is able to access the material when they want to and they can follow links to additional material. This exploratory learning increases the knowledge base of the students and contributes to their academic success.\n\nMany students regard computers are relevant tools both for school and future work life. Gaining proficiency in computers is therefore a common goal for many youth. Statistics show that students are in favor of using computers since they believe that the skills they gain by using these devices will be beneficial in their future lives. Use of computers can therefore assist in mitigating the student dropout rates in our country since students find computers relevant in their lives.\n\nArguments against Computers\n\nIn spite of the many advantages of computers in schools, some real problems may arise from computer use by students. The autonomy given to the students might decrease productivity in class, especially if the teacher is unable to monitor the students effectively.\n\nThis is a major problem since students are likely to engage in non-educational activities using their computers during class time if they are not closely supervised. Unlike in the traditional classroom setting where the teacher can easily tell if the students are engaged in other activities while teaching is going on, it is hard to detect this when students are using computers.\n\nThis lack of accountability facilitated by computer use will degrade the learning experience of the student. This problem can be solved by installing monitoring software on each student\u2019s computer to help the teacher to remotely observe what the student is doing.\n\nComputers make it easier for students to engage in academic misconduct. In classrooms where computers are being used, students can easily provide each other with solutions to problems using e-mail or chatting tools. The collaboration tools can be used by students to obtain solutions from their peers instead of trying to solve the class problems on their own.\n\nThis is a major problem since a student will not improve his/her performance if he relies on other people\u2019s efforts. This problem can be addressed by imposing huge penalties for identical work by students. Such a move would deter students from engaging in academic misconduct.\n\nStudents can easily be overwhelmed by the vast amount of information that is accessible using the computer. When carrying out research work using the computer, students might end up finding too much information from the internet. Sorting through the information is a hard task and students may end up providing poor quality work.\n\nTeacher input during research can help students to better utilize material from the internet. The teacher can provide the best sources and save the student from having to deal with too many irrelevant resources.\n\nConclusion\n\nThis paper set out to argue that computer use by students is mostly beneficial and it should therefore be encouraged. To reinforce this position, the paper has discussed some of the most significant merits of computer use. The paper has noted that computers provide an element of fun, involve participants actively in the learning process and make self-pacing possible.\n\nHowever, the paper has acknowledged that computers have some inherent demerits which include too much autonomy, higher risk of academic misconduct, and exposure to irrelevant material. The negative impact of these problems can be addressed therefore ensuring that computers remain beneficial to students.\n",
        "label": "human"
    },
    {
        "input": "Impact of Computer Based Communication Report\n\nTable of Contents\n 1. Executive Summary\n 2. Introduction\n 3. Effect of internet on communication\n 4. Instant Messaging\n 5. Impact of Instant Messages in business\n 6. Impact of Blog in business\n 7. Negative Impact on Communication\n 8. Conclusion\n 9. Reference List\n\nExecutive Summary\n\nComputer based communication has become popular in the recent past especially due to the advancement in Internet technology. Email, wikis, blogs, instant messaging and video conferencing are some computer-based communications that have had a great impact on communication.\n\nBlogging and Instant Messaging (IM) are the most popular of these computer based communication. Computer based communication has transformed the way business is carried out. It has lead to introduction of new business channels and new ways for interacting with the customers.\n\nHowever computer based communication systems have some negative impacts such as the ease of spreading negative information. Computer based communication is slowly replacing conventional communication. With advancement in internet technology, various computer based communication will continue to gain popularity.\n\nThis research looked at the impact of computer based communication on business. It started by explaining the impact of the internet in general then the paper will concentrate on the use of Instant Messaging and blogs. The research looked at both the positive and the negative impacts of the internet in business.\n\nThe main research method that was used will be literature review. Material containing information on computer communication was reviewed and used to build up the subject of this study. From the finding the researcher concluded that there are many advantages of the advancement in computer communication in business.\n\nThis is especially so with regard to public relations that has improved due to the use of the internet. This implies that a lot has to be done to ensure that businesses take advantage of the use of computer based communication for the wellbeing of their businesses.\n\nIntroduction\n\nAdvancement in computer technology has had a great impact on business communication. Computer based communication has almost replaced conventional means of communication. Internet technology has led to a revolution in communication with the advent of such applications as Email, wikis, blogs, instant messaging and video conferencing (Nicole, 2007, par 7).\n\nNot only has internet technology led to a reduction in cost of communication but has also provided an effective means of communication across long distances. As a consequence of advancement in computer and internet technology, computer based communication is slowly replacing conventional communication\n\nEffect of internet on communication\n\nImpact of computer based communication, particularly those supported by the internet is evident. Computer technology has led to great impact on people\u2019s lives. Today, carrying a laptop or palmtop is becoming part of people\u2019s culture.\n\nNot only do people have access to the internet in their work places or at homes, but modern technology has allowed portable internet supporting devices (Freeman, 2003, p. 53). As a result of internet portability, people can access the internet at any place from their home to right in their vehicles. This makes it possible for people to work away from offices. For example lawyers are able to work with their clients remotely and sort out legal issues.\n\nInternet technology has revolutionized communities around the world. It has transformed these communities from a tightly knit society into a loosely bounded and networked society. Wellman et al (2003, para 4) points out how governments and citizens interact through use of internet. For instance they singled out Scottish government\u2019s use of email in accepting petitions from the Scottish citizens.\n\nMoran (2008) asserts that internet use will bring great impact on the way people carry out their daily business. Spaeth (2008, par 9) warns that organizations that not take advantage of social media are likely to get out of business. This is because other firms will use internet social media as a means of gaining competitive edge over rivals.\n\nTo drive other competitors out of business, such firms will utilize internet in reducing competitive disadvantage and meeting other strategic organization objectives. Blogging, video sharing services, micro-blogging, and other social networking are impacting on people\u2019s lives, particularly in communication. Instant Messaging has completely changed the way communication is carried out.\n\nMany Instant Messaging programs are in use today with about eighty percent of internet users using one or more of the programs to communicate. Low cost of using Instant Messaging and ability to use the service at virtually any place with internet access has motivated many users (Wilkins, 2007, par 5).\n\nInstant Messaging\n\nInstant Messaging is a recent introduction to communication and has had a great impact on communication. IM is an online communication whereby individuals key in messages through the keyboard of their computers. Unlike conventional message communication, IM is almost synchronous enabling real time message communication.\n\nPopularity of Instance Messaging can be traced in 1990 (Isaacs, Walendowki, Whittaker and Schiano, 2002, p. 121). Internet Relay Chat (IRC) and Multi User Dungeons (MUD) were the initial Instant Messaging supporting technologies.\n\nIRC, still in use today, allows users to communicate with many servers that are dedicated to a particular topic. MUD on the other hand enables users to participate in real time chats in virtual communities. Unlike these older programs, Instant Messaging enables users to communicate through real time message with other people that they already know. Among the popular instant Messaging companies today include Yahoo! Messenger, MSN Messenger, AOL Instant Messenger and ICQ.\n\nImpact of Instant Messages in business\n\nInstant Messages is among the cheapest means of communication. While other internet communications such as email may cost an organization significant amount, Instant Messaging is provided virtually free of charge. IM has significant impact on communication in business and Media.\n\nIn business, IM is being used as an effective way of providing support to customers. Through live chats customer can raise their inquiries to an organization. An online live chat support can be able to respond to customers addressing their issues interactively. Unlike call support, IM is cheap and fast. In addition, a customer support can be able to respond to many inquiries simultaneously.\n\nImpact of Blog in business\n\nBlogs provide a means for people from any part in the world to contribute to a particular subject or topic. Some of the important impacts of blogs in business are on market research and public relation (Braffort, 1999, p. 57).\n\nThrough blogs, organizations or businesses can be able to get customer/ public\u2019s opinions over their business, products or services. Blogs provides an important source of information therefore helping in business research (Mangold and Faulds, 2007, p. 263). With regard to public relations, blogs enable people to raise issues over a company\u2019s products.\n\nAbility to express opinion over an organization, a product or a service is by itself a public relation act. In addition, through the blogs public relations officers in organization can be able to respond to the issues raised. Many organizations today run blogs to enable them to communicate with their customers (Kent, 2009, p. 33). The blogs mainly act as gateways to communication with the outside world.\n\nConventionally the mass media was the primary means for public relations. Before the internet become famous, people had limited sources of information and mainly relied on the mass media for information. In addition, people had limited means for expressing their grievance, suggestions or inquiry. Customers who were not happy with an organization had very limited means of raising their issues (Bryant and Oliver, 2009, p. 65).\n\nEmergence of blogs provided new ways for connecting with an organization. According to (Kent p.1 2007) blogs are a recent addition to the use of technology by an organization and it is having a significant impact on public relations. Blogs have empowered the public significantly therefore making organizations to take them seriously.\n\nThrough blogs, people discuss freely about various topics including products provided by organization. Failure of an organization to respond to blogs can therefore have significant negative impacts on its public relations. To be able to respond to issues raised by customers, organizations have to use the same channels of communication as the public.\n\nCommunications through blogs therefore become an effective way for public relation. Contributions in blogs are usually personal in nature thus the use of this channel by organizations for Pr will enable organizations to retain customers by giving personalized customer services.\n\nThe personal nature and closeness provided by blog enable blogs to be a preferred means for expressing opinion (Macias, Freimuth & Hilyard, 2008, par. 7). Personal contribution of senior employees to a blog has very positive impact in public relation.\n\nNegative Impact on Communication\n\nAlthough computer based communications have brought significant positive impact on communication, they have some negative impacts. Blogging enables individuals to contribute freely to a topic. Therefore, there is likelihood for biased or false information in the contributions.\n\nIn business, blogs can be used by competitors to raise negative issues about a company. Although an organization has an opportunity to correct some of the misinformation in communication, some of it may have long-term negative impact on the company. Instant Messages have wide usage in organizations. Addiction to IM leads to loss of a manpower and time in organizations (Perkins, 2008, par 8; Grinter and Palen, 2002, p. 73).\n\nConclusion\n\nComputer based communication has had a great impact on communication. Considering the positive and negative impacts of this type of communication on businesses advantages outweigh the negative impacts and businesses have to take advantage of this technology to advance their businesses.\n\nEmails, blogs, wikis, IM and other computer based communication have changed communication at various levels of the society. As some of the most popular computer based communication, blogs and Instant Messages have had significant impacts on communication.\n\nIn business the internet is not only used for advertising but also for direct interaction with the customer. Through blogs organizations can understand their customers and be able to respond to their needs appropriately. Instant Messaging is among the cheapest computer based communication and organizations need to take advantage of this so as to reduce communication cost.\n\nWith advancement in internet technology, various computer based communication will continue to gain popularity and organizations need to keep themselves up to date with these developments. Some issues that may need to be looked into include the ability of all organizations worldwide to keep themselves up to date with the advancements in technology which affects their competitiveness.\n\nReference List\n\nBraffort, A. 1999. Gesture-based communication in human-computer interaction: proceeding. New York: Springer.\n\nBryant, J. and Oliver, B. 2009. Media effects: advancement in theory and research. New York: Taylor & Francis.\n\nFreeman, L. 2003. The impact of computer based communication on the social structure on an emerging scientific specialty. Social Networks Vol. 6 No. 3, pp 201-221.\n\nGrinter, R. and Palen, L. 2002. Instant messaging in teen life. New York: Association for Computer Machinery.\n\nIsaacs, E., Walendowki, A., Whittaker, S. and Schiano, D. 2002.The Character, functions, and styles on instant messaging in the workplace. New York: ACM.\n\nKent, M.L. (2008). Critical analysis of blogging in public relations. Public Relations Review, Vol. 34 No 1, pp32-40.\n\nMacias, W., Freimuth, V. & Hilyard K. (2008). Blog Functions as Risk and Crisis Communication During Hurricane Katrina. Web.\n\nMangold, G., AND Faulds, D. 2007. Social media: The new hybrid element of the promotion mix. Business Horizon Vol. 52, pp 357-365.\n\nMoran, M. (2008). How the Web Changes the Old Marketing Rules . Web.\n\nNicole, M. 2007. Keep Your Eyes on the Enterprise: Emails, Wikis, Blogs and Corporate Risk. Web.\n\nPerkins, B. 2008. The Pitfalls of Social Networking. Computerworld Vol. 42 No. 7, pp 44.\n\nSpaeth, M. 2008. The Next Generation of Communication. Web.\n\nWellman, B, Quan-Haase, A, Boase, J, Chen, W, Hampton, K, Diaz, I & Miyata, K., 2003, The social affordance of the internet for networked individualism. Journal of Computer-Medicated Communication, Vol. 8, No. 3.\n\nWilkins, J. 2007. RU READY FOR IM? Information Management Journal Vol. 41 No. 3.\n",
        "label": "human"
    },
    {
        "input": "Introduction to Computers Malicious Software (Trojan horses) Term Paper\n\nAbstract\n\nThe Trojan horse term emanates from some story in Homer\u2019s Iliad, where the Greeks gave their foes a huge horse made of wood, apparently, as a symbol of offering. But, immediately after the Trojans draw the horse into the walls of the city, the solders in Greek creep out of the horse\u2019s bellies which are seemingly hollow and unfasten the city gates, enabling their partners to sneak in and incarcerate Troy.\n\nTherefore, Trojan horses are simply undesired material or operations that are sneaked into firmware, software, hardware or wetware leading to undesired and/or unsuitable performance (DuBois, 2001). Detection of Trojan horses seems an almost unsolvable problem despite the fact that some inadequate analysis by use of mathematics has been applied in this area to provide further clarity.\n\nThis paper therefore seeks to provide more information regarding Trojan horses it begins by giving the techniques used by Trojan horses, then some ways to prevent them; thirdly, it gives some few examples of recent Trojan horses attacks and finally giving a summary and conclusion.\n\nTechniques used by Trojan horses\n\nTrojan horses depend mostly on users to set up them; alternatively, they can be set up by internal or external intruders who have gained illegal access by some way. Intruders that attempt to sabotage a system also depend on other users who run the Trojans to be triumphant. System users can also be fooled into setting up Trojan horses by simple means such as enticing of threats.\n\nA simple example is a Trojan attached to email with a pop up informing one that he/she has won a lottery such that a user attempts to claim the lottery and in that event installing the Trojan into the system (Fontes, Fontes, Purcell, Schulz, and Virgil., 2007).\n\nSome people might decide to use Software distribution websites to temper with real legitimate software versions with Trojans. In case the software distributers website is the main distribution point where other sites pick versions from, the Trojan horses would be spread so much fast thus affecting many systems within the internet community.\n\nAnother loophole is the DNS which provides inadequate security, hence, users might be cheated into connecting to undesired websites. Intruders would then exploit them by making them download Trojan horses (Forrest & Gross, 2004).\n\nTrojan horses might also be injected into programs through compilers that are affected by the Trojans. They may also be placed into websites from where users can be cheated to download. The Trojans can take forms such as JavaScript, ActiveX, Java applets or executable files (Fontes, Fontes, Purcell, Schulz, and Virgil., 2007).\n\nRecent scenario that involved Trojan horses\n\nMicrosoft suffer attack\n\nDuring the fall 2000, attackers downloaded and modified the original code of an operating system designed for future use. This happened due to a Trojan concealing a worm a program component that copies itself onto other computers across a network. After being set up on a Microsoft computer, the code stretched until it located a machine that contained secrets worth pilfering. The Trojan then alerted its existence to an attacker, opening a loophole to the network (Forrest & Gross, 2004).\n\nTechniques of preventing Trojan horses\n\nThe most suitable method of protecting a system from Trojan horses\u2019 attacks is avoiding them. Some other forms have been devised to try preventing Trojan attacks first; do not download unknowingly from websites or persons that you do not trust. You also should be aware of hidden file extensions windows operating system for example hides executables file extensions by default.\n\nSecond, familiarize your system users with regard to its dangers and ways to prevent them. And do not use program components that auto runs files however much convenience they may come with, they still can be harmful to ones system, for example, Trojans that might have infected removable storage media might automatically install themselves at the time of plug in.\n\nThirdly, when online, for example, in chat rooms, do not type in commands that are recommended to you by others or run program files suggested by friends. Also be cautionary when dealing with ActiveX, JavaScript or Java applets controls from pages of websites. It is advisable to configure one browser to disable execution of files automatically.\n\nFourthly, apply firewalls and virus protection software\u2019s that adequately combat Trojan horses. Despite the fact that it might be difficult to detect all Trojan horses, the software\u2019s will aid along way in combating them (DuBois, 2001).\n\nConclusion\n\nTrojans reflect a great internal threat to the integrity of an institutions entire network. They seem pretty tough to detect as opposed to viruses and other malicious software since they are distributed alongside recompiled file name attributes. They may also vary from simple keystrokes log made on personal computers to fully developed remote control Trojans that affect commercial systems to look vague by giving some authority to compromised users who can temper with the entire network.\n\nDespite the security, mechanisms put in place close to all networks are exposed to weaknesses of human gullibility (Forrest & Gross, 2004). Hence, all system users must consider the small mistakes we neglect since those are the loopholes that Trojans exploit.\n\nReferences\n\nDuBois, P. (2001). Trojan horses : saving the classics from conservatives. New York : New York Univ. Press.\n\nFontes, J., Fontes, R., Purcell, G., Schulz, B. J., & Virgil. (2007). The Trojan horse : the fall of Troy : a Greek legend. Minneapolis: Graphic Universe.\n\nForrest, B., & Gross, P. R. (2004). Creationism\u2019s Trojan horse : the wedge of intelligent design. New York: Oxford University Press.\n",
        "label": "human"
    },
    {
        "input": "State Laws Regarding Computer Use and Abuse Report\n\nTable of Contents\n 1. Introduction\n 2. Discussion\n 3. Conclusion\n 4. References\n\nIntroduction\n\nIn the United States, different states have passed various laws with regards to computer crime. This has been necessitated by the numerous acts of hacking into public and private computer systems by individuals or groups, something that has brutalized computer use all over the world.\n\nThe computerization of almost every aspect of modern society means that information, sensitive or otherwise is stored in computers and thus the risk factor rears its ugly head that other unauthorized users can access that same information. The safe guards meant to keep such information safe have more often than not been rendered ineffective by highly skilled hackers.\n\nAny device vital to national security that is controlled using computers could be used against the U.S if it ever fell into the wrong hands. It was therefore deemed necessary to pass laws regulating the use of computers in a bid to promote safety when using them (New York State Law, 2009). This study is about the laws of the state with regard to the use of computers and the abuse thereof. Computer crime is divided into three categories according to Flexispy (2010);\n\n 1. Illegally obtaining computer materials such as hardware, software, and peripherals.\n 2. Crimes targeting computer networks and or devices. Hacking and the generation of viruses, worms etc fall under this category.\n 3. Committing crimes by using computers and computer networks. Piracy of music, movies, software, and cyber terrorism falls under this category.\n\nDiscussion\n\nThe New York state law according to the Legal Directories (2011) has criminalized the unauthorized use of a computer, computer trespass, tampering with a computer; this has four degrees, pirating of computer materials, \u201cclass e felony\u201d, unlawful possession of computer materials and Unlawful entry into a computer \u201cclass e felony\u201d, unauthorized use of a computer, \u201dclass a misdemeanor\u201d. Tampering with data,\u201d class d felony\u201d (U.S. Department of Homeland Security, 2010).\n\n 1. If anyone violates these laws his/her defense can be that he /she had logical grounds and or rights to tamper with, reproduce, destroy and or use the computer and or material in contention but if found guilty, is subject to punishment as provided for by the law( New York State Law, 2010).\n 2. The Alabama computer crimes act criminalizes the unauthorized access of data, software, or any material within a computer, its manipulation, destruction, examination, and possession (Iron Geek, 2011). The violation of such would constitute a violation of intellectual property rights. If the damage caused to the said intellectual property exceeds ($2500), and or it results in the disruption of any public utility or service, the offender is guilty of a class b felony. Unauthorized removal and alteration of data causing physical harm to a person not involved in the crime is categorized as a class a felony (Cornell University Law School, 2012).\n 3. The modification willful or otherwise of any equipment that is used or intended for use in a computer is guilty of an offence against computer equipment and supplies. This crime is classified as a class a crime. Samson (2012). Alabama computer crime Act.\n\nConclusion\n\nThe laws against computer related crimes are broad in both states but adequately address the problem of cyber crime and other computer crimes. Most of the laws are similar in classification of crimes and in their definition of abuses, but differ in terms of penalties.\n\nReferences\n\nCornell University Law School. (2012). Fraud and related activity in connection with computers . Web.\n\nFlexispy. (2010). The world\u2019s most powerful spyphone . Web.\n\nIron Geek. (2011). State Hacking/Computer Security Laws . Web.\n\nLegal Directories. (2011). Law and practice . Web.\n\nNew York State Law. (2009). Offenses involving computers; definition of terms . Web.\n\nNew York State Law. (2010). Article 156 \u2013 Penal Law . Web.\n\nSamson, M. (2012). Alabama computer crime Act, Internet Library of Law and court Decisions . Web.\n\nU.S. Department of Homeland Security. (2010). United States computer emergency readiness team. Cyber threats to mobile devices . Web.\n",
        "label": "human"
    },
    {
        "input": "Apple Computer: The Twenty-First Century Innovator Term Paper\n\nBrief History of the Industry\n\nComputers were invented due to man\u2019s quest to tinker on many things. Calculators as counting machines were the first to be invented. Early humans used manual calculators to help in counting and calculation. Early tools used pebbles and sticks to help in the counting process.\n\nThe method of calculation uses algorithm to count or manipulate numbers. A manual calculator needs an operator to help in the process of tracking the algorithm. The Greeks and the Romans, and later the peoples of China, Japan and India, used the abacus in counting. The abacus is a rectangular wooden frame consisting of beads that represent certain quantity or numbers.\n\nMore inventions came in that improved the calculator. An example is the Napier\u2019s Bones. John Napier made improvements through his discovery of logarithms. The logarithms became the basis for the discovery of the slide rule. The slide rule is a perfect aid and tool for engineers and students of engineering even up to today.\n\nWithout an electronic circuit, an electronic computer cannot function. Therefore, the first computers functioned manually. With the invention of electricity and electronic circuits, inventions in computer engineering became more advanced.\n\nIt is not certain who invented the first electronic computer. But with the invention of the vacuum tube, a professor of Iowa State University, named John V. Atanasoff, and a student named Clifford E. Berry, were successful in using an electronic circuit to function on an electronic computer. (Parsons & Oja, 2010, p. 489)\n\nHistory of Apple Inc.\n\nWorking in a garage to produce products isn\u2019t extraordinary. But working in a garage to create a company that is considered one of the greatest innovators in the age of globalization and information technology is something extraordinary.\n\nJobs and Wozniak were young and aspiring computer hobbyists. They were merely looking for new things to boast about. But at the back of their minds, they were trying ways to invent and make a fortune out of it. This was the start of Apple Computer.\n\nThey first named their small company Apple Computer but later changed it to Apple Inc.\n\nFrom selling PCs, Apple grew into an organization specializing in multi-media products and services.\n\nThe success of Apple can be attributed to a clever and successful management of human resource. As music, movies, and photography were becoming digital, Jobs introduced elegant and simple devices which attracted millions of customers, particularly the young. Apple also focused on counterculture themes.\n\nCurrent status of the industry and the company\n\nComputer technology and the information revolution have dominated our lives and our businesses. Without a computer, an organization cannot function well. Organizations have their respective databases which contain information or organizational knowledge that has been accumulated through the years.\n\nOrganizational knowledge is a part of the resources of an organization. Another important development in the new business arena is the emergence of the Internet. Communication has been enhanced through the Internet. Almost all organizations have their own websites connected via the Internet. Fast and effective communication is an outcome of the Internet.\n\nThe company Apple Inc. has been headed by Steve Jobs through the rough and tumble ways of the technology craze business. He has led it to its success and leadership in the industry. Recently, he has announced that he is leaving the company for reasons of health (Robertson, 2011). But he is leaving with many products already introduced to the market, such as iPods, iPhones, iTunes, and iPads, and many more products with mobile technology application.\n\nApple Inc. was awarded by the Guinness World Record for being the most popular technology marketplace in the world today. It is now the leading company in the manufacture of mobile gadgets. Apple\u2019s App Store has also been awarded innovator beating other manufacturing companies of mobile gadgets. (International Business Times, 2011)\n\nIn 2009, Apple\u2019s 275 stores around the world had a net operating income of $1.4 billion. The Apple Store has become the best place for anyone wanting to experience music, games, or video. (Copeland, 2011)\n\nApple\u2019s Market Structure\n\nApple has many retail stores throughout the United States and in many parts of the world. It also uses the Internet in selling its products. The Retail Stores are part of the retail strategy of Apple.\n\nThere was the popularity of Gateway Country Store but which later waned down and all the stores were closed. Apple has now reached quite a number of countries in propagating its retail stores, and this includes the United States, Canada, the United Kingdom, Japan, and the emerging countries of Asia, the so-called \u201cTiger Economies\u201d \u2013 Singapore, Taiwan, and now China and India.\n\nCompared to other retail giants, including high-end luxury goods, Apple\u2019s Stores have maintained an impressive performance. (Bernstein Research, 2011, p. 91)\n\nIn 2006, Apple\u2019s manufacturing profit of its products alone attributed $663 million, while the rest (non-Apple products) accounted for only $200 million. Manufacturing profit is solved by calculating the difference between the cost of sales of that retail sector and Apple\u2019s cost of sales.\n\nSupply and demand for Apple products\n\nThe supply and demand for Apple products, such as iTunes, iPods, iPads, is something other competitors in the mobile technology industry are envious about. Whenever Steve Jobs announced that he was going to present a new product, the customers, especially the young technology users and enthusiasts, would wait overnight in front of Apple Stores. (Robertson, 2011)\n\nApple products are so in demand nowadays. The iPad alone sold 300,000 units on its first launching, surpassing other competitors. (Schermerhorn, 2011, p. 23)\n\nIphones and Ipods can download thousands of songs through the Internet. Apple mobile gadgets are also in demand but Apple is trying to keep the supply up while demand continues.\n\nThe download actions emanate from approximately 160 million owners of iPhone, iPod and iPad. The App Store has more than 350,000 downloadable apps available to 90 countries. There are more apps added. Categories range from games to business, news, sports, travel, and so on. (M2 Communications, 2011)\n\nPrice elasticity of demand for Apple products and services\n\nPrices of Apple products react to the demand of the products. The downloadable songs have quite a steady price with respect to the other products. Other product prices have been maintained by Apple, despite the high demand from local and foreign customers.\n\nIn 2006, iPods contributed to about one-third of Apple revenues from the Apple Stores. Bernstein Research states that the Apple Stores have added reputation to the Apple products.\n\nApple competitors\n\nGraph shows a comparison of Apple Stores\u2019 sales, Best Buy and other luxury retailers\n\nSOURCE: Corporate reports and Bernstein estimates and analysis\n\nApple\u2019s performance in the PC market retailing has been quite an astonishing performance. Compare to Gateway\u2019s performance, Apple\u2019s unique style of marketing of PCs has made it a leader in the PC market. Apple has beaten Gateway and Best Buy, two competitors in electronic retail.\n\nCompetitor Sony will continue to dominate the electronics sector but Apple\u2019s dominance in the mobile technologies will be retained even after the economic crisis and when the U.S. has battled the Standard & Poor\u2019s credit downgrade.\n\nIdentify those trends and discuss what relationship they have to the trends of the 3 major macroeconomic indicators over that 3 year period. Use GDP, CPI and Unemployment.\n\nDespite Steve Jobs\u2019s resignation as CEO, Apple will continue to dominate the mobile gadgets\u2019 market. Apple has announced that Jobs will remain as Chairman and consultant.\n\nThe downgrade of the U.S. credit rating by Standard & Poor\u2019s will affect GDP for the next three years. This in turn will affect the mobile communications sales, especially for Microsoft and Apple. Apple will continue to dominate the sales of mobile gadgets, but not on PCs. The PC sales will decline for the next three years.\n\nUnemployment is nothing to be feared of by employees (associates of Apple). The company is sure to retain their employees despite economic crisis. This was shown in their experience during the 2008 financial crisis.\n\nChallenges and Opportunities\n\nSony has its own style of \u2018store-keeping\u2019. It has been hitting the malls, particularly the kiosks, and tapping the women demographic. Sony showcased some of its features such as turning home movies into DVDs, taking of Sony video footage and uploading it on a PC then playing it on a TV. Sony however picked the higher income demographic or those earning $90,000 and above. (Anderson, 2008)\n\nThe Mac family, the iPod family, iPhone, and iTunes, will continue to be relevant for the next ten years. The target market is also important for Apple. These kinds of products are important to the so-called \u2018new generation\u2019 and beyond who value music with accompanying video. Apple has tapped the emerging market with emerging technologies coupled with the Internet. There is no other more important innovation than this. It has given Apple billions in profit.\n\nReferences\n\nAnderson, D. (2004). Microsoft, Gateway, Sony Heed the Call of the Mall. Adweek Magazines\u2019 Technology Marketing , 15362272. City University London.\n\nBernstein Research (2011). Apple Stores: Clearly Successful, But What is Their Real Impact? Apple Computer: Not Ready To Bet on the Unknown. Black Book \u2013 Apple Computer , 2007. Web.\n\nCopeland, M. (2011). The Apple Ecosystem. Fortune , 00158259, 11/23/2009, Vol. 160, Issue 10. Web.\n\nInternational Business Times (2011). Apple\u2019s App Store Awarded the Biggest App Store by Guinness World Records. Sun .\n\nM2 Communications (2011). Ten Billion Apps Downloaded from Apple\u2019s App Store Worldwide Computer Products News . M2 13639889. Web.\n\nParsons, J. J. & Oja, D. (2010). New perspectives on computer concepts 2011 . United States of America: Cengage Learning.\n\nRobertson, J. (2011). Jobs at Apple: master inventor, master marketer . Web.\n\nSchermerhorn, J. R. (2011). Management . United States of America: John Wiley & Sons, Inc.\n",
        "label": "human"
    },
    {
        "input": "Computer Crimes Defense and Prevention Essay\n\nTable of Contents\n 1. Defense and Prevention\n 2. Conclusion\n 3. Works Cited\n\nIn 1998 former president Clinton spoke to the graduating class of the U.S. Naval Academy and he said that the security of the United States is threatened by a new breed of adversaries that has found a way to harass and terrorize America.\n\nClinton said that they are international criminals and terrorists who cannot defeat us in traditional theaters of battle, but search instead for new ways to attack by exploiting new technologies and the world\u2019s increasing openness\u201d (Aldrich, p.1). He is talking about cyberspace and one of its unwanted by-product cyberterror and cybercrime.\n\nIt is important for the general public to know more about it in order to pressure government officials to invest more to protect them from unscrupulous men.\n\nCybercrime is the act of committing crime using the Internet as a tool to bypass security and increase its capability to cross borders and victimize people beyond the hacker\u2019s country of origin. Cyberterror on the other hand is the use of the Internet to send malicious code in order to disrupt, contaminate and cause accidents in transportation hubs and at government utility facilities.\n\nThe problem here is the commission of a crime without being physically present in the target area. The second major problem is the ability of criminals and terrorists to create havoc to countries like the United States, even if they are physically located thousands of miles away in another continent.\n\nDefense and Prevention\n\nThe first line of defense must be international cooperation. It is useless to develop elaborate defense systems without the capability of catching criminals and terrorists that are developing their attacks outside the United States.\n\nThe clamor reached a higher level when in October of 1998 Russian Foreign Minister Ivanov communicated to then General Secretary of the UN Kofi Annan and made it known that even Russia is felt what he called as the \u201cincreasing danger of information warfare\u201d (Aldrich, p.4).\n\nInternational cooperation can be intensified by refining International laws concerning transnational crimes especially those related to cybercrime and cyberterror.\n\nThere are at least two issues that have to be resolved in order to win in this battle. First, there is a need to develop laws and defense mechanisms that are effective but at the same time does not violate privacy of individuals (Committee on the Judiciary House of Representatives, p.36). Secondly, there is a need to be one step ahead of criminals.\n\nExperts are lamenting the fact that \u201cIn this dynamic threat environment, attackers are constantly ahead of defenders, and yet the PCI standards are updated only by unanimous consent every 2 years\u201d (Committee on Homeland Security House of Representatives, p.1). Citizens of the United States must constantly pressure their government officials to make the war against cybercrime and cyberterror one of their priorities.\n\nConclusion\n\nThe Internet created a high-level of global interconnectedness. It is a blessing for commerce but a major problem when it comes to security.\n\nBecause of the I Internet, it has become increasingly difficult to deal with criminals and terrorists that can operate beyond the reach of American authorities and yet still possess the capability to wreak havoc by commuting crimes and acts of terror. International cooperation is key as well as the realization that governments must invest heavily in the fight against cybercrime and terror.\n\nWorks Cited\n\nAldrich, Richard. \u201c Cyberterrorism and computer crimes: issues surrounding the establishment of an international legal regime .\u201d USAF Institute for National Security Studies. 2000. Web.\n\nCommittee on Homeland Security House of Representatives. \u201c Do the payment card industry data standard reduce cybercrime? \u201d Government Printing Office . 2009. Web.\n\nCommittee on the Judiciary House of Representatives. \u201c Hearing on data retention as tool for investigating internet child pornography and other internet crimes .\u201d Center for Democracy and Technology. 2011. Web.\n",
        "label": "human"
    },
    {
        "input": "Ethical and Illegal Computer Hacking Argumentative Essay\n\nComputer hacking refers to the art of identifying and exploiting the weaknesses that exist in computer systems. Hacking can be done on a malicious basis or on good grounds. Hackers use their knowledge and skills to exploit the flaws that exist in certain systems in consideration to their specialization. Hacking exists in two forms, which are ethical and illegal hacking.\n\nFor the ethical hackers, they pursue hacking in order to identify the unexploited areas or determine weaknesses in systems in order to fix them. On the contrary, illegal hackers\u2019 interests are for personal gains such as money and prestige.\n\nThrough hacking, they gain access into confidential information and manipulate it for their own gain or cause losses to the system owners (Caldwell, 2003). Repeatedly, it is realized that ethical hackers are the real hackers because of their expertise in computer systems while illegal hackers are known as crackers.\n\nIn the process of hacking, hackers gain different information depending on their core objective for hacking. There exist white hat, grey hat and black hat hackers.\n\nWhite hat hackers perceive the challenges that are manifested in computer systems and seek ways in which they can develop measures to counter any eventualities in addition to development of up-to-date security systems. The grey hat hackers are individuals who break into other individuals\u2019 or organizations\u2019 systems in order to establish the deficiencies or illegal acts they conduct.\n\nBlack hat hackers use the profit motive, protest or individual challenges from the knowledge they have to break into systems (Gunkel, 2001). The whole act of hacking demands creative, focused, and endowed individuals with problem-solving skills. Generally, hackers have an in-depth passion to explore into computer systems for various reasons.\n\nIn the exercise of hacking, it is realized that most hackers are senior individuals in the society. This norm is enhanced by the fact that they have autonomous access to various organizations\u2019 security systems and have the necessary input required for hacking such as monetary resources and hacking experience. In their act of hacking, they explore the daily challenges they encounter in their field of work, and explore new ideas.\n\nThis culture propagates a competitive notion, which could either have negative or positive results. Meanwhile, the outcome of the identified continuous challenge for effectiveness or inefficiencies in security systems make the hackers contented (Parks, 2008). Furthermore, hacking becomes beneficial if pursued on a positive mindset in the innovation of appropriate security systems.\n\nOver the decades, it has been realized that the young adults have the greatest passion for hacking. This norm is because of their magnificent energy and eagerness to explore the unique areas unidentified by their predecessors.\n\nThe presumption that hackers have negative and positive impacts to the society attracts them to explore the challenges attributed to hacking in the process of learning about computer hardware and software. As seen from the destructive nature that can be imparted on systems software and hardware by hackers, it is necessary to conduct a continuous search for efficient systems that can ensure protection of organization resources.\n\nTo counter the acts of illegal hackers, it is mandatory to adopt measures such as firewalls and other appropriate security systems that prevent intruders or phreakers from accessing confidential information. On this note, organizations are protected from harm despite the hackers skills that are either beneficial or destructive to the society as a whole.\n\nReferences\n\nCaldwell, W. (2003). Computer security sourcebook . Detroit, MI: Omnigraphics.\n\nGunkel, D. J. (2001). Hacking cyberspace . Boulder, Colo.: Westview Press.\n\nParks, P. J. (2008). Computer hacking . Farmington Hills, MI: Lucent Books.\n",
        "label": "human"
    },
    {
        "input": "Computer Security Breaches and Hacking Report\n\nTable of Contents\n 1. Hackers gain access to RSA tokens\n 2. Peer-to-peer app DC++ hijacked for denial-of-service attacks\n 3. 63 percent of schools suffer IT security breaches\n 4. References\n\nHackers gain access to RSA tokens\n\n 1. The attack on RSA is in a class known as APTs (Advanced Persistent Threats). These threats are mainly aimed at stealing valuable information and unlike other forms of attacks; these are implemented over a period of time. First the attacker makes a series of trials before gaining access to the system and then once access is gained, information siphoning is done over time quietly in the background to avoid being noticed by legitimate users. This makes it difficult to determine what compromised the system at the start, but generally, spear phishing is the most common method that is used by hackers in this type of attack.\n 2. It may be pretty difficult to avoid such attacks completely but it is important to keep measures in place to detect them in their earliest stages. However, we cannot sit back and wait for it to happen so that we can detect it and do something. Precautionary measures such as these listed below should be implemented. Strong passwords and PIN policies coupled with regular changing of the same should be enforced while at the same time monitoring changes in user privileges and system access both remotely and locally. The security and use for social media applications should be placed on high priority. The target victims of these hackers are mostly those high in authority as they are likely to have better privileges, therefore the rule of least privilege should be followed in role assignment. Operating systems and security applications should be maintained up to date and employees should be educated on social engineering tactics and how to avoid them.\n 3. The time loss associated with this attack includes the time spent trying to figure out the weak points in the system and the time spent communicating with clients and discussing mitigation measures. These processes also involve monetary expenses that otherwise could not have been incurred.\n 4. It may be difficult to fend off this type of attack by any hardware device implementation but some antimalware software have capabilities such as anomaly detection, pattern discovery, malicious IP tracking and many others which can help detect an upcoming threat.\n 5. No. All are equally susceptible, what matters is the precautionary measures in place.\n 6. The local user should avoid opening emails and messages from suspicious senders as well as pop up windows and links. He should also avoid giving out information except to authorized personnel only, (William, 2011).\n\nPeer-to-peer app DC++ hijacked for denial-of-service attacks\n\n 1. This attack is known as a Distributed Denial Of Service attack. From the given information, it is clear that the computers that were used to implement the attack are those whose IP addresses had been compromised through out of date client applications. This means that the Initial problem was failure to update the client application.\n 2. To avoid such an attack in the future, it is advisable to keep both the client and server applications up to date. Also using self updating applications especially when it comes to third party applications, such as the peer to peer application in this case. Packet filtering techniques should be employed to ensure that all packets that get into the network are legitimate. Use of IP verification features on the interface, to verify the legitimacy of the source of requests is advantageous.\n 3. The monetary or time value lost in the attack may not be clearly quoted, but we know that in any DOS attack, there is a major system downtime involved. This time has got monetary value in terms of IP hosting, system restore, employee idle time and many more. Also there may be need to hire experts to help in system restoration which translates to additional monetary expenses. In cases where the attacker is after extortion benefits then the organization under attack may be in for bigger losses. The affected companies may not have been exceptional.\n 4. Packet filters, Firewalls and Software patches are measures that can be put into place to fend off such an attack in the future.\n 5. No. All systems are equally susceptible.\n 6. The local user should ensure that third party applications that he is using are maintained up to date. Also any abnormal delays in server access should be reported to the person in charge, (Jeremy, 2007 ).\n\n63 percent of schools suffer IT security breaches\n\n 1. This is not about a specific case but a general study hence it is hard to determine what went wrong. However, we can be justified to conclude that many school attacks are malware and virus attacks propagated through the social media networks. This means that the major compromise of such systems is lack of proper antivirus programs and firewalls.\n 2. To avoid future attacks, it is important for schools to put self updating antivirus and antimalware applications in place. External media introduction into the network should be discouraged and in the event that they have to be used, then security measures should be enhanced. Internet usage should be monitored and any suspicious websites blocked. Use of firewalls and IP filters should be emphasized. User authentication measures should be employed while at the same time educating users on the benefits of securing the network as this encourages user responsibility.\n 3. Significant system downtime is experienced every time there is an attack and time meant for other businesses especially education related issues which is the core business for schools, is sometimes spent in restoration. Also there is monetary value incurred in system recovery, that is, money spent to hire experts and sometimes to replace parts such as hard drives that have been completely destroyed by viruses.\n 4. Installation of Firewalls, antimalware and Antivirus software can help fend off such attacks in the future.\n 5. Yes. Certain operating systems like Linux are less vulnerable to virus attacks.\n 6. The local user can avoid use of vulnerable external media such as USB devices if the organization does not have proper mitigation measures in place to control virus transmission into the network. Also they can avoid connecting personal computers and laptops to this network. (Panda Security, 2011)\n\nReferences\n\nJeremy R. (2007). Peer-to-peer app DC++ hijacked for denial-of-service attacks . Ars technica . Web.\n\nPanda Security. (2011) Study: 63 Percent of Schools Suffer IT Security Breaches Twice a Year . Security Products. Web.\n\nWilliam J. (2011). Hackers gain access to RSA\u2019s SecurID security tokens . Government Computer News. Web.\n",
        "label": "human"
    },
    {
        "input": "Computer Viruses: Spreading, Multiplying and Damaging Essay\n\nA computer virus is a software program designed to interfere with the normal computer functioning by infecting the computer operating system (Szor, 2005). These viruses have the capability of spreading from one computer to another. They are also capable of multiplying. Viruses cause myriads of working challenges to computers ranging from destruction of files, slowing down computer performance, renaming files in computers and eventually making it difficult to access stored files (Szor, 2005).\n\nThese viruses can spread from one computer to another through an internet connection and sharing of floppy disks, flash discs and any other external and portable device that can transfer data from one machine to another. Viruses are distinguished from worms because they cannot run automatically (Szor, 2005). Therefore, the user of a computer must perform a function that will necessitate their operations.\n\nThe virus programs are not-self generated within a computer system. However, they are designed and produced by computer hackers. The hackers who design most common computer viruses usually come up with the names (US-CERT , 2012).\n\nSome of these viruses include Melissa, the Anna Kournikova, MyDoom, Sasser & Netsky, the ILOVEYOU virus, The Klez Virus, Code Red and Code Red II, Nimda virus, SQL Slammer/Sapphire virus, Leap-A/Oompa-A virus, and Storm Worm (US-CERT , 2012). All these viruses are tools used by hackers to destroy and access remote computer files.\n\nThere are many ways through which a computer can be infected by a virus. These may range from direct access such as file sharing to remote access portals such as using internet connectivity. For a computer virus to be effective, it must be allowed to run by the user through various means (US-CERT, 2012). A computer user can accept any new message on the screen without knowing the information in the file.\n\nIn this way, a computer virus is then allowed to run, causing harm to the computer. If a computer user opens unknown attachments, then viruses can easily find access into files in a computer. If a computer\u2019s operating system is not updated, it will not be able to filter viruses, making it more vulnerable to attacks (US-CERT , 2012).\n\nIn addition, downloading files from unsecure sources allows viruses to gain access to a computer. Viruses can also be hidden within some software. For instance, during software installation, viruses are allowed to infiltrate into computer files (US-CERT , 2012).\n\nViruses have become a major challenge to several companies across the world. However, some measures can be taken to regulate the spread of these computer viruses. For instance, the use of an updated antivirus program sourced from a reputable vendor offers computer protection against most viruses (US-CERT , 2012).\n\nThe antivirus must be frequently updated and configured to automatically start as the computer is switched on. Besides, the antivirus should be configured to allow regular virus scanning and virus check when running executable file on the computer. Backing up data is also an important method of protecting a computer from loss of data due to virus infection. The internet is the most common source of viruses.\n\nWhen dealing with any form of data from an online source, make sure that the source is well known. The use of USB data transfer gadgets should be well monitored and conducted with caution. The antivirus in this case should be used to scan USB drives before accessing the files. The most important protection is making sure that the operating system is updated and the firewall is always activated.\n\nIt is also possible to erase crucial information from hard disc drives using viruses. This information may be data collected from a crime scene, banking records, confidential files, and even important government files (Szor, 2005). The use of viruses can manipulate important computer information such as bank records resulting into an individual accessing vital and confidential records. The development and use of viruses has always been aimed at committing crime either directly or indirectly.\n\nReferences\n\nSzor, P. (2005). The Art of Computer Virus Research and Defense. Boston, MA: Addison-Wesley Professional. US-CERT (2012). Virus Basics. Retrieved from https://www.us-cert.gov/publications/virus-basics\n",
        "label": "human"
    },
    {
        "input": "Purchasing or Leasing Computer Equipment: Advantages and Disadvantages Report\n\nTable of Contents\n 1. Summary\n 2. Introduction\n 3. Discussion\n 4. Conclusions\n 5. Recommendations\n 6. Appendices\n 7. References\n\nSummary\n\nThe report details the best way a company should go in its process to upgrade its computer equipment. In fact, any organization should opt for the most advantageous way in terms of savings and efficiency (Zimberoff, 2002). This report will weigh the option of purchasing and leasing this equipment in terms of benefits the company expects. Leasing computer equipment ought to be cheap compared to purchasing new pieces. The company will be recommended to go for the leasing option, as opposed to purchasing new equipment (Gelinas et al, 2004).\n\nIntroduction\n\nThis report is based on a research carried out to establish the most viable option for the company wishing to upgrade their computer equipment (Weaver & Weston, 2007). The research was done by interviewing several dealers in computer equipment with the aim of established which a cheap option (Oz, 2008).\n\nPeople in computer equipment businesses were involved whereby they were requested to give their quotations for new equipment. In addition, enterprises that lease computer equipment were consulted in order to get useful information on prices. This computer equipment may include, keyboards, mice, monitors, CPUs, printers and UPSs.\n\nThis equipment advances at a high rate hence creating the need for upgrades in the organization (Harold Bierman, 2010). It was found that leasing equipment is cheap as compared to purchasing new ones because, there are many costs involved with installation of new equipment. When the organization decides to lease this equipment for the installation, will be on the part of the owners and maintenance, as well.\n\nDiscussion\n\nLeasing computer equipment is cheap as compared to purchasing new equipment. This is because some of this equipment needs to be upgraded often due to changes in the technological world. It becomes so uneconomical for one to purchase new equipment after every six months or one year (Nevitt et al, 2011).\n\nIn case, they lease this equipment and at expiry of the lease period they are phased out by advancement in technology, then they easily change to the latest technology. The company is saved on the cost of disposing outdated equipment by opting to lease this equipment (Chandra, 2005).\n\nIn fact, by leasing computer equipment, the organization saves a lot of money in terms of maintenance cost. This is because tear and wear of the equipment are in the hands of the owners and not the organization. Depending with the agreement, the organization may lay off its technicians hence becoming abundantly economical (Kendall, 2008).\n\nFrom most of the enterprises consulted on this matter, many advised that the organization should lease equipment because they keep on advancing. These advancements create the need of constant upgrading hence becoming an expensive activity for companies that have to purchase new equipment every time they have to upgrade (Hosford-Dunn, 2008).\n\nConclusions\n\nLeasing equipment was found to have many advantages as a lot of costs are cut. This means that, for organizations dealing with constantly advancing equipment, it is extremely crucial that they arrange with dealers and attain such equipment for some time and take them back. In addition, organizations may get a chance to lay off some employees since their efforts cease to be required hence cutting down some costs.\n\nRecommendations\n\nThe research data details several aspects of considerations but eventually, leasing was found to be most viable idea for the organization (Harder, 2004). This is because; it saves the organization on operational and managerial costs. Therefore, the organization should lease computer equipment hence maximizing out of the idea.\n\nAppendices\n\nThese cost analysis for an organization having leased equipment for five years.\n\nCost for the proposed system (figures in USD Thousands)\n\nBenefit for the propose system\n\nProfit = Benefits \u2013 Costs = 300, 000 -154, 000 = USD 146, 000\n\nReferences\n\nChandra, H. (2005). Fundamentals of financial management . New York: Tata McGraw-Hill.\n\nGelinas, U. J., Sutton, S. G., Hunton, J. E. & Hunton, J. (2004 ). Acquiring, developing, and implementing accounting information systems . New Jersey: Thomson/South-Western.\n\nHarder, F. (2004). Fa$hion for profit: from design concept to apparel manufacturing \u2026 a professional\u2019s complete guide. London: Frances Harder.\n\nHarold Bierman, J. R. (2010). An Introduction to Accounting and Managerial Finance: A Merger of Equals. London: World Scientific.\n\nHosford-Dunn, H., Roeser, R. J. & Valente, M. (2008). Audiology practice management. New Zealand: Thieme.\n\nKendall, K. E. & Kendall, J.E. (2008). Systems analysis and design . San Jos: Pearson/Prentice Hall.\n\nNevitt, P.K., Fabozzi, F. J. & Mathew, J. V. (2011). Equipment leasing. Sydney: John Wiley and Sons.\n\nOz, E. (2008). Management Information Systems. Michigan: Cengage Learning.\n\nWeaver, S. C. & Weston, J. F. (2007). Strategic financial management: applications of corporate finance. Michigan: Cengage Learning.\n\nZimberoff, T. (2002). Photography: Focus on Profit. London: Skyhorse Publishing Inc.\n",
        "label": "human"
    },
    {
        "input": "Print and Broadcast Computer Advertisements Essay\n\nTable of Contents\n 1. Print advertisement\n 2. Broadcast advertisement\n 3. Overview of computer advertisements\n 4. Conclusion\n 5. Works Cited\n\nAdvertisement is the means through which a company or an organization tries to endorse its products to the public. Computer advisement, therefore, involves the promotion of computers to the public by making known their advantages and intended benefits one is to get after purchasing it or after using the computer-related service that is advertised.\n\nThis is done by use of different methods and is intended to cause a rise it the quantity of the services rendered or computers sold. Potential buyers are informed of computers, their uses, and other essential benefits and are persuaded to buy them after being convinced about the need to do so.\n\nAdvertisement is a very useful tool in the expansion and growth of a company or an organization dealing with computers. There are different ways that computer advertisement can be carried out. They include the use of print media and broadcast among many other forms. In this paper, print and broadcast computer advertisements are going to be tackled.\n\nPrint advertisement\n\nThis is a form of advertising that involves the use of print media for example newspapers, magazines, brochures, fliers, and other printed material. By placing advertisements in print media, people can see and get attracted to the new or existing products advertised, get attracted to them and also go ahead and spend money to purchase the goods or services (Management Study Guide p. g 1). This is especially important when advertising computers.\n\nAlthough this is a very effective way of advertising, it also requires a lot of arrangement and preparation by a group of creative experts. This is because for any advertisement to attract enough attention from people and cause them to use their money to acquire the advertised products, it has to be appealing and convincing enough.\n\nPurchasing of the advertised computers may not be immediate, but once potential buyers learn about the presence of the computers in the market, they may find themselves buying it at a later date.\n\nThe use of pictures and words to bring out the special features in any given computer and types of computers is therefore crucial in this type of advertisement because people have to see to be persuaded to buy them.\n\nThis type of advertisement can also be very costly. Putting an ad in a newspaper, for example, is charged depending on the size and even where its place. Big sized advertisements tend to attract more attention, but on the other hand, they are more expensive. It is also costly because placing an advert just once is not enough, but there should be consistency in advertising.\n\nPrint advertisement, therefore, becomes an essential source of income for the media offering this service (Wheeler, p. g 1). It can be particularly useful to use big pictured computer advertisement to capture the attention of potential buyers because though this is a bit expensive, it is more rewarding in the long run.\n\nAnother form of print media that deals with individuals directly is the use of brochures and fliers. People are given leaflets and brochures as they walk along the streets as they drive when stuck in traffic.\n\nThe logic behind this is that people tend to be keener on things received directly from another person. This brochures and fliers should have all the vital information about the computers being advertised and if possible brightly colored pictures.\n\nAnother form of print media is mailers. This entails the use leaflets made from uncomplicated material for example paper. Postcards can also be used. The leaflets or postcards are distributed to mailboxes belonging to different people. Most people, however, do not value such material and they end up treating them as trash and dispose of them without even reading.\n\nThe use of billboards is also another form of print advertisement. Billboards are intended to catch the attention of people who are using highways and other major roads. Computer advertisements on billboards should, therefore, be exact and should be able to create a lasting impact on the people.\n\nThis is because those targeted by this form of advertisement are people who are driving and may not have enough time to read a lot of material on the billboards. They should be eye-catching and large font should be used for them to be legible from a distance. Bright colors are also encouraged.\n\nThe invention of the internet has been a significant threat to print media. This is due to the internet\u2019s ability to reach to a larger population online.\n\nCompanies have therefore resulted in the use of the internet to advertise which is more rewarding than using print media. Although there is still significant use of print media, more people prefer to compliment it with other forms of media. This makes advertising more efficient.\n\nBroadcast advertisement\n\nThis is a way of passing on the message to the people by use of radio and television. This is one of the most effective methods that can be used by computer companies and organizations to advertise because of its ability to reach many people at once both within the country and abroad.\n\nIt is also possible to reach the people targeted mainly by placing the advertisements in particular programs which have large audiences both locally and internationally. This would work especially for computer advertisement because computers are no longer liabilities but a necessity for many people.\n\nSome of the factors that determine the cost of broadcast advertisement include the duration of the ad. Longer duration advertisements cost more than shorter duration ones. The time of the airing of the advertisements is also a factor. According to Management Study Guide (p. g 1), Advertisements aired during peak times would cost more than those aired during other times.\n\nWhen a broadcast advertisement is used in advertising computers, these two crucial issues should put into consideration: the cost and the time of airing the ad. Most advertisements are given a short duration and therefore when advertising things like computers one should be very precise and use captivating features, for example, pleasant voice and attractive images.\n\nFor people computer advertisements, one should also be willing to spend some money to get prime time advertisements which are more likely to get to the target audience and potential customers than adverts aired during regular times (Wheeler, p. g 1).\n\nThis is because people tend to pay more attention to programs aired during this time and therefore is better to invest on such time for advertising because it will eventually bring good returns.\n\nThis, however, does not mean that people should drain their accounts while trying to advertise computers. Moderation is also essential in maintaining the balance between the money spent on advertisement and that spent on other marketing strategies.\n\nFrequency in advertising is also required to make the products known and appealing to the public. Advertising should be done regularly and consistently for it to be an effective marketing strategy and bring positive returns to the company (Management Study Guide p. g 1). Once is not enough.\n\nAdvertising computers using the broadcast method is especially very effective because of the high demand for computers nowadays (Wheeler, p. g 1). Most of the things are being done online; for example, it is possible to shop, learn or sell things online among many other things possible to do online.\n\nMany people are therefore in need of good computers which are able to carry out different functions. It is also essential for people to have confidence in computers one is advertising. This can be possible by one offering accurate information and not just trying to lure people into buying the products for the sake of making a profit.\n\nUse of internet services can also be very effective when advertising computers. There are usually many people accessing the internet at any particular time, and hence, it is a crucial site where potential customers can be found.\n\nSince online buying and selling of products is possible, one can use this as a site for advertising. With consistency in advertising, it is elementary to increase the sale of products for any company and especially the computers which are a commodity in demand for this generation where everything is computerized.\n\nOverview of computer advertisements\n\nComputer advertisement is very imperative for anyone in the computer business. Different methods can be used to advertise, two of which have been discussed in this paper.\n\nIf both methods are used conveniently and to complement each other, the possibility of an increase in productivity of the company involved in advertising is very high. This is because for any business to grow, advertisement is an important marketing strategy.\n\nJust like other products need to be advertised to increase awareness to the public, the computers also need to be promoted. Many people interact with computers in their everyday life, and therefore any information about computers is beneficial to many people. With such information, potential customers are well equipped to make informed decisions.\n\nConclusion\n\nAdvertisement is an essential strategy in the marketing of any product. Different methods work for different products. It is vital to know which method works best for the product one intends to market. The computers for example as discussed in this paper can be advertised using print media or broadcast method or one can use both methods depending on how effective each method is when used alone.\n\nWorks Cited\n\nManagement Study Guide. Classification of Advertising, 2011. Web. < https://www.managementstudyguide.com/classification-of-advertising.htm >\n\nWheeler, Nikki. Guide to Computer Advertising and Marketing Key Terms . Santa Monica, CA: Business.com, Inc., 2011. < https://www.business.com/ >\n",
        "label": "human"
    },
    {
        "input": "How computers influence our life Essay\n\nIntroduction\n\nComputers are a common phenomenon in the lives of people in today\u2019s world. Computers are very vital especially to those people who run businesses, industries and other organizations. Today, almost everything that people engage in makes use of a computer. Take for instance, the transport sector: vehicles, trains, airplanes, and even traffic lights on our roads are controlled by computers.\n\nIn hospitals, most of the equipments use or are run by computers. Look at space exploration; it was all made possible with the advent of computer technology. In the job sector, many of the jobs require knowledge in computers because they mostly involve the use of computers.\n\nIn short, these machines have become so important and embedded in the lives of humans, they have hugely impacted on the whole society to the extent that it will be very hard to survive now, without them. This article discusses the influence of computers on the everyday life of human beings.\n\nOne can guess what will exactly happen if the world had no computers. Many of the cures found with help of computer technology would not have been developed without computer technology, meaning that many people would have died from diseases that are now curable. In the entertainment industry, many of the movies and even songs will not be in use without computers because most of the graphics used and the animations we see are only possible with the help of a computer (Saimo 1).\n\nIn the field of medicine, pharmacies, will find it hard in determining the type of medication to give to the many patients. Computers have also played a role in the development of democracy in the world. Today votes are counted using computers and this has greatly reduced incidences of vote rigging and consequently reduced conflicts that would otherwise arise from the same.\n\nAnd as we have already seen, no one would have known anything about space because space explorations become possible only with the help of computer technology. However, the use of computers has generated public discourses whereby people have emerged with different views, some supporting their use and others criticizing them (Saimo 1).\n\nHistory of computers\n\nTo better understand how computers influence the lives of people, we will have to start from the history, from their invention to the present day. Early computers did not involve complex technologies as the ones that are used today; neither did they employ the use of monitors or chips that are common today.\n\nThe early computers were not that small as those used today and they were commonly used to help in working out complex calculations in mathematics that proved tedious to be done manually. This is why the first machine was called by some as a calculator and others as a computer because it was used for making calculations.\n\nBlaise Pascal is credited with the first digital machine that could add and subtract. Many versions of calculators and computers borrowed from his ideas. And as time went by, many developed more needs, which lead to modifications to bring about new and more efficient computers (Edwards 4).\n\nPositive Impacts of computers\n\nComputer influence in the life of man became widely felt during World War II where computers were used to calculate and track the movements and also strategize the way military attacks were done (Edwards 4). It is therefore clear, that computers and its influence on man have a long history.\n\nIts invention involved hard work dedication and determination, and in the end it paid off. The world was and is still being changed by computers. Man has been able to see into the future and plan ahead because of computers. Life today has been made easier with the help of computers, although some people may disagree with this, but am sure many will agree with me.\n\nThose who disagree say that computers have taken away the role of man, which is not wrong at all, but we must also acknowledge the fact what was seen as impossible initially, become possible because of computers (Turkle 22).\n\nAs we mentioned in the introduction, computers are useful in the running of the affairs of many companies today. Companies nowadays use a lot of data that can only be securely stored with the help of computers. This data is then used in operations that are computer run. Without computers companies will find it difficult store thousands of records that are made on a daily basis.\n\nTake for instance, what will happen to a customer checking his or her balance, or one who just want to have information on transactions made. In such a case, it will take long to go through all the transactions to get a particular one.\n\nThe invention of computers made this easier; bank employees today give customers their balances, transaction information, and other services just by tapping the computer keyboard. This would not be possible without computers (Saimo 1).\n\nIn personal life\n\nToday individuals can store all information be it personal or that of a business nature in a computer. It is even made better by being able to make frequent updates and modifications to the information. This same information can be easily retrieved whenever it is needed by sending it via email or by printing it.\n\nAll this have been made possible with the use of computers. Life is easier and enjoyable, individuals now can comfortably entertain themselves at home by watching TV with their families or they can work from the comfort of their home thanks to computer technology.\n\nComputers feature in the everyday life of people. Today one can use a computer even without being aware of it: people use their credit cards when buying items from stores; this has become a common practice that few know that the transaction is processed through computer technology.\n\nIt is the computer which process customer information that is fed to it through the credit card, it detects the transaction, and it then pays the bill by subtracting the amount from the credit card. Getting cash has also been made easier and faster, an individual simply walks to an ATM machine to withdraw any amount of cash he requires. ATM machines operate using computer technology (Saimo 1).\n\nI mentioned the use of credit cards as one of the practical benefits of using computers. Today, individual do not need to physically visit shopping stores to buy items. All one needs is to be connected on the internet and by using a computer one can pay for items using the credit card.\n\nThese can then be delivered at the door step. The era where people used to queue in crowded stores to buy items, or wasting time in line waiting to buy tickets is over. Today, travelers can buy tickets and make travel arrangements via the internet at any time thanks to the advent of computer technology (Saimo 1).\n\nIn communication\n\nThrough the computer, man now has the most effective means of communication. The internet has made the world a global village. Today people carry with them phones, which are basically small computers, others carry laptops, all these have made the internet most effective and affordable medium of communication for people to contact their friends, their families, contact business people, from anywhere in the world.\n\nBusinesses are using computer technology to keep records and track their accounts and the flow of money (Lee 1). In the area of entertainment, computers have not been left behind either.\n\nAction and science fiction movies use computers to incorporated visual effects that make them look real. Computer games, a common entertainer especially to teenagers, have been made more entertaining with the use of advanced computer technology (Frisicaro et.al 1).\n\nIn Education\n\nThe education sector has also been greatly influenced by computer technology. Much of the school work is done with the aid of a computer. If students are given assignments all they have to do is search for the solution on the internet using Google. The assignments can then be neatly presented thanks to computer software that is made specifically for such purposes.\n\nToday most high schools have made it mandatory for students to type out their work before presenting it for marking. This is made possible through computers. Teachers have also found computer technology very useful as they can use it to track student performance. They use computers to give out instructions.\n\nComputers have also made online learning possible. Today teachers and students do not need to be physically present in class in order to be taught. Online teaching has allowed students to attend class from any place at any time without any inconveniences (Computers 1).\n\nIn the medical sector\n\nAnother very crucial sector in the life of man that computers has greatly influenced and continues to influence is the health sector. It was already mentioned in the introduction that hospitals and pharmacies employ the use of computers in serving people.\n\nComputers are used in pharmacies to help pharmacists determine what type and amount of medication patients should get. Patient data and their health progress are recorded using computers in many hospitals. The issue of equipment status and placement in hospitals is recorded and tracked down using computers.\n\nResearch done by scientists, doctors, and many other people in the search to find cures for many diseases and medical complications is facilitated through computer technology. Many of the diseases that were known to be dangerous such as malaria are now treatable thanks to computer interventions (Parkin 615).\n\nComputers replacing man\n\nMany of the opponents of computer technology have argued against the use of computers basing their arguments on the fact that computers are replacing man when carrying out the basic activities that are naturally human in nature.\n\nHowever, it should be noted that there are situations that call for extraordinary interventions. In many industries, machines have replaced human labor. Use of machines is usually very cheap when compared to human labor.\n\nIn addition machines give consistent results in terms of quality. There are other instances where the skills needed to perform a certain task are too high for an ordinary person to do. This is usually experienced in cases of surgery where man\u2019s intervention alone is not sufficient. However, machines that are computer operated have made complex surgeries successful.\n\nThere are also cases where the tasks that are to be performed may be too dangerous for a normal human being. Such situations have been experienced during disasters such as people being trapped underground during mining. It is usually dangerous to use people in such situations, and even where people are used, the rescue is usually delayed.\n\nRobotic machines that are computer operated have always helped in such situations and people have been saved. It is not also possible to send people in space duration space explorations, but computer machines such as robots have been effectively used to make exploration outside our world (Gupta 1).\n\nNegative impacts\n\nDespite all these good things that computers have done to humans, their opponents also have some vital points that should not just be ignored. There are many things that computers do leaving many people wondering whether they are really helping the society, or they are just being used to deprive man his God given ability to function according to societal ethics.\n\nTake for instance in the workplace and even at home; computers have permeated in every activity done by an individual thereby compromising personal privacy. Computers have been used to expose people to unauthorized access to personal information. There is some personal information, which if exposed can impact negatively to someone\u2019s life.\n\nToday the world does not care about ethics to the extent that it is very difficulty for one to clearly differentiate between what is and is not authentic or trustful. Computers have taken up every aspect of human life, from house chores in the home to practices carried out in the social spheres.\n\nThis has seen people lose their human element to machines. Industries and organizations have replaced human labor for the cheap and more effective machine labor. This means that people have lost jobs thanks to the advances made in the computer technology. Children using computers grow up with difficulties of differentiating between reality and fiction (Subrahmanyam et.al 139).\n\nPeople depend on computers to do tasks. Students generate solutions to assignments using computers; teachers on the other hand use computers to mark assignments. Doctors in hospitals depend on machines to make patient diagnoses, to perform surgeries and to determine type of medications (Daley 56).\n\nIn the entertainment industry, computer technology has been used to modify sound to make people think that person singing is indeed great, but the truth of the matter is that it is simply the computer. This has taken away the really function of a musician in the music sector.\n\nIn the world of technology today, we live as a worried lot. The issue of hacking is very common and even statistics confirm that huge amounts of money are lost every year through hacking. Therefore, as much as people pride themselves that they are computer literate, they deeply worried that they may be the next victim to practices such as hacking (Bynum 1).\n\nConflict with religious beliefs\n\nThere is also the problem of trying to imitate God. It is believed that in 20 years time, man will come up with another form of life, a man made being. This will not only affect how man will be viewed in terms of his intelligence, but it will also break the long held view that God is the sole provider of life.\n\nComputers have made it possible to create artificial intelligence where machines are given artificial intelligence so that they can behave and act like man. This when viewed from the religious point of view creates conflicts in human beliefs.\n\nIt has been long held that man was created in the image of God. Creating a machine in the image of money will distort the way people conceive of God. Using artificial methods to come up with new forms of life with man like intelligence will make man equate himself to God.\n\nThis carries the risk of changing the beliefs that mankind has held for millions of years. If this happens, the very computer technology will help by the use of mass media to distribute and convince people to change their beliefs and conceptions of God (Krasnogor 1).\n\nConclusion\n\nWe have seen that computer have and will continue to influence our lives. The advent of the computers has changed man as much as it has the world he lives in.\n\nIt is true that many of the things that seemed impossible have been made possible with computer technology. Medical technologies have led to discoveries in medicine, which have in turn saved many lives. Communication is now easy and fast. The world has been transformed into a virtual village.\n\nComputers have made education accessible to all. In the entertainment sector, people are more satisfied. Crime surveillance is better and effective. However, we should be ware not to imitate God. As much as computers have positively influenced our lives, it is a live bomb that is waiting to explode.\n\nWe should tread carefully not to be overwhelmed by its sophistication (Computers 1). Many technologies have come with intensities that have seen them surpass their productivity levels thereby destroying themselves in the process. This seems like one such technology.\n\nWorks Cited\n\nBynum, Terrell. Computer and Information Ethics . Plato, 2008. Web.\n\nComputers. Institutional Impacts . Virtual Communities in a Capitalist World, n.d. Web.\n\nDaley, Bill. Computers Are Your Future: Introductory. New York: Prentice, 2007. Print.\n\nEdwards, Paul. From \u201cImpact\u201d to Social Process . Computers in Society and Culture,1994. Web.\n\nFrisicaro et.al. So What\u2019s the Problem? The Impact of Computers, 2011. Web.\n\nGupta, Satyandra. We, robot: What real-life machines can and can\u2019t do . Science News, 2011. Web.\n\nKrasnogor, Ren. Advances in Artificial Life. Impacts on Human Life. n.d. Web.\n\nLee, Konsbruck. Impacts of Information Technology on Society in the new Century . Zurich. Web.\n\nParkin, Andrew. Computers in clinical practice . Applying experience from child psychiatry. 2004. Web.\n\nSaimo. The impact of computer technology in Affect human life . Impact of Computer, 2010. Web.\n\nSubrahmanyam et al. The Impact of Home Computer Use on Children\u2019s Activities and Development. Princeton, 2004. Web.\n\nTurkle, Sherry. The second self : Computers and the human spirit, 2005. Web.\n",
        "label": "human"
    },
    {
        "input": "The Impact of Computer-Based Technologies on Business Communication Report\n\nExecutive Summary\n\nThe impact of computer-based communication can be considered as a revolution of communication. Computer based communications are currently the most common tools of communication not only in organizations, but also in social lives. Many people spend most of their time on a computer working, studying or having fun by playing various computer games.\n\nBlogs and Facebook are some of computer-based communication tools, which have had a great impact on communication. Blogs are used as tools through which debate over various issues is conducted. As a result, they allow individuals to participate in debates on subjects of their interest. In business, customer, employees and other groups can participate in debates.\n\nThrough blogs, market information related to various issues such as taste, customer preference, competition and customer satisfaction can be obtained. They are also important public relation tools where public relation officers can respond to issues arising from an organization.\n\nFacebook is the most popular social networking tool at present. Through facebook, individuals can share their social lives such as photos, birthday celebrations, updates and events. Facebook has provided an affordable, fast and effective tool through which communication is relayed.\n\nHowever, it has been noted that long hours spent on facebook and blogs can affect an individual\u2019s productivity adversely, however it is clear that if well managed, these tools can enhance the performance of an organization greatly. Some of the areas where they can be effectively be used is in advertisement, public relations and organizational communication.\n\nIntroduction\n\nDevelopment in computer and internet technology has had a significant impact on communication. Currently, computer based communication has become very popular. This is evident in its increased adoption by many individuals and organizations on daily basis (Perkins, 2008, p. 44). Many people spend most of their time on the computer working, studying or having fun by playing computer games.\n\nComputer technology has increasingly been integrated into people\u2019s cultures. For example, carrying a laptop has become a norm rather that an exception. Consequently, the computer has become a major tool for communication. Today, using email, instant messaging, Facebook, blogs and other computer-based communications has become very common in that they are almost replacing other conventional means of communication (Kent, 2008, p. 32).\n\nFacebook and blogs have become very common communication tools. This report addresses the impact of these tools on communication. In addition, the report focuses on how these tools are currently being used, the challenges experienced when using them and what can be done in order to effectively address the challenges observed.\n\nEnhanced Business communication through Computer-based Technologies\n\nImportance of Blogs to Business Communication\n\nBlogging involves development of blogs or web logs which are internet-based journals that can be accessed by customers for vital information about an organization.\n\nThe impact of blogging in business communication is based on their ability to replace the conventional means of communicating with customers by sending them newsletters through the post office (Cox, Martinez & Quinlan, 2008, p. 4). The main objective of blogs is to support active debates. Blogs allow people from different parts of the world to contribute to various debatable issues of interest.\n\nIn this way, a business organization can pose questions to their clients and in turn get a feedback particularly useful in product development and investment ideas (Cox et al, 2008, p. 5). Additionally, through blogs, customers can participate in debates concerning an organization\u2019s operations, products or services. However, in order to catch the attention of regular bloggers, the web log or blog must be frequently updated.\n\nFrom the above discussion, it is evident that blogs have two main roles in business. These include market research and public relation. This is because market information that relate to taste and preference, competition and customer satisfaction is accessible through blogs.\n\nBy providing blogs where customers can raise their issues, an organization can succeed in customer relations management. Blogs allow public relation officers in an organization to respond to issues raised in subsequent debates. In addition, their ability to facilitate debates with former employees and loyal customers, they can help create a positive image of an organization (Cox et al, 2008, p. 5).\n\nAnother important aspect of blogging entails customer-generated advertising, which can allow a business organization to communicate directly with its clients through blog entries such as opinions, reviews, discussions, and feedbacks.\n\nThe Importance of Facebook to Business Communication\n\nFacebook is one of the most popular social networking tools among college students and businesspersons. Currently, Facebook allows subscription from all over the world thus connecting millions of people globally. Through Facebook, people share their personal and business posts on a daily basis (Pannunzio & Nelson, 2008, p. 8). Facebook has thus succeeded in creating a virtual community where individuals can participate as family members. In addition, Facebook support live chats where friends can chat freely.\n\nFurthermore, Facebook consists of useful applications such as MyOffice, which allows users to communicate business posts, news, and ideas to other users at no cost. This is a great business opportunity for organizations looking forward to reach out to their customers on a worldwide platform (Pannunzio & Nelson, 2008, p. 8).\n\nImpended Business Communication through Computer-based Technologies\n\nThe negative effects of Blogging on Business Communication\n\nBlogs pose both ethical and business threats to many business organizations. This is because they lack clear guidelines regarding the amount and type of information that an employee or organizations can share with customers. In this case, many organizations and employees have found themselves giving out more information about their practices than required (Cox et al, 2008, p. 6).\n\nAnother problem of blogging is related to misguided or malevolent corporation/ employee weblogs. This practice has led to defamation of corporations\u2019 names including their artistically developed brands.\n\nIn addition, employees can be fired or reprimanded for posting damaging information about their organizations. For instance, Google, which forbids external blogging for its employees, fired one of its employees for displaying information about the corporation on an external blog.\n\nAnother aspect of blogging which can lead to a damaged organization or brand name is astroturfing. This entails marketing campaigns comprising of paid participants with the aim of deceiving the public. If discovered by the public, the organization stands to loose trust from customers and it may damage the credibility of its blogs (Cox et al., 2008, p. 12).\n\nThe negative effects of Facebook on Business Communication\n\nAs important as it may seem, Facebook has a variety of negative impacts on business communication. Communication through social networks disrupts normal social interactions between business organizations and their clients. Therefore, this derails social development especially among the young generations (Pannunzio & Nelson, 2008, p. 9).\n\nIn addition, lack of normal social interactions such as face-to-face communications can undermine basic communication skills such as negotiation and bargaining skills, which are important in business communication. Moreover, social networks consume a large percentage of employees\u2019 time and organizational resources thereby lowering productivity, efficiency, and performance (Pannunzio & Nelson, 2008, p. 10).\n\nFurthermore, the probability of giving out personal and corporate information is higher in social networks than is the case with other conventional platforms of interaction. It is also argued that the level of office gossip is bound to be higher through social networks than in real world communication (Pannunzio & Nelson, 2008, p. 8).\n\nThe solutions/ risk management strategies\n\nBefore joining any social network, it is advisable to consider a number of factors. Experts recommend that these factors should be considered in four major steps which include investigating, observing, joining in and moving up.\n\nHowever, for many business corporations, it is important to follow certain guidelines which govern the development of internal and external blogs. In addition, these corporations should design plans aimed at assessing the applicability and the level of accessibility of the computer-based technologies amongst their employees (Kent, 2008, p. 35).\n\nOn the other hand, because of the sensitivity of the information displayed on social networks, there is the need to select one employee who can be trusted with the corporation\u2019s internal secrets and with the responsibility of communicating with others and the general public. Finally, business organizations should attempt to monitor their online presence in order to track blog entries that may be malicious and damaging to its brand names (Kent, 2008, p. 38).\n\nConclusion\n\nThis report explores the impact of computer-based technologies on business communication and the possible solutions to the identified impediments. The impact of computer-based communication is evident in various ways. For example, blogs, Facebook and other computer-based communications have become very common in various areas.\n\nBlogs and facebook have become preferred tools in debate, communication and social life. Blogs are used as tools through which debate over various business issues is conducted. On the other hand, Facebook has become an important tool in social life.\n\nBlogs and Facebook can be used for the benefit of an organization. Blogs can be used to facilitate positive debates which lead to product development and service improvement. For example, they can be used by employees, customers and other groups to debate issues of common interest. Through blogs, an organization can be able to keep track of issues affecting its operations.\n\nThis will enable the organization to respond effectively. On the other hand, Facebook can be used in various ways for the benefit of an organization. For example, it is used to conduct advertisements, public relations management and communication. However, the two computer-based technologies can also act as impediments to successful business communication. Therefore, there is the need to strike balance between social networking and business practices.\n\nReference List\n\nCox, J.L., Martinez, E.R. & Quinlan, K.B. 2008. Blogs and the corporation: managing the risk, reaping the benefits. Journal Business Strategy . Vol. 29, issue 3, pp. 4-12.\n\nKent, M.L. 2008. Critical analysis of blogging in public relations. Public Relations Review . Vol. 34, issue 1, pp. 32-40.\n\nPannunzio, C. & Nelson, C. 2008. Leverage the power of social media. Journal of Financial Planning . Vol. 4, issue 1, pp. 1-10.\n\nPerkins, B. 2008. The Pitfalls of Social Networking. Computerworld . Vol. 42, issue 7, pp. 40-44.\n",
        "label": "human"
    }
]