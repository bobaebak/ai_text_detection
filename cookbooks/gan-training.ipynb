{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, RobertaForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/data/bobae/ai_text_detection\")\n",
    "# sys.path.append(\"/Users/bobaebak/git/ai_text_detection\")\n",
    "\n",
    "from utils.cuda_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 0.00 MB\n",
      "Maximum GPU memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 0: NVIDIA GeForce RTX 2080 Ti\n",
      "cuda 1: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "print_gpu_devie_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "if num_gpus>1:\n",
    "    device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Total GPU memory: 10.75 GB\n"
     ]
    }
   ],
   "source": [
    "print_total_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4894 MiB |   9499 MiB | 342616 MiB | 337721 MiB |\n",
      "|       from large pool |   4892 MiB |   9496 MiB | 336061 MiB | 331169 MiB |\n",
      "|       from small pool |      2 MiB |     98 MiB |   6555 MiB |   6552 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4894 MiB |   9499 MiB | 342616 MiB | 337721 MiB |\n",
      "|       from large pool |   4892 MiB |   9496 MiB | 336061 MiB | 331169 MiB |\n",
      "|       from small pool |      2 MiB |     98 MiB |   6555 MiB |   6552 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4885 MiB |   9489 MiB | 341880 MiB | 336994 MiB |\n",
      "|       from large pool |   4882 MiB |   9486 MiB | 335339 MiB | 330456 MiB |\n",
      "|       from small pool |      2 MiB |     98 MiB |   6540 MiB |   6538 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   7032 MiB |  10770 MiB |  12270 MiB |   5238 MiB |\n",
      "|       from large pool |   7028 MiB |  10766 MiB |  12168 MiB |   5140 MiB |\n",
      "|       from small pool |      4 MiB |    102 MiB |    102 MiB |     98 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   2137 MiB |   2826 MiB | 259579 MiB | 257442 MiB |\n",
      "|       from large pool |   2135 MiB |   2825 MiB | 251967 MiB | 249831 MiB |\n",
      "|       from small pool |      1 MiB |     32 MiB |   7612 MiB |   7610 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1328    |    1529    |  121622    |  120294    |\n",
      "|       from large pool |     692    |     822    |   11210    |   10518    |\n",
      "|       from small pool |     636    |     767    |  110412    |  109776    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1328    |    1529    |  121622    |  120294    |\n",
      "|       from large pool |     692    |     822    |   11210    |   10518    |\n",
      "|       from small pool |     636    |     767    |  110412    |  109776    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     189    |     239    |     240    |      51    |\n",
      "|       from large pool |     187    |     188    |     189    |       2    |\n",
      "|       from small pool |       2    |      51    |      51    |      49    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      91    |      98    |   54162    |   54071    |\n",
      "|       from large pool |      58    |      68    |    2067    |    2009    |\n",
      "|       from small pool |      33    |      61    |   52095    |   52062    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load json data file and convert to dataframe\n",
    "- real -> human: 1\n",
    "- fake -> ai: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human data\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/human/tofel.json', \"r\") as f:\n",
    "    h_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads an arxiv dataset\n",
    "with open('../dataset/human/arxiv.json', \"r\") as f:\n",
    "    h_arxiv_dataset = json.load(f)\n",
    "\n",
    "# loads student essay\n",
    "with open('../dataset/human/student_essay.json', \"r\") as f:\n",
    "    h_essay_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay\n",
    "with open('../dataset/human/student_cs_essay.json', \"r\") as f:\n",
    "    h_essay_cs_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt data\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/ai/gpt2medium_tofel.json', \"r\") as f:\n",
    "    gpt_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads an arxiv dataset\n",
    "with open('../dataset/ai/gpt2medium_arxiv.json', \"r\") as f:\n",
    "    gpt_arxiv_dataset = json.load(f)\n",
    "\n",
    "# loads student essay\n",
    "with open('../dataset/ai/gpt2medium_essay.json', \"r\") as f:\n",
    "    gpt_essay_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay\n",
    "with open('../dataset/ai/gpt2medium_essay_cs.json', \"r\") as f:\n",
    "    gpt_essay_cs_dataset = json.load(f)\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/ai/gpt35_tofel.json', \"r\") as f:\n",
    "    gpt_35_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay\n",
    "with open('../dataset/ai/gpt35_essay_cs.json', \"r\") as f:\n",
    "    gpt_35_essay_cs_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_dataset = []\n",
    "for i in [h_tofel_dataset, h_arxiv_dataset, h_essay_dataset, h_essay_cs_dataset]:\n",
    "    h_dataset.extend(i)\n",
    "\n",
    "len(h_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_dataset = []\n",
    "for i in [gpt_35_tofel_dataset, gpt_arxiv_dataset, gpt_essay_dataset, gpt_35_essay_cs_dataset]:\n",
    "    gpt_dataset.extend(i)\n",
    "\n",
    "len(gpt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 387)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_essay_cs_dataset), len(gpt_35_essay_cs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_text</th>\n",
       "      <th>h_label</th>\n",
       "      <th>h_target</th>\n",
       "      <th>m_text</th>\n",
       "      <th>m_label</th>\n",
       "      <th>m_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I prefer to cook at home. First of all, it is ...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>I enjoy cooking at home for several reasons. F...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The place I would like to visit most is the ou...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>The place I would most like to visit is outer ...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I imagine my life ten years in the future to b...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>I envision my life a decade from now as comple...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Personally, I would like to say that the schoo...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Personally, I have to say that the school that...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I believe that I will follow my interest. I'm ...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm confident I will pursue my passion. I'm no...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>Computer Viruses: Spreading, Multiplying and D...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Viruses: Spreading, Multiplying, and ...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>Purchasing or Leasing Computer Equipment: Adva...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Purchasing vs Leasing Computer Equipment: Pros...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>Print and Broadcast Computer Advertisements Es...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Table of Contents\\n 1. Print advertisement\\n 2...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>How computers influence our life Essay\\n\\nIntr...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Influence of Computers on Our Lives\\n\\nIntrodu...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>The Impact of Computer-Based Technologies on B...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Executive Summary\\n\\nThe impact of computer-ba...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 h_text h_label  h_target  \\\n",
       "0     I prefer to cook at home. First of all, it is ...   human         1   \n",
       "1     The place I would like to visit most is the ou...   human         1   \n",
       "2     I imagine my life ten years in the future to b...   human         1   \n",
       "3     Personally, I would like to say that the schoo...   human         1   \n",
       "4     I believe that I will follow my interest. I'm ...   human         1   \n",
       "...                                                 ...     ...       ...   \n",
       "2473  Computer Viruses: Spreading, Multiplying and D...   human         1   \n",
       "2474  Purchasing or Leasing Computer Equipment: Adva...   human         1   \n",
       "2475  Print and Broadcast Computer Advertisements Es...   human         1   \n",
       "2476  How computers influence our life Essay\\n\\nIntr...   human         1   \n",
       "2477  The Impact of Computer-Based Technologies on B...   human         1   \n",
       "\n",
       "                                                 m_text m_label  m_target  \n",
       "0     I enjoy cooking at home for several reasons. F...      ai         0  \n",
       "1     The place I would most like to visit is outer ...      ai         0  \n",
       "2     I envision my life a decade from now as comple...      ai         0  \n",
       "3     Personally, I have to say that the school that...      ai         0  \n",
       "4     I'm confident I will pursue my passion. I'm no...      ai         0  \n",
       "...                                                 ...     ...       ...  \n",
       "2473  Computer Viruses: Spreading, Multiplying, and ...      ai         0  \n",
       "2474  Purchasing vs Leasing Computer Equipment: Pros...      ai         0  \n",
       "2475  Table of Contents\\n 1. Print advertisement\\n 2...      ai         0  \n",
       "2476  Influence of Computers on Our Lives\\n\\nIntrodu...      ai         0  \n",
       "2477  Executive Summary\\n\\nThe impact of computer-ba...      ai         0  \n",
       "\n",
       "[2478 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = {\n",
    "    \"h_text\": [item['input'] for item in h_dataset],\n",
    "    \"h_label\": [item['label'] for item in h_dataset],\n",
    "    \"h_target\": np.ones(len(h_dataset), dtype=int),\n",
    "    \"m_text\": [item['input'] for item in gpt_dataset],\n",
    "    \"m_label\": [item['label'] for item in gpt_dataset],\n",
    "    \"m_target\": np.zeros(len(gpt_dataset), dtype=int),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dct)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator (Paraphraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, model_name=\"t5-large\", lr=0.1):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # hyperparameters\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    \n",
    "    def generate_text(self, input):\n",
    "        # Paraphrase AI text make it better\n",
    "        \n",
    "        arguments = {\"padding\": True, \"truncation\": True, \"return_tensors\": \"pt\"}\n",
    "        # encode and obtain ids\n",
    "        encoded_inputs = self.tokenizer(input, **arguments)\n",
    "        encoded_inputs_input_ids = encoded_inputs.input_ids.to(device, dtype=torch.long)\n",
    "        \n",
    "        # generate\n",
    "        outputs = self.model.generate(encoded_inputs_input_ids,\n",
    "                                    max_new_tokens=200,\n",
    "                                    # num_beams=5,\n",
    "                                    # num_return_sequences=1,\n",
    "                                    # temperature=1.5,\n",
    "                                    # num_beam_groups=5,\n",
    "                                    # diversity_penalty=2.0,\n",
    "                                    # no_repeat_ngram_size=2,\n",
    "                                    # early_stopping=True,\n",
    "                                    # length_penalty=2.0\n",
    "                                    )\n",
    "\n",
    "        sequences_list = outputs.tolist()\n",
    "\n",
    "        # decode \n",
    "        decoded_outputs = self.tokenizer.batch_decode(sequences_list, skip_special_tokens=True)\n",
    "        return decoded_outputs\n",
    "\n",
    "\n",
    "    def train_generator(self, fake_texts, discriminator, criterion):\n",
    "        labels = torch.ones(len(fake_texts)).to(device)\n",
    "\n",
    "        # fake_inputs = discriminator.tokenizer.encode_plus(\n",
    "        #     fake_texts,\n",
    "        #     None,\n",
    "        #     add_special_tokens=True,\n",
    "        #     pad_to_max_length=True,\n",
    "        #     max_length=512,\n",
    "        #     return_token_type_ids=True\n",
    "        # )\n",
    "        fake_inputs = discriminator.tokenizer(fake_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        fake_inputs_input_ids = fake_inputs.input_ids.to(device, dtype=torch.long)\n",
    "        fake_inputs_attention_mask = fake_inputs.attention_mask.to(device, dtype=torch.long)\n",
    "        \n",
    "        fake_outputs = discriminator.model(fake_inputs_input_ids, fake_inputs_attention_mask)\n",
    "\n",
    "        loss = criterion(fake_outputs.logits[:, 1], labels)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        # return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, model_name=\"roberta-base\", lr=0.1):\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def classify_text(self, input):\n",
    "        # given text, discriminate whether the given text is from human or ai\n",
    "        \"\"\"\n",
    "        Assume the label y for the true data is \n",
    "            1 -> human\n",
    "            0 -> AI\n",
    "        \"\"\"\n",
    "\n",
    "        arguments = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":512, \"return_token_type_ids\": True, \"return_tensors\": \"pt\"}\n",
    "        encoded_inputs = self.tokenizer.encode_plus(input, None, **arguments)\n",
    "        ids = encoded_inputs['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = encoded_inputs['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = encoded_inputs[\"token_type_ids\"].to(device, dtype = torch.long)\n",
    "        # targets = data['target'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = self.model(ids, mask, token_type_ids)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        # predicted_class = torch.argmax(probabilities).item()\n",
    "        return logits, probabilities\n",
    "    \n",
    "    def train_discriminator(self, real_texts, fake_texts, criterion):\n",
    "        real_labels = torch.ones(len(real_texts)).to(device)\n",
    "        fake_labels = torch.zeros(len(fake_texts)).to(device)\n",
    "\n",
    "        real_inputs = self.tokenizer(real_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        fake_inputs = self.tokenizer(fake_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        real_inputs_input_ids = real_inputs.input_ids.to(device, dtype=torch.long)\n",
    "        real_inputs_attention_mask = real_inputs.attention_mask.to(device, dtype=torch.long)\n",
    "        fake_inputs_input_ids = fake_inputs.input_ids.to(device, dtype=torch.long)\n",
    "        fake_inputs_attention_mask = fake_inputs.attention_mask.to(device, dtype=torch.long)\n",
    "\n",
    "        real_outputs = self.model(real_inputs_input_ids, real_inputs_attention_mask)\n",
    "        fake_outputs = self.model(fake_inputs_input_ids, fake_inputs_attention_mask)\n",
    "\n",
    "        real_loss = criterion(real_outputs.logits[:, 1], real_labels)\n",
    "        fake_loss = criterion(fake_outputs.logits[:, 0], fake_labels)\n",
    "        loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_epochs=2\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.model.to('cpu')\n",
    "discriminator = discriminator.model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Assume you have a list of real texts and corresponding prompts\n",
    "    real_texts = [item['input'] for item in h_tofel_dataset[:2]]\n",
    "    prompts = [\"Paraphrase: \"+item['input'] for item in gpt_tofel_dataset[:2]]\n",
    "    fake_texts = generator.generate_text(prompts)\n",
    "\n",
    "    # Train Discriminator\n",
    "    d_loss = discriminator.train_discriminator(real_texts, prompts, criterion)\n",
    "    print(f\"Epoch {epoch + 1}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "    # Train Generator\n",
    "    g_loss = generator.train_generator(fake_texts, discriminator, criterion)\n",
    "    print(f\"Epoch {epoch + 1}, Generator Loss: {g_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.h_text = df['h_text']\n",
    "        self.h_target = df['h_target']\n",
    "        self.m_text = df['m_text']\n",
    "        self.m_target = df['m_target']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        h_text = str(self.h_text[index])\n",
    "        h_text = \" \".join(h_text.split())\n",
    "\n",
    "        m_text = str(self.m_text[index])\n",
    "        m_text = \" \".join(m_text.split())\n",
    "\n",
    "        return {\n",
    "            'real_texts': h_text,\n",
    "            # 'real_target': torch.tensor(self.h_target[index], dtype=torch.float), \n",
    "            'fake_texts': m_text,\n",
    "            # 'fake_target': torch.tensor(self.m_target[index], dtype=torch.float), \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2478, 6)\n",
      "TRAIN Dataset: (1982, 6)\n",
      "TEST Dataset: (496, 6)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=df.sample(frac=train_size, random_state=42)\n",
    "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "#### Dataset\n",
    "training_set = GANDataset(train_data)\n",
    "testing_set = GANDataset(test_data)\n",
    "\n",
    "#### DataLoader \n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, generator, discriminator, training_loader, testing_loader):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        with tqdm(training_loader, desc=f\"Epoch {epoch}\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                real_texts = data['real_texts']\n",
    "                prompts = [\"\".join(\"Paraphrase: \"+real_text) for real_text in real_texts]\n",
    "                fake_texts = generator.generate_text(prompts)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                d_loss = discriminator.train_discriminator(real_texts, prompts, criterion)\n",
    "                print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "                # Train Generator\n",
    "                g_loss = generator.train_generator(fake_texts, discriminator, criterion)\n",
    "                print(f\"Epoch {epoch}, Generator Loss: {g_loss}\")\n",
    "\n",
    "                d_losses.append(d_loss)\n",
    "                g_losses.append(g_loss)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                tepoch.set_postfix(discriminator_loss=d_loss, generator_loss=g_loss)\n",
    "\n",
    "                if d_loss == 0 and g_loss == 0:\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epochs, generator, discriminator, training_loader, testing_loader):\n",
    "\n",
    "#     for epoch in range(1, epochs+1):\n",
    "#         for data in training_loader:\n",
    "#             real_texts = data['real_texts']\n",
    "#             prompts = [\"\".join(\"Paraphrase: \"+real_text) for real_text in real_texts]\n",
    "#             fake_texts = generator.generate_text(prompts)\n",
    "            \n",
    "#             # Train Discriminator\n",
    "#             d_loss = discriminator.train_discriminator(real_texts, prompts, criterion)\n",
    "#             print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "#             # Train Generator\n",
    "#             g_loss = generator.train_generator(fake_texts, discriminator, criterion)\n",
    "#             print(f\"Epoch {epoch}, Generator Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                | 0/991 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Epoch 1:   0%|                       | 1/991 [00:05<1:24:56,  5.15s/it, discriminator_loss=0.696, generator_loss=9.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Discriminator Loss: 0.6956241130828857\n",
      "Epoch 1, Generator Loss: 9.08476448059082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                    | 2/991 [00:07<1:01:39,  3.74s/it, discriminator_loss=9.32, generator_loss=1.12e-22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Discriminator Loss: 9.318075180053711\n",
      "Epoch 1, Generator Loss: 1.1216403003666852e-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                | 3/991 [00:11<1:05:34,  3.98s/it, discriminator_loss=5.61e-23, generator_loss=1.16e-40]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Discriminator Loss: 5.608201501833426e-23\n",
      "Epoch 1, Generator Loss: 1.1565476745458445e-40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.58 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# with gpt paraphrased data, epoch=10\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/gan_models/20240528_v1_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(epochs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, generator, discriminator, training_loader, testing_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m real_texts \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal_texts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParaphrase: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mreal_text) \u001b[38;5;28;01mfor\u001b[39;00m real_text \u001b[38;5;129;01min\u001b[39;00m real_texts]\n\u001b[0;32m---> 10\u001b[0m fake_texts \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train Discriminator\u001b[39;00m\n\u001b[1;32m     13\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_discriminator(real_texts, prompts, criterion)\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mGenerator.generate_text\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     17\u001b[0m encoded_inputs_input_ids \u001b[38;5;241m=\u001b[39m encoded_inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# generate\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_inputs_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# num_beams=5,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# num_return_sequences=1,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# temperature=1.5,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# num_beam_groups=5,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# diversity_penalty=2.0,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# no_repeat_ngram_size=2,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# early_stopping=True,\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# length_penalty=2.0\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m sequences_list \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# decode \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/generation/utils.py:1597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   1592\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39mpad_token_id, generation_config\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m   1593\u001b[0m     )\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1597\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/generation/utils.py:523\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    521\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    522\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 523\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1094\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         output_attentions,\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:687\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    697\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:594\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    592\u001b[0m ):\n\u001b[1;32m    593\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 594\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:544\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    541\u001b[0m         position_bias \u001b[38;5;241m=\u001b[39m position_bias[:, :, \u001b[38;5;241m-\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) :, :]\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m         position_bias \u001b[38;5;241m=\u001b[39m \u001b[43mposition_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruned_heads:\n\u001b[1;32m    547\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(position_bias\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.58 GiB. GPU "
     ]
    }
   ],
   "source": [
    "train(epochs, generator, discriminator, training_loader, testing_loader)\n",
    "\n",
    "# with gpt paraphrased data, epoch=10\n",
    "file_name = \"\".join('../models/gan_models/20240528_v1_epoch'+str(epochs)+'.pth')\n",
    "torch.save(model, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(discriminator, generator, data_iter, num_epochs, latent_dim, data):\n",
    "#     loss = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "#     for w in discriminator.model.parameters():\n",
    "#         nn.init.normal_(w, 0, 0.02)\n",
    "#     for w in generator.model.parameters():\n",
    "#         nn.init.normal_(w, 0, 0.02)\n",
    "\n",
    "#     animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "#                             xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "#                             legend=['discriminator', 'generator'])\n",
    "#     animator.fig.subplots_adjust(hspace=0.3)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Train one epoch\n",
    "#         timer = d2l.Timer()\n",
    "#         metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "#         for (X,) in data_iter:\n",
    "#             batch_size = X.shape[0]\n",
    "#             Z = torch.normal(0, 1, size=(batch_size, latent_dim))\n",
    "#             metric.add(discriminator.update_model(X, Z, generator, loss),\n",
    "#                        generator.update_model(Z, discriminator, loss),\n",
    "#                        batch_size)\n",
    "#         # Visualize generated examples\n",
    "#         Z = torch.normal(0, 1, size=(100, latent_dim))\n",
    "#         fake_X = generator(Z).detach().numpy()\n",
    "#         animator.axes[1].cla()\n",
    "#         animator.axes[1].scatter(data[:, 0], data[:, 1])\n",
    "#         animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n",
    "#         animator.axes[1].legend(['real', 'generated'])\n",
    "#         # Show the losses\n",
    "#         loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]\n",
    "#         animator.add(epoch + 1, (loss_D, loss_G))\n",
    "#     print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "#           f'{metric[2] / timer.stop():.1f} examples/sec')\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaliGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.name = 'vanilla'\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp, hidden, need_hidden=False):\n",
    "        \"\"\"\n",
    "        Embeds input and applies LSTM\n",
    "        :param inp: batch_size * seq_len\n",
    "        :param hidden: (h, c)\n",
    "        :param need_hidden: if return hidden, use for sampling\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n",
    "        if len(inp.size()) == 1:\n",
    "            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
    "\n",
    "        out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n",
    "        out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n",
    "        # out = self.temperature * out  # temperature\n",
    "        pred = self.softmax(out)\n",
    "\n",
    "        if need_hidden:\n",
    "            return pred, hidden\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "    def sample(self, num_samples, batch_size, start_letter=cfg.start_letter):\n",
    "        \"\"\"\n",
    "        Samples the network and returns num_samples samples of length max_seq_len.\n",
    "        :return samples: num_samples * max_seq_length (a sampled sequence in each row)\n",
    "        \"\"\"\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n",
    "\n",
    "        # Generate sentences with multinomial sampling strategy\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size)\n",
    "            if self.gpu:\n",
    "                inp = inp.cuda()\n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                out, hidden = self.forward(inp, hidden, need_hidden=True)  # out: batch_size * vocab_size\n",
    "                next_token = torch.multinomial(torch.exp(out), 1)  # batch_size * 1 (sampling from each row)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token.view(-1)\n",
    "                inp = next_token.view(-1)\n",
    "        samples = samples[:num_samples]\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if cfg.gen_init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif cfg.gen_init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif cfg.gen_init == 'truncated_normal':\n",
    "                    truncated_normal_(param, std=stddev)\n",
    "\n",
    "    def init_oracle(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                torch.nn.init.normal_(param, mean=0, std=1)\n",
    "\n",
    "    def init_hidden(self, batch_size=cfg.batch_size):\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "\n",
    "        if self.gpu:\n",
    "            return h.cuda(), c.cuda()\n",
    "        else:\n",
    "            return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, gpu=False,\n",
    "                 dropout=0.2):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Get final predictions of discriminator\n",
    "        :param inp: batch_size * seq_len\n",
    "        :return: pred: batch_size * 2\n",
    "        \"\"\"\n",
    "        feature = self.get_feature(inp)\n",
    "        pred = self.feature2out(self.dropout(feature))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def get_feature(self, inp):\n",
    "        \"\"\"\n",
    "        Get feature vector of given sentences\n",
    "        :param inp: batch_size * max_seq_len\n",
    "        :return: batch_size * feature_dim\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n",
    "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if cfg.dis_init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif cfg.dis_init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif cfg.dis_init == 'truncated_normal':\n",
    "                    truncated_normal_(param, std=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaliGAN_G(LSTMGenerator):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n",
    "        super(MaliGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n",
    "        self.name = 'maligan'\n",
    "\n",
    "    def adv_loss(self, inp, target, reward):\n",
    "        \"\"\"\n",
    "        Returns a MaliGAN loss\n",
    "\n",
    "        :param inp: batch_size x seq_len, inp should be target with <s> (start letter) prepended\n",
    "        :param target: batch_size x seq_len\n",
    "        :param reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding sentence)\n",
    "        :return loss: policy loss\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inp.size()\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        out = self.forward(inp, hidden).view(batch_size, self.max_seq_len, self.vocab_size)\n",
    "        target_onehot = F.one_hot(target, self.vocab_size).float()  # batch_size * seq_len * vocab_size\n",
    "        pred = torch.sum(out * target_onehot, dim=-1)  # batch_size * seq_len\n",
    "        loss = -torch.sum(pred * reward)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaliGAN_D(CNNDiscriminator):\n",
    "    def __init__(self, embed_dim, vocab_size, padding_idx, gpu=False, dropout=0.25):\n",
    "        super(MaliGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx, gpu,\n",
    "                                        dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, df, tokenizer_G, tokenizer_D, max_len):\n",
    "        self.tokenizer_G = tokenizer_G\n",
    "        self.tokenizer_D = tokenizer_D\n",
    "        self.max_len = max_len\n",
    "        self.h_text = df['h_text']\n",
    "        self.h_target = df['h_target']\n",
    "        self.m_text = df['m_text']\n",
    "        self.m_target = df['m_target']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h_text)\n",
    "\n",
    "    def set_human(self, index):\n",
    "        text = str(self.h_text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "         # generator inputs\n",
    "        arguments_G = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_G = self.tokenizer_G.encode_plus(text, None, **arguments_G)\n",
    "\n",
    "        # discriminator inputs\n",
    "        arguments_D = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_D = self.tokenizer_D.encode_plus(text, None, **arguments_D)\n",
    "\n",
    "        return {\n",
    "            'h_ids_G': torch.tensor(inputs_G['input_ids'], dtype=torch.long),\n",
    "            'h_mask_G': torch.tensor(inputs_G['attention_mask'], dtype=torch.long),\n",
    "            'h_ids_D': torch.tensor(inputs_D['input_ids'], dtype=torch.long),\n",
    "            'h_mask_D': torch.tensor(inputs_D['attention_mask'], dtype=torch.long),\n",
    "            'h_token_type_ids_D': torch.tensor(inputs_D[\"token_type_ids\"], dtype=torch.long),\n",
    "            'h_target': torch.tensor(self.h_target[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def set_ai(self, index):\n",
    "        text = str(self.m_text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "         # generator inputs\n",
    "        arguments_G = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_G = self.tokenizer_G.encode_plus(text, None, **arguments_G)\n",
    "\n",
    "        # discriminator inputs\n",
    "        arguments_D = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_D = self.tokenizer_D.encode_plus(text, None, **arguments_D)\n",
    "\n",
    "        return {\n",
    "            'm_ids_G': torch.tensor(inputs_G['input_ids'], dtype=torch.long),\n",
    "            'm_mask_G': torch.tensor(inputs_G['attention_mask'], dtype=torch.long),\n",
    "            'm_ids_D': torch.tensor(inputs_D['input_ids'], dtype=torch.long),\n",
    "            'm_mask_D': torch.tensor(inputs_D['attention_mask'], dtype=torch.long),\n",
    "            'm_token_type_ids_D': torch.tensor(inputs_D[\"token_type_ids\"], dtype=torch.long),\n",
    "            'm_target': torch.tensor(self.m_target[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        human_ = self.set_human(index)\n",
    "        ai_ = self.set_ai(index)\n",
    "        return {\n",
    "            'real_texts': human_, \n",
    "            'fake_texts': ai_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_G = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "tokenizer_D = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_data=df.sample(frac=train_size, random_state=42)\n",
    "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "#### Dataset\n",
    "training_set = SentimentData(train_data, tokenizer_G, tokenizer_D, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer_G, tokenizer_D, MAX_LEN)\n",
    "\n",
    "#### DataLoader \n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = df['target'].value_counts()\n",
    "# majority_class = class_counts.idxmax()\n",
    "# minority_class = class_counts.idxmin()\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_df = df[df['target'] == majority_class]\n",
    "# minority_df = df[df['target'] == minority_class]\n",
    "\n",
    "# # Undersample majority class\n",
    "# undersampled_majority_df = resample(majority_df,\n",
    "#                                     replace=False,  # Sample without replacement\n",
    "#                                     n_samples=len(minority_df),  # Match minority class size\n",
    "#                                     random_state=42)  # For reproducibility\n",
    "\n",
    "# # Combine minority class with undersampled majority class\n",
    "# undersampled_df = pd.concat([undersampled_majority_df, minority_df])\n",
    "# undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df = undersampled_df\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_df = df[df['label'] == \"human\"]\n",
    "# ai_df = df[df['label'] == \"ai\"]\n",
    "\n",
    "# human_df = human_df.sample(frac=1).reset_index(drop=True)\n",
    "# ai_df = ai_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 0.2\n",
    "\n",
    "# total_size = len(human_df)\n",
    "# train_size = int(train_ratio * total_size)\n",
    "# test_size = total_size - train_size\n",
    "\n",
    "# print(total_size, train_size, test_size)\n",
    "\n",
    "\n",
    "# human_train_df = human_df[:train_size]\n",
    "# human_valid_df = human_df[train_size:]\n",
    "# ai_train_df = ai_df[:train_size]\n",
    "# ai_valid_df = ai_df[train_size:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
