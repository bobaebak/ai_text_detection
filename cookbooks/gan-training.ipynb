{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, RobertaForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"/mnt/data/bobae/ai_text_detection\")\n",
    "sys.path.append(\"/Users/bobaebak/git/ai_text_detection\")\n",
    "\n",
    "from utils.cuda_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_devie_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "if num_gpus>1:\n",
    "    device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_total_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load json data file and convert to dataframe\n",
    "- real -> human: 1\n",
    "- fake -> ai: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human data\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/human/tofel.json', \"r\") as f:\n",
    "    h_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads an arxiv dataset\n",
    "with open('../dataset/human/arxiv.json', \"r\") as f:\n",
    "    h_arxiv_dataset = json.load(f)\n",
    "\n",
    "# loads student essay\n",
    "with open('../dataset/human/student_essay.json', \"r\") as f:\n",
    "    h_essay_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay\n",
    "with open('../dataset/human/student_cs_essay.json', \"r\") as f:\n",
    "    h_essay_cs_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt data\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/ai/gpt2medium_tofel.json', \"r\") as f:\n",
    "    gpt_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads an arxiv dataset\n",
    "with open('../dataset/ai/gpt2medium_arxiv.json', \"r\") as f:\n",
    "    gpt_arxiv_dataset = json.load(f)\n",
    "\n",
    "# loads student essay\n",
    "with open('../dataset/ai/gpt2medium_essay.json', \"r\") as f:\n",
    "    gpt_essay_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay\n",
    "with open('../dataset/ai/gpt2medium_essay_cs.json', \"r\") as f:\n",
    "    gpt_essay_cs_dataset = json.load(f)\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/ai/gpt35_tofel.json', \"r\") as f:\n",
    "    gpt_35_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay\n",
    "with open('../dataset/ai/gpt35_essay_cs.json', \"r\") as f:\n",
    "    gpt_35_essay_cs_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_dataset = []\n",
    "for i in [h_tofel_dataset, h_arxiv_dataset, h_essay_dataset, h_essay_cs_dataset]:\n",
    "    h_dataset.extend(i)\n",
    "\n",
    "len(h_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_dataset = []\n",
    "for i in [gpt_35_tofel_dataset, gpt_arxiv_dataset, gpt_essay_dataset, gpt_35_essay_cs_dataset]:\n",
    "    gpt_dataset.extend(i)\n",
    "\n",
    "len(gpt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 387)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_essay_cs_dataset), len(gpt_35_essay_cs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_text</th>\n",
       "      <th>h_label</th>\n",
       "      <th>h_target</th>\n",
       "      <th>m_text</th>\n",
       "      <th>m_label</th>\n",
       "      <th>m_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I prefer to cook at home. First of all, it is ...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>I enjoy cooking at home for several reasons. F...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The place I would like to visit most is the ou...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>The place I would most like to visit is outer ...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I imagine my life ten years in the future to b...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>I envision my life a decade from now as comple...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Personally, I would like to say that the schoo...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Personally, I have to say that the school that...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I believe that I will follow my interest. I'm ...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm confident I will pursue my passion. I'm no...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>Computer Viruses: Spreading, Multiplying and D...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Viruses: Spreading, Multiplying, and ...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>Purchasing or Leasing Computer Equipment: Adva...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Purchasing vs Leasing Computer Equipment: Pros...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>Print and Broadcast Computer Advertisements Es...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Table of Contents\\n 1. Print advertisement\\n 2...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>How computers influence our life Essay\\n\\nIntr...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Influence of Computers on Our Lives\\n\\nIntrodu...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>The Impact of Computer-Based Technologies on B...</td>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>Executive Summary\\n\\nThe impact of computer-ba...</td>\n",
       "      <td>ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 h_text h_label  h_target  \\\n",
       "0     I prefer to cook at home. First of all, it is ...   human         1   \n",
       "1     The place I would like to visit most is the ou...   human         1   \n",
       "2     I imagine my life ten years in the future to b...   human         1   \n",
       "3     Personally, I would like to say that the schoo...   human         1   \n",
       "4     I believe that I will follow my interest. I'm ...   human         1   \n",
       "...                                                 ...     ...       ...   \n",
       "2473  Computer Viruses: Spreading, Multiplying and D...   human         1   \n",
       "2474  Purchasing or Leasing Computer Equipment: Adva...   human         1   \n",
       "2475  Print and Broadcast Computer Advertisements Es...   human         1   \n",
       "2476  How computers influence our life Essay\\n\\nIntr...   human         1   \n",
       "2477  The Impact of Computer-Based Technologies on B...   human         1   \n",
       "\n",
       "                                                 m_text m_label  m_target  \n",
       "0     I enjoy cooking at home for several reasons. F...      ai         0  \n",
       "1     The place I would most like to visit is outer ...      ai         0  \n",
       "2     I envision my life a decade from now as comple...      ai         0  \n",
       "3     Personally, I have to say that the school that...      ai         0  \n",
       "4     I'm confident I will pursue my passion. I'm no...      ai         0  \n",
       "...                                                 ...     ...       ...  \n",
       "2473  Computer Viruses: Spreading, Multiplying, and ...      ai         0  \n",
       "2474  Purchasing vs Leasing Computer Equipment: Pros...      ai         0  \n",
       "2475  Table of Contents\\n 1. Print advertisement\\n 2...      ai         0  \n",
       "2476  Influence of Computers on Our Lives\\n\\nIntrodu...      ai         0  \n",
       "2477  Executive Summary\\n\\nThe impact of computer-ba...      ai         0  \n",
       "\n",
       "[2478 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = {\n",
    "    \"h_text\": [item['input'] for item in h_dataset],\n",
    "    \"h_label\": [item['label'] for item in h_dataset],\n",
    "    \"h_target\": np.ones(len(h_dataset), dtype=int),\n",
    "    \"m_text\": [item['input'] for item in gpt_dataset],\n",
    "    \"m_label\": [item['label'] for item in gpt_dataset],\n",
    "    \"m_target\": np.zeros(len(gpt_dataset), dtype=int),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dct)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator (Paraphraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, model_name=\"t5-large\", lr=0.1):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # hyperparameters\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    \n",
    "    def generate_text(self, input):\n",
    "        # Paraphrase AI text make it better\n",
    "        \n",
    "        arguments = {\"padding\": True, \"truncation\": True, \"return_tensors\": \"pt\"}\n",
    "        # encode and obtain ids\n",
    "        encoded_inputs = self.tokenizer(input, **arguments).to(device)\n",
    "\n",
    "        # generate\n",
    "        outputs = self.model.generate(encoded_inputs.input_ids,\n",
    "                                    max_new_tokens=200,\n",
    "                                    # num_beams=5,\n",
    "                                    # num_return_sequences=1,\n",
    "                                    # temperature=1.5,\n",
    "                                    # num_beam_groups=5,\n",
    "                                    # diversity_penalty=2.0,\n",
    "                                    # no_repeat_ngram_size=2,\n",
    "                                    # early_stopping=True,\n",
    "                                    # length_penalty=2.0\n",
    "                                    )\n",
    "\n",
    "        sequences_list = outputs.tolist()\n",
    "\n",
    "        # decode \n",
    "        decoded_outputs = self.tokenizer.batch_decode(sequences_list, skip_special_tokens=True)\n",
    "        return decoded_outputs\n",
    "\n",
    "\n",
    "    # def update_model(self, Z, discriminator, loss):\n",
    "    #     \"\"\"Update generator.\"\"\"\n",
    "    #     batch_size = Z.shape[0]\n",
    "    #     ones = torch.ones((batch_size,), device=device)\n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     # We could reuse `fake_X` from `update_D` to save computation\n",
    "    #     fake_X = self.__call__(Z)\n",
    "    #     # Recomputing `fake_Y` is needed since `net_D` is changed\n",
    "    #     fake_Y = discriminator(fake_X)\n",
    "    #     loss_G = loss(fake_Y, ones.reshape(fake_Y.shape))\n",
    "    #     loss_G.backward()\n",
    "    #     self.optimizer.step()\n",
    "    #     return loss_G\n",
    "    \n",
    "\n",
    "    def train_generator(self, fake_texts, discriminator, criterion):\n",
    "        labels = torch.ones(len(fake_texts)).to(device)\n",
    "\n",
    "        fake_inputs = discriminator.tokenizer(fake_texts, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "        fake_outputs = discriminator.model(**fake_inputs)\n",
    "        # fake_inputs = discriminator.tokenizer(\n",
    "        #     fake_texts,  \n",
    "        #     add_special_tokens=True, pad_to_max_length=True, max_length=512, return_token_type_ids=True, return_tensors=True\n",
    "        # )\n",
    "        # fake_inputs = fake_inputs.to(device)\n",
    "        # fake_outputs = discriminator.model(**fake_inputs).to(device)\n",
    "\n",
    "        loss = criterion(fake_outputs.logits[:, 1], labels)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        # return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, model_name=\"roberta-base\", lr=0.1):\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def classify_text(self, input):\n",
    "        # given text, discriminate whether the given text is from human or ai\n",
    "        \"\"\"\n",
    "        Assume the label y for the true data is \n",
    "            1 -> human\n",
    "            0 -> AI\n",
    "        \"\"\"\n",
    "\n",
    "        arguments = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":512, \"return_token_type_ids\": True, \"return_tensors\": \"pt\"}\n",
    "        encoded_inputs = self.tokenizer.encode_plus(input, None, **arguments)\n",
    "        ids = encoded_inputs['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = encoded_inputs['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = encoded_inputs[\"token_type_ids\"].to(device, dtype = torch.long)\n",
    "        # targets = data['target'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = self.model(ids, mask, token_type_ids)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        # predicted_class = torch.argmax(probabilities).item()\n",
    "        return logits, probabilities\n",
    "    \n",
    "\n",
    "    # def update_model(self, real_texts, Z, generator, loss):\n",
    "    #     \"\"\"Update discriminator.\"\"\"\n",
    "    #     batch_size = X.shape[0]\n",
    "    #     ones = torch.ones((batch_size,), device=X.device)\n",
    "    #     zeros = torch.zeros((batch_size,), device=X.device)\n",
    "        \n",
    "    #     real_inputs = self.tokenizer(real_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    #     fake_X = generator(Z)\n",
    "\n",
    "    #     real_outputs = self.classify_text(real_inputs)\n",
    "    #     fake_outputs = self.classify_text(fake_X.detach())\n",
    "        \n",
    "    #     real_loss = loss(real_outputs, ones.reshape(real_outputs.shape))\n",
    "    #     fake_loss = loss(fake_outputs, zeros.reshape(fake_outputs.shape))\n",
    "    #     loss_D = (real_loss + fake_loss) / 2\n",
    "        \n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     loss_D.backward()\n",
    "    #     self.optimizer.step()\n",
    "\n",
    "    #     return loss_D\n",
    "    \n",
    "    def train_discriminator(self, real_texts, fake_texts, criterion):\n",
    "        real_labels = torch.ones(len(real_texts)).to(device)\n",
    "        fake_labels = torch.zeros(len(fake_texts)).to(device)\n",
    "\n",
    "        real_inputs = self.tokenizer(real_texts, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "        fake_inputs = self.tokenizer(fake_texts, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "        real_outputs = self.model(**real_inputs)\n",
    "        fake_outputs = self.model(**fake_inputs)\n",
    "\n",
    "        real_loss = criterion(real_outputs.logits[:, 1], real_labels)\n",
    "        fake_loss = criterion(fake_outputs.logits[:, 0], fake_labels)\n",
    "        loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_epochs=2\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Assume you have a list of real texts and corresponding prompts\n",
    "    real_texts = [item['input'] for item in h_tofel_dataset[:2]]\n",
    "    prompts = [\"Paraphrase: \"+item['input'] for item in gpt_tofel_dataset[:2]]\n",
    "    fake_texts = generator.generate_text(prompts)\n",
    "\n",
    "    # Train Discriminator\n",
    "    d_loss = discriminator.train_discriminator(real_texts, prompts, criterion)\n",
    "    print(f\"Epoch {epoch + 1}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "    # Train Generator\n",
    "    g_loss = generator.train_generator(fake_texts, discriminator, criterion)\n",
    "    print(f\"Epoch {epoch + 1}, Generator Loss: {g_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.h_text = df['h_text']\n",
    "        self.h_target = df['h_target']\n",
    "        self.m_text = df['m_text']\n",
    "        self.m_target = df['m_target']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        h_text = str(self.h_text[index])\n",
    "        h_text = \" \".join(h_text.split())\n",
    "\n",
    "        m_text = str(self.m_text[index])\n",
    "        m_text = \" \".join(m_text.split())\n",
    "\n",
    "        return {\n",
    "            'real_texts': h_text,\n",
    "            'real_target': torch.tensor(self.h_target[index], dtype=torch.float), \n",
    "            'fake_texts': m_text,\n",
    "            'fake_target': torch.tensor(self.m_target[index], dtype=torch.float), \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2478, 6)\n",
      "TRAIN Dataset: (1982, 6)\n",
      "TEST Dataset: (496, 6)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=df.sample(frac=train_size, random_state=42)\n",
    "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "#### Dataset\n",
    "training_set = GANDataset(train_data)\n",
    "testing_set = GANDataset(test_data)\n",
    "\n",
    "#### DataLoader \n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'real_texts': ['Yelling in Past and Modern Society Essay Yelling, once associated with toughness, strength, and power, is now seen as imposing and aggressive, a negative side consequence of arrogance and unearthed advantage. With the widespread public reaction towards the online clip of Republican Majority Leader Jake Corman yelling at his Democratic opponent Kate Muth, the precarious position of yelling in modern American consciousness became apparent. Screaming is currently associated with toxic masculinity and anger issues that frequently stem from it. People who lose control and start yelling are met with a mixture of surprise and humiliation. It’s not an appropriate course of action for anyone who wishes to achieve public respect and be taken seriously. A large part of this movement is undoubtedly due to a wider trend away from conduct that promotes abusive and blatantly patriarchal structures. While there is no proof that men yell more than women, a man yelling is nearly always a more physically and psychologically dangerous experience. Another element of the transition seems reasonable to attribute to a shift in generational temperament, assisted, maybe, by the rise of internet culture (Petrusich, 2019). Today, there’s less need for someone to raise his voice when it’s quite easy to be cruel, abusive, and disruptive trough other communication channels. Personally, I find the evolution of the understanding of yelling rather fascinating. Social connotations of behaviors are not fixed, but rather evolve with time, depending on the state of society itself. As the general public becomes progressively more and more respectful towards one another, at least hypothetically, it is natural for screaming to become somewhat obsolete. I only hope, that respectable people are allowed to deal with the emotions conveyed previously trough yelling in healthy ways, rather then forced to suppress them for as long as possible. Reference Petrusich, A. (2019). The Decline of Yelling . The New Yorker. Web.',\n",
       "  'Quantitative Blood Loss in Obstetric Hemorrhage Research Paper Table of Contents 1. Introduction 2. Pregnancy Associated Mortality and According to Racial Rate 3. Dominant Causes of Mortality 4. Frightening Racial Contrast in Maternal Mortality 5. Quantitative Blood Loss in Obstetric Hemorrhage 6. Obstetric Hemorrhage Classification 7. Terms of Prevention 8. Conclusion 9. References Introduction While childbirth is one of the fascinating issues for many people, it can become the most horrifying accident. The baby can lose the closest person on the first day of life, while the family will simultaneously deal with the happiness of the baby’s emergence and the loss of the beloved person. A considerable number of ladies die every day because of pregnancy-associated issues. This assignment will analyze one of the most prevalent causes of mortality – blood loss in obstetric hemorrhage, figure out the alarming differences in the death rate among African American women during childbirth, and provide current proposals and directions on prevention issues. The thesis statement of the assignment includes the message for the needed attention towards such terrifying causes and suppose possible variants to overcome these awful current aspects. Pregnancy Associated Mortality and According to Racial Rate The death rate during pregnancy, during childbirth, or one year after birth is about the same. A total of about 200 women die each year in the United States due to pregnancy-related complications. Significantly, 54–93% of maternal deaths due to obstetric hemorrhage may be escapable ( Quantitative Blood Loss in Obstetric Hemorrhage , 2019). This is a relatively high rate for developed countries. There are persistent differences in pregnancy-related death rates among women of different racial and ethnic groups in the United States. According to a recently published study by experts from the U.S. Centers for Disease Control and Prevention’s Division of Reproductive Health (Petersen et al., 2019), maternal mortality rates are much higher among black and Native American women than white or Asian women. Dominant Causes of Mortality Most countries worldwide have adopted the term “maternal mortality. The term “maternal mortality” refers to all deaths of women due to pregnancy, irrespective of the duration of pregnancy. The term refers to all deaths of women due to pregnancy, irrespective of its duration and location, which occur during pregnancy or within 42 days after its termination. Pregnancy-associated conditions or complications arise from pregnancy or management, excluding accidents, miscarriages, or cardiovascular disease (Boyd et al., 2020). The following applies in the case of pregnancy, except for accidents or unexpected circumstances. Some experts believe that 10-15% of maternal deaths are due to medical causes, while 85-90% are due to poor environmental conditions, lack of nutrition, or education (Petersen et al., 2019). Poor nutrition, lack of education, and other factors have been identified as underlying causes of maternal mortality in CIS countries. The quality of life determines what share of medical deaths is attributable to such causes as hemorrhage, hemorrhage, hepatitis B and C, etc. The leading causes of maternal mortality are bleeding, gestosis and sepsis. Frightening Racial Contrast in Maternal Mortality The surprising fact of the statistics is connected with the racial differences in mortality rate differences between African American women and others, which commonly do not include the reasons for such awful circumstances. For a long time, the high mortality rate during childbirth was associated with undeveloped countries. Still, statistics in the USA from 1990-2016 show the highest rate among the developed countries, which sounds awful. Despite this, the ratio of African American women’s deaths is worse than in Mexico, where 39% of the population lives in absolute poverty (Levy, 2019). Compared to white women (Peterson et al., 2019), black women have a maternal mortality rate that is 3.2 times higher and 4.3 times higher at ages 30-34. Black women have 5.2 times the maternal mortality rate from pregnancy complications among women with a college degree than white women (40.2 per 100,000 vs. 7.8 per 100,000). Thus, inequality in maternal mortality is a serious national problem. Experts suggest that this inequality is due to a combination of factors, led by differences in the availability and quality of health care and the prevalence of chronic diseases among women of different ethnic groups (McKinney et al., 2021). Most pregnancy-related deaths are preventable, and ways to reduce maternal mortality are known. Coordination is needed at the level of the patient, family, community, physicians, health care organizations, and the health care system as a whole. Quantitative Blood Loss in Obstetric Hemorrhage Currently, the main problem of obstetric hemorrhage has not been solved. Obstetric hemorrhage consistently maintains a leading position in the structure of maternal morbidity and mortality, is a critical, life-threatening condition, complicates 3-5% of births, predictable in the development of anemia, ischemia anterior pituitary lobe (Sheehan’s syndrome or postpartum pituitary necrosis), hemotransfusion, coagulopathy, myocardial ischemia, orthostatic hypotension, postpartum depression ( Quantitative Blood Loss in Obstetric Hemorrhage ,2019). In developed countries, the rate of MS is relatively low, so it takes 3 to 10 years to collect a sufficient sample based on which to assess the quality of care and the causes leading to MS. Obstetric Hemorrhage Classification This part will focus on the simple period-based classification of obstetric hemorrhage and some aspects of this period. Bleeding during pregnancy is distinguished as not related to the pathology of the fetus, which means such occasions as cervical ectopy, polyps of the cervical canal, cervical cancer, varicose veins of the vagina, and vaginal trauma (Konar, 2017). Also, there are bleedings associated with fetal egg pathology, caused by such factors as ectopic pregnancy, spontaneous miscarriage, cervical pregnancy, bubble skid, placenta previa, and premature detachment of the customarily located placenta. Speaking about bleeding during labor is divided into periods. The classification also distinguishes postpartum bleeding, which is associated with such concepts as hypo-atonic uterine bleeding, retention in the uterine cavity of the placenta, soft tissue rupture of the birth canal, and congenital and acquired disorders of the hemostatic system. Terms of Prevention An essential factor in the development of obstetric hemorrhage can be called an organizational issue. The main barriers to adequate bleeding prevention are: * Lack of flowcharts on bleeding control measures or clinical protocols in the delivery room within walking distance. * Overestimation by professionals of their knowledge regarding the definition of high-risk categories of patients for PPH and their treatment. * Lack of communication in the team, lack of clarity in the management due to ignorance of each team member, their skills, and experience, frequent changes in team composition. * Disagreement between team members and allied professionals (blood bank staff, anesthesiologists) about the seriousness of the situation. * Costs of hierarchy. All these issues should be addressed at the level of the highest authorities and the organizations; it is necessary to create coordinating bodies and communities that will draw up all the required protocols and schemes (Muñoz,2019). It is needed for such organizations to conduct the required training and staff studies and perhaps provide the required materials even to ordinary people. So that at every level of protection and attention, there is a concrete understanding and capacity to analyze the woman’s condition during, before, and after childbirth. Conclusion Obstetric hemorrhage remains relevant to the present day and requires continued research. The wide possibility of open access to the data of the world databases of clinical and scientific research allows one to get acquainted remotely with the latest trends in the prognosis and prevention of obstetric bleeding, study their effectiveness, and implement the obtained knowledge into real practice. While modern realities allow consolidating international scientific and clinical experience in the study of features, differences in the prevention and treatment of obstetric bleeding will update current clinical protocols to adapt them to modern conditions. References Boyd, L., Chazotte, C., Illescas, A., Johansson, E., Koch, A., Langston, A., Nathan, L., Searing, H., & Ma. (n.d.). (2020). Pregnancy-Associated Mortality in New York City, 2011-2015. New York City Department of Health and Mental Hygiene. Konar, H. (Ed.). (2017). DC Dutta’s Textbook of Obstetrics (9 th ed.). Jaypee Brothers, Medical Publishers Prt. Limited. Levy, B.S. (2019). Social Injustice and Public Health . Oxford University Press. McKinney, E.S., James, S.R., Murray, S.S., Nelson, K. & Ashwill, J. (2021). Mental -Child Nursing [eBook]. Elsevier Health Sciences. Muñoz, M., Stensballe, J., Ducloy-Bouthors, A. S., Bonnet, M. P., De Robertis, E., Fornet, I., Goffinet, F., Hofer, S., Holzgreve, W., Manrique, S., Nizard, J., Christory, F., Samama, C. M., & Hardy, J. F. (2019). Patient Blood Management in Obstetrics: Prevention and Treatment of Postpartum Hemorrhage . A NATA consensus statement. Blood transfusion = Trasfusione del sangue, 17(2), 112–136. Petersen E.E., Davis N.L. & Goodman D. (2019). Racial/Ethnic Disparities in Pregnancy-Related Deaths —United States, 2007–2016. MMWR Mortal Weekly (68) , 762–765. Quantitative Blood Loss in Obstetric Hemorrhage. ACOG Committee Opinion. (2019). American College of Obstetricians and Gynecologists, 794.',\n",
       "  'The Siemens Company’s Ethical Culture Change Essay Table of Contents 1. Summary of the Case 2. Ethical Violations Involved in the Case 3. Siemen’s Response to the Recovery 4. Ethical Guidelines Implemented by Siemens 5. Basic Principles of Ethics Related to the Case 6. Boundary Crossings and Violations 7. Recommended Model for the Ethical Dilemma 8. References Summary of the Case Siemens is a telecommunications company based in Munich, Germany but serves across the world. In 2006, the company was involved in a corruption scandal that led to approximately 2.6 billion euros in fines and penalties. There were investigations from international investigators and law firms. From the reports, it was found that 80% of senior employees serving as the chief executive officer (CEO), chief finance officer (CFO), and staff in human resource (HR) management were corrupt (CGMA, 2014). 70% of the next management level and 40% of the junior staff were also said to be corrupt (CGMA, 2014). It took the efforts of Peter Loscher 2007, who came as the CEO, to change the organization’s culture. Dozens of employees in Siemens then used bribes and embezzled millions of funds to win company contracts. From the investigations done, there was a massive corruption scandal that had altered the telecom units of the company to almost $128 million, as reported by the German Focus magazine in 2006 (Blanc et al., 2017). The reports showed more than 80 million euros in the bank accounts of executives in Greece and Austria. The money was said to be part of the slush fund used to give bribes to the contractors who placed bids on the security systems at the 2004 Olympic Games held in Athens (Blanc et al., 2017). Members of the board and other employees were nubbed and held in custody after police raided the offices in Munich, Germany. The company faced charges from US authorities and German authorities due to bribery allegations. The giant engineering company parted with $800 million and an additional 395 million euros after pleading guilty to corruption charges. The reports indicated that the company had paid officials about 4,000 times to the extent of parting with 1.3 billion euros between 2000 and 2006, whereby the goal was to win contracts globally (Blanc et al., 2017). The US Securities and Exchange Commission (SEC) investigated the company in collaboration with the Department of Justice. According to the researchers, the record penalty levied in the US is 800 million dollars, almost 20 times more than any other foreign firm had come across in the US in corruption issues (Blanc et al., 2017). Three hundred ninety-five million euros paid to Germany was due to the lack of control in terms of business tasks (Vernand, 2018). The company faced tough as there was a fine of 201 million euros (Vernand, 2018). The money was levied against the German judges in 2007 for the misappropriation of the funds at the telecommunications group. There were specific people mentioned as the key drivers for the corruption allegations. They include Thomas Ganswindt, the manager in charge of the telecoms division, and Johannes Feldmayer, a board member (CGMA, 2014). Heinrich von Pierer, the head of Siemen’s supervisions, and Klaus Kleinfield, the CEO, were also found to have grossly violated the company’s monetary policies despite denying the investigators’ wrongdoings. Ethical Violations Involved in the Case The major ethical violation involved in this case was corruption. The reason is that corrupt officials showed a lack of integrity and transparency, which are requirements when holding such offices (CGMA, 2014). The issue undermined the democracy of the firm and eroded the economy in the market, as well as caused instability in terms of resources. It is against ethics to conspire to fraud a company for malicious gains, and therefore, the people involved deliberately organized to embezzle the funds contrary to the company’s policies (Blanc et al., 2017). They also violated the need to have transparency and fairness in undertaking its activities while serving in their respective offices. Siemen’s Response to the Recovery The company made a raft of changes in the administration and the roles played by the key people serving in the giant engineering company. First, the company’s CEO, Klaus Kleinfield, and the chairman Heinrich von Pierer resigned (CGMA, 2014). There was the appointment of Peter Loscher, who announced that employees were to come forward and testify against any charges. There were more than 40 informers who gave incriminating evidence, which led to the replacement of the previous board. Siemens appointed Michael Hershman, the co-founder of Transparency International, who came to advise hence, making a move as a leading anti-corruption expert (Blanc et al., 2017). The firm established strict regulations on anti-corruption whereby there were 500 full-time compliance officers to spearhead the recovery (Blanc et al., 2017). The new CEO led to training and education programs on anti-corruption for staff (CGMA, 2014). The company also started working with Interpol to assist in any required investigations. Ethical Guidelines Implemented by Siemens The company started training employees on avoiding corruption and being accountable for any resource. By 2008, Siemens had offered training to 0.4 million employees globally on anti-corruption matters (Vernand, 2018). In this case, Loscher ensured the commencement of complex matrices to streamline the company’s financial divisions (CGMA, 2014). All finance officers were required to produce bank account statements and all the documents involved in transactions from that time. All employees were required to be head high in whatever departments they served to ensure no loopholes that enabled company funds loss (CGMA, 2014). Each staff was required to have a valid certificate from the ethics agencies showing that they had not participated in any fraud or misappropriation of funds. Basic Principles of Ethics Related to the Case 1. Trustworthiness- the CEO, CFO, and HR were not trustworthy since they conspired against Siemens to fraud its funds through bribery in securing lucrative foreign contracts. 2. Respect- the company’s officials failed to respect the terms and conditions and policies set against the misappropriation of resources. 3. Responsibility- the employees lacked the responsibility to highlight corruption cases and scandals at the ground level, which led to a global escalation of the matter. 4. Fairness- The company was fair in relieving duties for the members found guilty and replacing them with competent people who would recover the lost resources from the previous regime. 5. Citizenship- Embezzling a multi-global company means economic and political instability ensued, thus, causing a state of scarcity and lack of developmental opportunities in countries where Siemens had reached operations. Boundary Crossings and Violations The boundary crossing in the case study is the deviation from normal operations that do not have embezzlement of resources, that which does not exploit the company and supports the company’s objectives (Blanc et al., 2017). The boundary violation, in this case, was the bribery done to secure contracts using the company’s money. The illegal exploitation of Siemens caused economic sanctions after the company was fined heavily by the US and Germany. Recommended Model for the Ethical Dilemma The appropriate model for solving this ethical dilemma would be the Rion model. In this model, people would ask why the situation bothers the normal working environment and whether or not the decision needs input from other parties (Robert, 2017). Through Rion’s framework, it is easy to know the specific problem to solve, and it calls for being true to the situation and getting options from the external parties watching from other perspectives (Robert, 2017). Therefore, the model would meticulously deal with the dilemma by making a decision that would be fair to the perpetrators of the rules as well as the company itself. References Blanc, R., Cho, C., Sopt, J., & Branco, M. (2017). Disclosure responses to a corruption scandal: The case of Siemens AG. Journal of Business Ethics , 156 (2), 545-561. Web. CGMA. (2014). Rethinking the value chain – Ethical culture change at Siemens: a case study [PDF] (pp. 2-6). AWCPA. Web. Robert, C. (2017). Theoretical models in identifying & resolving ethical dilemmas . Bizfluent. Web. Vernand, B. (2018). Lessons from the Siemens corruption scandal ten years on . News24. Web.',\n",
       "  'This work reports a theoretical study of the gas phase unimolecular decomposition of cyclobutane, cyclopentane and cyclohexane by means of quantum chemical calculations. A biradical mechanism has been envisaged for each cycloalkane, and the main routes for the decomposition of the biradicals formed have been investigated at the CBS-QB3 level of theory. Thermochemical data (\\\\delta H^0_f, S^0, C^0_p) for all the involved species have been obtained by means of isodesmic reactions. The contribution of hindered rotors has also been included. Activation barriers of each reaction have been analyzed to assess the 1 energetically most favorable pathways for the decomposition of biradicals. Rate constants have been derived for all elementary reactions using transition state theory at 1 atm and temperatures ranging from 600 to 2000 K. Global rate constant for the decomposition of the cyclic alkanes in molecular products have been calculated. Comparison between calculated and experimental results allowed to validate the theoretical approach. An important result is that the rotational barriers between the conformers, which are usually neglected, are of importance in decomposition rate of the largest biradicals. Ring strain energies (RSE) in transition states for ring opening have been estimated and show that the main part of RSE contained in the cyclic reactants is removed upon the activation process.'],\n",
       " 'real_target': tensor([1., 1., 1., 1.]),\n",
       " 'fake_texts': ['The first thing you need to do is to get your hands on a copy of the game. You can find it on Amazon.com or from your local game store. The game is called \"The Last of Us\" and it\\'s available for $19.99. The game is set in a post-apocalyptic world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only way to survive is to survive in a world where the only',\n",
       "  'Quantitative Blood Loss in Obstetric Hemorrhage Research Paper Table of Contents 1. Introduction 2. Pregnancy Associated Mortality in the United States 3. Pregnancy Associated Mortality in the United States: United States, 2000–2010 4. Pregnancy Associated Mortality in the United States: United States, 2010 5. Pregnancy Associated Mortality in the United States: United States, 2011 6. Pregnancy Associated Mortality in the United States: United States, 2012 7. Pregnancy Associated Mortality in the United States: United States, 2013 8. Pregnancy Associated Mortality in the United States: United States, 2014 9. Pregnancy Associated Mortality in the United States: United States, 2015 10. Pregnancy Associated Mortality in the United States: United States, 2016 11. Pregnancy Associated Mortality in the United States: United States, 2017',\n",
       "  \"The Siemens Company’s Ethical Culture Change Essay Table of Contents 1. Summary of the Case 2. Ethical Culture Change Essay 3. Case Study 4. Conclusion 1. Summary of the Case The Siemens Company is a global leader in the development of sustainable energy solutions. The company's mission is to create a world where people can live and work in harmony with nature and the environment. The company's mission is to create a world where people can live and work in harmony with nature and the environment. The Siemens Company is a global leader in the development of sustainable energy solutions. The company's mission is to create a world where people can live and work in harmony with nature and the environment. The Siemens Company is a global leader in the development of sustainable energy solutions. The company's mission is to create a world where people can live and work in harmony with nature and the environment.\",\n",
       "  'This work reports a theoretical study of the gas phase unimolecular decomposition of cyclobutane, cyclopentane and cyclohexane. The gas phase decomposition is characterized by the formation of a cyclobutane-like compound, cyclopentane-like compound and a cyclohexane-like compound. The gas phase decomposition is characterized by the formation of a cyclobutane-like compound, cyclopentane-like compound and a cyclohexane-like compound. The gas phase decomposition of cyclobutane, cyclopentane and cyclohexane is characterized by the formation of a cyclobutane-like compound, cyclopentane-like compound and a cyclohexane-like compound. The gas phase decomposition of cyclobutane, cyclopentane and cyclohexane is characterized by the formation of a cyclobutane-like compound, cyclop'],\n",
       " 'fake_target': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, generator, discriminator, training_loader, testing_loader):\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        with tqdm(training_loader, desc=f\"Epoch {epoch}\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                real_texts = data['real_texts']\n",
    "                prompts = [\"\".join(\"Paraphrase: \"+real_text) for real_text in real_texts]\n",
    "                fake_texts = generator.generate_text(prompts)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                d_loss = discriminator.train_discriminator(real_texts, prompts, criterion)\n",
    "                print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "                # Train Generator\n",
    "                g_loss = generator.train_generator(fake_texts, discriminator, criterion)\n",
    "                print(f\"Epoch {epoch}, Generator Loss: {g_loss}\")\n",
    "\n",
    "                tepoch.set_postfix(discriminator_loss=d_loss, generator_loss=g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, generator, discriminator, training_loader, testing_loader):\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for data in training_loader:\n",
    "            real_texts = data['real_texts']\n",
    "            prompts = [\"\".join(\"Paraphrase: \"+real_text) for real_text in real_texts]\n",
    "            fake_texts = generator.generate_text(prompts)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            d_loss = discriminator.train_discriminator(real_texts, prompts, criterion)\n",
    "            print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "            # Train Generator\n",
    "            g_loss = generator.train_generator(fake_texts, discriminator, criterion)\n",
    "            print(f\"Epoch {epoch}, Generator Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, generator, discriminator, training_loader, testing_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m real_texts \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal_texts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParaphrase: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mreal_text) \u001b[38;5;28;01mfor\u001b[39;00m real_text \u001b[38;5;129;01min\u001b[39;00m real_texts]\n\u001b[0;32m----> 7\u001b[0m fake_texts \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train Discriminator\u001b[39;00m\n\u001b[1;32m     10\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_discriminator(real_texts, prompts, criterion)\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mGenerator.generate_text\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     16\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# generate\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# num_beams=5,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# num_return_sequences=1,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# temperature=1.5,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# num_beam_groups=5,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# diversity_penalty=2.0,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# no_repeat_ngram_size=2,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# early_stopping=True,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# length_penalty=2.0\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m sequences_list \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# decode \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1742\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1757\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1109\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1095\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1096\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         output_attentions,\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:689\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    699\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:596\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    587\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    594\u001b[0m ):\n\u001b[1;32m    595\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 596\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    606\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:515\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# get query states\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m query_states \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    518\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    519\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs, generator, discriminator, training_loader, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(discriminator, generator, data_iter, num_epochs, latent_dim, data):\n",
    "#     loss = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "#     for w in discriminator.model.parameters():\n",
    "#         nn.init.normal_(w, 0, 0.02)\n",
    "#     for w in generator.model.parameters():\n",
    "#         nn.init.normal_(w, 0, 0.02)\n",
    "\n",
    "#     animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "#                             xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "#                             legend=['discriminator', 'generator'])\n",
    "#     animator.fig.subplots_adjust(hspace=0.3)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Train one epoch\n",
    "#         timer = d2l.Timer()\n",
    "#         metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "#         for (X,) in data_iter:\n",
    "#             batch_size = X.shape[0]\n",
    "#             Z = torch.normal(0, 1, size=(batch_size, latent_dim))\n",
    "#             metric.add(discriminator.update_model(X, Z, generator, loss),\n",
    "#                        generator.update_model(Z, discriminator, loss),\n",
    "#                        batch_size)\n",
    "#         # Visualize generated examples\n",
    "#         Z = torch.normal(0, 1, size=(100, latent_dim))\n",
    "#         fake_X = generator(Z).detach().numpy()\n",
    "#         animator.axes[1].cla()\n",
    "#         animator.axes[1].scatter(data[:, 0], data[:, 1])\n",
    "#         animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n",
    "#         animator.axes[1].legend(['real', 'generated'])\n",
    "#         # Show the losses\n",
    "#         loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]\n",
    "#         animator.add(epoch + 1, (loss_D, loss_G))\n",
    "#     print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "#           f'{metric[2] / timer.stop():.1f} examples/sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaliGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.name = 'vanilla'\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp, hidden, need_hidden=False):\n",
    "        \"\"\"\n",
    "        Embeds input and applies LSTM\n",
    "        :param inp: batch_size * seq_len\n",
    "        :param hidden: (h, c)\n",
    "        :param need_hidden: if return hidden, use for sampling\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n",
    "        if len(inp.size()) == 1:\n",
    "            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
    "\n",
    "        out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n",
    "        out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n",
    "        # out = self.temperature * out  # temperature\n",
    "        pred = self.softmax(out)\n",
    "\n",
    "        if need_hidden:\n",
    "            return pred, hidden\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "    def sample(self, num_samples, batch_size, start_letter=cfg.start_letter):\n",
    "        \"\"\"\n",
    "        Samples the network and returns num_samples samples of length max_seq_len.\n",
    "        :return samples: num_samples * max_seq_length (a sampled sequence in each row)\n",
    "        \"\"\"\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n",
    "\n",
    "        # Generate sentences with multinomial sampling strategy\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size)\n",
    "            if self.gpu:\n",
    "                inp = inp.cuda()\n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                out, hidden = self.forward(inp, hidden, need_hidden=True)  # out: batch_size * vocab_size\n",
    "                next_token = torch.multinomial(torch.exp(out), 1)  # batch_size * 1 (sampling from each row)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token.view(-1)\n",
    "                inp = next_token.view(-1)\n",
    "        samples = samples[:num_samples]\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if cfg.gen_init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif cfg.gen_init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif cfg.gen_init == 'truncated_normal':\n",
    "                    truncated_normal_(param, std=stddev)\n",
    "\n",
    "    def init_oracle(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                torch.nn.init.normal_(param, mean=0, std=1)\n",
    "\n",
    "    def init_hidden(self, batch_size=cfg.batch_size):\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "\n",
    "        if self.gpu:\n",
    "            return h.cuda(), c.cuda()\n",
    "        else:\n",
    "            return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, gpu=False,\n",
    "                 dropout=0.2):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Get final predictions of discriminator\n",
    "        :param inp: batch_size * seq_len\n",
    "        :return: pred: batch_size * 2\n",
    "        \"\"\"\n",
    "        feature = self.get_feature(inp)\n",
    "        pred = self.feature2out(self.dropout(feature))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def get_feature(self, inp):\n",
    "        \"\"\"\n",
    "        Get feature vector of given sentences\n",
    "        :param inp: batch_size * max_seq_len\n",
    "        :return: batch_size * feature_dim\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n",
    "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if cfg.dis_init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif cfg.dis_init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif cfg.dis_init == 'truncated_normal':\n",
    "                    truncated_normal_(param, std=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaliGAN_G(LSTMGenerator):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n",
    "        super(MaliGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n",
    "        self.name = 'maligan'\n",
    "\n",
    "    def adv_loss(self, inp, target, reward):\n",
    "        \"\"\"\n",
    "        Returns a MaliGAN loss\n",
    "\n",
    "        :param inp: batch_size x seq_len, inp should be target with <s> (start letter) prepended\n",
    "        :param target: batch_size x seq_len\n",
    "        :param reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding sentence)\n",
    "        :return loss: policy loss\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inp.size()\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        out = self.forward(inp, hidden).view(batch_size, self.max_seq_len, self.vocab_size)\n",
    "        target_onehot = F.one_hot(target, self.vocab_size).float()  # batch_size * seq_len * vocab_size\n",
    "        pred = torch.sum(out * target_onehot, dim=-1)  # batch_size * seq_len\n",
    "        loss = -torch.sum(pred * reward)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaliGAN_D(CNNDiscriminator):\n",
    "    def __init__(self, embed_dim, vocab_size, padding_idx, gpu=False, dropout=0.25):\n",
    "        super(MaliGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx, gpu,\n",
    "                                        dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, df, tokenizer_G, tokenizer_D, max_len):\n",
    "        self.tokenizer_G = tokenizer_G\n",
    "        self.tokenizer_D = tokenizer_D\n",
    "        self.max_len = max_len\n",
    "        self.h_text = df['h_text']\n",
    "        self.h_target = df['h_target']\n",
    "        self.m_text = df['m_text']\n",
    "        self.m_target = df['m_target']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h_text)\n",
    "\n",
    "    def set_human(self, index):\n",
    "        text = str(self.h_text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "         # generator inputs\n",
    "        arguments_G = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_G = self.tokenizer_G.encode_plus(text, None, **arguments_G)\n",
    "\n",
    "        # discriminator inputs\n",
    "        arguments_D = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_D = self.tokenizer_D.encode_plus(text, None, **arguments_D)\n",
    "\n",
    "        return {\n",
    "            'h_ids_G': torch.tensor(inputs_G['input_ids'], dtype=torch.long),\n",
    "            'h_mask_G': torch.tensor(inputs_G['attention_mask'], dtype=torch.long),\n",
    "            'h_ids_D': torch.tensor(inputs_D['input_ids'], dtype=torch.long),\n",
    "            'h_mask_D': torch.tensor(inputs_D['attention_mask'], dtype=torch.long),\n",
    "            'h_token_type_ids_D': torch.tensor(inputs_D[\"token_type_ids\"], dtype=torch.long),\n",
    "            'h_target': torch.tensor(self.h_target[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def set_ai(self, index):\n",
    "        text = str(self.m_text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "         # generator inputs\n",
    "        arguments_G = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_G = self.tokenizer_G.encode_plus(text, None, **arguments_G)\n",
    "\n",
    "        # discriminator inputs\n",
    "        arguments_D = {\"add_special_tokens\": True, \"pad_to_max_length\": True, \"max_length\":self.max_len, \"return_token_type_ids\": True}\n",
    "        inputs_D = self.tokenizer_D.encode_plus(text, None, **arguments_D)\n",
    "\n",
    "        return {\n",
    "            'm_ids_G': torch.tensor(inputs_G['input_ids'], dtype=torch.long),\n",
    "            'm_mask_G': torch.tensor(inputs_G['attention_mask'], dtype=torch.long),\n",
    "            'm_ids_D': torch.tensor(inputs_D['input_ids'], dtype=torch.long),\n",
    "            'm_mask_D': torch.tensor(inputs_D['attention_mask'], dtype=torch.long),\n",
    "            'm_token_type_ids_D': torch.tensor(inputs_D[\"token_type_ids\"], dtype=torch.long),\n",
    "            'm_target': torch.tensor(self.m_target[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        human_ = self.set_human(index)\n",
    "        ai_ = self.set_ai(index)\n",
    "        return {\n",
    "            'real_texts': human_, \n",
    "            'fake_texts': ai_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_G = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "tokenizer_D = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_data=df.sample(frac=train_size, random_state=42)\n",
    "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "#### Dataset\n",
    "training_set = SentimentData(train_data, tokenizer_G, tokenizer_D, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer_G, tokenizer_D, MAX_LEN)\n",
    "\n",
    "#### DataLoader \n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = df['target'].value_counts()\n",
    "# majority_class = class_counts.idxmax()\n",
    "# minority_class = class_counts.idxmin()\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_df = df[df['target'] == majority_class]\n",
    "# minority_df = df[df['target'] == minority_class]\n",
    "\n",
    "# # Undersample majority class\n",
    "# undersampled_majority_df = resample(majority_df,\n",
    "#                                     replace=False,  # Sample without replacement\n",
    "#                                     n_samples=len(minority_df),  # Match minority class size\n",
    "#                                     random_state=42)  # For reproducibility\n",
    "\n",
    "# # Combine minority class with undersampled majority class\n",
    "# undersampled_df = pd.concat([undersampled_majority_df, minority_df])\n",
    "# undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df = undersampled_df\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_df = df[df['label'] == \"human\"]\n",
    "# ai_df = df[df['label'] == \"ai\"]\n",
    "\n",
    "# human_df = human_df.sample(frac=1).reset_index(drop=True)\n",
    "# ai_df = ai_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 0.2\n",
    "\n",
    "# total_size = len(human_df)\n",
    "# train_size = int(train_ratio * total_size)\n",
    "# test_size = total_size - train_size\n",
    "\n",
    "# print(total_size, train_size, test_size)\n",
    "\n",
    "\n",
    "# human_train_df = human_df[:train_size]\n",
    "# human_valid_df = human_df[train_size:]\n",
    "# ai_train_df = ai_df[:train_size]\n",
    "# ai_valid_df = ai_df[train_size:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
