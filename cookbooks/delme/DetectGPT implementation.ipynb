{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"/Users/bobaebak/git/ai_text_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_helper import *\n",
    "from utils.text_helper import *\n",
    "from utils.plot_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.detectgpt_model import DetectGPTRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Human Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_texts = load_json_list(\"../data/student_answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for Human: 92.83%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentences': ['Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world'],\n",
       " 'prob': '92.83%',\n",
       " 'label': 1,\n",
       " 'out': 'This text is most likely written by an Human'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DetectGPTRunner('mps', 'gpt2-medium')\n",
    "sentence = \"Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world Hello world\" \n",
    "\n",
    "result_dict = model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in H_texts:\n",
    "    try:\n",
    "        model = DetectGPTRunner('mps', 'gpt2-medium')\n",
    "        sentence = item['raw']\n",
    "\n",
    "        result_dict = model(sentence)\n",
    "        item['detectgpt'] = result_dict \n",
    "        print(\"============\")\n",
    "    \n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected {err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPYTORCH_ENABLE_MPS_FALLBACK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/os.py:685\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[1;32m    684\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)\n\u001b[0;32m--> 685\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     putenv(key, value)\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_detector/lib/python3.10/os.py:757\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(value):\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 757\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr expected, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mencode(encoding, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: str expected, not int"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in H_texts:\n",
    "    try:\n",
    "        model = DetectGPTRunner('mps', 'gpt2-medium')\n",
    "        sentence = item['raw']\n",
    "        \n",
    "        detectgpt_dict = model(sentence)\n",
    "        item['detectgpt'] = detectgpt_dict\n",
    "        print(\"============\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected {err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for Human: 60.04%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'prob': '60.04%', 'label': 1},\n",
       " 'This text is most likely written by an Human')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DetectGPTRunner('mps', 'gpt2-medium')\n",
    "sentence = \"\"\"\n",
    "The probability output from a T5 model, or any neural language model like it, typically represents the likelihood of a particular sequence of tokens given the context provided to the model. This probability is calculated using the model's learned parameters and is often normalized across all possible sequences to sum up to 1.\n",
    "\n",
    "Using log-probabilities in inference has several advantages:\n",
    "\n",
    "Numerical Stability: When dealing with probabilities, especially when multiplying many small probabilities together (which can lead to underflow issues), taking logarithms helps maintain numerical stability.\n",
    "Mathematical Convenience: Logarithms simplify mathematical operations, especially multiplication and division, into addition and subtraction. This simplifies calculations in the model and makes it more computationally efficient.\n",
    "Prediction Confidence: Log-probabilities provide a clearer representation of the model's confidence in its predictions. A higher log-probability indicates a higher confidence in the predicted sequence, whereas a lower log-probability suggests less confidence.\n",
    "Training Optimization: In the context of training neural networks, using log-probabilities is common because it allows the loss function to be formulated as the negative log-likelihood. This is a standard formulation in maximum likelihood estimation, which is widely used in training neural networks.\n",
    "\"\"\"\n",
    "model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for Human: 86.98%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'prob': '86.98%', 'label': 1},\n",
       " 'This text is most likely written by an Human')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "The explosive growth in AI is one of the most conspicuous signs of technological development. Due to the exponential usage of AI tools, many students employ them as co-pilots.\n",
    "\n",
    "The problem is that it was deemed acceptable to use them only for reference purposes, but reality wasnâ€™t like that.\n",
    "\n",
    "Many cases of abuse were detected, where students simply copied machine-generated texts and pasted them onto their assessments and essays.\n",
    "\n",
    "Some social media platforms exhibit tricky methods to bypass AI detection by regenerating or paraphrasing the machine-written text.\n",
    "\n",
    "Given this social phenomenon of exploiting AI, many stakeholders in high-level educational organizations must be aware of this.\n",
    "\n",
    "This project aims to help raise awareness of educational integrity for the future education landscape.\n",
    "\"\"\"\n",
    "model(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect_gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
