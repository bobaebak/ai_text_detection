{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together to calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statisitcal (Perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Filters logits to only the top k choices\n",
    "    from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values,\n",
    "                       torch.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "                       logits)\n",
    "\n",
    "class LM:\n",
    "    def __init__(self, model_name_or_path=\"gpt2\", device=\"mps\"):\n",
    "        self.device = device\n",
    "        self.enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.start_token = self.enc(self.enc.bos_token, return_tensors='pt').data['input_ids'][0]\n",
    "        print(f\"Loaded GPT-2 model! on {self.device}\")\n",
    "\n",
    "    def check_probabilities(self, in_text, topk=40):\n",
    "        # Process input\n",
    "        token_ids = self.enc(in_text, return_tensors='pt').data['input_ids'][0]\n",
    "        token_ids = torch.concat([self.start_token, token_ids])\n",
    "        # Forward through the model\n",
    "        output = self.model(token_ids.to(self.device))\n",
    "        all_logits = output.logits[:-1].detach().squeeze()\n",
    "        # construct target and pred\n",
    "        # yhat = torch.softmax(logits[0, :-1], dim=-1)\n",
    "        all_probs = torch.softmax(all_logits, dim=1)\n",
    "\n",
    "        y = token_ids[1:]\n",
    "        # Sort the predictions for each timestep\n",
    "        sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()\n",
    "        # [(pos, prob), ...]\n",
    "        real_topk_pos = list(\n",
    "            [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
    "             for i in range(y.shape[0])])\n",
    "        real_topk_probs = all_probs[np.arange(\n",
    "            0, y.shape[0], 1), y].data.cpu().numpy().tolist()\n",
    "        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))\n",
    "\n",
    "        real_topk = list(zip(real_topk_pos, real_topk_probs))\n",
    "        # [str, str, ...]\n",
    "        bpe_strings = self.enc.convert_ids_to_tokens(token_ids[:])\n",
    "\n",
    "        bpe_strings = [self.postprocess(s) for s in bpe_strings]\n",
    "\n",
    "        topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)\n",
    "\n",
    "        pred_topk = [list(zip(self.enc.convert_ids_to_tokens(topk_prob_inds[i]),\n",
    "                              topk_prob_values[i].data.cpu().numpy().tolist()\n",
    "                              )) for i in range(y.shape[0])]\n",
    "        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
    "\n",
    "\n",
    "        # pred_topk = []\n",
    "        payload = {'bpe_strings': bpe_strings,\n",
    "                   'real_topk': real_topk,\n",
    "                   'pred_topk': pred_topk}\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def sample_unconditional(self, length=100, topk=5, temperature=1.0):\n",
    "        '''\n",
    "        Sample `length` words from the model.\n",
    "        Code strongly inspired by\n",
    "        https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Get the token ID\n",
    "        start_token_id = self.enc.encode('<|endoftext|>', add_special_tokens=False)[0]\n",
    "\n",
    "        context = torch.full((1, 1),\n",
    "                            #  self.enc.encoder[self.start_token],\n",
    "                             start_token_id, \n",
    "                             device=self.device,\n",
    "                             dtype=torch.long)\n",
    "        prev = context\n",
    "        output = context\n",
    "        past = None\n",
    "        # Forward through the model\n",
    "        with torch.no_grad():\n",
    "            for i in range(length):\n",
    "                # logits, past_key_values = self.model(prev, past_key_values=past)\n",
    "                outputs = self.model(prev, past_key_values=past)\n",
    "                logits = outputs.logits\n",
    "                past = outputs.past_key_values\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                # Filter predictions to topk and softmax\n",
    "                probs = torch.softmax(top_k_logits(logits, k=topk),\n",
    "                                      dim=-1)\n",
    "                # Sample\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "                # Construct output\n",
    "                output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "        output_text = self.enc.decode(output[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    def postprocess(self, token):\n",
    "        with_space = False\n",
    "        with_break = False\n",
    "        if token.startswith('Ġ'):\n",
    "            with_space = True\n",
    "            token = token[1:]\n",
    "            # print(token)\n",
    "        elif token.startswith('â'):\n",
    "            token = ' '\n",
    "        elif token.startswith('Ċ'):\n",
    "            token = ' '\n",
    "            with_break = True\n",
    "\n",
    "        token = '-' if token.startswith('â') else token\n",
    "        token = '“' if token.startswith('ľ') else token\n",
    "        token = '”' if token.startswith('Ŀ') else token\n",
    "        token = \"'\" if token.startswith('Ļ') else token\n",
    "\n",
    "        if with_space:\n",
    "            token = '\\u0120' + token\n",
    "        if with_break:\n",
    "            token = '\\u010A' + token\n",
    "\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT-2 model! on mps\n",
      "0.41 Seconds for a check with GPT-2\n",
      "2.37 Seconds for a sample from GPT-2\n",
      "SAMPLE: <|endoftext|>\n",
      "\"We are very proud that our members are now able to join our community,\" said Mayor Ed Murray, who announced his support for the initiative on Twitter on Tuesday.\n",
      "\n",
      "Murray added that he was \"very excited to hear about our supporters and their support of the initiative.\"\n",
      "\n",
      "The city will be donating $1,500 each to the campaign, which aims to help fund the project by paying for its initial $10,000 goal.\n",
      "\n",
      "The city's first phase of the initiative\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tests for GPT-2\n",
    "'''\n",
    "\n",
    "raw_text = \"\"\"\n",
    "My name is bobae bak where lived in korea. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lm = LM()\n",
    "start = time.time()\n",
    "payload = lm.check_probabilities(raw_text, topk=40)\n",
    "end = time.time()\n",
    "print(\"{:.2f} Seconds for a check with GPT-2\".format(end - start))\n",
    "\n",
    "start = time.time()\n",
    "sample = lm.sample_unconditional()\n",
    "end = time.time()\n",
    "print(\"{:.2f} Seconds for a sample from GPT-2\".format(end - start))\n",
    "print(\"SAMPLE:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bpe_strings': ['<|endoftext|>',\n",
       "  'Ċ ',\n",
       "  'In',\n",
       "  'Ġa',\n",
       "  'Ġshocking',\n",
       "  'Ġfinding',\n",
       "  ',',\n",
       "  'Ġscientist',\n",
       "  'Ġdiscovered',\n",
       "  'Ġa',\n",
       "  'Ġherd',\n",
       "  'Ġof',\n",
       "  'Ġunic',\n",
       "  'orns',\n",
       "  'Ġliving',\n",
       "  'Ġin',\n",
       "  'Ġa',\n",
       "  'Ġremote',\n",
       "  ',',\n",
       "  'Ġpreviously',\n",
       "  'Ġunexpl',\n",
       "  'ored',\n",
       "  'Ġvalley',\n",
       "  ',',\n",
       "  'Ġin',\n",
       "  'Ġthe',\n",
       "  'ĠAnd',\n",
       "  'es',\n",
       "  'ĠMountains',\n",
       "  '.',\n",
       "  'ĠEven',\n",
       "  'Ġmore',\n",
       "  'Ġsurprising',\n",
       "  'Ġto',\n",
       "  'Ġthe',\n",
       "  'Ġresearchers',\n",
       "  'Ġwas',\n",
       "  'Ġthe',\n",
       "  'Ġfact',\n",
       "  'Ġthat',\n",
       "  'Ġthe',\n",
       "  'Ġunic',\n",
       "  'orns',\n",
       "  'Ġspoke',\n",
       "  'Ġperfect',\n",
       "  'ĠEnglish',\n",
       "  '.',\n",
       "  'Ċ '],\n",
       " 'real_topk': [(0, 0.0623),\n",
       "  (2, 0.02149),\n",
       "  (1, 0.09866),\n",
       "  (16, 0.00723),\n",
       "  (82, 0.00113),\n",
       "  (0, 0.48222),\n",
       "  (1516, 4e-05),\n",
       "  (82, 0.00165),\n",
       "  (1, 0.10015),\n",
       "  (2767, 2e-05),\n",
       "  (0, 0.97458),\n",
       "  (869, 0.00016),\n",
       "  (0, 0.99857),\n",
       "  (11, 0.01713),\n",
       "  (0, 0.54261),\n",
       "  (0, 0.34731),\n",
       "  (7, 0.01207),\n",
       "  (5, 0.04114),\n",
       "  (417, 0.00027),\n",
       "  (2, 0.0744),\n",
       "  (0, 0.99256),\n",
       "  (8, 0.01505),\n",
       "  (4, 0.04377),\n",
       "  (1, 0.06449),\n",
       "  (0, 0.17177),\n",
       "  (68, 0.00269),\n",
       "  (0, 0.56245),\n",
       "  (1, 0.12028),\n",
       "  (0, 0.31169),\n",
       "  (68, 0.0006),\n",
       "  (1, 0.09908),\n",
       "  (2, 0.09151),\n",
       "  (6, 0.00971),\n",
       "  (1, 0.12462),\n",
       "  (0, 0.07961),\n",
       "  (1, 0.26692),\n",
       "  (1, 0.35176),\n",
       "  (0, 0.5261),\n",
       "  (0, 0.92771),\n",
       "  (0, 0.41228),\n",
       "  (0, 0.17672),\n",
       "  (0, 0.99786),\n",
       "  (181, 0.00036),\n",
       "  (170, 0.00055),\n",
       "  (0, 0.59236),\n",
       "  (0, 0.46646),\n",
       "  (0, 0.6702)],\n",
       " 'pred_topk': [[('Ċ ', 0.06230081245303154),\n",
       "   ('The', 0.03769966959953308),\n",
       "   ('\"', 0.02411375567317009),\n",
       "   ('A', 0.019402500241994858),\n",
       "   ('I', 0.018320579081773758)],\n",
       "  [('The', 0.08049516379833221),\n",
       "   ('A', 0.041296493262052536),\n",
       "   ('In', 0.021486543118953705),\n",
       "   ('\"', 0.02127091959118843),\n",
       "   ('This', 0.01705935411155224)],\n",
       "  [('Ġthe', 0.1574123501777649),\n",
       "   ('Ġa', 0.0986565575003624),\n",
       "   ('Ġthis', 0.07084096223115921),\n",
       "   ('Ġan', 0.03587526082992554),\n",
       "   ('Ġhis', 0.0236755833029747)],\n",
       "  [('Ġrecent', 0.04634993523359299),\n",
       "   ('Ġnew', 0.03410128876566887),\n",
       "   ('Ġstatement', 0.031161589547991753),\n",
       "   ('Ġmove', 0.0292548518627882),\n",
       "   ('Ġrare', 0.01812889240682125)],\n",
       "  [('Ġtwist', 0.081727534532547),\n",
       "   ('Ġdevelopment', 0.07948243618011475),\n",
       "   ('Ġmove', 0.06181273236870766),\n",
       "   ('Ġnew', 0.045099806040525436),\n",
       "   ('Ġincident', 0.035901330411434174)],\n",
       "  [(',', 0.4822215139865875),\n",
       "   ('Ġthat', 0.11817482113838196),\n",
       "   ('Ġin', 0.05890744552016258),\n",
       "   ('Ġfrom', 0.026670336723327637),\n",
       "   ('Ġon', 0.024244925007224083)],\n",
       "  [('Ġa', 0.22066177427768707),\n",
       "   ('Ġthe', 0.19108925759792328),\n",
       "   ('Ġan', 0.03025779128074646),\n",
       "   ('Ġpolice', 0.025184497237205505),\n",
       "   ('Ġtwo', 0.01914926990866661)],\n",
       "  [('ĠDr', 0.08546074479818344),\n",
       "   ('Ġat', 0.033409252762794495),\n",
       "   ('Ġwho', 0.032886020839214325),\n",
       "   ('Ġand', 0.027222316712141037),\n",
       "   ('ĠDavid', 0.023536495864391327)],\n",
       "  [('Ġthat', 0.5031324625015259),\n",
       "   ('Ġa', 0.10014915466308594),\n",
       "   ('Ġthe', 0.07146178930997849),\n",
       "   ('Ġan', 0.016896849498152733),\n",
       "   ('Ġin', 0.008413509465754032)],\n",
       "  [('Ġnew', 0.13334842026233673),\n",
       "   ('Ġway', 0.02023237943649292),\n",
       "   ('Ġmysterious', 0.01722702756524086),\n",
       "   ('Ġsecret', 0.014007077552378178),\n",
       "   ('Ġrare', 0.013952468521893024)],\n",
       "  [('Ġof', 0.9745827317237854),\n",
       "   ('Ġthat', 0.004117267671972513),\n",
       "   ('-', 0.002004943322390318),\n",
       "   ('Ġin', 0.0010776732815429568),\n",
       "   ('Ġanimal', 0.0007665788871236145)],\n",
       "  [('Ġwolves', 0.02047210931777954),\n",
       "   ('Ġsheep', 0.017611877992749214),\n",
       "   ('Ġcows', 0.016569841653108597),\n",
       "   ('Ġwild', 0.015278717502951622),\n",
       "   ('Ġelephants', 0.015270676463842392)],\n",
       "  [('orns', 0.9985699653625488),\n",
       "   ('urs', 0.0003584900696296245),\n",
       "   ('orn', 0.00035716197453439236),\n",
       "   ('am', 0.0001565547863719985),\n",
       "   ('ast', 0.0001403487112838775)],\n",
       "  [('Ġthat', 0.10291559249162674),\n",
       "   ('Ġin', 0.08531416952610016),\n",
       "   ('Ġwere', 0.04447026178240776),\n",
       "   (',', 0.03531709685921669),\n",
       "   ('Ġhad', 0.026172257959842682)],\n",
       "  [('Ġin', 0.5426099300384521),\n",
       "   ('Ġon', 0.2033875435590744),\n",
       "   ('Ġnear', 0.01936705783009529),\n",
       "   ('Ġat', 0.019098540768027306),\n",
       "   ('Ġunder', 0.017682408913969994)],\n",
       "  [('Ġa', 0.34731006622314453),\n",
       "   ('Ġthe', 0.1866420954465866),\n",
       "   ('Ġan', 0.06568270176649094),\n",
       "   ('Ġtheir', 0.015082304365932941),\n",
       "   ('Ġone', 0.00979539193212986)],\n",
       "  [('Ġcave', 0.055220283567905426),\n",
       "   ('Ġforest', 0.03967941924929619),\n",
       "   ('Ġsmall', 0.024834154173731804),\n",
       "   ('Ġdesert', 0.02223709039390087),\n",
       "   ('Ġtiny', 0.019785473123192787)],\n",
       "  [('Ġarea', 0.0882231816649437),\n",
       "   ('Ġpart', 0.07736581563949585),\n",
       "   ('Ġforest', 0.043899137526750565),\n",
       "   ('Ġvillage', 0.04353325814008713),\n",
       "   ('Ġmountain', 0.041313111782073975)],\n",
       "  [('Ġremote', 0.085575170814991),\n",
       "   ('Ġbarren', 0.04471978172659874),\n",
       "   ('Ġisolated', 0.04429161548614502),\n",
       "   ('Ġmountainous', 0.03833061084151268),\n",
       "   ('Ġdesert', 0.026989949867129326)],\n",
       "  [('Ġunknown', 0.20461462438106537),\n",
       "   ('Ġund', 0.07678041607141495),\n",
       "   ('Ġunexpl', 0.07439825683832169),\n",
       "   ('Ġuntouched', 0.07140850275754929),\n",
       "   ('Ġunin', 0.06731021404266357)],\n",
       "  [('ored', 0.9925630688667297),\n",
       "   ('o', 0.005456747021526098),\n",
       "   ('oded', 0.0017637377604842186),\n",
       "   ('oted', 3.3507898479001597e-05),\n",
       "   ('oring', 2.8897957236040384e-05)],\n",
       "  [('Ġarea', 0.235423281788826),\n",
       "   ('Ġregion', 0.08906190097332001),\n",
       "   ('Ġpart', 0.0572243370115757),\n",
       "   ('Ġworld', 0.04775293543934822),\n",
       "   (',', 0.021045073866844177)],\n",
       "  [('Ġin', 0.24121110141277313),\n",
       "   ('.', 0.16547474265098572),\n",
       "   ('Ġof', 0.08799703419208527),\n",
       "   ('Ġnear', 0.055169738829135895),\n",
       "   (',', 0.04377457872033119)],\n",
       "  [('Ġand', 0.09624765068292618),\n",
       "   ('Ġin', 0.06449153274297714),\n",
       "   ('Ġwhich', 0.0374363474547863),\n",
       "   ('Ġthat', 0.030384723097085953),\n",
       "   ('Ġwhere', 0.023953914642333984)],\n",
       "  [('Ġthe', 0.17176887392997742),\n",
       "   ('Ġa', 0.0873422771692276),\n",
       "   ('Ġan', 0.0385473407804966),\n",
       "   ('Ġwhat', 0.028151964768767357),\n",
       "   ('Ġnorthern', 0.024478081613779068)],\n",
       "  [('ĠHimal', 0.07072335481643677),\n",
       "   ('Ġmiddle', 0.03686785325407982),\n",
       "   ('Ġearly', 0.019767392426729202),\n",
       "   ('ĠSouth', 0.016791094094514847),\n",
       "   ('Ġwild', 0.014945250935852528)],\n",
       "  [('es', 0.5624487996101379),\n",
       "   ('aman', 0.32368284463882446),\n",
       "   ('ean', 0.06460593640804291),\n",
       "   ('hra', 0.012445658445358276),\n",
       "   ('am', 0.005189903546124697)],\n",
       "  [('.', 0.3385791778564453),\n",
       "   ('ĠMountains', 0.12027731537818909),\n",
       "   (',', 0.11130031198263168),\n",
       "   ('Ġmountains', 0.03983793780207634),\n",
       "   ('Ġin', 0.031345829367637634)],\n",
       "  [('.', 0.3116893470287323),\n",
       "   ('Ġof', 0.2705381214618683),\n",
       "   (',', 0.1378088742494583),\n",
       "   ('Ġin', 0.06050584465265274),\n",
       "   ('Ġnear', 0.021979570388793945)],\n",
       "  [('Ċ ', 0.5706056952476501),\n",
       "   ('ĠThe', 0.11535035818815231),\n",
       "   ('ĠThey', 0.039252568036317825),\n",
       "   ('Ċ ', 0.023633789271116257),\n",
       "   ('ĠIt', 0.0139614874497056)],\n",
       "  [('Ġthough', 0.4125211536884308),\n",
       "   ('Ġmore', 0.09908139705657959),\n",
       "   ('Ġafter', 0.05375876650214195),\n",
       "   ('Ġif', 0.044673800468444824),\n",
       "   ('Ġthe', 0.042655687779188156)],\n",
       "  [('Ġshocking', 0.4690558612346649),\n",
       "   ('Ġdisturbing', 0.1144605204463005),\n",
       "   ('Ġsurprising', 0.09150686860084534),\n",
       "   ('Ġstartling', 0.05409083887934685),\n",
       "   ('Ġalarming', 0.03843231126666069)],\n",
       "  [(',', 0.6053997874259949),\n",
       "   ('Ġwas', 0.14690352976322174),\n",
       "   ('Ġis', 0.10927964001893997),\n",
       "   ('Ġthan', 0.03721223771572113),\n",
       "   (':', 0.01834670640528202)],\n",
       "  [('Ġscientists', 0.15192168951034546),\n",
       "   ('Ġthe', 0.12461511045694351),\n",
       "   ('Ġus', 0.06157422438263893),\n",
       "   ('Ġresearchers', 0.057110439985990524),\n",
       "   ('Ġme', 0.04694480821490288)],\n",
       "  [('Ġresearchers', 0.07960709929466248),\n",
       "   ('Ġlocals', 0.07864604145288467),\n",
       "   ('Ġscientists', 0.056351784616708755),\n",
       "   ('Ġworld', 0.05577094852924347),\n",
       "   ('Ġpublic', 0.03372731804847717)],\n",
       "  [(',', 0.3591326177120209),\n",
       "   ('Ġwas', 0.26691651344299316),\n",
       "   ('Ġis', 0.2098969668149948),\n",
       "   ('Ġwere', 0.05550704523921013),\n",
       "   ('Ġare', 0.018270516768097878)],\n",
       "  [('Ġthat', 0.3662281036376953),\n",
       "   ('Ġthe', 0.35176125168800354),\n",
       "   ('Ġhow', 0.11521528661251068),\n",
       "   ('Ġtheir', 0.0590423084795475),\n",
       "   ('Ġa', 0.02719181776046753)],\n",
       "  [('Ġfact', 0.5260979533195496),\n",
       "   ('Ġdiscovery', 0.06952428817749023),\n",
       "   ('Ġfinding', 0.024355469271540642),\n",
       "   ('Ġpresence', 0.024059418588876724),\n",
       "   ('Ġlocation', 0.013662180863320827)],\n",
       "  [('Ġthat', 0.9277086853981018),\n",
       "   ('Ġthe', 0.023571938276290894),\n",
       "   ('Ġthey', 0.01971629075706005),\n",
       "   (',', 0.0064380415715277195),\n",
       "   ('Ġof', 0.0022415569983422756)],\n",
       "  [('Ġthe', 0.4122809171676636),\n",
       "   ('Ġthey', 0.17532485723495483),\n",
       "   ('Ġthese', 0.04886047542095184),\n",
       "   ('Ġthis', 0.03487268090248108),\n",
       "   ('Ġtheir', 0.03170061856508255)],\n",
       "  [('Ġunic', 0.17671671509742737),\n",
       "   ('Ġanimals', 0.1306062936782837),\n",
       "   ('Ġunicorn', 0.0898766964673996),\n",
       "   ('Ġherd', 0.03799700364470482),\n",
       "   ('Ġcreatures', 0.021766802296042442)],\n",
       "  [('orns', 0.9978576302528381),\n",
       "   ('orn', 0.0018685766262933612),\n",
       "   ('urs', 0.00010832364205271006),\n",
       "   ('opes', 1.793662340787705e-05),\n",
       "   ('ur', 9.246658009942621e-06)],\n",
       "  [('Ġwere', 0.27887752652168274),\n",
       "   ('Ġhad', 0.0900326818227768),\n",
       "   ('Ġare', 0.044749654829502106),\n",
       "   (\"'\", 0.03777595981955528),\n",
       "   (',', 0.03506094962358475)],\n",
       "  [('Ġa', 0.11430884897708893),\n",
       "   ('Ġin', 0.06802714616060257),\n",
       "   ('Ġthe', 0.058395277708768845),\n",
       "   ('ĠEnglish', 0.04698210582137108),\n",
       "   ('Ġonly', 0.04169216752052307)],\n",
       "  [('ĠEnglish', 0.5923593640327454),\n",
       "   ('ĠSpanish', 0.08719471842050552),\n",
       "   ('ĠItalian', 0.022190388292074203),\n",
       "   ('ĠFrench', 0.020802723243832588),\n",
       "   (',', 0.014082272537052631)],\n",
       "  [('.', 0.4664572775363922),\n",
       "   (',', 0.2577751874923706),\n",
       "   ('Ġand', 0.08018714934587479),\n",
       "   (' ', 0.017638448625802994),\n",
       "   ('Ġ-', 0.01171545498073101)],\n",
       "  [('Ċ ', 0.6702011227607727),\n",
       "   ('ĠThe', 0.0607115775346756),\n",
       "   ('Ċ ', 0.022992761805653572),\n",
       "   ('ĠThey', 0.019006051123142242),\n",
       "   ('Ġ\"', 0.016180021688342094)]]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT-2 model! on mps\n",
      "0.12 Seconds for a check with GPT-2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m Seconds for a check with GPT-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n\u001b[1;32m     10\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_unconditional\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m Seconds for a sample from GPT-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "Cell \u001b[0;32mIn[41], line 95\u001b[0m, in \u001b[0;36mLM.sample_unconditional\u001b[0;34m(self, length, topk, temperature)\u001b[0m\n\u001b[1;32m     91\u001b[0m logits, past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(prev, past_key_values\u001b[38;5;241m=\u001b[39mpast)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# outputs = self.model(prev, past_key_values=past)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# logits = outputs.logits\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# past = outputs.past_key_values\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Filter predictions to topk and softmax\u001b[39;00m\n\u001b[1;32m     98\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(top_k_logits(logits, k\u001b[38;5;241m=\u001b[39mtopk),\n\u001b[1;32m     99\u001b[0m                       dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tests for GPT-2\n",
    "'''\n",
    "lm = LM()\n",
    "start = time.time()\n",
    "payload = lm.check_probabilities(raw_text, topk=5)\n",
    "end = time.time()\n",
    "print(\"{:.2f} Seconds for a check with GPT-2\".format(end - start))\n",
    "\n",
    "start = time.time()\n",
    "sample = lm.sample_unconditional()\n",
    "end = time.time()\n",
    "print(\"{:.2f} Seconds for a sample from GPT-2\".format(end - start))\n",
    "print(\"SAMPLE:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the start token (usually <|endoftext|> for GPT-2)\n",
    "start_token = '<|endoftext|>'\n",
    "\n",
    "# Get the token ID\n",
    "start_token_id = enc.encode(start_token, add_special_tokens=False)[0]\n",
    "start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "full() received an invalid combination of arguments - got (tuple, list, dtype=torch.dtype, device=str), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# enc.encoder[start_token],\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m context\n",
      "\u001b[0;31mTypeError\u001b[0m: full() received an invalid combination of arguments - got (tuple, list, dtype=torch.dtype, device=str), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "context = torch.full((1, 1),\n",
    "        # enc.encoder[start_token],\n",
    "        50256,\n",
    "\n",
    "        device=\"mps\",\n",
    "        dtype=torch.long)\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DetectGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npct_words_masked: \\n    how many words masked for purturbation\\n    pct_words_masked * (span_length / (span_length + 2 * buffer_size))\\n    (default) 0.3\\nspan_length:\\n    (default) 2  \\nbase_model_name:\\n    target model\\n    (default) gpt2-medium\\nmask_filling_model_name:\\n    masking model\\n    (default) t5-large\\n  \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters:\n",
    "\"\"\"\n",
    "pct_words_masked: \n",
    "    how many words masked for purturbation\n",
    "    pct_words_masked * (span_length / (span_length + 2 * buffer_size))\n",
    "    (default) 0.3\n",
    "span_length:\n",
    "    (default) 2  \n",
    "base_model_name:\n",
    "    target model\n",
    "    (default) gpt2-medium\n",
    "mask_filling_model_name:\n",
    "    masking model\n",
    "    (default) t5-large\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/Users/bobaebak/git/ai_text_detection')\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from models.detectgpt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectGPTArgs(BaseModel):\n",
    "    base_model_name: str=\"gpt2\"\n",
    "    mask_filling_model_name: str=\"t5-small\"\n",
    "    pct_words_masked: float=0.3 \n",
    "    span_length: int=2 \n",
    "    n_perturbation_list: str=\"1,10\"\n",
    "    n_perturbation_rounds: int=1\n",
    "    cache_dir:str = \"../cache\"\n",
    "    n_samples: int = 1\n",
    "args = DetectGPTArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87330eb260bc48fcb24ed8673cc71992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bd90da29b248d5aecd579553c4ad0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3891630cf7964280a7b5158be0a72e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generic generative model\n",
    "# base_model, base_tokenizer = load_base_model_and_tokenizer(args.base_model_name)\n",
    "base_model_kwargs = {}\n",
    "optional_tok_kwargs = {}\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(args.base_model_name, **base_model_kwargs, cache_dir=args.cache_dir)\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(args.base_model_name, **optional_tok_kwargs, cache_dir=args.cache_dir)\n",
    "base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "# mask filling t5 model\n",
    "int8_kwargs = {}\n",
    "half_kwargs = {}\n",
    "# int8_kwargs = dict(load_in_8bit=True, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "# half_kwargs = dict(torch_dtype=torch.bfloat16)\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(args.mask_filling_model_name, **int8_kwargs, **half_kwargs, cache_dir=args.cache_dir)\n",
    "n_positions = mask_model.config.n_positions\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(args.mask_filling_model_name, model_max_length=n_positions, cache_dir=args.cache_dir)\n",
    "\n",
    "n_perturbation_list = [int(x) for x in args.n_perturbation_list.split(\",\")]\n",
    "n_perturbation_rounds = 1\n",
    "\n",
    "\n",
    "for n_perturbations in n_perturbation_list:\n",
    "    perturbation_results = get_perturbation_results(args.span_length, n_perturbations, args.n_samples)\n",
    "    for perturbation_mode in ['d', 'z']:\n",
    "        output = run_perturbation_experiment(\n",
    "            perturbation_results, perturbation_mode, span_length=args.span_length, n_perturbations=n_perturbations, n_samples=n_samples)\n",
    "        outputs.append(output)\n",
    "        with open(os.path.join(SAVE_FOLDER, f\"perturbation_{n_perturbations}_{perturbation_mode}_results.json\"), \"w\") as f:\n",
    "            json.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" # example: cuda:0\n",
    "detector_path_or_id = \"TrustSafeAI/RADAR-Vicuna-7B\"\n",
    "detector = transformers.AutoModelForSequenceClassification.from_pretrained(detector_path_or_id)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(detector_path_or_id)\n",
    "detector.eval()\n",
    "detector.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
