{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|█████████████████| 25.0/25.0 [00:00<00:00, 41.5kB/s]\n",
      "config.json: 100%|█████████████████████████████| 519/519 [00:00<00:00, 1.87MB/s]\n",
      "vocab.json: 100%|█████████████████████████████| 899k/899k [00:00<00:00, 935kB/s]\n",
      "merges.txt: 100%|█████████████████████████████| 456k/456k [00:00<00:00, 647kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:01<00:00, 1.12MB/s]\n",
      "model.safetensors: 100%|███████████████████| 1.43G/1.43G [04:08<00:00, 5.74MB/s]\n",
      "Some weights of the model checkpoint at openai-community/roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "{'Fake': 0.5336293, 'Real': 0.46637073}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/openai_detector.py --text \"My name is bobae bak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "{'Fake': 0.6938189, 'Real': 0.30618104}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/openai_detector.py --text \"I believe that I will follow my interest. I'm not saying that studying a subject for job opportunities is wrong, it's just that I'm not that kind of person. Me myself want to be a scientist in the future, and following my own interests are rather important, because doing research can be tedious or frustrating in many situations, and my interests may be the only thing to keep me going on and on. If you are only driven by profit, it's likely that you will abandon your current subject once it seems not so profitable, and that's clearly not good for the development of science.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuned Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "{'Fake': 0.17135951, 'Real': 0.8286405}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/openai_finetune_detector.py --text \"My name is bobae bak\" --model_name \"20240523_v1_epoch4.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "{'Fake': 0.036566377, 'Real': 0.9634336}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/openai_finetune_detector.py --text \"The place I'm most eager to visit is outer space, where many of the physical laws that govern the earth no longer apply. It's fascinating to see everything suspended in mid-air. And, of course, the phrase \\\"in mid-air\\\" needs a tweak, given that there's no air to speak of. It's exhilarating to move about in a completely novel way. And it's breathtaking to gaze upon our home planet from a radically new perspective. The big blue crystal sphere, as captured in the stunning images taken by astronauts, is simply awe-inspiring. I'm desperate to lay eyes on this magnificent blue orb in person.\" --model_name \"fine_tune_epoch1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Vicuna Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fake': 0.9963961243629456, 'Real': 0.0036038756370544434}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/radar_detector.py --text \"My name is bobae bak\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lines': [\"The system's availability was compromised due to a wiped master boot record and overwritten files by a wiper. \", 'To prevent such attacks, updating the OS to a newer version with enhanced security features and using reliable, up-to-date antivirus software are essential measures to protect against malware.'], 'burstiness': 135, 'perplexity': 37, 'perplexity_per_line_avg': 78.0, 'perplexity_per_line': [135, 21], 'msg': 'The Text is most probably contain parts which are generated by AI. (require more text for better Judgement)', 'label': 0, 'threshold': (60, 80)}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/gptzero_detector.py --text \"The system's availability was compromised due to a wiped master boot record and overwritten files by a wiper. To prevent such attacks, updating the OS to a newer version with enhanced security features and using reliable, up-to-date antivirus software are essential measures to protect against malware.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DetectGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "{'mean_prob': 55.02629053529829, 'mean_score': 0.5736743956804276, 'lines': '', 'msg': 'This text is most likely written by an Human', 'label': 1, 'threshold': 0.7}\n"
     ]
    }
   ],
   "source": [
    "!python ../models/detectgpt_detector.py --text \"A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
