{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDdhG63BE9vl"
      },
      "source": [
        "# Fine-Tuning of OpenAI detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lNNCcY5FFCM",
        "outputId": "a726d8f5-031d-4c95-f1b5-69a9b97c6ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpFwRO2mFVkm",
        "outputId": "3df71a40-f2f7-4d39-9efa-94bc54fddeb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/ai_text_detection/cookbooks\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/Colab Notebooks/ai_text_detection/cookbooks\n",
        "# modify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4-68HlTGE9vx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import resample\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qJveZ7hHE9v3"
      },
      "outputs": [],
      "source": [
        "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IrVcIBIE9v5"
      },
      "source": [
        "### Load json data file and convert to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kS-tbweBE9v6"
      },
      "outputs": [],
      "source": [
        "# human data\n",
        "\n",
        "# loads a tofel dataset\n",
        "with open('../dataset/human/tofel.json', \"r\") as f:\n",
        "    h_tofel_dataset = json.load(f)\n",
        "\n",
        "# loads an arxiv dataset\n",
        "with open('../dataset/human/arxiv.json', \"r\") as f:\n",
        "    h_arxiv_dataset = json.load(f)\n",
        "\n",
        "# loads student essay\n",
        "with open('../dataset/human/student_essay.json', \"r\") as f:\n",
        "    h_essay_dataset = json.load(f)\n",
        "\n",
        "# loads student computer essay\n",
        "with open('../dataset/human/student_cs_essay.json', \"r\") as f:\n",
        "    h_essay_cs_dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w6f6HRR9E9v8"
      },
      "outputs": [],
      "source": [
        "# gpt data\n",
        "\n",
        "# loads a tofel dataset\n",
        "with open('../dataset/ai/gpt2medium_tofel.json', \"r\") as f:\n",
        "    gpt_tofel_dataset = json.load(f)\n",
        "\n",
        "# loads an arxiv dataset\n",
        "with open('../dataset/ai/gpt2medium_arxiv.json', \"r\") as f:\n",
        "    gpt_arxiv_dataset = json.load(f)\n",
        "\n",
        "# loads student essay\n",
        "with open('../dataset/ai/gpt2medium_essay.json', \"r\") as f:\n",
        "    gpt_essay_dataset = json.load(f)\n",
        "\n",
        "# loads student computer essay\n",
        "with open('../dataset/ai/gpt2medium_essay_cs.json', \"r\") as f:\n",
        "    gpt_essay_cs_dataset = json.load(f)\n",
        "\n",
        "# loads a tofel dataset\n",
        "with open('../dataset/ai/gpt35_tofel.json', \"r\") as f:\n",
        "    gpt_35_tofel_dataset = json.load(f)\n",
        "\n",
        "# loads student computer essay\n",
        "with open('../dataset/ai/gpt35_essay_cs.json', \"r\") as f:\n",
        "    gpt_35_essay_cs_dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epBape2VE9v-",
        "outputId": "feac056d-5be9-4c79-9573-85466f7b898f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2478"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h_dataset = []\n",
        "for i in [h_tofel_dataset, h_arxiv_dataset, h_essay_dataset, h_essay_cs_dataset]:\n",
        "    h_dataset.extend(i)\n",
        "\n",
        "len(h_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaHJOc_mE9wB",
        "outputId": "6df71e53-bfac-435a-e62a-06a29c15ff80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2956"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_dataset = []\n",
        "for i in [gpt_tofel_dataset, gpt_arxiv_dataset, gpt_essay_dataset, gpt_essay_cs_dataset,\n",
        "          gpt_35_tofel_dataset, gpt_35_essay_cs_dataset]:\n",
        "    gpt_dataset.extend(i)\n",
        "\n",
        "len(gpt_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yvdPDLgYE9wD"
      },
      "outputs": [],
      "source": [
        "dct = {\n",
        "    \"text\": [item['input'] for item in h_dataset] + [item['input'] for item in gpt_dataset],\n",
        "    \"label\": [item['label'] for item in h_dataset] + [item['label'] for item in gpt_dataset],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(dct)\n",
        "\n",
        "def label_to_numeric(value):\n",
        "    if value == \"human\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['target'] = df['label'].apply(lambda x: label_to_numeric(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "mhFxK9EFE9wE",
        "outputId": "3105f1ae-91cb-462c-f82f-24751c1d358d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4956,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4348,\n        \"samples\": [\n          \"Computerized Systems: Material Requirements Planning Report (Assessment)\\n\\nTable of Contents\\n 1. Introduction\\n 2. Operation of MRP in UAE\\n 3. Challenges of implementing MRP in Local UAE Organizations\\n 4. Works Cited\\n\\nIntroduction\\n\\nTo efficiently handle inventory and run businesses smoothly, entrepreneurs in the UAE have turned to computerized operational activities. The widely used system is Material Requirement Planning (MRP). According to Groover (741), MRP is a computer-based system that translates the operational master plan into a series of activities and operations from the final product to the acquisition of raw materials.\\n\\nBy embracing state-of-the-art technology, firms have been able to effectively manage their inventory and improve decision-making related to inventory.\\n\\nMRP ensures that three critical goals in inventory management are met. Firstly, through the use of MRP, organizations ensure a stable supply of raw materials for processing and availability of finished products to meet customer demand. Secondly, the application of MRP systems helps to maintain a low stock level to avoid tying up cash in excess inventory.\\n\\nFinally, MRP governs key operational activities such as manufacturing, delivery, and purchasing activities to prevent overlapping and interruptions in production (Stevenson and Hojati 16). MRP is customized to meet the specifications of each enterprise based on its main activities.\\n\\nOperation of MRP in UAE\\n\\nMRP represents an evolution in line with Economic Order Quantity (EOQ) principles, aimed at maintaining the lowest level of inventory. It specifies the replenishing amount, known as the reorder level, as well as the maximum and minimum stock levels.\\n\\nUnlike the manual execution of the EOQ system, MRP is automated and provides a systematic response based on inventory levels, recommending corrective actions for fluctuations.\\n\\nDue to the dynamic nature of demand and market uncertainty, production decisions are often challenging to predict, and MRP systems assist production managers in making informed decisions.\\n\\nAll manufacturing firms face production dilemmas because consumers demand supplies promptly while a significant amount of time is required for manufacturing. To address these conflicting time demands, MRP remains the most reliable tool for enhancing continuous production.\\n\\nThe MRP system is designed to work backward from an estimated finished product amount to determine the labor and raw material requirements. These processes are systematically scheduled and broken down into subassemblies, components, parts, and materials (Groover 742).\\n\\nEach process is assigned a specific time limit and cost function, enabling the operations manager to understand the duration and cost of producing each product. According to Stevenson and Hojati (16), MRP helps management answer three key production questions: what is needed, how much is required, and when it is needed.\\n\\nA thorough understanding of what to produce, how much to produce, and when simplifies operational decision-making to ensure that only the necessary supplies are purchased in the correct quantities. Childe (77) emphasizes the importance of verifying the data entered into the system to avoid producing incorrect final products. For example, inaccurate input compositions will not yield the expected final products.\\n\\nMRP subdivides inventory requirements into planning periods to ensure a timely and systematic production process. All planning periods are optimized to maintain low inventory levels and minimize carrying costs.\\n\\nEffectively adopting an optimized MRP helps management understand how to scale up production capacity based on available capital and time considerations.\\n\\nFurthermore, successful implementation of MRP helps firms avoid contractual conflicts with customers and suppliers because only the necessary raw materials and supplies are ordered and supplied. Stevenson and Hojati (18) highlight the importance of understanding inventory requirements to operate within the operational range.\\n\\nMRP input is derived from three sources: the bill of materials, master schedule, and inventory records. The bill of materials contains all the details of the inventory, including raw materials, components, and assemblies needed to complete each unit of production.\\n\\nBragg (22) asserts that the bill of materials is the foundation of all operations, and high accuracy must be maintained to avoid negative impacts on subsequent operations.\\n\\nGiven the importance of this item, each manufacturer should maintain a bill of materials based on the type of products they produce. The MRP system deducts the quantity of materials used to produce a complete product from the inventory records to meet the ordered requirements.\\n\\nAccording to Groover (742), the master schedule forecasts the production activities of the firm. Production projections are estimated using internal projections and customer order rates. The master schedule indicates outstanding orders in terms of products and their collection times.\\n\\nThe schedule is designed to cover a sufficient period to produce the final product. In other words, the schedule is a function of demand and does not solely depend on the production capacity of the plant.\\n\\nIt is important to note that the period should accommodate all subassemblies and operations until the final product is produced. However, the feasibility of the schedule is not guaranteed until prototypes are tested and their reliability is proven.\\n\\nInventory records provide information regarding inventory levels in the warehouse compared to the amount ordered. In inventory accounts, current stock is subtracted from material requirements. Bragg (20) adds that inventory records include details such as gross requirements, planned receipts, projected stock levels, supplier information, reorder levels, and reorder quantities.\\n\\nChallenges of Implementing MRP in Local UAE Organizations\\n\\nThere are several challenges that hinder the successful implementation of the MRP system. The primary challenge is data integrity, which is often overlooked but has significant consequences. Thorough screening of the data input is necessary to achieve the expected output. For example, incorrect input compositions can result in lower quality final products.\\n\\nIn general, computerized systems produce quality output based on the input they receive. Childe (76) commented on how data integrity is compromised by inaccurate phase adjustments, input errors, unrecorded by-products, spoiled materials, and arithmetic errors.\\n\\nSimilar sentiments were shared by Groover (758), who emphasizes the need to verify data entry to prevent distortions in subsequent processes. These errors can lead to incorrect inventory assessments and process disruptions, potentially causing the collapse of a firm.\\n\\nAnother challenge facing MRP systems is determining the time required for product development from the initial stage to the final product (Childe 73). A uniform lead time assumption is often imposed on all products, although this is not always the case due to changes in demand and other uncertainties.\\n\\nThis assumption is also flawed because lead time is affected by the quantity of products being produced simultaneously in the plant; larger quantities require longer lead times. Therefore, management should allow for spare time to account for risks and uncertainties inherent in random elements like demand and supply.\\n\\nFirms operating multiple branches in different regions face the dilemma of whether to order new supplies when they have a large backlog in other branches. Each factory should maintain a separate MRP to avoid future challenges and confusion between branches.\\n\\nHowever, with proper communication, MRP can still link various branches, provided the systems adopted by the firm function efficiently before implementing the MRP system. Additionally, attention should be paid to lead times, which may be affected by unforeseen circumstances during carriage.\\n\\nAnother challenge that hinders the successful implementation of MRP is the lack of technological integration. Technology is constantly evolving, and each firm should embrace new technology to ensure smooth operations without delays and maintain output quality. For large firms, the sheer number of components and processes involved can be overwhelming, requiring specialized computers to handle transactions smoothly.\\n\\nWorks Cited\\n\\nBragg, Steven M. Inventory Accounting: A Comprehensive Guide. New Jersey: John Wiley & Sons, 2005.\\n\\nChilde, Stephen J. An Introduction to Computer-Aided Production Management. London: St. Edmundsbury Press, 1997.\\n\\nGroover, Mikell P. Automation, Production Systems, and Computer-Integrated Manufacturing. New Jersey: Prentice Hall, 2008.\\n\\nStevenson, William J. & Hojati, Mehran. Production/Operation/Management. New York: McGraw-Hill, 2001.\",\n          \"We subject the phenomenologically successful large volume scenario of\\nhep-th/0502058 to a first consistency check in string theory. In particular, we\\nconsider whether the expansion of the string effective action is consistent in\\nthe presence of D-branes and O-planes. Due to the no-scale structure at\\ntree-level, the scenario is surprisingly robust. We compute the modification of\\nsoft supersymmetry breaking terms, and find only subleading corrections. We\\nalso comment that for large-volume limits of toroidal orientifolds and fibered\\nCalabi-Yau manifolds the corrections can be more important, and we discuss\\nfurther checks that need to be performed.\",\n          \"The status of 38 halo white dwarf candidates identified by Oppenheimer et al.\\n(2001) has been intensively discussed by various authors. In analyses\\nundertaken to date, trigonometric parallaxes are crucial missing data. Distance\\nmeasurements are mandatory to kinematically segregate halo object from disk\\nobjects and hence enable a more reliable estimate of the local density of halo\\ndark matter residing in such objects.\\n  We present trigonometric parallax measurements for 15 candidate halo white\\ndwarfs (WDs) selected from the Oppenheimer et al. (2001) list. We observed the\\nstars using the ESO 1.56-m Danish Telescope and ESO 2.2-m telescope from August\\n2001 to July 2004. Parallaxes with accuracies of 1--2 mas were determined\\nyielding relative errors on distances of $\\\\sim5$% for 6 objects, $\\\\sim12$% for\\n3 objects, and $\\\\sim20$% for two more objects. Four stars appear to be too\\ndistant (probably farther than 100 pc) to have measurable parallaxes in our\\nobservations. Distances, absolute magnitudes and revised space velocities were\\nderived for the 15 halo WDs from the Oppenheimer et al. (2001) list. Halo\\nmembership is confirmed unambiguously for 6 objects while 5 objects may be\\nthick disk members and 4 objects are too distant to draw any conclusion based\\nsolely on kinematics. Comparing our trigonometric parallaxes with photometric\\nparallaxes used in previous work reveals an overestimation of distance as\\nderived from photometric techniques. This new data set can be used to revise\\nthe halo white dwarf space density, and that analysis will be presented in a\\nsubsequent publication.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ai\",\n          \"human\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3c7b7d55-b4fe-4782-aaf1-36637e38a6fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The spectral action on noncommutative torus is...</td>\n",
              "      <td>human</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Computerized Ordering System Thesis\\n\\nIntrodu...</td>\n",
              "      <td>ai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We report observation of spin-orbit Berry's ph...</td>\n",
              "      <td>human</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We report observation of spin-orbit Berry's ph...</td>\n",
              "      <td>ai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\nThe first thing you need to do is to get y...</td>\n",
              "      <td>ai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4951</th>\n",
              "      <td>The Lessons from Yusufiyah Essay\\n\\nThe Lesson...</td>\n",
              "      <td>human</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4952</th>\n",
              "      <td>We present an algorithm that produces the clas...</td>\n",
              "      <td>ai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4953</th>\n",
              "      <td>In this paper we investigate the optimal contr...</td>\n",
              "      <td>human</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4954</th>\n",
              "      <td>Correlation between Socioeconomic Status and H...</td>\n",
              "      <td>human</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4955</th>\n",
              "      <td>\\n\\nThe first thing you need to do is to get y...</td>\n",
              "      <td>ai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4956 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c7b7d55-b4fe-4782-aaf1-36637e38a6fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c7b7d55-b4fe-4782-aaf1-36637e38a6fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c7b7d55-b4fe-4782-aaf1-36637e38a6fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f509241-e501-43b5-965e-1ba6366ae20e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f509241-e501-43b5-965e-1ba6366ae20e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f509241-e501-43b5-965e-1ba6366ae20e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_14dd9599-148c-4a91-9018-74c3f1c4250e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_14dd9599-148c-4a91-9018-74c3f1c4250e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                   text  label  target\n",
              "0     The spectral action on noncommutative torus is...  human       1\n",
              "1     Computerized Ordering System Thesis\\n\\nIntrodu...     ai       0\n",
              "2     We report observation of spin-orbit Berry's ph...  human       1\n",
              "3     We report observation of spin-orbit Berry's ph...     ai       0\n",
              "4     \\n\\nThe first thing you need to do is to get y...     ai       0\n",
              "...                                                 ...    ...     ...\n",
              "4951  The Lessons from Yusufiyah Essay\\n\\nThe Lesson...  human       1\n",
              "4952  We present an algorithm that produces the clas...     ai       0\n",
              "4953  In this paper we investigate the optimal contr...  human       1\n",
              "4954  Correlation between Socioeconomic Status and H...  human       1\n",
              "4955  \\n\\nThe first thing you need to do is to get y...     ai       0\n",
              "\n",
              "[4956 rows x 3 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_counts = df['target'].value_counts()\n",
        "majority_class = class_counts.idxmax()\n",
        "minority_class = class_counts.idxmin()\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_df = df[df['target'] == majority_class]\n",
        "minority_df = df[df['target'] == minority_class]\n",
        "\n",
        "# Undersample majority class\n",
        "undersampled_majority_df = resample(majority_df,\n",
        "                                    replace=False,  # Sample without replacement\n",
        "                                    n_samples=len(minority_df),  # Match minority class size\n",
        "                                    random_state=42)  # For reproducibility\n",
        "\n",
        "# Combine minority class with undersampled majority class\n",
        "undersampled_df = pd.concat([undersampled_majority_df, minority_df])\n",
        "undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df = undersampled_df\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI5TS8TiE9wG"
      },
      "source": [
        "### Preparing the Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Nw9xGDeVE9wH"
      },
      "outputs": [],
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = self.data.text\n",
        "        self.target = self.data.target\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.target[index], dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aWzXey0E9wI",
        "outputId": "e5f09c67-fa49-4090-9d0c-c2de14aa0e5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at openai-community/roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/roberta-large-openai-detector\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"openai-community/roberta-large-openai-detector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZSzTfZGcE9wJ"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLnyur5yE9wK",
        "outputId": "53439859-7fd6-4c53-d8c9-ea59323287f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FULL Dataset: (4956, 3)\n",
            "TRAIN Dataset: (3965, 3)\n",
            "TEST Dataset: (991, 3)\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.8\n",
        "train_data=df.sample(frac=train_size, random_state=42)\n",
        "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uj4NJRDpE9wL"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE, 'shuffle': True, 'num_workers': 0}\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnkqpeAkE9wL"
      },
      "source": [
        "### Fine Tuning the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NLnVtFQ6E9wM"
      },
      "outputs": [],
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jyF2MALDE9wM"
      },
      "outputs": [],
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "VPbMBOgbRtFF"
      },
      "outputs": [],
      "source": [
        "# Training Loop with Early Stopping:**\n",
        "def train_model(epochs, file_name, model):\n",
        "  best_loss = float('inf')\n",
        "  best_model_weights = None\n",
        "  patience = 10\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "\n",
        "  model = model.to(device)\n",
        "  for epoch in range(1, epochs+1):\n",
        "    n_correct = 0\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0; nb_tr_examples = 0;\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for i, data in enumerate(tqdm(training_loader)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss_ = loss_function(outputs.logits, targets)\n",
        "\n",
        "        tr_loss += loss_.item()\n",
        "        big_val, big_idx = torch.max(outputs.logits, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if i%100==0:\n",
        "            step_loss = tr_loss/nb_tr_steps\n",
        "            step_acc = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"Training Loss per 100 steps: {step_loss}\")\n",
        "            print(f\"Training Accuracy per 100 steps: {step_acc}\")\n",
        "            print(\"==\"*50)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    train_epoch_loss = tr_loss/nb_tr_steps\n",
        "    train_epoch_acc = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {train_epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {train_epoch_acc}\")\n",
        "    train_losses.append(train_epoch_loss)\n",
        "\n",
        "    # ######################\n",
        "    # # validate the model #\n",
        "    # ######################\n",
        "    # model.eval()  # Set model to evaluation mode\n",
        "    # val_n_correct = 0; val_n_wrong = 0; total = 0;\n",
        "    # val_loss = 0;\n",
        "    # nb_val_steps = 0; nb_val_example = 0;\n",
        "    # with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    #     for i, data in enumerate(tqdm(testing_loader)):\n",
        "    #       ids = data['ids'].to(device, dtype = torch.long)\n",
        "    #       mask = data['mask'].to(device, dtype = torch.long)\n",
        "    #       token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "    #       targets = data['target'].to(device, dtype = torch.long)\n",
        "\n",
        "    #       outputs = model(ids, mask, token_type_ids)\n",
        "    #       loss_ = loss_function(outputs.logits, targets)\n",
        "\n",
        "    #       val_loss += loss_.item()\n",
        "    #       big_val, big_idx = torch.max(outputs.logits, dim=1)\n",
        "    #       val_n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "    #       nb_val_steps += 1\n",
        "    #       nb_val_examples += targets.size(0)\n",
        "\n",
        "    #       if i%100==0:\n",
        "    #         step_loss = tr_loss/nb_tr_steps\n",
        "    #         step_acc = (n_correct*100)/nb_tr_examples\n",
        "    #         print(f\"Validation Loss per 100 steps: {step_loss}\")\n",
        "    #         print(f\"Validation Accuracy per 100 steps: {step_acc}\")\n",
        "\n",
        "    #       if val_loss < best_loss:\n",
        "    #         best_loss = val_loss\n",
        "    #         best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    #         patience = 10  # Reset patience counter\n",
        "    #       else:\n",
        "    #         patient -= 1\n",
        "    #         if patient == 0:\n",
        "    #           print(\"=\"*50)\n",
        "    #           print(\"=\"*20, \"early stop\", \"=\"*20)\n",
        "    #           print(\"=\"*50)\n",
        "    #           break\n",
        "\n",
        "    # valid_epoch_loss = val_loss/nb_val_steps\n",
        "    # valid_epoch_accu = (val_n_correct*100)/nb_val_example\n",
        "    # print(f\"Validation Loss Epoch: {valid_epoch_loss}\")\n",
        "    # print(f\"Validation Accuracy Epoch: {valid_epoch_accu}\")\n",
        "    # valid_losses.append(valid_epoch_loss)\n",
        "\n",
        "\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/ai_text_detection/models/'+file_name+'_epoch'+str(epoch)+'.pth'\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "  return train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w18RAjtzdiU_",
        "outputId": "7fc3456a-ccd0-46b3-f7ea-9d4947f80f1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.7031255960464478\n",
            "Training Accuracy per 100 steps: 87.5\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:50<07:19,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5198084413267599\n",
            "Training Accuracy per 100 steps: 83.91089108910892\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:41<05:28,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5553283661103515\n",
            "Training Accuracy per 100 steps: 83.3955223880597\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:32<03:37,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5559600055576658\n",
            "Training Accuracy per 100 steps: 83.80398671096346\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:23<01:46,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5571192507585627\n",
            "Training Accuracy per 100 steps: 83.8216957605985\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:09<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 1: 83.5561160151324\n",
            "Training Loss Epoch: 0.5589507854864707\n",
            "Training Accuracy Epoch: 83.5561160151324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 1.6776599884033203\n",
            "Training Accuracy per 100 steps: 75.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:50<07:23,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5222560936668058\n",
            "Training Accuracy per 100 steps: 84.77722772277228\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:41<05:27,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5769979786475314\n",
            "Training Accuracy per 100 steps: 83.33333333333333\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:32<03:37,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5503550191855435\n",
            "Training Accuracy per 100 steps: 83.63787375415282\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:23<01:46,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5658189091736223\n",
            "Training Accuracy per 100 steps: 83.22942643391521\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:10<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 2: 83.95964691046659\n",
            "Training Loss Epoch: 0.555270833052477\n",
            "Training Accuracy Epoch: 83.95964691046659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.12199705839157104\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:50<07:22,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5885796867274117\n",
            "Training Accuracy per 100 steps: 83.7871287128713\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:41<05:29,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5667090179463524\n",
            "Training Accuracy per 100 steps: 84.32835820895522\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:33<03:38,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5505517761461264\n",
            "Training Accuracy per 100 steps: 84.59302325581395\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:24<01:46,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5587301412527623\n",
            "Training Accuracy per 100 steps: 84.03990024937656\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:10<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 3: 84.01008827238336\n",
            "Training Loss Epoch: 0.5591309158311736\n",
            "Training Accuracy Epoch: 84.01008827238336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.45658066868782043\n",
            "Training Accuracy per 100 steps: 87.5\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:50<07:22,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.6353326649543377\n",
            "Training Accuracy per 100 steps: 81.55940594059406\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:42<05:30,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5853137497266578\n",
            "Training Accuracy per 100 steps: 82.96019900497512\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:33<03:36,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5735118876016417\n",
            "Training Accuracy per 100 steps: 83.34717607973423\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:24<01:46,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.5700660363905726\n",
            "Training Accuracy per 100 steps: 83.16708229426433\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:10<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 4: 83.203026481715\n",
            "Training Loss Epoch: 0.5654975044805812\n",
            "Training Accuracy Epoch: 83.203026481715\n"
          ]
        }
      ],
      "source": [
        "train_losses, valid_losses = train_model(4, str('23May2024_2'), model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD_0ukY11lVK",
        "outputId": "c13f6dc6-5ed4-461e-a3a0-2468206427ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0.5589507854864707,\n",
              "  0.555270833052477,\n",
              "  0.5591309158311736,\n",
              "  0.5654975044805812],\n",
              " [])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eXXVGn48E9wN"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "def train(epoch, model):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # for i, data in tqdm(enumerate(training_loader, 0), total=len(training_loader)):\n",
        "    # for _, data in tqdm(enumerate(training_loader, 0)):\n",
        "    for i, data in enumerate(tqdm(training_loader)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['target'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs.logits, targets)\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.logits, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if i%100==0:\n",
        "            step_loss = tr_loss/nb_tr_steps\n",
        "            step_acc = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"Training Loss per 100 steps: {step_loss}\")\n",
        "            print(f\"Training Accuracy per 100 steps: {step_acc}\")\n",
        "            print(\"==\"*50)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_acc = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_acc}\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, step_loss, step_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_HKcT-E0E9wO",
        "outputId": "63b71508-2b47-47f3-f058-2e5b97db880e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.29653799533843994\n",
            "Training Accuracy per 100 steps: 87.5\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:56<07:53,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.1919173747785205\n",
            "Training Accuracy per 100 steps: 94.05940594059406\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:55<05:53,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.14656814898448806\n",
            "Training Accuracy per 100 steps: 94.96268656716418\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:55<03:55,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.13486164158803451\n",
            "Training Accuracy per 100 steps: 95.34883720930233\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:55<01:55,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.118978393176005\n",
            "Training Accuracy per 100 steps: 95.82294264339153\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:50<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 0: 96.14123581336696\n",
            "Training Loss Epoch: 0.11149888655446345\n",
            "Training Accuracy Epoch: 96.14123581336696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.015021657571196556\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:59<07:53,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.02984088203480172\n",
            "Training Accuracy per 100 steps: 99.38118811881188\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:59<05:55,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.025363737390178547\n",
            "Training Accuracy per 100 steps: 99.37810945273633\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:59<03:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.019061782061330437\n",
            "Training Accuracy per 100 steps: 99.54318936877077\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:59<01:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.029492534709321522\n",
            "Training Accuracy per 100 steps: 99.09600997506234\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:53<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 1: 98.99117276166457\n",
            "Training Loss Epoch: 0.031799002159635666\n",
            "Training Accuracy Epoch: 98.99117276166457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.0004704270395450294\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:59<07:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.00332287800645843\n",
            "Training Accuracy per 100 steps: 99.87623762376238\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:59<05:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.009660644453993086\n",
            "Training Accuracy per 100 steps: 99.6268656716418\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:59<03:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.01671150113390975\n",
            "Training Accuracy per 100 steps: 99.29401993355482\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:59<01:55,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.01584218728031349\n",
            "Training Accuracy per 100 steps: 99.34538653366583\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:54<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 2: 99.34426229508196\n",
            "Training Loss Epoch: 0.019170242963839207\n",
            "Training Accuracy Epoch: 99.34426229508196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.001520379213616252\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:59<07:53,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.003298978157004913\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:59<05:55,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.006936699594848845\n",
            "Training Accuracy per 100 steps: 99.93781094527363\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 300/496 [05:59<03:55,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.011242661749490806\n",
            "Training Accuracy per 100 steps: 99.66777408637874\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 400/496 [07:59<01:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.015470851579694973\n",
            "Training Accuracy per 100 steps: 99.59476309226933\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 496/496 [09:54<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 3: 99.54602774274906\n",
            "Training Loss Epoch: 0.01697757558348155\n",
            "Training Accuracy Epoch: 99.54602774274906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/496 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.0008747725514695048\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 100/496 [01:59<07:53,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.0020462061337017832\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 200/496 [03:59<05:54,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 100 steps: 0.0014261899499914178\n",
            "Training Accuracy per 100 steps: 100.0\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 232/496 [04:39<05:17,  1.20s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-65de569e9a2b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# with gpt paraphrased data, epoch=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-6d8c0d90f022>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mbig_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalcuate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "train_result = []\n",
        "for epoch in range(EPOCHS):\n",
        "    train_result.append(train(epoch, model))\n",
        "\n",
        "    # with gpt paraphrased data, epoch=10\n",
        "    torch.save(model, '../models/23May2024_epoch10.pth')\n",
        "    # loaded_model = torch.load('../models/fine_tune_epoch10.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KrU538YZRPi5"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '../models/fine_tune_epoch4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6qJWwbYYqBJ"
      },
      "outputs": [],
      "source": [
        "# Load the best model weights\n",
        "train_result.append(train(epoch, model))\n",
        "\n",
        "model.load_state_dict(best_model_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0di8SitGE9wP"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    with torch.no_grad():\n",
        "        # for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "        for i, data in enumerate(iter(testing_loader)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['target'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            loss = loss_function(outputs.logits, targets)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.logits, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "\n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return epoch_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CFuONuuE9wQ",
        "outputId": "8fe05ffd-9997-4522-e0d0-7c2d56f39be7"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on test data = \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m acc)\n",
            "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(ids, mask, token_type_ids)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs\u001b[38;5;241m.\u001b[39mlogits, targets)\n\u001b[0;32m---> 16\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m big_val, big_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m n_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calcuate_accuracy(big_idx, targets)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "acc = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdi2AYPbE9wR",
        "outputId": "d476a13e-ad72-4a71-bfbe-ff8d5a64262b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.) [9.9980313e-01 1.9692969e-04]\n",
            "tensor(0.) [9.9980086e-01 1.9911495e-04]\n",
            "tensor(0.) [9.998054e-01 1.946893e-04]\n",
            "tensor(0.) [9.9980313e-01 1.9692969e-04]\n",
            "tensor(1.) [0.00323358 0.99676645]\n",
            "tensor(0.) [9.9979335e-01 2.0667377e-04]\n",
            "tensor(1.) [2.0647602e-04 9.9979359e-01]\n",
            "tensor(0.) [9.9980313e-01 1.9692969e-04]\n",
            "tensor(1.) [1.847777e-04 9.998153e-01]\n",
            "tensor(1.) [2.2683069e-04 9.9977320e-01]\n",
            "tensor(1.) [2.1246231e-04 9.9978751e-01]\n",
            "tensor(1.) [0.00831866 0.9916814 ]\n",
            "tensor(1.) [2.248533e-04 9.997751e-01]\n",
            "tensor(1.) [1.8204887e-04 9.9981803e-01]\n",
            "tensor(0.) [9.9980408e-01 1.9599737e-04]\n",
            "tensor(1.) [1.9227664e-04 9.9980778e-01]\n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(iter(testing_loader)):\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "    targets = data['target']\n",
        "\n",
        "    outputs = model(ids, mask, token_type_ids)\n",
        "    logits = outputs.logits\n",
        "    prob = F.softmax(logits, dim=-1)[:, :].detach().cpu().numpy()\n",
        "    # 0: fake, 1: real\n",
        "    for _, a in enumerate(prob):\n",
        "        print(targets[_], a)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16D9b2FyE9wR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LquW0WyHE9wS",
        "outputId": "05daa4cf-9e8e-4a0f-c542-b35bc1bdff80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Fake': 0.0004347022, 'Real': 0.9995653}\n",
            "{'Fake': 0.00055805565, 'Real': 0.9994419}\n",
            "{'Fake': 0.00038633827, 'Real': 0.9996137}\n",
            "{'Fake': 0.0010733912, 'Real': 0.9989266}\n",
            "{'Fake': 0.00028868736, 'Real': 0.99971133}\n",
            "{'Fake': 0.0004142776, 'Real': 0.9995857}\n",
            "{'Fake': 0.00023252562, 'Real': 0.9997675}\n",
            "{'Fake': 0.00033761762, 'Real': 0.9996624}\n",
            "{'Fake': 0.000558327, 'Real': 0.9994417}\n",
            "{'Fake': 0.0007697495, 'Real': 0.99923027}\n",
            "{'Fake': 0.00022874387, 'Real': 0.9997713}\n",
            "{'Fake': 0.0003081829, 'Real': 0.99969184}\n",
            "{'Fake': 0.00021044456, 'Real': 0.99978954}\n",
            "{'Fake': 0.000567336, 'Real': 0.9994326}\n",
            "{'Fake': 0.0011065092, 'Real': 0.99889356}\n",
            "{'Fake': 0.00071633206, 'Real': 0.99928373}\n",
            "{'Fake': 0.0002635781, 'Real': 0.99973637}\n",
            "{'Fake': 0.000228002, 'Real': 0.999772}\n",
            "{'Fake': 0.001737065, 'Real': 0.99826294}\n",
            "{'Fake': 0.00028268207, 'Real': 0.9997173}\n",
            "{'Fake': 0.00028259447, 'Real': 0.9997174}\n",
            "{'Fake': 0.0015291135, 'Real': 0.99847084}\n",
            "{'Fake': 0.005053571, 'Real': 0.9949464}\n",
            "{'Fake': 0.00028413007, 'Real': 0.99971586}\n",
            "{'Fake': 0.0009320253, 'Real': 0.99906796}\n",
            "{'Fake': 0.00056867185, 'Real': 0.9994313}\n",
            "{'Fake': 0.00025135523, 'Real': 0.99974865}\n",
            "{'Fake': 0.00025514577, 'Real': 0.99974483}\n",
            "{'Fake': 0.00036632826, 'Real': 0.99963367}\n",
            "{'Fake': 0.0004729971, 'Real': 0.999527}\n",
            "{'Fake': 0.0007488885, 'Real': 0.99925107}\n",
            "{'Fake': 0.00035051783, 'Real': 0.9996495}\n",
            "{'Fake': 0.00042329804, 'Real': 0.99957675}\n",
            "{'Fake': 0.0002380067, 'Real': 0.999762}\n",
            "{'Fake': 0.0001986038, 'Real': 0.99980146}\n",
            "{'Fake': 0.0010808392, 'Real': 0.9989191}\n",
            "{'Fake': 0.00051211927, 'Real': 0.9994879}\n",
            "{'Fake': 0.00028664578, 'Real': 0.99971336}\n",
            "{'Fake': 0.00027063396, 'Real': 0.99972934}\n",
            "{'Fake': 0.00031751697, 'Real': 0.99968255}\n",
            "{'Fake': 0.00026811167, 'Real': 0.99973184}\n",
            "{'Fake': 0.0003640448, 'Real': 0.99963593}\n",
            "{'Fake': 0.006745775, 'Real': 0.99325424}\n",
            "{'Fake': 0.0002687182, 'Real': 0.99973124}\n",
            "{'Fake': 0.00029815425, 'Real': 0.9997018}\n",
            "{'Fake': 0.0002454879, 'Real': 0.9997545}\n",
            "{'Fake': 0.0002041872, 'Real': 0.99979585}\n",
            "{'Fake': 0.00033390214, 'Real': 0.9996661}\n",
            "{'Fake': 0.00017674548, 'Real': 0.9998233}\n",
            "{'Fake': 0.00025417673, 'Real': 0.9997458}\n",
            "{'Fake': 0.00026372515, 'Real': 0.99973625}\n",
            "{'Fake': 0.00040084336, 'Real': 0.99959916}\n",
            "{'Fake': 0.00017265993, 'Real': 0.9998273}\n",
            "{'Fake': 0.00022902514, 'Real': 0.99977094}\n",
            "{'Fake': 0.00032240333, 'Real': 0.99967766}\n",
            "{'Fake': 0.0002955942, 'Real': 0.9997044}\n",
            "{'Fake': 0.00017406984, 'Real': 0.999826}\n",
            "{'Fake': 0.0011319623, 'Real': 0.9988681}\n",
            "{'Fake': 0.00030340295, 'Real': 0.9996966}\n",
            "{'Fake': 0.00038026663, 'Real': 0.9996197}\n",
            "{'Fake': 0.0002938042, 'Real': 0.9997062}\n",
            "{'Fake': 0.00034712817, 'Real': 0.99965286}\n",
            "{'Fake': 0.002625908, 'Real': 0.99737406}\n",
            "{'Fake': 0.0005405709, 'Real': 0.99945945}\n",
            "{'Fake': 0.001106747, 'Real': 0.9988933}\n",
            "{'Fake': 0.0012401697, 'Real': 0.99875987}\n",
            "{'Fake': 0.0002933141, 'Real': 0.9997067}\n",
            "{'Fake': 0.00025322445, 'Real': 0.99974674}\n",
            "{'Fake': 0.00041021456, 'Real': 0.99958974}\n",
            "{'Fake': 0.0013057597, 'Real': 0.99869424}\n",
            "{'Fake': 0.0056880424, 'Real': 0.9943119}\n",
            "{'Fake': 0.00026301728, 'Real': 0.99973696}\n",
            "{'Fake': 0.00093903445, 'Real': 0.9990609}\n",
            "{'Fake': 0.00031383452, 'Real': 0.99968624}\n",
            "{'Fake': 0.00027486138, 'Real': 0.99972516}\n",
            "{'Fake': 0.00085714454, 'Real': 0.9991429}\n",
            "{'Fake': 0.0004939298, 'Real': 0.9995061}\n",
            "{'Fake': 0.00019941555, 'Real': 0.9998006}\n",
            "{'Fake': 0.00033975625, 'Real': 0.99966025}\n",
            "{'Fake': 0.00025227643, 'Real': 0.9997477}\n",
            "{'Fake': 0.00039702098, 'Real': 0.999603}\n",
            "{'Fake': 0.0018408081, 'Real': 0.9981592}\n",
            "{'Fake': 0.00035600446, 'Real': 0.99964404}\n",
            "{'Fake': 0.0002844667, 'Real': 0.9997155}\n",
            "{'Fake': 0.0001955145, 'Real': 0.99980456}\n",
            "{'Fake': 0.007114911, 'Real': 0.99288505}\n",
            "{'Fake': 0.00081397797, 'Real': 0.999186}\n",
            "{'Fake': 0.0008879377, 'Real': 0.99911207}\n",
            "{'Fake': 0.0008049108, 'Real': 0.99919504}\n",
            "{'Fake': 0.00034405378, 'Real': 0.99965596}\n",
            "{'Fake': 0.0003386301, 'Real': 0.9996613}\n"
          ]
        }
      ],
      "source": [
        "# check the model\n",
        "\n",
        "for item in h_tofel_dataset:\n",
        "    test = item['input']\n",
        "    encoded_inputs = tokenizer.encode_plus(\n",
        "        test,   # Tokenize the sentence.\n",
        "        None,   # Prepend the `[CLS]` token to the start.\n",
        "        add_special_tokens=True,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=512,\n",
        "        return_token_type_ids=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    encoded_inputs = encoded_inputs.to(device)\n",
        "\n",
        "    output = model(\n",
        "        encoded_inputs.input_ids,\n",
        "        encoded_inputs.attention_mask,\n",
        "        encoded_inputs.token_type_ids,\n",
        "    )\n",
        "\n",
        "    logits = output.logits\n",
        "    prob = F.softmax(logits, dim=-1)[:, :].detach().cpu().numpy().squeeze()\n",
        "    print({\"Fake\": prob[0], \"Real\": prob[1]})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ad0q7xE9wT"
      },
      "source": [
        "### save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOywiBJgE9wT"
      },
      "outputs": [],
      "source": [
        "# output_model_file = 'pytorch_roberta_sentiment.bin'\n",
        "# output_vocab_file = './'\n",
        "\n",
        "# model_to_save = model\n",
        "# torch.save(model_to_save, output_model_file)\n",
        "# tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "# print('All files saved')\n",
        "# print('This tutorial is completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3ibUhDhE9wU"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '../models/fine_tune_epoch1.pth')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = torch.load('../models/fine_tune_epoch1.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
