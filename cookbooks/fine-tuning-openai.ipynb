{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning of OpenAI detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bobae\\anaconda3\\envs\\ai_detector\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm \n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load json data file and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human data\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/human/tofel.json', \"r\") as f:\n",
    "    h_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads an arxiv dataset\n",
    "with open('../dataset/human/arxiv.json', \"r\") as f:\n",
    "    h_arxiv_dataset = json.load(f)\n",
    "\n",
    "# loads student essay\n",
    "with open('../dataset/human/student_essay.json', \"r\") as f:\n",
    "    h_essay_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay \n",
    "with open('../dataset/human/student_cs_essay.json', \"r\") as f:\n",
    "    h_essay_cs_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt data\n",
    "\n",
    "# loads a tofel dataset\n",
    "with open('../dataset/ai/gpt2medium_tofel.json', \"r\") as f:\n",
    "    gpt_tofel_dataset = json.load(f)\n",
    "\n",
    "# loads an arxiv dataset\n",
    "with open('../dataset/ai/gpt2medium_arxiv.json', \"r\") as f:\n",
    "    gpt_arxiv_dataset = json.load(f)\n",
    "\n",
    "# loads student essay\n",
    "with open('../dataset/ai/gpt2medium_essay.json', \"r\") as f:\n",
    "    gpt_essay_dataset = json.load(f)\n",
    "\n",
    "# loads student computer essay \n",
    "with open('../dataset/ai/gpt2medium_essay_cs.json', \"r\") as f:\n",
    "    gpt_essay_cs_dataset = json.load(f)\n",
    "    \n",
    "# loads a tofel dataset    \n",
    "with open('../dataset/ai/gpt35_tofel.json', \"r\") as f:\n",
    "    gpt_35_tofel_dataset = json.load(f)\n",
    "    \n",
    "# loads student computer essay\n",
    "with open('../dataset/ai/gpt35_essay_cs.json', \"r\") as f:\n",
    "    gpt_35_essay_cs_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_dataset = []\n",
    "for i in [h_tofel_dataset, h_arxiv_dataset, h_essay_dataset, h_essay_cs_dataset]:\n",
    "    h_dataset.extend(i)\n",
    "\n",
    "len(h_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2956"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_dataset = []\n",
    "for i in [gpt_tofel_dataset, gpt_arxiv_dataset, gpt_essay_dataset, gpt_essay_cs_dataset,\n",
    "          gpt_35_tofel_dataset, gpt_35_essay_cs_dataset]:\n",
    "    gpt_dataset.extend(i)\n",
    "\n",
    "len(gpt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\n",
    "    \"text\": [item['input'] for item in h_dataset] + [item['input'] for item in gpt_dataset],\n",
    "    \"label\": [item['label'] for item in h_dataset] + [item['label'] for item in gpt_dataset],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dct) \n",
    "\n",
    "def label_to_numeric(value):\n",
    "    if value == \"human\": \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['target'] = df['label'].apply(lambda x: label_to_numeric(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = self.data.text\n",
    "        self.target = self.data.target\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.target[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/roberta-base-openai-detector\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"openai-community/roberta-base-openai-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (4956, 3)\n",
      "TRAIN Dataset: (3965, 3)\n",
      "TEST Dataset: (991, 3)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=df.sample(frac=train_size, random_state=42)\n",
    "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 16, 'shuffle': True, 'num_workers': 0}\n",
    "test_params = {'batch_size': 16, 'shuffle': True, 'num_workers': 0}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "def train(epoch, model):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # for i, data in tqdm(enumerate(training_loader, 0), total=len(training_loader)):\n",
    "    # for _, data in tqdm(enumerate(training_loader, 0)):\n",
    "    for i, data in enumerate(iter(training_loader)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs.logits, targets)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.logits, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if i%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "            print(\"==\"*50)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.17000845074653625\n",
      "Training Accuracy per 5000 steps: 93.75\n",
      "====================================================================================================\n",
      "Training Loss per 5000 steps: 0.03668684901503643\n",
      "Training Accuracy per 5000 steps: 98.9480198019802\n",
      "====================================================================================================\n",
      "Training Loss per 5000 steps: 0.023941905792010252\n",
      "Training Accuracy per 5000 steps: 99.25373134328358\n",
      "====================================================================================================\n",
      "The Total Accuracy for Epoch 0: 99.39470365699874\n",
      "Training Loss Epoch: 0.01973094394311521\n",
      "Training Accuracy Epoch: 99.39470365699874\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        # for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "        for i, data in enumerate(iter(testing_loader)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['target'].to(device, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_function(outputs.logits, targets)\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.logits, dim=1)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on test data = \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m acc)\n",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(ids, mask, token_type_ids)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs\u001b[38;5;241m.\u001b[39mlogits, targets)\n\u001b[0;32m---> 16\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m big_val, big_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m n_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calcuate_accuracy(big_idx, targets)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bobaebak/anaconda3/envs/ai_detector/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) [9.9980313e-01 1.9692969e-04]\n",
      "tensor(0.) [9.9980086e-01 1.9911495e-04]\n",
      "tensor(0.) [9.998054e-01 1.946893e-04]\n",
      "tensor(0.) [9.9980313e-01 1.9692969e-04]\n",
      "tensor(1.) [0.00323358 0.99676645]\n",
      "tensor(0.) [9.9979335e-01 2.0667377e-04]\n",
      "tensor(1.) [2.0647602e-04 9.9979359e-01]\n",
      "tensor(0.) [9.9980313e-01 1.9692969e-04]\n",
      "tensor(1.) [1.847777e-04 9.998153e-01]\n",
      "tensor(1.) [2.2683069e-04 9.9977320e-01]\n",
      "tensor(1.) [2.1246231e-04 9.9978751e-01]\n",
      "tensor(1.) [0.00831866 0.9916814 ]\n",
      "tensor(1.) [2.248533e-04 9.997751e-01]\n",
      "tensor(1.) [1.8204887e-04 9.9981803e-01]\n",
      "tensor(0.) [9.9980408e-01 1.9599737e-04]\n",
      "tensor(1.) [1.9227664e-04 9.9980778e-01]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(iter(testing_loader)):\n",
    "    ids = data['ids'].to(device, dtype = torch.long)\n",
    "    mask = data['mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "    targets = data['target']\n",
    "    \n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    logits = outputs.logits\n",
    "    prob = F.softmax(logits, dim=-1)[:, :].detach().cpu().numpy()\n",
    "    # 0: fake, 1: real\n",
    "    for _, a in enumerate(prob):\n",
    "        print(targets[_], a)        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fake': 0.0004347022, 'Real': 0.9995653}\n",
      "{'Fake': 0.00055805565, 'Real': 0.9994419}\n",
      "{'Fake': 0.00038633827, 'Real': 0.9996137}\n",
      "{'Fake': 0.0010733912, 'Real': 0.9989266}\n",
      "{'Fake': 0.00028868736, 'Real': 0.99971133}\n",
      "{'Fake': 0.0004142776, 'Real': 0.9995857}\n",
      "{'Fake': 0.00023252562, 'Real': 0.9997675}\n",
      "{'Fake': 0.00033761762, 'Real': 0.9996624}\n",
      "{'Fake': 0.000558327, 'Real': 0.9994417}\n",
      "{'Fake': 0.0007697495, 'Real': 0.99923027}\n",
      "{'Fake': 0.00022874387, 'Real': 0.9997713}\n",
      "{'Fake': 0.0003081829, 'Real': 0.99969184}\n",
      "{'Fake': 0.00021044456, 'Real': 0.99978954}\n",
      "{'Fake': 0.000567336, 'Real': 0.9994326}\n",
      "{'Fake': 0.0011065092, 'Real': 0.99889356}\n",
      "{'Fake': 0.00071633206, 'Real': 0.99928373}\n",
      "{'Fake': 0.0002635781, 'Real': 0.99973637}\n",
      "{'Fake': 0.000228002, 'Real': 0.999772}\n",
      "{'Fake': 0.001737065, 'Real': 0.99826294}\n",
      "{'Fake': 0.00028268207, 'Real': 0.9997173}\n",
      "{'Fake': 0.00028259447, 'Real': 0.9997174}\n",
      "{'Fake': 0.0015291135, 'Real': 0.99847084}\n",
      "{'Fake': 0.005053571, 'Real': 0.9949464}\n",
      "{'Fake': 0.00028413007, 'Real': 0.99971586}\n",
      "{'Fake': 0.0009320253, 'Real': 0.99906796}\n",
      "{'Fake': 0.00056867185, 'Real': 0.9994313}\n",
      "{'Fake': 0.00025135523, 'Real': 0.99974865}\n",
      "{'Fake': 0.00025514577, 'Real': 0.99974483}\n",
      "{'Fake': 0.00036632826, 'Real': 0.99963367}\n",
      "{'Fake': 0.0004729971, 'Real': 0.999527}\n",
      "{'Fake': 0.0007488885, 'Real': 0.99925107}\n",
      "{'Fake': 0.00035051783, 'Real': 0.9996495}\n",
      "{'Fake': 0.00042329804, 'Real': 0.99957675}\n",
      "{'Fake': 0.0002380067, 'Real': 0.999762}\n",
      "{'Fake': 0.0001986038, 'Real': 0.99980146}\n",
      "{'Fake': 0.0010808392, 'Real': 0.9989191}\n",
      "{'Fake': 0.00051211927, 'Real': 0.9994879}\n",
      "{'Fake': 0.00028664578, 'Real': 0.99971336}\n",
      "{'Fake': 0.00027063396, 'Real': 0.99972934}\n",
      "{'Fake': 0.00031751697, 'Real': 0.99968255}\n",
      "{'Fake': 0.00026811167, 'Real': 0.99973184}\n",
      "{'Fake': 0.0003640448, 'Real': 0.99963593}\n",
      "{'Fake': 0.006745775, 'Real': 0.99325424}\n",
      "{'Fake': 0.0002687182, 'Real': 0.99973124}\n",
      "{'Fake': 0.00029815425, 'Real': 0.9997018}\n",
      "{'Fake': 0.0002454879, 'Real': 0.9997545}\n",
      "{'Fake': 0.0002041872, 'Real': 0.99979585}\n",
      "{'Fake': 0.00033390214, 'Real': 0.9996661}\n",
      "{'Fake': 0.00017674548, 'Real': 0.9998233}\n",
      "{'Fake': 0.00025417673, 'Real': 0.9997458}\n",
      "{'Fake': 0.00026372515, 'Real': 0.99973625}\n",
      "{'Fake': 0.00040084336, 'Real': 0.99959916}\n",
      "{'Fake': 0.00017265993, 'Real': 0.9998273}\n",
      "{'Fake': 0.00022902514, 'Real': 0.99977094}\n",
      "{'Fake': 0.00032240333, 'Real': 0.99967766}\n",
      "{'Fake': 0.0002955942, 'Real': 0.9997044}\n",
      "{'Fake': 0.00017406984, 'Real': 0.999826}\n",
      "{'Fake': 0.0011319623, 'Real': 0.9988681}\n",
      "{'Fake': 0.00030340295, 'Real': 0.9996966}\n",
      "{'Fake': 0.00038026663, 'Real': 0.9996197}\n",
      "{'Fake': 0.0002938042, 'Real': 0.9997062}\n",
      "{'Fake': 0.00034712817, 'Real': 0.99965286}\n",
      "{'Fake': 0.002625908, 'Real': 0.99737406}\n",
      "{'Fake': 0.0005405709, 'Real': 0.99945945}\n",
      "{'Fake': 0.001106747, 'Real': 0.9988933}\n",
      "{'Fake': 0.0012401697, 'Real': 0.99875987}\n",
      "{'Fake': 0.0002933141, 'Real': 0.9997067}\n",
      "{'Fake': 0.00025322445, 'Real': 0.99974674}\n",
      "{'Fake': 0.00041021456, 'Real': 0.99958974}\n",
      "{'Fake': 0.0013057597, 'Real': 0.99869424}\n",
      "{'Fake': 0.0056880424, 'Real': 0.9943119}\n",
      "{'Fake': 0.00026301728, 'Real': 0.99973696}\n",
      "{'Fake': 0.00093903445, 'Real': 0.9990609}\n",
      "{'Fake': 0.00031383452, 'Real': 0.99968624}\n",
      "{'Fake': 0.00027486138, 'Real': 0.99972516}\n",
      "{'Fake': 0.00085714454, 'Real': 0.9991429}\n",
      "{'Fake': 0.0004939298, 'Real': 0.9995061}\n",
      "{'Fake': 0.00019941555, 'Real': 0.9998006}\n",
      "{'Fake': 0.00033975625, 'Real': 0.99966025}\n",
      "{'Fake': 0.00025227643, 'Real': 0.9997477}\n",
      "{'Fake': 0.00039702098, 'Real': 0.999603}\n",
      "{'Fake': 0.0018408081, 'Real': 0.9981592}\n",
      "{'Fake': 0.00035600446, 'Real': 0.99964404}\n",
      "{'Fake': 0.0002844667, 'Real': 0.9997155}\n",
      "{'Fake': 0.0001955145, 'Real': 0.99980456}\n",
      "{'Fake': 0.007114911, 'Real': 0.99288505}\n",
      "{'Fake': 0.00081397797, 'Real': 0.999186}\n",
      "{'Fake': 0.0008879377, 'Real': 0.99911207}\n",
      "{'Fake': 0.0008049108, 'Real': 0.99919504}\n",
      "{'Fake': 0.00034405378, 'Real': 0.99965596}\n",
      "{'Fake': 0.0003386301, 'Real': 0.9996613}\n"
     ]
    }
   ],
   "source": [
    "# check the model\n",
    "\n",
    "for item in h_tofel_dataset:\n",
    "    test = item['input']\n",
    "    encoded_inputs = tokenizer.encode_plus(\n",
    "        test,   # Tokenize the sentence.\n",
    "        None,   # Prepend the `[CLS]` token to the start.\n",
    "        add_special_tokens=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    encoded_inputs = encoded_inputs.to(device)\n",
    "\n",
    "    output = model(\n",
    "        encoded_inputs.input_ids, \n",
    "        encoded_inputs.attention_mask, \n",
    "        encoded_inputs.token_type_ids,\n",
    "    )\n",
    "\n",
    "    logits = output.logits\n",
    "    prob = F.softmax(logits, dim=-1)[:, :].detach().cpu().numpy().squeeze()\n",
    "    print({\"Fake\": prob[0], \"Real\": prob[1]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_model_file = 'pytorch_roberta_sentiment.bin'\n",
    "# output_vocab_file = './'\n",
    "\n",
    "# model_to_save = model\n",
    "# torch.save(model_to_save, output_model_file)\n",
    "# tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "# print('All files saved')\n",
    "# print('This tutorial is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../models/fine_tune_epoch1.pth')\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = torch.load('../models/fine_tune_epoch1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
